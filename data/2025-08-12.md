<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 144]
- [cs.GT](#cs.GT) [Total: 10]
- [cs.DB](#cs.DB) [Total: 11]
- [cs.AI](#cs.AI) [Total: 65]
- [cs.IT](#cs.IT) [Total: 15]
- [cs.DS](#cs.DS) [Total: 10]
- [cs.IR](#cs.IR) [Total: 19]
- [cs.MM](#cs.MM) [Total: 8]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Self-Organizing Survival Manifolds: A Theory for Unsupervised Discovery of Prognostic Structures in Biological Systems](https://arxiv.org/abs/2508.06539)
*Atahan Karagoz*

Main category: cs.LG

TL;DR: 论文提出了一种新的生存模型理论（SOSM），将生存视为生物状态空间中的几何属性，而非传统的有监督学习任务。


<details>
  <summary>Details</summary>
Motivation: 传统生存模型依赖标注数据和固定协变量，而作者认为生存是生物状态空间几何特性的自然结果。

Method: 提出了自组织生存流形（SOSM）理论，基于低曲率测地流和几何能量泛函建模生存动态。

Result: 理论证明了在生物合理条件下，生存对齐轨迹的涌现和收敛，并将健康、疾病等视为流形结构的几何相变。

Conclusion: 该理论为生存建模提供了无标签的通用框架，连接了机器学习、生物物理和生命几何学。

Abstract: Survival is traditionally modeled as a supervised learning task, reliant on
curated outcome labels and fixed covariates. This work rejects that premise. It
proposes that survival is not an externally annotated target but a geometric
consequence: an emergent property of the curvature and flow inherent in
biological state space. We develop a theory of Self-Organizing Survival
Manifolds (SOSM), in which survival-relevant dynamics arise from low-curvature
geodesic flows on latent manifolds shaped by internal biological constraints. A
survival energy functional based on geodesic curvature minimization is
introduced and shown to induce structures where prognosis aligns with geometric
flow stability. We derive discrete and continuous formulations of the objective
and prove theoretical results demonstrating the emergence and convergence of
survival-aligned trajectories under biologically plausible conditions. The
framework draws connections to thermodynamic efficiency, entropy flow, Ricci
curvature, and optimal transport, grounding survival modeling in physical law.
Health, disease, aging, and death are reframed as geometric phase transitions
in the manifold's structure. This theory offers a universal, label-free
foundation for modeling survival as a property of form, not annotation-bridging
machine learning, biophysics, and the geometry of life itself.

</details>


### [2] [Semi-Supervised Supply Chain Fraud Detection with Unsupervised Pre-Filtering](https://arxiv.org/abs/2508.06574)
*Fatemeh Moradi,Mehran Tarif,Mohammadhossein Homaei*

Main category: cs.LG

TL;DR: 提出了一种结合无监督异常检测和半监督学习的两阶段框架，用于供应链欺诈检测，效果显著。


<details>
  <summary>Details</summary>
Motivation: 现代供应链欺诈检测面临数据复杂性和标签稀缺的挑战，传统方法效果有限。

Method: 第一阶段使用Isolation Forest进行无监督异常检测，第二阶段用自训练SVM进行半监督学习。

Result: 在DataCo数据集上F1-score达0.817，误报率低于3.0%。

Conclusion: 该方法有效结合了无监督和半监督学习，但需进一步解决概念漂移和与深度学习的对比。

Abstract: Detecting fraud in modern supply chains is a growing challenge, driven by the
complexity of global networks and the scarcity of labeled data. Traditional
detection methods often struggle with class imbalance and limited supervision,
reducing their effectiveness in real-world applications. This paper proposes a
novel two-phase learning framework to address these challenges. In the first
phase, the Isolation Forest algorithm performs unsupervised anomaly detection
to identify potential fraud cases and reduce the volume of data requiring
further analysis. In the second phase, a self-training Support Vector Machine
(SVM) refines the predictions using both labeled and high-confidence
pseudo-labeled samples, enabling robust semi-supervised learning. The proposed
method is evaluated on the DataCo Smart Supply Chain Dataset, a comprehensive
real-world supply chain dataset with fraud indicators. It achieves an F1-score
of 0.817 while maintaining a false positive rate below 3.0%. These results
demonstrate the effectiveness and efficiency of combining unsupervised
pre-filtering with semi-supervised refinement for supply chain fraud detection
under real-world constraints, though we acknowledge limitations regarding
concept drift and the need for comparison with deep learning approaches.

</details>


### [3] [GFlowNets for Learning Better Drug-Drug Interaction Representations](https://arxiv.org/abs/2508.06576)
*Azmine Toushik Wasi*

Main category: cs.LG

TL;DR: 提出了一种结合GFlowNet和VGAE的框架，用于生成罕见药物相互作用的合成样本，以解决数据不平衡问题并提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 药物相互作用（DDI）预测中，严重的数据不平衡问题导致罕见但关键的相互作用预测效果不佳，现有方法多为二分类，忽略了类别特异性。

Method: 结合生成流网络（GFlowNet）和变分图自编码器（VGAE），生成罕见类别的合成样本，优化模型平衡。

Result: 提升了各类药物相互作用的预测性能，增强了临床可靠性。

Conclusion: 该框架有效解决了数据不平衡问题，生成了新颖且有效的DDI对，为临床提供了更可靠的预测。

Abstract: Drug-drug interactions pose a significant challenge in clinical pharmacology,
with severe class imbalance among interaction types limiting the effectiveness
of predictive models. Common interactions dominate datasets, while rare but
critical interactions remain underrepresented, leading to poor model
performance on infrequent cases. Existing methods often treat DDI prediction as
a binary problem, ignoring class-specific nuances and exacerbating bias toward
frequent interactions. To address this, we propose a framework combining
Generative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE)
to generate synthetic samples for rare classes, improving model balance and
generate effective and novel DDI pairs. Our approach enhances predictive
performance across interaction types, ensuring better clinical reliability.

</details>


### [4] [Hypergraph Neural Network with State Space Models for Node Classification](https://arxiv.org/abs/2508.06587)
*A. Quadir,M. Tanveer*

Main category: cs.LG

TL;DR: 提出了一种新型超图神经网络HGMN，结合角色感知表示和状态空间模型，显著提升了节点分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统GNN主要关注节点间的邻接关系，忽略了角色特征的重要性，现有方法多为无监督且性能不足。

Method: HGMN通过超图构建技术建模高阶关系，结合角色和邻接表示，使用可学习的mamba transformer机制，并引入残差网络解决过平滑问题。

Result: 在多个数据集上的实验表明，HGMN在节点分类任务中优于现有GNN方法。

Conclusion: HGMN通过有效融合角色特征和邻接信息，提供了更丰富的节点表示，适用于多种图学习任务。

Abstract: In recent years, graph neural networks (GNNs) have gained significant
attention for node classification tasks on graph-structured data. However,
traditional GNNs primarily focus on adjacency relationships between nodes,
often overlooking the rich role-based characteristics that are crucial for
learning more expressive node representations. Existing methods for capturing
role-based features are largely unsupervised and fail to achieve optimal
performance in downstream tasks. To address these limitations, we propose a
novel hypergraph neural network with state space model (HGMN) that effectively
integrates role-aware representations into GNNs and the state space model. HGMN
utilizes hypergraph construction techniques to model higher-order relationships
and combines role-based and adjacency-based representations through a learnable
mamba transformer mechanism. By leveraging two distinct hypergraph construction
methods-based on node degree and neighborhood levels, it strengthens the
connections among nodes with similar roles, enhancing the model's
representational power. Additionally, the inclusion of hypergraph convolution
layers enables the model to capture complex dependencies within hypergraph
structures. To mitigate the over-smoothing problem inherent in deep GNNs, we
incorporate a residual network, ensuring improved stability and better feature
propagation across layers. Extensive experiments conducted on one newly
introduced dataset and four benchmark datasets demonstrate the superiority of
HGMN. The model achieves significant performance improvements on node
classification tasks compared to state-of-the-art GNN methods. These results
highlight HGMN's ability to provide enriched node representations by
effectively embedding role-based features alongside adjacency information,
making it a versatile and powerful tool for a variety of graph-based learning
applications.

</details>


### [5] [Graph is a Natural Regularization: Revisiting Vector Quantization for Graph Representation Learning](https://arxiv.org/abs/2508.06588)
*Zian Zhai,Fan Li,Xingyu Tan,Xiaoyang Wang,Wenjie Zhang*

Main category: cs.LG

TL;DR: 论文研究了图数据中向量量化（VQ）的代码本崩溃问题，提出了RGVQ框架以增强代码本利用率和令牌多样性。


<details>
  <summary>Details</summary>
Motivation: 图数据中VQ的代码本崩溃问题限制了图令牌的表达能力和泛化性，目前缺乏深入研究。

Method: 提出RGVQ框架，结合图拓扑和特征相似性作为正则化信号，使用Gumbel-Softmax重参数化和结构感知对比正则化。

Result: RGVQ显著提高了代码本利用率，并在多个下游任务中提升了图VQ骨干模型的性能。

Conclusion: RGVQ通过增强代码本利用和令牌多样性，为图数据提供了更具表达力和可迁移性的令牌表示。

Abstract: Vector Quantization (VQ) has recently emerged as a promising approach for
learning discrete representations of graph-structured data. However, a
fundamental challenge, i.e., codebook collapse, remains underexplored in the
graph domain, significantly limiting the expressiveness and generalization of
graph tokens.In this paper, we present the first empirical study showing that
codebook collapse consistently occurs when applying VQ to graph data, even with
mitigation strategies proposed in vision or language domains. To understand why
graph VQ is particularly vulnerable to collapse, we provide a theoretical
analysis and identify two key factors: early assignment imbalances caused by
redundancy in graph features and structural patterns, and self-reinforcing
optimization loops in deterministic VQ. To address these issues, we propose
RGVQ, a novel framework that integrates graph topology and feature similarity
as explicit regularization signals to enhance codebook utilization and promote
token diversity. RGVQ introduces soft assignments via Gumbel-Softmax
reparameterization, ensuring that all codewords receive gradient updates. In
addition, RGVQ incorporates a structure-aware contrastive regularization to
penalize the token co-assignments among similar node pairs. Extensive
experiments demonstrate that RGVQ substantially improves codebook utilization
and consistently boosts the performance of state-of-the-art graph VQ backbones
across multiple downstream tasks, enabling more expressive and transferable
graph token representations.

</details>


### [6] [A Federated Learning Framework for Handling Subtype Confounding and Heterogeneity in Large-Scale Neuroimaging Diagnosis](https://arxiv.org/abs/2508.06589)
*Xinglin Zhao,Yanwen Wang,Xiaobo Liu,Yanrong Hao,Rui Cao,Xin Wen*

Main category: cs.LG

TL;DR: 提出了一种针对神经影像CAD系统的联邦学习框架，通过动态导航和元整合模块处理数据异质性和亚型混杂问题，显著提高了诊断准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决小样本研究可重复性低和大规模数据集中亚型混杂导致的异质性问题，提升神经影像CAD系统的可靠性。

Method: 采用联邦学习框架，包含动态导航模块（基于潜在亚型表示路由样本）和元整合模块（整合异构局部模型的预测）。

Result: 在1300多名MDD患者和1100名健康对照的fMRI数据上测试，平均准确率达74.06%，显著优于传统方法。

Conclusion: 该框架有效处理亚型异质性，提升模型泛化能力，对个性化医疗和临床决策具有重要潜力。

Abstract: Computer-aided diagnosis (CAD) systems play a crucial role in analyzing
neuroimaging data for neurological and psychiatric disorders. However,
small-sample studies suffer from low reproducibility, while large-scale
datasets introduce confounding heterogeneity due to multiple disease subtypes
being labeled under a single category. To address these challenges, we propose
a novel federated learning framework tailored for neuroimaging CAD systems. Our
approach includes a dynamic navigation module that routes samples to the most
suitable local models based on latent subtype representations, and a
meta-integration module that combines predictions from heterogeneous local
models into a unified diagnostic output. We evaluated our framework using a
comprehensive dataset comprising fMRI data from over 1300 MDD patients and 1100
healthy controls across multiple study cohorts. Experimental results
demonstrate significant improvements in diagnostic accuracy and robustness
compared to traditional methods. Specifically, our framework achieved an
average accuracy of 74.06\% across all tested sites, showcasing its
effectiveness in handling subtype heterogeneity and enhancing model
generalizability. Ablation studies further confirmed the importance of both the
dynamic navigation and meta-integration modules in improving performance. By
addressing data heterogeneity and subtype confounding, our framework advances
reliable and reproducible neuroimaging CAD systems, offering significant
potential for personalized medicine and clinical decision-making in neurology
and psychiatry.

</details>


### [7] [Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials](https://arxiv.org/abs/2508.06591)
*Rachel K. Luu,Jingyu Deng,Mohammed Shahrudin Ibrahim,Nam-Joon Cho,Ming Dao,Subra Suresh,Markus J. Buehler*

Main category: cs.LG

TL;DR: 论文提出了一种结合生成式AI与多学科文献的框架，用于材料科学中的实验设计和知识提取，展示了AI辅助创意在材料设计中的实际应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在跨学科实验科学（如材料科学）中的应用仍有限，本文旨在填补这一空白。

Method: 采用BioinspiredLLM、检索增强生成（RAG）、代理系统和分层采样策略，从植物科学、仿生学和材料工程中提取结构-性能关系。

Result: 成功验证了AI生成的材料设计和预测，并制造出一种新型花粉基粘合剂，具有可调形态和实测剪切强度。

Conclusion: AI辅助创意可推动实际材料设计，并为人类与AI协作提供有效途径。

Abstract: Large language models (LLMs) have reshaped the research landscape by enabling
new approaches to knowledge retrieval and creative ideation. Yet their
application in discipline-specific experimental science, particularly in highly
multi-disciplinary domains like materials science, remains limited. We present
a first-of-its-kind framework that integrates generative AI with literature
from hitherto-unconnected fields such as plant science, biomimetics, and
materials engineering to extract insights and design experiments for materials.
We focus on humidity-responsive systems such as pollen-based materials and
Rhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and
adaptive performance. Using a suite of AI tools, including a fine-tuned model
(BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a
Hierarchical Sampling strategy, we extract structure-property relationships and
translate them into new classes of bioinspired materials. Structured inference
protocols generate and evaluate hundreds of hypotheses from a single query,
surfacing novel and experimentally tractable ideas. We validate our approach
through real-world implementation: LLM-generated procedures, materials designs,
and mechanical predictions were tested in the laboratory, culminating in the
fabrication of a novel pollen-based adhesive with tunable morphology and
measured shear strength, establishing a foundation for future plant-derived
adhesive design. This work demonstrates how AI-assisted ideation can drive
real-world materials design and enable effective human-AI collaboration.

</details>


### [8] [Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs](https://arxiv.org/abs/2508.06601)
*Kyle O'Brien,Stephen Casper,Quentin Anthony,Tomek Korbak,Robert Kirk,Xander Davies,Ishan Mishra,Geoffrey Irving,Yarin Gal,Stella Biderman*

Main category: cs.LG

TL;DR: 研究探讨通过过滤训练数据中的双用途主题文本，防止开放权重AI系统被篡改攻击，并提出一种多阶段数据过滤方法，显著提升模型对抗攻击的能力。


<details>
  <summary>Details</summary>
Motivation: 开放权重AI系统虽具透明性和开放性，但易受篡改攻击，现有安全微调方法效果有限，需探索更有效的防护手段。

Method: 提出多阶段数据过滤管道，预训练多个6.9B参数模型，测试其对生物威胁相关文本的抵抗能力。

Result: 过滤后的模型在对抗10,000步微调和3亿标记攻击时表现优异，优于现有基线，且不影响其他能力。但模型仍可通过上下文利用危险信息。

Conclusion: 预训练数据过滤是开放权重AI系统的有效防护层，但需结合其他防御手段。

Abstract: Open-weight AI systems offer unique benefits, including enhanced
transparency, open research, and decentralized access. However, they are
vulnerable to tampering attacks which can efficiently elicit harmful behaviors
by modifying weights or activations. Currently, there is not yet a robust
science of open-weight model risk management. Existing safety fine-tuning
methods and other post-training techniques have struggled to make LLMs
resistant to more than a few dozen steps of adversarial fine-tuning. In this
paper, we investigate whether filtering text about dual-use topics from
training data can prevent unwanted capabilities and serve as a more
tamper-resistant safeguard. We introduce a multi-stage pipeline for scalable
data filtering and show that it offers a tractable and effective method for
minimizing biothreat proxy knowledge in LLMs. We pretrain multiple
6.9B-parameter models from scratch and find that they exhibit substantial
resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M
tokens of biothreat-related text -- outperforming existing post-training
baselines by over an order of magnitude -- with no observed degradation to
unrelated capabilities. However, while filtered models lack internalized
dangerous knowledge, we find that they can still leverage such information when
it is provided in context (e.g., via search tool augmentation), demonstrating a
need for a defense-in-depth approach. Overall, these findings help to establish
pretraining data curation as a promising layer of defense for open-weight AI
systems.

</details>


### [9] [From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations](https://arxiv.org/abs/2508.08061)
*Sven Weinzierl,Sandra Zilker,Annina Liessmann,Martin Käppel,Weixin Wang,Martin Matzner*

Main category: cs.LG

TL;DR: 提出了一种基于迁移学习的预测性流程监控（PPM）技术，帮助缺乏足够事件数据的组织实现有效决策支持。


<details>
  <summary>Details</summary>
Motivation: 现有PPM技术需要大量事件数据或其他资源，限制了部分组织的应用。本文旨在解决这一问题。

Method: 采用迁移学习技术，将知识从一个业务流程迁移到类似流程（同一或不同组织），并基于两个真实案例进行实验验证。

Result: 实验表明，迁移学习可在目标环境中实现有效的PPM，支持跨组织资源共享。

Conclusion: 该技术为资源有限的组织提供了可行的PPM解决方案，支持跨组织知识迁移。

Abstract: Event logs reflect the behavior of business processes that are mapped in
organizational information systems. Predictive process monitoring (PPM)
transforms these data into value by creating process-related predictions that
provide the insights required for proactive interventions at process runtime.
Existing PPM techniques require sufficient amounts of event data or other
relevant resources that might not be readily available, preventing some
organizations from utilizing PPM. The transfer learning-based PPM technique
presented in this paper allows organizations without suitable event data or
other relevant resources to implement PPM for effective decision support. The
technique is instantiated in two real-life use cases, based on which numerical
experiments are performed using event logs for IT service management processes
in an intra- and inter-organizational setting. The results of the experiments
suggest that knowledge of one business process can be transferred to a similar
business process in the same or a different organization to enable effective
PPM in the target context. With the proposed technique, organizations can
benefit from transfer learning in an intra- and inter-organizational setting,
where resources like pre-trained models are transferred within and across
organizational boundaries.

</details>


### [10] [Local Diffusion Models and Phases of Data Distributions](https://arxiv.org/abs/2508.06614)
*Fangjun Hu,Guangkuo Liu,Yifan Zhang,Xun Gao*

Main category: cs.LG

TL;DR: 扩散模型通过分数函数逐步去噪生成复杂数据分布，但忽略了数据的局部结构。本文提出数据分布相的概念，证明去噪过程分为早期平凡相和晚期数据相，中间存在快速相变。通过信息论界限和实验验证，提出在相变点外使用小型局部网络，仅在相变区间需全局网络，从而简化模型架构。


<details>
  <summary>Details</summary>
Motivation: 普通扩散模型忽略数据的局部结构，学习全局分数函数计算成本高。本文旨在通过数据分布相的概念，揭示局部去噪器的设计思路，降低计算成本。

Method: 定义数据分布相，证明去噪过程的相变特性，提出基于条件互信息的信息论界限，并通过数值实验验证局部去噪器的有效性。

Result: 发现去噪过程分为平凡相和数据相，中间存在快速相变。实验证明相变点外可使用小型局部网络，仅在相变区间需全局网络。

Conclusion: 本文提出数据分布相的概念，简化扩散模型架构，为生成式AI和神经网络设计提供新方向。

Abstract: As a class of generative artificial intelligence frameworks inspired by
statistical physics, diffusion models have shown extraordinary performance in
synthesizing complicated data distributions through a denoising process
gradually guided by score functions. Real-life data, like images, is often
spatially structured in low-dimensional spaces. However, ordinary diffusion
models ignore this local structure and learn spatially global score functions,
which are often computationally expensive. In this work, we introduce a new
perspective on the phases of data distributions, which provides insight into
constructing local denoisers with reduced computational costs. We define two
distributions as belonging to the same data distribution phase if they can be
mutually connected via spatially local operations such as local denoisers.
Then, we show that the reverse denoising process consists of an early trivial
phase and a late data phase, sandwiching a rapid phase transition where local
denoisers must fail. To diagnose such phase transitions, we prove an
information-theoretic bound on the fidelity of local denoisers based on
conditional mutual information, and conduct numerical experiments in a
real-world dataset. This work suggests simpler and more efficient architectures
of diffusion models: far from the phase transition point, we can use small
local neural networks to compute the score function; global neural networks are
only necessary around the narrow time interval of phase transitions. This
result also opens up new directions for studying phases of data distributions,
the broader science of generative artificial intelligence, and guiding the
design of neural networks inspired by physics concepts.

</details>


### [11] [Generalizing Scaling Laws for Dense and Sparse Large Language Models](https://arxiv.org/abs/2508.06617)
*Md Arafat Hossain,Xingfu Wu,Valerie Taylor,Ali Jannesari*

Main category: cs.LG

TL;DR: 本文提出了一种适用于稠密和稀疏大型语言模型的通用缩放定律，以解决现有缩放定律架构特定性的问题。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型规模和训练计算成本的快速增长，研究人员致力于提高训练效率，但模型规模预测和资源分配仍具挑战性。

Method: 重新审视现有缩放定律，提出一种通用缩放定律框架，适用于稠密和稀疏模型。

Result: 通过评估和比较，证明了所提缩放定律的有效性。

Conclusion: 通用缩放定律为大型语言模型的训练提供了更灵活和统一的框架。

Abstract: Over the past few years, the size of language models has grown exponentially,
as has the computational cost to train these large models. This rapid growth
has motivated researchers to develop new techniques aimed at enhancing the
efficiency of the training process. Despite these advancements, optimally
predicting the model size or allocating optimal resources remains a challenge.
Several efforts have addressed the challenge by proposing different scaling
laws, but almost all of them are architecture-specific (dense or sparse). In
this work we revisit existing scaling laws and propose a generalized scaling
law to provide a unified framework that is applicable to both dense and sparse
large language models. We evaluate and compare our proposed scaling law with
existing scaling laws to demonstrate its effectiveness.

</details>


### [12] [Learning to Forget with Information Divergence Reweighted Objectives for Noisy Labels](https://arxiv.org/abs/2508.06622)
*Jeremiah Birrell,Reza Ebrahimi*

Main category: cs.LG

TL;DR: ANTIDOTE是一种针对噪声标签学习的新目标函数，通过信息散度邻域的松弛定义，利用凸对偶性转化为对抗训练方法，计算成本与标准交叉熵损失相近。


<details>
  <summary>Details</summary>
Motivation: 解决训练数据中固有或对抗性噪声标签的问题，减少噪声样本对学习的影响。

Method: 通过信息散度邻域的松弛定义目标函数，利用凸对偶性转化为对抗训练方法。

Result: 在不同类型的噪声标签（对称、非对称、人工标注和真实噪声）下表现优于现有方法，计算效率接近标准交叉熵损失。

Conclusion: ANTIDOTE是一种高效且适应性强的噪声标签学习方法，适用于实际场景。

Abstract: We introduce ANTIDOTE, a new class of objectives for learning under noisy
labels which are defined in terms of a relaxation over an
information-divergence neighborhood. Using convex duality, we provide a
reformulation as an adversarial training method that has similar computational
cost to training with standard cross-entropy loss. We show that our approach
adaptively reduces the influence of the samples with noisy labels during
learning, exhibiting a behavior that is analogous to forgetting those samples.
ANTIDOTE is effective in practical environments where label noise is inherent
in the training data or where an adversary can alter the training labels.
Extensive empirical evaluations on different levels of symmetric, asymmetric,
human annotation, and real-world label noise show that ANTIDOTE outperforms
leading comparable losses in the field and enjoys a time complexity that is
very close to that of the standard cross entropy loss.

</details>


### [13] [Strategic Incentivization for Locally Differentially Private Federated Learning](https://arxiv.org/abs/2508.07138)
*Yashwant Krishna Pagoti,Arunesh Sinha,Shamik Sural*

Main category: cs.LG

TL;DR: 论文研究了联邦学习中的隐私与准确性权衡问题，通过博弈论建模并提出基于令牌的激励机制。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中因局部差分隐私（LDP）噪声添加导致的模型准确性下降问题。

Method: 将隐私-准确性权衡建模为博弈，引入基于令牌的激励机制，激励客户端减少噪声添加。

Result: 通过实验验证了不同参数对博弈结果的影响，展示了激励机制的有效性。

Conclusion: 提出的博弈模型和激励机制能有效平衡隐私保护与模型准确性。

Abstract: In Federated Learning (FL), multiple clients jointly train a machine learning
model by sharing gradient information, instead of raw data, with a server over
multiple rounds. To address the possibility of information leakage in spite of
sharing only the gradients, Local Differential Privacy (LDP) is often used. In
LDP, clients add a selective amount of noise to the gradients before sending
the same to the server. Although such noise addition protects the privacy of
clients, it leads to a degradation in global model accuracy. In this paper, we
model this privacy-accuracy trade-off as a game, where the sever incentivizes
the clients to add a lower degree of noise for achieving higher accuracy, while
the clients attempt to preserve their privacy at the cost of a potential loss
in accuracy. A token based incentivization mechanism is introduced in which the
quantum of tokens credited to a client in an FL round is a function of the
degree of perturbation of its gradients. The client can later access a newly
updated global model only after acquiring enough tokens, which are to be
deducted from its balance. We identify the players, their actions and payoff,
and perform a strategic analysis of the game. Extensive experiments were
carried out to study the impact of different parameters.

</details>


### [14] [Early Detection of Pancreatic Cancer Using Multimodal Learning on Electronic Health Record](https://arxiv.org/abs/2508.06627)
*Mosbah Aouad,Anirudh Choudhary,Awais Farooq,Steven Nevers,Lusine Demirkhanyan,Bhrandon Harris,Suguna Pappu,Christopher Gondi,Ravishankar Iyer*

Main category: cs.LG

TL;DR: 提出了一种多模态方法，结合电子健康记录中的诊断代码和实验室数据，用于早期检测胰腺导管腺癌（PDAC），显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: PDAC早期检测困难，缺乏特异性症状和可靠生物标志物。

Method: 结合神经控制微分方程、预训练语言模型、循环网络和交叉注意力机制，整合诊断代码和实验室数据。

Result: 在4700名患者数据集上，AUC提升6.5%至15.5%，并识别出新的高风险生物标志物。

Conclusion: 该方法在PDAC早期检测中表现出色，具有临床潜力。

Abstract: Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest cancers, and
early detection remains a major clinical challenge due to the absence of
specific symptoms and reliable biomarkers. In this work, we propose a new
multimodal approach that integrates longitudinal diagnosis code histories and
routinely collected laboratory measurements from electronic health records to
detect PDAC up to one year prior to clinical diagnosis. Our method combines
neural controlled differential equations to model irregular lab time series,
pretrained language models and recurrent networks to learn diagnosis code
trajectory representations, and cross-attention mechanisms to capture
interactions between the two modalities. We develop and evaluate our approach
on a real-world dataset of nearly 4,700 patients and achieve significant
improvements in AUC ranging from 6.5% to 15.5% over state-of-the-art methods.
Furthermore, our model identifies diagnosis codes and laboratory panels
associated with elevated PDAC risk, including both established and new
biomarkers. Our code is available at
https://github.com/MosbahAouad/EarlyPDAC-MML.

</details>


### [15] [Multi-Hop Privacy Propagation for Differentially Private Federated Learning in Social Networks](https://arxiv.org/abs/2508.07676)
*Chenchen Lin,Xuehe Wang*

Main category: cs.LG

TL;DR: 本文提出了一种社交感知的联邦学习隐私保护机制，通过多跳传播模型量化间接隐私泄露，并采用Stackelberg博弈优化激励策略，提升客户效用并降低服务器成本。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中社交网络连接的隐私外部性问题，即客户的隐私损失不仅取决于自身策略，还受他人决策影响。

Method: 提出多跳传播模型量化隐私泄露，采用Stackelberg博弈设计激励机制，引入均值场估计器解决信息不对称问题。

Result: 实验证明该方法显著提升客户效用、降低服务器成本，同时保持模型性能，优于基线方法。

Conclusion: 该机制在客户激励视角下实现近似最优社会福利，为联邦学习中的隐私保护提供了有效解决方案。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients without sharing local data, thereby enhancing privacy and
facilitating collaboration among clients connected via social networks.
However, these social connections introduce privacy externalities: a client's
privacy loss depends not only on its privacy protection strategy but also on
the privacy decisions of others, propagated through the network via multi-hop
interactions. In this work, we propose a socially-aware privacy-preserving FL
mechanism that systematically quantifies indirect privacy leakage through a
multi-hop propagation model. We formulate the server-client interaction as a
two-stage Stackelberg game, where the server, as the leader, optimizes
incentive policies, and clients, as followers, strategically select their
privacy budgets, which determine their privacy-preserving levels by controlling
the magnitude of added noise. To mitigate information asymmetry in networked
privacy estimation, we introduce a mean-field estimator to approximate the
average external privacy risk. We theoretically prove the existence and
convergence of the fixed point of the mean-field estimator and derive
closed-form expressions for the Stackelberg Nash Equilibrium. Despite being
designed from a client-centric incentive perspective, our mechanism achieves
approximately-optimal social welfare, as revealed by Price of Anarchy (PoA)
analysis. Experiments on diverse datasets demonstrate that our approach
significantly improves client utilities and reduces server costs while
maintaining model performance, outperforming both Social-Agnostic (SA)
baselines and methods that account for social externalities.

</details>


### [16] [Using Imperfect Synthetic Data in Downstream Inference Tasks](https://arxiv.org/abs/2508.06635)
*Yewon Byun,Shantanu Gupta,Zachary C. Lipton,Rachel Leah Childers,Bryan Wilder*

Main category: cs.LG

TL;DR: 提出一种基于广义矩估计的新方法，用于结合大语言模型生成的合成数据与真实数据，以提升统计结论的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用大语言模型生成的合成数据（如调查响应）与真实数据结合，以解决有限数据场景下的统计问题。

Method: 引入基于广义矩估计的新估计器，无需超参数调整，具有理论保证。

Result: 发现合成数据与真实数据的矩残差交互能提升目标参数估计，实验验证了其在小样本场景下的显著效果。

Conclusion: 新方法为计算社会科学中的合成数据应用提供了统计有效的解决方案。

Abstract: Predictions and generations from large language models are increasingly being
explored as an aid to computational social science and human subject research
in limited data regimes. While previous technical work has explored the
potential to use model-predicted labels for unlabeled data in a principled
manner, there is increasing interest in using large language models to generate
entirely new synthetic samples (also termed as synthetic simulations), such as
in responses to surveys. However, it is not immediately clear by what means
practitioners can combine such data with real data and yet produce
statistically valid conclusions upon them. In this work, we introduce a new
estimator based on generalized method of moments, providing a
hyperparameter-free solution with strong theoretical guarantees to address the
challenge at hand. Surprisingly, we find that interactions between the moment
residuals of synthetic data and those of real data can improve estimates of the
target parameter. We empirically validate the finite-sample performance of our
estimator across different regression tasks in computational social science
applications, demonstrating large empirical gains.

</details>


### [17] [Segmented Confidence Sequences and Multi-Scale Adaptive Confidence Segments for Anomaly Detection in Nonstationary Time Series](https://arxiv.org/abs/2508.06638)
*Muyan Anna Li,Aditi Gautam*

Main category: cs.LG

TL;DR: 论文提出了两种自适应阈值框架（SCS和MACS），用于非平稳时间序列中的异常检测，显著提升了F1分数。


<details>
  <summary>Details</summary>
Motivation: 传统静态阈值在非平稳环境中容易失效，需要适应统计特性随时间变化的场景。

Method: 引入基于统计在线学习和分段原理的自适应阈值框架SCS和MACS。

Result: 在Wafer Manufacturing数据集上，相比传统方法显著提升了F1分数。

Conclusion: 自适应阈值框架能实现可靠、可解释且及时的异常检测。

Abstract: As time series data become increasingly prevalent in domains such as
manufacturing, IT, and infrastructure monitoring, anomaly detection must adapt
to nonstationary environments where statistical properties shift over time.
Traditional static thresholds are easily rendered obsolete by regime shifts,
concept drift, or multi-scale changes. To address these challenges, we
introduce and empirically evaluate two novel adaptive thresholding frameworks:
Segmented Confidence Sequences (SCS) and Multi-Scale Adaptive Confidence
Segments (MACS). Both leverage statistical online learning and segmentation
principles for local, contextually sensitive adaptation, maintaining guarantees
on false alarm rates even under evolving distributions. Our experiments across
Wafer Manufacturing benchmark datasets show significant F1-score improvement
compared to traditional percentile and rolling quantile approaches. This work
demonstrates that robust, statistically principled adaptive thresholds enable
reliable, interpretable, and timely detection of diverse real-world anomalies.

</details>


### [18] [Fractal Language Modelling by Universal Sequence Maps (USM)](https://arxiv.org/abs/2508.06641)
*Jonas S Almeida,Daniel E Russ,Susana Vinga,Ines Duarte,Lee Mason,Praphulla Bhawsar,Aaron Ge,Arlindo Oliveira,Jeya Balaji Balasubramanian*

Main category: cs.LG

TL;DR: 论文提出了一种改进的通用序列映射（USM）方法，解决了迭代过程中的种子偏差问题，并揭示了USM作为高效数值过程的性质。


<details>
  <summary>Details</summary>
Motivation: 随着基于Transformer的语言模型（如ChatGPT）的兴起，研究者对多尺度和嵌入维度下符号序列的数值表示方法重新产生兴趣。编码的挑战在于需要保留符号序列的上下文信息，以便通过非线性模型（如神经网络）进行建模。

Method: 通用序列映射（USM）是一种双射迭代函数，通过正向和反向的混沌游戏表示（CGR）将符号序列编码到数值空间，并可投影到频域（FCGR）。USM坐标可用于计算切比雪夫距离和k-mer频率，无需重新计算嵌入数值坐标。

Result: 研究解决了USM迭代过程中的种子偏差问题，实现了数值定位与序列身份的全匹配，并揭示了USM作为高效数值过程收敛于稳态序列嵌入解的性质。结果以基因组序列为例展示，但适用于任意基数字母表。

Conclusion: 改进后的USM方法不仅解决了种子偏差问题，还揭示了其作为高效数值过程的性质，为符号序列的数值表示提供了更优的解决方案。

Abstract: Motivation: With the advent of Language Models using Transformers,
popularized by ChatGPT, there is a renewed interest in exploring encoding
procedures that numerically represent symbolic sequences at multiple scales and
embedding dimensions. The challenge that encoding addresses is the need for
mechanisms that uniquely retain contextual information about the succession of
individual symbols, which can then be modeled by nonlinear formulations such as
neural networks.
  Context: Universal Sequence Maps(USM) are iterated functions that bijectively
encode symbolic sequences onto embedded numerical spaces. USM is composed of
two Chaos Game Representations (CGR), iterated forwardly and backwardly, that
can be projected into the frequency domain (FCGR). The corresponding USM
coordinates can be used to compute a Chebyshev distance metric as well as k-mer
frequencies, without having to recompute the embedded numeric coordinates, and,
paradoxically, allowing for non-integers values of k.
  Results: This report advances the bijective fractal encoding by Universal
Sequence Maps (USM) by resolving seeding biases affecting the iterated process.
The resolution had two results, the first expected, the second an intriguing
outcome: 1) full reconciliation of numeric positioning with sequence identity;
and 2) uncovering the nature of USM as an efficient numeric process converging
towards a steady state sequence embedding solution. We illustrate these results
for genomic sequences because of the convenience of a planar representation
defined by an alphabet with only 4 tokens (the 4 nucleotides). Nevertheless,
the application to alphabet of arbitrary cardinality was found to be
straightforward.

</details>


### [19] [PROPS: Progressively Private Self-alignment of Large Language Models](https://arxiv.org/abs/2508.06783)
*Noel Teku,Fengwei Tian,Payel Bhattacharjee,Souradip Chakraborty,Amrit Singh Bedi,Ravi Tandon*

Main category: cs.LG

TL;DR: PROPS是一种多阶段隐私保护对齐框架，用于LLM对齐，通过渐进式隐私保护提高模型效用，同时保护人类偏好标签的隐私。


<details>
  <summary>Details</summary>
Motivation: 依赖人类反馈的LLM对齐可能泄露标注者的个人价值观和隐私，现有方法（如DP-SGD）可能过度保护隐私并降低模型效用。

Method: 提出PROPS框架，利用前一阶段对齐的模型作为标注者补充训练数据，实现多阶段隐私保护对齐。

Result: 在相同隐私预算下，PROPS的胜率比DP-SGD高3倍，比基于随机响应的方法高2.5倍。

Conclusion: PROPS在保护隐私的同时显著提升了模型效用，为LLM对齐提供了一种高效且隐私安全的解决方案。

Abstract: Alignment is a key step in developing Large Language Models (LLMs) using
human feedback to ensure adherence to human values and societal norms.
Dependence on human feedback raises privacy concerns about how much a labeler's
preferences may reveal about their personal values, beliefs, and personality
traits. Existing approaches, such as Differentially Private SGD (DP-SGD),
provide rigorous privacy guarantees by privatizing gradients during fine-tuning
and alignment but can provide more privacy than necessary as human preferences
are tied only to labels of (prompt, response) pairs and can degrade model
utility. This work focuses on LLM alignment with preference-level privacy,
which preserves the privacy of preference labels provided by humans. We propose
PROPS (PROgressively Private Self-alignment), a multi-stage privacy preserving
alignment framework where privately aligned models in previous stages can serve
as labelers for supplementing training data in the subsequent stages of
alignment. We present theoretical guarantees for PROPS as well as comprehensive
validation using multiple models (Pythia and GPT) and datasets (AlpacaEval,
Anthropic HH-RLHF, truthy-dpo-v0.1) to demonstrate the utility of PROPS over
existing methods while still providing high privacy. For the same privacy
budget, alignment via PROPS can achieve up to 3x higher win-rates compared to
DP-SGD, and 2.5x higher win-rates compared to Randomized Response (RR) based
alignment.

</details>


### [20] [Privacy-Preserving Tabular Synthetic Data Generation Using TabularARGN](https://arxiv.org/abs/2508.06647)
*Andrey Sidorenko,Paul Tiwald*

Main category: cs.LG

TL;DR: TabularARGN是一种专为生成高质量合成表格数据设计的神经网络架构，在保持高数据保真度的同时计算高效。


<details>
  <summary>Details</summary>
Motivation: 传统匿名化技术难以充分保护隐私，合成数据生成成为安全共享和分析敏感数据的关键。

Method: 采用基于离散化的自回归方法设计TabularARGN。

Result: 在统计相似性、机器学习实用性和检测鲁棒性方面表现优异，并通过系统化的成员推理攻击验证了隐私保护效果。

Conclusion: TabularARGN在隐私与实用性之间实现了有效平衡，是一种高效的合成数据生成方法。

Abstract: Synthetic data generation has become essential for securely sharing and
analyzing sensitive data sets. Traditional anonymization techniques, however,
often fail to adequately preserve privacy. We introduce the Tabular
Auto-Regressive Generative Network (TabularARGN), a neural network architecture
specifically designed for generating high-quality synthetic tabular data. Using
a discretization-based auto-regressive approach, TabularARGN achieves high data
fidelity while remaining computationally efficient. We evaluate TabularARGN
against existing synthetic data generation methods, showing competitive results
in statistical similarity, machine learning utility, and detection robustness.
We further perform an in-depth privacy evaluation using systematic
membership-inference attacks, highlighting the robustness and effective
privacy-utility balance of our approach.

</details>


### [21] [SGD Convergence under Stepsize Shrinkage in Low-Precision Training](https://arxiv.org/abs/2508.07142)
*Vincent-Daniel Yun*

Main category: cs.LG

TL;DR: 论文研究了低精度训练中梯度收缩和量化噪声对SGD收敛的影响，证明其会降低收敛速度并增加误差。


<details>
  <summary>Details</summary>
Motivation: 低精度训练虽能降低计算和内存成本，但梯度量化的收缩和噪声可能影响SGD的收敛行为，需深入研究。

Method: 通过梯度收缩模型分析SGD收敛，将梯度缩放和量化噪声纳入标准SGD框架。

Result: 低精度SGD仍收敛，但速度因梯度收缩而降低，且量化噪声导致渐近误差增加。

Conclusion: 低精度训练通过梯度收缩和噪声影响收敛性能，需权衡计算效率与收敛效果。

Abstract: Low-precision training has become essential for reducing the computational
and memory costs of large-scale deep learning. However, quantization of
gradients introduces both magnitude shrinkage and additive noise, which can
alter the convergence behavior of stochastic gradient descent (SGD). In this
work, we study the convergence of SGD under a gradient shrinkage model, where
each stochastic gradient is scaled by a factor $q_k \in (0,1]$ and perturbed by
zero-mean quantization noise. We show that this shrinkage is equivalent to
replacing the nominal stepsize $\mu_k$ with an effective stepsize $\mu_k q_k$,
which slows convergence when $q_{\min} < 1$. Under standard smoothness and
bounded-variance assumptions, we prove that low-precision SGD still converges,
but at a reduced rate determined by $q_{\min}$, and with an increased
asymptotic error floor due to quantization noise. We theoretically analyze how
reduced numerical precision slows down training by modeling it as gradient
shrinkage in the standard SGD convergence framework.

</details>


### [22] [In-Context Reinforcement Learning via Communicative World Models](https://arxiv.org/abs/2508.06659)
*Fernando Martinez-Lopez,Tao Li,Yingdong Lu,Juntao Chen*

Main category: cs.LG

TL;DR: 本文提出CORAL框架，通过将表征学习与控制解耦，提升强化学习代理的上下文适应能力。


<details>
  <summary>Details</summary>
Motivation: 强化学习代理在新任务和环境中泛化能力不足，因其表征和策略过度拟合训练环境。

Method: CORAL将上下文强化学习建模为双代理通信问题，引入信息代理（IA）和控制代理（CA），通过因果影响损失优化通信协议。

Result: 实验表明，CORAL显著提升样本效率，并在未见稀疏奖励环境中实现零样本适应。

Conclusion: 学习可迁移的通信表征能有效提升强化学习代理的适应能力。

Abstract: Reinforcement learning (RL) agents often struggle to generalize to new tasks
and contexts without updating their parameters, mainly because their learned
representations and policies are overfit to the specifics of their training
environments. To boost agents' in-context RL (ICRL) ability, this work
formulates ICRL as a two-agent emergent communication problem and introduces
CORAL (Communicative Representation for Adaptive RL), a framework that learns a
transferable communicative context by decoupling latent representation learning
from control. In CORAL, an Information Agent (IA) is pre-trained as a world
model on a diverse distribution of tasks. Its objective is not to maximize task
reward, but to build a world model and distill its understanding into concise
messages. The emergent communication protocol is shaped by a novel Causal
Influence Loss, which measures the effect that the message has on the next
action. During deployment, the previously trained IA serves as a fixed
contextualizer for a new Control Agent (CA), which learns to solve tasks by
interpreting the provided communicative context. Our experiments demonstrate
that this approach enables the CA to achieve significant gains in sample
efficiency and successfully perform zero-shot adaptation with the help of
pre-trained IA in entirely unseen sparse-reward environments, validating the
efficacy of learning a transferable communicative representation.

</details>


### [23] [Multimodal Remote Inference](https://arxiv.org/abs/2508.07555)
*Keyuan Zhang,Yin Sun,Bo Ji*

Main category: cs.LG

TL;DR: 研究多模态远程推理系统中的调度问题，提出基于索引的阈值策略以最小化推理误差，证明其最优性，并在数值实验中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 远程传感器采集的多模态特征对实时推理至关重要，但网络资源有限导致特征更新不及时，需优化调度以减少推理误差。

Method: 提出基于索引的阈值调度策略，动态切换模态以最小化基于AoI的推理误差，适用于非单调、非加性AoI函数和异构传输时间。

Result: 数值实验显示，该策略比轮询和随机策略降低推理误差高达55%，且计算高效。

Conclusion: 优化面向任务的AoI函数可显著提升远程推理准确性，为多模态系统调度提供了有效解决方案。

Abstract: We consider a remote inference system with multiple modalities, where a
multimodal machine learning (ML) model performs real-time inference using
features collected from remote sensors. As sensor observations may change
dynamically over time, fresh features are critical for inference tasks.
However, timely delivering features from all modalities is often infeasible due
to limited network resources. To this end, we study a two-modality scheduling
problem to minimize the ML model's inference error, which is expressed as a
penalty function of AoI for both modalities. We develop an index-based
threshold policy and prove its optimality. Specifically, the scheduler switches
modalities when the current modality's index function exceeds a threshold. We
show that the two modalities share the same threshold, and both the index
functions and the threshold can be computed efficiently. The optimality of our
policy holds for (i) general AoI functions that are \emph{non-monotonic} and
\emph{non-additive} and (ii) \emph{heterogeneous} transmission times. Numerical
results show that our policy reduces inference error by up to 55% compared to
round-robin and uniform random policies, which are oblivious to the AoI-based
inference error function. Our results shed light on how to improve remote
inference accuracy by optimizing task-oriented AoI functions.

</details>


### [24] [Transferring Social Network Knowledge from Multiple GNN Teachers to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.06663)
*Yuan-Hung Chao,Chia-Hsun Lu,Chih-Ya Shen*

Main category: cs.LG

TL;DR: 将KANs集成到GNN架构中，提出KGAT、KSGC和KAPPNP模型，并通过多教师知识融合提升性能。


<details>
  <summary>Details</summary>
Motivation: GNN依赖图连接性，限制了可扩展性和效率；KANs具有强非线性表达能力和高效推理能力。

Method: 将KANs集成到GAT、SGC和APPNP中，形成新模型，并采用多教师知识融合框架。

Result: 实验表明新模型提升了节点分类准确率，知识融合显著提升学生模型性能。

Conclusion: KANs能增强GNN表达能力，实现高效、无图推理。

Abstract: Graph Neural Networks (GNNs) have shown strong performance on
graph-structured data, but their reliance on graph connectivity often limits
scalability and efficiency. Kolmogorov-Arnold Networks (KANs), a recent
architecture with learnable univariate functions, offer strong nonlinear
expressiveness and efficient inference. In this work, we integrate KANs into
three popular GNN architectures-GAT, SGC, and APPNP-resulting in three new
models: KGAT, KSGC, and KAPPNP. We further adopt a multi-teacher knowledge
amalgamation framework, where knowledge from multiple KAN-based GNNs is
distilled into a graph-independent KAN student model. Experiments on benchmark
datasets show that the proposed models improve node classification accuracy,
and the knowledge amalgamation approach significantly boosts student model
performance. Our findings highlight the potential of KANs for enhancing GNN
expressiveness and for enabling efficient, graph-free inference.

</details>


### [25] [Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations](https://arxiv.org/abs/2508.07722)
*Pietro Talli,Federico Mason,Federico Chiariotti,Andrea Zanella*

Main category: cs.LG

TL;DR: 提出了一种名为HR3L的新架构，用于在非理想无线信道上训练远程强化学习（RL）代理，解决了传统方法中因信息延迟或丢失导致的问题。


<details>
  <summary>Details</summary>
Motivation: RL代理需要即时感知状态变化，但在无线通信网络中，信息可能延迟或丢失，导致代理无法准确推断环境变化。现有解决方案计算负担大且不够灵活。

Method: HR3L由发射器和接收器组成，发射器编码环境信息，接收器解码并执行动作以最大化奖励信号，无需交换梯度信息。

Result: 实验表明，HR3L在样本效率和适应不同通信场景（如丢包、延迟和容量限制）方面显著优于基线方法。

Conclusion: HR3L为远程RL训练提供了一种高效且适应性强的解决方案，降低了通信开销和计算负担。

Abstract: In this work, we address the problem of training Reinforcement Learning (RL)
agents over communication networks. The RL paradigm requires the agent to
instantaneously perceive the state evolution to infer the effects of its
actions on the environment. This is impossible if the agent receives state
updates over lossy or delayed wireless systems and thus operates with partial
and intermittent information. In recent years, numerous frameworks have been
proposed to manage RL with imperfect feedback; however, they often offer
specific solutions with a substantial computational burden. To address these
limits, we propose a novel architecture, named Homomorphic Robust Remote
Reinforcement Learning (HR3L), that enables the training of remote RL agents
exchanging observations across a non-ideal wireless channel. HR3L considers two
units: the transmitter, which encodes meaningful representations of the
environment, and the receiver, which decodes these messages and performs
actions to maximize a reward signal. Importantly, HR3L does not require the
exchange of gradient information across the wireless channel, allowing for
quicker training and a lower communication overhead than state-of-the-art
solutions. Experimental results demonstrate that HR3L significantly outperforms
baseline methods in terms of sample efficiency and adapts to different
communication scenarios, including packet losses, delayed transmissions, and
capacity limitations.

</details>


### [26] [Watermarking Kolmogorov-Arnold Networks for Emerging Networked Applications via Activation Perturbation](https://arxiv.org/abs/2508.06676)
*Chia-Hsun Lu,Guan-Jhih Wu,Ya-Chi Ho,Chih-Ya Shen*

Main category: cs.LG

TL;DR: 本文提出了一种针对Kolmogorov-Arnold Networks（KAN）的新型水印方法DCT-AW，利用可学习激活函数嵌入水印，具有任务独立性和高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习中知识产权保护的重要性增加，水印技术受到关注。KAN的独特架构为水印带来了新挑战，需要专门的方法。

Method: 提出DCT-AW方法，通过离散余弦变换扰动激活输出来嵌入水印，适用于KAN的可学习激活函数。

Result: 实验表明，DCT-AW对模型性能影响小，且能抵抗微调、剪枝等攻击。

Conclusion: DCT-AW是一种高效且鲁棒的KAN水印方法，适用于复杂网络数据建模。

Abstract: With the increasing importance of protecting intellectual property in machine
learning, watermarking techniques have gained significant attention. As
advanced models are increasingly deployed in domains such as social network
analysis, the need for robust model protection becomes even more critical.
While existing watermarking methods have demonstrated effectiveness for
conventional deep neural networks, they often fail to adapt to the novel
architecture, Kolmogorov-Arnold Networks (KAN), which feature learnable
activation functions. KAN holds strong potential for modeling complex
relationships in network-structured data. However, their unique design also
introduces new challenges for watermarking. Therefore, we propose a novel
watermarking method, Discrete Cosine Transform-based Activation Watermarking
(DCT-AW), tailored for KAN. Leveraging the learnable activation functions of
KAN, our method embeds watermarks by perturbing activation outputs using
discrete cosine transform, ensuring compatibility with diverse tasks and
achieving task independence. Experimental results demonstrate that DCT-AW has a
small impact on model performance and provides superior robustness against
various watermark removal attacks, including fine-tuning, pruning, and
retraining after pruning.

</details>


### [27] [Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent](https://arxiv.org/abs/2508.08222)
*Tong Yang,Yu Huang,Yingbin Liang,Yuejie Chi*

Main category: cs.LG

TL;DR: 论文研究了Transformer如何通过训练学习解决符号多步推理问题，特别是树中的路径查找任务，并提供了理论分析。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer通过训练获得多步推理能力的机制，尤其是从理论角度分析其学习过程。

Method: 分析两个任务：反向推理（从目标节点到根节点的路径）和更复杂的前向推理（两阶段推理）。通过梯度下降动态理论分析单层Transformer的能力。

Result: 训练后的单层Transformer能够解决这两个任务，并具有对未见树的泛化能力。多阶段训练动态揭示了注意力头的专业化与协作。

Conclusion: 研究揭示了Transformer如何实现顺序算法过程，并表明结构化任务设计可以使浅层Transformer有效解决复杂问题。

Abstract: Transformers have demonstrated remarkable capabilities in multi-step
reasoning tasks. However, understandings of the underlying mechanisms by which
they acquire these abilities through training remain limited, particularly from
a theoretical standpoint. This work investigates how transformers learn to
solve symbolic multi-step reasoning problems through chain-of-thought
processes, focusing on path-finding in trees. We analyze two intertwined tasks:
a backward reasoning task, where the model outputs a path from a goal node to
the root, and a more complex forward reasoning task, where the model implements
two-stage reasoning by first identifying the goal-to-root path and then
reversing it to produce the root-to-goal path. Our theoretical analysis,
grounded in the dynamics of gradient descent, shows that trained one-layer
transformers can provably solve both tasks with generalization guarantees to
unseen trees. In particular, our multi-phase training dynamics for forward
reasoning elucidate how different attention heads learn to specialize and
coordinate autonomously to solve the two subtasks in a single autoregressive
path. These results provide a mechanistic explanation of how trained
transformers can implement sequential algorithmic procedures. Moreover, they
offer insights into the emergence of reasoning abilities, suggesting that when
tasks are structured to take intermediate chain-of-thought steps, even shallow
multi-head transformers can effectively solve problems that would otherwise
require deeper architectures.

</details>


### [28] [Stabilizing Federated Learning under Extreme Heterogeneity with HeteRo-Select](https://arxiv.org/abs/2508.06692)
*Md. Akmol Masud,Md Abrar Jahin,Mahmud Hasan*

Main category: cs.LG

TL;DR: 论文提出HeteRo-Select框架，解决联邦学习中因数据异构性导致的训练不稳定问题，通过智能选择客户端子集提升性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）因客户端数据多样性常导致训练不稳定，现有方法（如Oort）在后期训练中精度下降显著。

Method: 提出HeteRo-Select框架，基于客户端有用性、公平性、更新速度和数据多样性设计评分系统，理论分析支持其在异构数据下的高效通信。

Result: 在CIFAR-10数据集上，HeteRo-Select峰值精度74.75%，最终精度72.76%，稳定性下降仅1.99%，优于Oort。

Conclusion: HeteRo-Select为异构FL问题提供了理论和实证支持的高效解决方案。

Abstract: Federated Learning (FL) is a machine learning technique that often suffers
from training instability due to the diverse nature of client data. Although
utility-based client selection methods like Oort are used to converge by
prioritizing high-loss clients, they frequently experience significant drops in
accuracy during later stages of training. We propose a theoretical
HeteRo-Select framework designed to maintain high performance and ensure
long-term training stability. We provide a theoretical analysis showing that
when client data is very different (high heterogeneity), choosing a smart
subset of client participation can reduce communication more effectively
compared to full participation. Our HeteRo-Select method uses a clear,
step-by-step scoring system that considers client usefulness, fairness, update
speed, and data variety. It also shows convergence guarantees under strong
regularization. Our experimental results on the CIFAR-10 dataset under
significant label skew ($\alpha=0.1$) support the theoretical findings. The
HeteRo-Select method performs better than existing approaches in terms of peak
accuracy, final accuracy, and training stability. Specifically, HeteRo-Select
achieves a peak accuracy of $74.75\%$, a final accuracy of $72.76\%$, and a
minimal stability drop of $1.99\%$. In contrast, Oort records a lower peak
accuracy of $73.98\%$, a final accuracy of $71.25\%$, and a larger stability
drop of $2.73\%$. The theoretical foundations and empirical performance in our
study make HeteRo-Select a reliable solution for real-world heterogeneous FL
problems.

</details>


### [29] [TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations](https://arxiv.org/abs/2508.07016)
*Jianfei Wu,Wenmian Yang,Bingning Liu,Weijia Jia*

Main category: cs.LG

TL;DR: 提出了一种基于时间滞后交叉相关性的序列预测框架（TLCCSP），通过整合滞后相关序列提升预测精度，实验证明其在天气、金融和房地产数据集上显著降低了均方误差（MSE）。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型常忽略时间滞后交叉相关性，而这对捕捉复杂时序关系至关重要。

Method: 采用序列偏移动态时间规整（SSDTW）算法捕捉滞后相关性，并结合对比学习编码器（CLE）高效近似SSDTW距离。

Result: 在天气、金融和房地产数据集上，SSDTW和CLE分别显著降低了MSE，且对比学习方法将SSDTW计算时间减少约99%。

Conclusion: TLCCSP框架通过整合时间滞后交叉相关性和高效计算，显著提升了时序预测的准确性和实用性。

Abstract: Time series forecasting is critical across various domains, such as weather,
finance and real estate forecasting, as accurate forecasts support informed
decision-making and risk mitigation. While recent deep learning models have
improved predictive capabilities, they often overlook time-lagged
cross-correlations between related sequences, which are crucial for capturing
complex temporal relationships. To address this, we propose the Time-Lagged
Cross-Correlations-based Sequence Prediction framework (TLCCSP), which enhances
forecasting accuracy by effectively integrating time-lagged cross-correlated
sequences. TLCCSP employs the Sequence Shifted Dynamic Time Warping (SSDTW)
algorithm to capture lagged correlations and a contrastive learning-based
encoder to efficiently approximate SSDTW distances.
  Experimental results on weather, finance and real estate time series datasets
demonstrate the effectiveness of our framework. On the weather dataset, SSDTW
reduces mean squared error (MSE) by 16.01% compared with single-sequence
methods, while the contrastive learning encoder (CLE) further decreases MSE by
17.88%. On the stock dataset, SSDTW achieves a 9.95% MSE reduction, and CLE
reduces it by 6.13%. For the real estate dataset, SSDTW and CLE reduce MSE by
21.29% and 8.62%, respectively. Additionally, the contrastive learning approach
decreases SSDTW computational time by approximately 99%, ensuring scalability
and real-time applicability across multiple time series forecasting tasks.

</details>


### [30] [CISO: Species Distribution Modeling Conditioned on Incomplete Species Observations](https://arxiv.org/abs/2508.06704)
*Hager Radi Abdelwahed,Mélisande Teng,Robin Zbinden,Laura Pollock,Hugo Larochelle,Devis Tuia,David Rolnick*

Main category: cs.LG

TL;DR: CISO是一种基于深度学习的物种分布建模方法，能够结合不完整的物种观察数据与环境变量，提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统物种分布模型（SDMs）通常忽略物种间的生物相互作用，且现有方法对数据要求严格，难以处理稀疏和不一致的物种观察数据。

Method: 提出CISO方法，利用深度学习灵活处理不完整的物种观察数据，并结合环境变量进行预测。

Result: 实验表明，CISO在预测物种分布时优于其他方法，尤其是在结合部分生物信息或多数据集时表现更佳。

Conclusion: CISO是一种有潜力的生态工具，能够整合不完整生物数据并识别跨类群物种间的潜在相互作用。

Abstract: Species distribution models (SDMs) are widely used to predict species'
geographic distributions, serving as critical tools for ecological research and
conservation planning. Typically, SDMs relate species occurrences to
environmental variables representing abiotic factors, such as temperature,
precipitation, and soil properties. However, species distributions are also
strongly influenced by biotic interactions with other species, which are often
overlooked. While some methods partially address this limitation by
incorporating biotic interactions, they often assume symmetrical pairwise
relationships between species and require consistent co-occurrence data. In
practice, species observations are sparse, and the availability of information
about the presence or absence of other species varies significantly across
locations. To address these challenges, we propose CISO, a deep learning-based
method for species distribution modeling Conditioned on Incomplete Species
Observations. CISO enables predictions to be conditioned on a flexible number
of species observations alongside environmental variables, accommodating the
variability and incompleteness of available biotic data. We demonstrate our
approach using three datasets representing different species groups: sPlotOpen
for plants, SatBird for birds, and a new dataset, SatButterfly, for
butterflies. Our results show that including partial biotic information
improves predictive performance on spatially separate test sets. When
conditioned on a subset of species within the same dataset, CISO outperforms
alternative methods in predicting the distribution of the remaining species.
Furthermore, we show that combining observations from multiple datasets can
improve performance. CISO is a promising ecological tool, capable of
incorporating incomplete biotic information and identifying potential
interactions between species from disparate taxa.

</details>


### [31] [Analysis of Schedule-Free Nonconvex Optimization](https://arxiv.org/abs/2508.06743)
*Connor Brown*

Main category: cs.LG

TL;DR: 论文提出了一种鲁棒的Lyapunov框架，用于分析非凸优化中的Schedule-Free方法，证明了其无需依赖总步数T的超参数独立性，并提供了多种收敛速率。


<details>
  <summary>Details</summary>
Motivation: 解决传统一阶方法需要依赖总步数T的步长调度问题，同时扩展Schedule-Free方法在非凸优化中的应用。

Method: 引入Lyapunov框架，结合Polyak-Ruppert平均和动量，分析非凸优化中的收敛行为。

Result: 证明了多种收敛速率，如O(1/log T)、O(log T/T)和O(T^{-(1-α)})，并通过PEP实验验证了理论结果。

Conclusion: 扩展了Schedule-Free方法在非凸优化中的适用性，为未来研究非凸最优速率提供了方向。

Abstract: First-order methods underpin most large-scale learning algorithms, yet their
classical convergence guarantees hinge on carefully scheduled step-sizes that
depend on the total horizon $T$, which is rarely known in advance. The
Schedule-Free (SF) method promises optimal performance with hyperparameters
that are independent of $T$ by interpolating between Polyak--Ruppert averaging
and momentum, but nonconvex analysis of SF has been limited or reliant on
strong global assumptions. We introduce a robust Lyapunov framework that, under
only $L$-smoothness and lower-boundedness, reduces SF analysis to a single-step
descent inequality. This yields horizon-agnostic bounds in the nonconvex
setting: $O(1/\log T)$ for constant step + PR averaging, $O(\log T/T)$ for a
linearly growing step-size, and a continuum of $O(T^{-(1-\alpha)})$ rates for
polynomial averaging. We complement these proofs with Performance Estimation
Problem (PEP) experiments that numerically validate our rates and suggest that
our $O(1/\log T)$ bound on the original nonconvex SF algorithm may tighten to
$O(1/T)$. Our work extends SF's horizon-free guarantees to smooth nonconvex
optimization and charts future directions for optimal nonconvex rates.

</details>


### [32] [Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning](https://arxiv.org/abs/2508.06765)
*Xingke Yang,Liang Li,Sicong Li,Liwei Guan,Hao Wang,Xiaoqi Qi,Jiang Liu,Xin Fu,Miao Pan*

Main category: cs.LG

TL;DR: Fed MobiLLM提出了一种高效的联邦学习方法，用于在异构移动设备上微调大语言模型（LLM），显著降低了计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法在移动设备上计算和内存负担过重，且同步模型聚合协议效率低下。

Method: 采用服务器辅助的联邦侧调优范式，设备仅执行轻量级前向传播并上传中间激活，服务器独立训练共享侧网络。

Result: 实验显示，Fed MobiLLM显著降低了计算和通信开销（分别减少95.2%和93.2%），并加速了5.1倍的收敛速度。

Conclusion: Fed MobiLLM为异构移动设备上的LLM微调提供了高效且实用的解决方案。

Abstract: Collaboratively fine-tuning (FT) large language models (LLMs) over
heterogeneous mobile devices fosters immense potential applications of
personalized intelligence. However, such a vision faces critical system
challenges. Conventional federated LLM FT approaches place prohibitive
computational and memory burdens on mobile hardware, and their synchronous
model aggregation protocols stall for slower devices. In this paper, we propose
Fed MobiLLM, a novel design to facilitate efficient federated LLM FT across
mobile devices with diverse computing/communication speeds and local model
architectures. In particular, Fed MobiLLM implements a pioneering
server-assisted federated side-tuning paradigm. Briefly, mobile devices perform
lightweight forward propagation computations on local data using their frozen
pre-scaled backbone LLMs, and then upload selected intermediate activations.
The server trains a shared side-network independently, eliminating client-side
backpropagation and enabling asynchronous updates. To bridge model
heterogeneity across different devices, we introduce an adaptive layer-wise
feature alignment method, which ensures consistent representations for
collaboratively tuning a shared side network. Extensive experimental results
demonstrate that Fed MobiLLM can maintain robust fine-tuning performance while
achieving extremely low on-device memory, with at least 95.2% reduction in
computation overhead, 93.2% reduction in communication costs and 5.1x faster
convergence compared to existing methods, validating its efficacy for practical
LLM adaptation over heterogeneous mobile devices.

</details>


### [33] [PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems](https://arxiv.org/abs/2508.06767)
*Arman Dogru,R. Irem Bor-Yaliniz,Nimal Gamini Senarath*

Main category: cs.LG

TL;DR: PANAMA算法通过优先不对称和网络感知的MARL框架，提升了多智能体路径规划的效率，优化了数据共享策略，适用于复杂环境。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生（DTs）和自动化系统的普及，高效的数据共享框架和算法变得至关重要。研究旨在解决DT生态系统中应用与网络提供商之间的动态数据管理问题。

Method: 提出PANAMA算法，结合优先不对称和网络感知的MARL框架，采用集中训练分散执行（CTDE）和异步执行者-学习者架构。

Result: PANAMA在路径规划的准确性、速度和扩展性上优于现有基准，并通过仿真验证了其在复杂环境中的优化数据共享策略。

Conclusion: PANAMA填补了网络感知决策与多智能体协调之间的空白，推动了DTs、无线网络和AI驱动自动化的协同发展。

Abstract: Digital Twins (DTs) are transforming industries through advanced data
processing and analysis, positioning the world of DTs, Digital World, as a
cornerstone of nextgeneration technologies including embodied AI. As robotics
and automated systems scale, efficient data-sharing frameworks and robust
algorithms become critical. We explore the pivotal role of data handling in
next-gen networks, focusing on dynamics between application and network
providers (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with
Priority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL)
based multi-agent path finding (MAPF). By adopting a Centralized Training with
Decentralized Execution (CTDE) framework and asynchronous actor-learner
architectures, PANAMA accelerates training while enabling autonomous task
execution by embodied AI. Our approach demonstrates superior pathfinding
performance in accuracy, speed, and scalability compared to existing
benchmarks. Through simulations, we highlight optimized data-sharing strategies
for scalable, automated systems, ensuring resilience in complex, real-world
environments. PANAMA bridges the gap between network-aware decision-making and
robust multi-agent coordination, advancing the synergy between DTs, wireless
networks, and AI-driven automation.

</details>


### [34] [Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift](https://arxiv.org/abs/2508.06776)
*Amit Pandey*

Main category: cs.LG

TL;DR: Zero-Direction Probing (ZDP) 是一种无需任务标签或输出评估的理论框架，通过检测 Transformer 激活的零方向来发现模型漂移。


<details>
  <summary>Details</summary>
Motivation: 研究模型漂移检测的理论基础，提供无需任务标签或输出的方法。

Method: 基于假设 A1-A6，提出方差泄漏定理、Fisher 零守恒、低秩更新的秩泄漏界限，以及在线零空间跟踪器的对数遗憾保证。

Result: 推导出谱零泄漏 (SNL) 度量，具有非渐近尾部边界和集中不等式，为高斯零模型下的漂移提供先验阈值。

Conclusion: 监控层激活的左右零空间及其 Fisher 几何，可为表示变化提供具体且可测试的保证。

Abstract: We present Zero-Direction Probing (ZDP), a theory-only framework for
detecting model drift from null directions of transformer activations without
task labels or output evaluations. Under assumptions A1--A6, we prove: (i) the
Variance--Leak Theorem, (ii) Fisher Null-Conservation, (iii) a Rank--Leak bound
for low-rank updates, and (iv) a logarithmic-regret guarantee for online
null-space trackers. We derive a Spectral Null-Leakage (SNL) metric with
non-asymptotic tail bounds and a concentration inequality, yielding a-priori
thresholds for drift under a Gaussian null model. These results show that
monitoring right/left null spaces of layer activations and their Fisher
geometry provides concrete, testable guarantees on representational change.

</details>


### [35] [Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning](https://arxiv.org/abs/2508.06784)
*Junjing Zheng,Chengliang Song,Weidong Jiang,Xinyu Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为MA-NTAE的新型自监督学习模型，通过非线性Tucker分解和灵活的模式编码策略，解决了高维张量数据学习中的计算和优化问题。


<details>
  <summary>Details</summary>
Motivation: 高维张量数据在自监督学习中面临维度灾难和计算负担问题，现有方法如MLP自编码器依赖展平操作，而张量网络则难以捕捉非线性关系。

Method: MA-NTAE将经典Tucker分解推广到非线性框架，采用Pick-and-Unfold策略，通过递归展开-编码-折叠操作实现灵活的模式编码。

Result: 实验表明，MA-NTAE在压缩和聚类任务中优于标准自编码器和现有张量网络，尤其在高阶高维张量上表现更优。

Conclusion: MA-NTAE通过结合张量结构先验和非线性学习，显著提升了高维张量数据的处理效率和性能。

Abstract: High-dimensional data, particularly in the form of high-order tensors,
presents a major challenge in self-supervised learning. While MLP-based
autoencoders (AE) are commonly employed, their dependence on flattening
operations exacerbates the curse of dimensionality, leading to excessively
large model sizes, high computational overhead, and challenging optimization
for deep structural feature capture. Although existing tensor networks
alleviate computational burdens through tensor decomposition techniques, most
exhibit limited capability in learning non-linear relationships. To overcome
these limitations, we introduce the Mode-Aware Non-linear Tucker Autoencoder
(MA-NTAE). MA-NTAE generalized classical Tucker decomposition to a non-linear
framework and employs a Pick-and-Unfold strategy, facilitating flexible
per-mode encoding of high-order tensors via recursive unfold-encode-fold
operations, effectively integrating tensor structural priors. Notably, MA-NTAE
exhibits linear growth in computational complexity with tensor order and
proportional growth with mode dimensions. Extensive experiments demonstrate
MA-NTAE's performance advantages over standard AE and current tensor networks
in compression and clustering tasks, which become increasingly pronounced for
higher-order, higher-dimensional tensors.

</details>


### [36] [Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities](https://arxiv.org/abs/2508.06800)
*Rui Liu,Haolin Zuo,Zheng Lian,Hongyu Yuan,Qi Fan*

Main category: cs.LG

TL;DR: 提出了一种名为HARDY-MER的动态课程学习框架，通过评估样本难度并动态调整训练重点，显著提升了多模态情感识别中缺失模态的处理能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理多模态情感识别中的缺失模态问题时，未能考虑样本间重建难度的差异，导致对困难样本的处理能力不足。

Method: HARDY-MER框架分两阶段：1）通过多视角硬度评估机制量化样本重建难度；2）采用检索式动态课程学习策略动态调整训练重点。

Result: 在基准数据集上的实验表明，HARDY-MER在缺失模态场景下优于现有方法。

Conclusion: HARDY-MER通过动态课程学习有效提升了模型对困难样本的处理能力，为缺失模态问题提供了新的解决方案。

Abstract: Missing modalities have recently emerged as a critical research direction in
multimodal emotion recognition (MER). Conventional approaches typically address
this issue through missing modality reconstruction. However, these methods fail
to account for variations in reconstruction difficulty across different
samples, consequently limiting the model's ability to handle hard samples
effectively. To overcome this limitation, we propose a novel Hardness-Aware
Dynamic Curriculum Learning framework, termed HARDY-MER. Our framework operates
in two key stages: first, it estimates the hardness level of each sample, and
second, it strategically emphasizes hard samples during training to enhance
model performance on these challenging instances. Specifically, we first
introduce a Multi-view Hardness Evaluation mechanism that quantifies
reconstruction difficulty by considering both Direct Hardness (modality
reconstruction errors) and Indirect Hardness (cross-modal mutual information).
Meanwhile, we introduce a Retrieval-based Dynamic Curriculum Learning strategy
that dynamically adjusts the training curriculum by retrieving samples with
similar semantic information and balancing the learning focus between easy and
hard instances. Extensive experiments on benchmark datasets demonstrate that
HARDY-MER consistently outperforms existing methods in missing-modality
scenarios. Our code will be made publicly available at
https://github.com/HARDY-MER/HARDY-MER.

</details>


### [37] [Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation](https://arxiv.org/abs/2508.06806)
*Xiao Huang,Xu Liu,Enze Zhang,Tong Yu,Shuai Li*

Main category: cs.LG

TL;DR: 论文提出了一种新的数据增强方法CFDG，通过无分类器扩散生成技术提升离线与在线数据生成质量，显著提高离线到在线强化学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的离线数据与在线数据分布存在差距，限制了性能提升。

Method: 采用无分类器引导扩散生成（CFDG）技术，结合重加权方法，使生成数据更贴近在线数据分布。

Result: CFDG在D4RL基准测试中平均提升15%的性能，优于现有方法。

Conclusion: CFDG是一种通用且高效的数据增强方法，可显著提升离线到在线强化学习的性能。

Abstract: Offline-to-online Reinforcement Learning (O2O RL) aims to perform online
fine-tuning on an offline pre-trained policy to minimize costly online
interactions. Existing work used offline datasets to generate data that conform
to the online data distribution for data augmentation. However, generated data
still exhibits a gap with the online data, limiting overall performance. To
address this, we propose a new data augmentation approach, Classifier-Free
Diffusion Generation (CFDG). Without introducing additional classifier training
overhead, CFDG leverages classifier-free guidance diffusion to significantly
enhance the generation quality of offline and online data with different
distributions. Additionally, it employs a reweighting method to enable more
generated data to align with the online data, enhancing performance while
maintaining the agent's stability. Experimental results show that CFDG
outperforms replaying the two data types or using a standard diffusion model to
generate new data. Our method is versatile and can be integrated with existing
offline-to-online RL algorithms. By implementing CFDG to popular methods IQL,
PEX and APL, we achieve a notable 15% average improvement in empirical
performance on the D4RL benchmark such as MuJoCo and AntMaze.

</details>


### [38] [Technical Report: Full-Stack Fine-Tuning for the Q Programming Language](https://arxiv.org/abs/2508.06813)
*Brendan R. Hogan,Will Brown,Adel Boyarsky,Anderson Schneider,Yuriy Nevmyvaka*

Main category: cs.LG

TL;DR: 论文提出了一种开源方法，用于将大语言模型（LLMs）适配到Q编程语言（一种在量化金融中常用但互联网上较少见的语言），并通过预训练、监督微调和强化学习训练了一系列模型，性能显著优于主流模型。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在Q编程语言等小众或私有领域任务中的表现不足问题。

Method: 引入新的Q语言Leetcode风格评估数据集，基于Qwen-2.5系列模型进行预训练、监督微调和强化学习，训练了五种参数规模的模型。

Result: 最佳模型在Q基准测试中达到59%的准确率，优于Claude Opus-4和GPT-4.1。

Conclusion: 该方法具有广泛适用性，可扩展到其他依赖软性或主观信号的任务。

Abstract: Even though large language models are becoming increasingly capable, it is
still unreasonable to expect them to excel at tasks that are under-represented
on the Internet. Leveraging LLMs for specialized applications, particularly in
niche programming languages and private domains, remains challenging and
largely unsolved. In this work, we address this gap by presenting a
comprehensive, open-source approach for adapting LLMs to the Q programming
language, a popular tool in quantitative finance that is much less present on
the Internet compared to Python, C, Java, and other ``mainstream" languages and
is therefore not a strong suit of general-purpose AI models. We introduce a new
Leetcode style evaluation dataset for Q, benchmark major frontier models on the
dataset, then do pretraining, supervised fine tuning, and reinforcement
learning to train a suite of reasoning and non-reasoning models based on the
Qwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our
best model achieves a pass@1 accuracy of 59 percent on our Q benchmark,
surpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent.
Additionally, all models, even our 1.5B model, outperform GPT-4.1 on this task.
In addition to releasing models, code, and data, we provide a detailed
blueprint for dataset construction, model pretraining, supervised fine-tuning,
and reinforcement learning. Our methodology is broadly applicable, and we
discuss how these techniques can be extended to other tasks, including those
where evaluation may rely on soft or subjective signals.

</details>


### [39] [Who's the Evil Twin? Differential Auditing for Undesired Behavior](https://arxiv.org/abs/2508.06827)
*Ishwar Balappanawar,Venkata Hasith Vattikuti,Greta Kintzley,Ronan Azimi-Mancel,Satvik Golechha*

Main category: cs.LG

TL;DR: 论文提出了一种对抗性游戏框架（红队与蓝队）来检测神经网络中的隐藏行为，实验表明基于对抗攻击的方法效果最佳，但对LLM的检测需要额外线索。


<details>
  <summary>Details</summary>
Motivation: 由于先验知识有限和对抗性混淆，检测神经网络中的隐藏行为具有挑战性。

Method: 通过红队训练两个相似模型（一个仅含良性数据，一个含隐藏有害行为），蓝队尝试识别被篡改的模型，实验采用CNN和多种蓝队策略。

Result: 基于对抗攻击的方法准确率最高（100%），其他方法表现不一；LLM检测需要额外线索。

Conclusion: 开源审计游戏框架，希望为设计更好的审计方法提供参考。

Abstract: Detecting hidden behaviors in neural networks poses a significant challenge
due to minimal prior knowledge and potential adversarial obfuscation. We
explore this problem by framing detection as an adversarial game between two
teams: the red team trains two similar models, one trained solely on benign
data and the other trained on data containing hidden harmful behavior, with the
performance of both being nearly indistinguishable on the benign dataset. The
blue team, with limited to no information about the harmful behaviour, tries to
identify the compromised model. We experiment using CNNs and try various blue
team strategies, including Gaussian noise analysis, model diffing, integrated
gradients, and adversarial attacks under different levels of hints provided by
the red team. Results show high accuracy for adversarial-attack-based methods
(100\% correct prediction, using hints), which is very promising, whilst the
other techniques yield more varied performance. During our LLM-focused rounds,
we find that there are not many parallel methods that we could apply from our
study with CNNs. Instead, we find that effective LLM auditing methods require
some hints about the undesired distribution, which can then used in standard
black-box and open-weight methods to probe the models further and reveal their
misalignment. We open-source our auditing games (with the model and data) and
hope that our findings contribute to designing better audits.

</details>


### [40] [Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning](https://arxiv.org/abs/2508.06871)
*Aleksandar Todorov,Juan Cardenas-Cartagena,Rafael F. Cunha,Marco Zullich,Matthia Sabatelli*

Main category: cs.LG

TL;DR: 论文研究了深度强化学习中可塑性下降的问题，探讨了稀疏化方法（GMP和SET）如何提升多任务强化学习（MTRL）的可塑性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决多任务强化学习中因可塑性下降导致的适应能力减弱问题。

Method: 采用渐进幅度剪枝（GMP）和稀疏进化训练（SET）方法，并在不同MTRL架构上评估其效果。

Result: GMP和SET有效缓解了可塑性下降的指标（如神经元休眠和表征崩溃），并提升了多任务性能。

Conclusion: 动态稀疏化是提升MTRL系统适应性的有效工具，但其效果依赖于具体场景。

Abstract: Plasticity loss, a diminishing capacity to adapt as training progresses, is a
critical challenge in deep reinforcement learning. We examine this issue in
multi-task reinforcement learning (MTRL), where higher representational
flexibility is crucial for managing diverse and potentially conflicting task
demands. We systematically explore how sparsification methods, particularly
Gradual Magnitude Pruning (GMP) and Sparse Evolutionary Training (SET), enhance
plasticity and consequently improve performance in MTRL agents. We evaluate
these approaches across distinct MTRL architectures (shared backbone, Mixture
of Experts, Mixture of Orthogonal Experts) on standardized MTRL benchmarks,
comparing against dense baselines, and a comprehensive range of alternative
plasticity-inducing or regularization methods. Our results demonstrate that
both GMP and SET effectively mitigate key indicators of plasticity degradation,
such as neuron dormancy and representational collapse. These plasticity
improvements often correlate with enhanced multi-task performance, with sparse
agents frequently outperforming dense counterparts and achieving competitive
results against explicit plasticity interventions. Our findings offer insights
into the interplay between plasticity, network sparsity, and MTRL designs,
highlighting dynamic sparsification as a robust but context-sensitive tool for
developing more adaptable MTRL systems.

</details>


### [41] [Conformal Prediction and Trustworthy AI](https://arxiv.org/abs/2508.06885)
*Anthony Bellotti,Xindi Zhao*

Main category: cs.LG

TL;DR: 本文回顾了保形预测在可信AI中的潜力，探讨了其超越边际有效性的应用，如泛化风险和AI治理，并通过实验展示了其校准预测和偏差识别的能力。


<details>
  <summary>Details</summary>
Motivation: 保形预测因其提供具有置信度保证的集合预测而受到关注，近年来成为机器学习中不确定性量化的主流方法。其可靠的不确定性量化能力使其在可信AI发展中极具价值。

Method: 文章通过实验和示例展示了保形预测作为校准预测器的应用，以及其在偏差识别和缓解中的作用。

Result: 保形预测不仅具备边际有效性，还能解决泛化风险和AI治理等问题，为可信AI提供了新的工具。

Conclusion: 保形预测在可信AI中具有广泛潜力，能够通过校准预测和偏差缓解增强AI的可靠性。

Abstract: Conformal predictors are machine learning algorithms developed in the 1990's
by Gammerman, Vovk, and their research team, to provide set predictions with
guaranteed confidence level. Over recent years, they have grown in popularity
and have become a mainstream methodology for uncertainty quantification in the
machine learning community. From its beginning, there was an understanding that
they enable reliable machine learning with well-calibrated uncertainty
quantification. This makes them extremely beneficial for developing trustworthy
AI, a topic that has also risen in interest over the past few years, in both
the AI community and society more widely. In this article, we review the
potential for conformal prediction to contribute to trustworthy AI beyond its
marginal validity property, addressing problems such as generalization risk and
AI governance. Experiments and examples are also provided to demonstrate its
use as a well-calibrated predictor and for bias identification and mitigation.

</details>


### [42] [QuiZSF: An efficient data-model interaction framework for zero-shot time-series forecasting](https://arxiv.org/abs/2508.06915)
*Shichao Ma,Zhengyang Zhou,Qihe Huang,Binwu Wang,Kuo Yang,Huan Li,Yang Wang*

Main category: cs.LG

TL;DR: 论文提出QuiZSF框架，结合检索增强生成（RAG）与时间序列预训练模型（TSPMs），提升零样本时间序列预测（ZSF）性能。


<details>
  <summary>Details</summary>
Motivation: 传统模型在数据稀缺场景（如领域迁移或极端条件）下难以处理ZSF，而现有TSPMs缺乏动态整合外部知识的机制。

Method: 提出QuiZSF框架，包括ChronoRAG Base（CRB）存储与检索、Multi-grained Series Interaction Learner（MSIL）提取特征，以及Model Cooperation Coherer（MCC）对齐知识与非LLM/LLM TSPMs。

Result: QuiZSF在75%（非LLM）和87.5%（LLM）预测场景中排名Top1，且内存和推理效率高。

Conclusion: QuiZSF通过结合RAG与TSPMs，显著提升了ZSF性能，适用于数据稀缺场景。

Abstract: Time series forecasting has become increasingly important to empower diverse
applications with streaming data. Zero-shot time-series forecasting (ZSF),
particularly valuable in data-scarce scenarios, such as domain transfer or
forecasting under extreme conditions, is difficult for traditional models to
deal with. While time series pre-trained models (TSPMs) have demonstrated
strong performance in ZSF, they often lack mechanisms to dynamically
incorporate external knowledge. Fortunately, emerging retrieval-augmented
generation (RAG) offers a promising path for injecting such knowledge on
demand, yet they are rarely integrated with TSPMs. To leverage the strengths of
both worlds, we introduce RAG into TSPMs to enhance zero-shot time series
forecasting. In this paper, we propose QuiZSF (Quick Zero-Shot Time Series
Forecaster), a lightweight and modular framework that couples efficient
retrieval with representation learning and model adaptation for ZSF.
Specifically, we construct a hierarchical tree-structured ChronoRAG Base (CRB)
for scalable time-series storage and domain-aware retrieval, introduce a
Multi-grained Series Interaction Learner (MSIL) to extract fine- and
coarse-grained relational features, and develop a dual-branch Model Cooperation
Coherer (MCC) that aligns retrieved knowledge with two kinds of TSPMs: Non-LLM
based and LLM based. Compared with contemporary baselines, QuiZSF, with Non-LLM
based and LLM based TSPMs as base model, respectively, ranks Top1 in 75% and
87.5% of prediction settings, while maintaining high efficiency in memory and
inference time.

</details>


### [43] [Class Unbiasing for Generalization in Medical Diagnosis](https://arxiv.org/abs/2508.06943)
*Lishi Zuo,Man-Wai Mak,Lu Yi,Youzhi Tu*

Main category: cs.LG

TL;DR: 论文提出了一种解决医学诊断中类别特征偏差和类别不平衡的方法，通过类别不平等损失和类别加权优化目标，提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学诊断中模型可能因类别特征偏差（依赖与部分类别强相关的特征）而表现不佳，同时类别不平衡问题也会影响性能。

Method: 提出类别不平等损失以平衡正负类样本的贡献，并采用类别加权分布鲁棒优化目标来增强损失效果。

Result: 实验证明类别特征偏差会损害模型性能，而所提方法能有效缓解偏差和不平衡问题。

Conclusion: 该方法显著提升了模型的泛化能力，适用于合成和真实数据集。

Abstract: Medical diagnosis might fail due to bias. In this work, we identified
class-feature bias, which refers to models' potential reliance on features that
are strongly correlated with only a subset of classes, leading to biased
performance and poor generalization on other classes. We aim to train a
class-unbiased model (Cls-unbias) that mitigates both class imbalance and
class-feature bias simultaneously. Specifically, we propose a class-wise
inequality loss which promotes equal contributions of classification loss from
positive-class and negative-class samples. We propose to optimize a class-wise
group distributionally robust optimization objective-a class-weighted training
objective that upweights underperforming classes-to enhance the effectiveness
of the inequality loss under class imbalance. Through synthetic and real-world
datasets, we empirically demonstrate that class-feature bias can negatively
impact model performance. Our proposed method effectively mitigates both
class-feature bias and class imbalance, thereby improving the model's
generalization ability.

</details>


### [44] [AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance](https://arxiv.org/abs/2508.06944)
*Lixuan He,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为AMFT的单阶段算法，通过动态平衡监督微调（SFT）和强化学习（RL）的奖励信号，解决了传统两阶段方法中的灾难性遗忘和探索-模仿权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段方法（SFT+RL）存在灾难性遗忘和探索-模仿权衡问题，而现有单阶段方法缺乏动态平衡机制。

Method: 提出AMFT算法，通过元梯度自适应权重控制器动态优化SFT和RL的奖励信号平衡，并结合策略熵正则化以提升稳定性。

Result: AMFT在数学推理、抽象视觉推理和视觉语言导航任务中均达到新SOTA，并在OOD任务中表现出更强的泛化能力。

Conclusion: AMFT通过元学习控制器实现了更稳定、高效的LLM对齐，为LLM微调提供了更优的范式。

Abstract: Large Language Models (LLMs) are typically fine-tuned for reasoning tasks
through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by
Reinforcement Learning (RL), a process fraught with catastrophic forgetting and
suboptimal trade-offs between imitation and exploration. Recent single-stage
methods attempt to unify SFT and RL using heuristics, but lack a principled
mechanism for dynamically balancing the two paradigms. In this paper, we
reframe this challenge through the theoretical lens of \textbf{implicit
rewards}, viewing SFT and RL not as distinct methods but as complementary
reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel
single-stage algorithm that learns the optimal balance between SFT's implicit,
path-level reward and RL's explicit, outcome-based reward. The core of AMFT is
a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL
balance as a learnable parameter, dynamically optimizing it to maximize
long-term task performance. This forward-looking approach, regularized by
policy entropy for stability, autonomously discovers an effective training
curriculum. We conduct a comprehensive evaluation on challenging benchmarks
spanning mathematical reasoning, abstract visual reasoning (General Points),
and vision-language navigation (V-IRL). AMFT consistently establishes a new
state-of-the-art and demonstrats superior generalization on out-of-distribution
(OOD) tasks. Ablation studies and training dynamic analysis confirm that the
meta-learning controller is crucial for AMFT's stability, sample efficiency,
and performance, offering a more principled and effective paradigm for LLM
alignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.

</details>


### [45] [BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity](https://arxiv.org/abs/2508.06953)
*Shiwei Li,Xiandi Luo,Haozhao Wang,Xing Tang,Ziqiang Cui,Dugang Liu,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.LG

TL;DR: BoRA是一种改进的低秩适应方法，通过块矩阵乘法和引入对角矩阵，以少量额外参数提升LoRA权重秩。


<details>
  <summary>Details</summary>
Motivation: LoRA在大型语言模型中的低秩适应方法通过增加秩提升性能，但参数数量显著增加。BoRA旨在以更少的额外参数实现更高的秩。

Method: BoRA将LoRA的权重矩阵分解为块矩阵，并为每个块乘法引入独特的对角矩阵，从而提升秩。

Result: 实验表明BoRA在多个数据集和模型上表现优异，且具有可扩展性。

Conclusion: BoRA通过块多样化的低秩适应，以更少的参数实现了更高的性能。

Abstract: Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method
widely used in large language models (LLMs). It approximates the update of a
pretrained weight matrix $W\in\mathbb{R}^{m\times n}$ by the product of two
low-rank matrices, $BA$, where $A \in\mathbb{R}^{r\times n}$ and
$B\in\mathbb{R}^{m\times r} (r\ll\min\{m,n\})$. Increasing the dimension $r$
can raise the rank of LoRA weights (i.e., $BA$), which typically improves
fine-tuning performance but also significantly increases the number of
trainable parameters. In this paper, we propose Block Diversified Low-Rank
Adaptation (BoRA), which improves the rank of LoRA weights with a small number
of additional parameters. Specifically, BoRA treats the product $BA$ as a block
matrix multiplication, where $A$ and $B$ are partitioned into $b$ blocks along
the columns and rows, respectively (i.e., $A=[A_1,\dots,A_b]$ and
$B=[B_1,\dots,B_b]^\top$). Consequently, the product $BA$ becomes the
concatenation of the block products $B_iA_j$ for $i,j\in[b]$. To enhance the
diversity of different block products, BoRA introduces a unique diagonal matrix
$\Sigma_{i,j} \in \mathbb{R}^{r\times r}$ for each block multiplication,
resulting in $B_i \Sigma_{i,j} A_j$. By leveraging these block-wise diagonal
matrices, BoRA increases the rank of LoRA weights by a factor of $b$ while only
requiring $b^2r$ additional parameters. Extensive experiments across multiple
datasets and models demonstrate the superiority of BoRA, and ablation studies
further validate its scalability.

</details>


### [46] [Can Multitask Learning Enhance Model Explainability?](https://arxiv.org/abs/2508.06966)
*Hiba Najjar,Bushra Alshbib,Andreas Dengel*

Main category: cs.LG

TL;DR: 论文提出了一种通过多任务学习利用遥感数据模态的方法，以提升模型可解释性，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 遥感数据多样但复杂，多模态学习虽能提升性能却牺牲了可解释性。研究旨在通过多任务学习利用模态信息，实现模型行为的内部解释。

Method: 将某些模态作为附加目标与主任务一起预测，而非额外输入。利用卫星数据的丰富信息作为输入模态。

Result: 方法在数据稀缺时无需额外模态，性能与多模态基线相当甚至更优，主任务预测误差可通过辅助任务解释。

Conclusion: 多任务学习方法在提升模型可解释性的同时保持了性能，适用于多种任务类型。

Abstract: Remote sensing provides satellite data in diverse types and formats. The
usage of multimodal learning networks exploits this diversity to improve model
performance, except that the complexity of such networks comes at the expense
of their interpretability. In this study, we explore how modalities can be
leveraged through multitask learning to intrinsically explain model behavior.
In particular, instead of additional inputs, we use certain modalities as
additional targets to be predicted along with the main task. The success of
this approach relies on the rich information content of satellite data, which
remains as input modalities. We show how this modeling context provides
numerous benefits: (1) in case of data scarcity, the additional modalities do
not need to be collected for model inference at deployment, (2) the model
performance remains comparable to the multimodal baseline performance, and in
some cases achieves better scores, (3) prediction errors in the main task can
be explained via the model behavior in the auxiliary task(s). We demonstrate
the efficiency of our approach on three datasets, including segmentation,
classification, and regression tasks. Code available at
git.opendfki.de/hiba.najjar/mtl_explainability/.

</details>


### [47] [Structure-Preserving Digital Twins via Conditional Neural Whitney Forms](https://arxiv.org/abs/2508.06981)
*Brooks Kinch,Benjamin Shaffer,Elizabeth Armstrong,Michael Meehan,John Hewson,Nathaniel Trask*

Main category: cs.LG

TL;DR: 提出了一种基于结构保持的降阶有限元模型和条件注意力机制的数字孪生框架，支持实时校准和复杂几何处理。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀疏或优化误差下数字孪生的数值稳定性和守恒量精确保持问题。

Method: 结合条件注意力机制和有限元外微积分（FEEC），学习降阶基和非线性守恒律。

Result: 在复杂几何和稀疏数据（25个LES模拟）下实现准确预测，速度提升3.1x10^8倍，实时推断约0.1秒。

Conclusion: 该框架非侵入式地与传统有限元工具集成，适用于复杂问题，如电池热失控，且开源实现可用。

Abstract: We present a framework for constructing real-time digital twins based on
structure-preserving reduced finite element models conditioned on a latent
variable Z. The approach uses conditional attention mechanisms to learn both a
reduced finite element basis and a nonlinear conservation law within the
framework of finite element exterior calculus (FEEC). This guarantees numerical
well-posedness and exact preservation of conserved quantities, regardless of
data sparsity or optimization error. The conditioning mechanism supports
real-time calibration to parametric variables, allowing the construction of
digital twins which support closed loop inference and calibration to sensor
data. The framework interfaces with conventional finite element machinery in a
non-invasive manner, allowing treatment of complex geometries and integration
of learned models with conventional finite element techniques.
  Benchmarks include advection diffusion, shock hydrodynamics, electrostatics,
and a complex battery thermal runaway problem. The method achieves accurate
predictions on complex geometries with sparse data (25 LES simulations),
including capturing the transition to turbulence and achieving real-time
inference ~0.1s with a speedup of 3.1x10^8 relative to LES. An open-source
implementation is available on GitHub.

</details>


### [48] [Discovery Learning accelerates battery design evaluation](https://arxiv.org/abs/2508.06985)
*Jiawei Zhang,Yifei Zhang,Baozhao Yi,Yao Ren,Qi Jiao,Hanyu Bai,Weiran Jiang,Ziyou Song*

Main category: cs.LG

TL;DR: 论文提出了一种名为Discovery Learning（DL）的科学机器学习范式，用于快速验证电池设计，显著减少时间和能源成本。


<details>
  <summary>Details</summary>
Motivation: 电池研发因原型设计和寿命测试的高成本而受限，现有数据驱动方法需目标设计的标记数据且效率不足。

Method: DL整合主动学习、物理引导学习和零样本学习，从历史电池设计中学习，减少原型需求。

Result: 在123个工业级锂离子电池上测试，DL仅用公开数据集训练，预测寿命误差为7.2%，节省98%时间和95%能源。

Conclusion: DL通过历史设计加速下一代电池技术开发，是数据驱动建模的重要进展。

Abstract: Fast and reliable validation of novel designs in complex physical systems
such as batteries is critical to accelerating technological innovation.
However, battery research and development remain bottlenecked by the
prohibitively high time and energy costs required to evaluate numerous new
design candidates, particularly in battery prototyping and life testing.
Despite recent progress in data-driven battery lifetime prediction, existing
methods require labeled data of target designs to improve accuracy and cannot
make reliable predictions until after prototyping, thus falling far short of
the efficiency needed to enable rapid feedback for battery design. Here, we
introduce Discovery Learning (DL), a scientific machine-learning paradigm that
integrates active learning, physics-guided learning, and zero-shot learning
into a human-like reasoning loop, drawing inspiration from learning theories in
educational psychology. DL can learn from historical battery designs and
actively reduce the need for prototyping, thus enabling rapid lifetime
evaluation for unobserved material-design combinations without requiring
additional data labeling. To test DL, we present 123 industrial-grade
large-format lithium-ion pouch cells, spanning eight material-design
combinations and diverse cycling protocols. Trained solely on public datasets
of small-capacity cylindrical cells, DL achieves 7.2% test error in predicting
the average cycle life under unknown device variability. This results in
savings of 98% in time and 95% in energy compared to industrial practices. This
work highlights the potential of uncovering insights from historical designs to
inform and accelerate the development of next-generation battery technologies.
DL represents a key advance toward efficient data-driven modeling and helps
realize the promise of machine learning for accelerating scientific discovery
and engineering innovation.

</details>


### [49] [UniMove: A Unified Model for Multi-city Human Mobility Prediction](https://arxiv.org/abs/2508.06986)
*Chonghua Han,Yuan Yuan,Yukun Liu,Jingtao Ding,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: UniMove是一个统一的多城市人类移动预测模型，通过通用空间表示和轨迹-位置双塔架构，解决了跨城市建模的挑战，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 人类移动预测对城市规划和服务优化至关重要，但现有方法因城市异质性需为每个城市单独建模，效率低下。

Method: 提出轨迹-位置双塔架构和MoE Transformer块，实现通用空间编码和自适应移动模式建模。

Result: 在多城市数据集上，UniMove的预测准确性提升了10.2%，展现了统一模型的潜力。

Conclusion: UniMove为人类移动预测的基础模型提供了关键进展，支持多城市联合训练与数据增强。

Abstract: Human mobility prediction is vital for urban planning, transportation
optimization, and personalized services. However, the inherent randomness,
non-uniform time intervals, and complex patterns of human mobility, compounded
by the heterogeneity introduced by varying city structures, infrastructure, and
population densities, present significant challenges in modeling. Existing
solutions often require training separate models for each city due to distinct
spatial representations and geographic coverage. In this paper, we propose
UniMove, a unified model for multi-city human mobility prediction, addressing
two challenges: (1) constructing universal spatial representations for
effective token sharing across cities, and (2) modeling heterogeneous mobility
patterns from varying city characteristics. We propose a trajectory-location
dual-tower architecture, with a location tower for universal spatial encoding
and a trajectory tower for sequential mobility modeling. We also design MoE
Transformer blocks to adaptively select experts to handle diverse movement
patterns. Extensive experiments across multiple datasets from diverse cities
demonstrate that UniMove truly embodies the essence of a unified model. By
enabling joint training on multi-city data with mutual data enhancement, it
significantly improves mobility prediction accuracy by over 10.2\%. UniMove
represents a key advancement toward realizing a true foundational model with a
unified architecture for human mobility. We release the implementation at
https://github.com/tsinghua-fib-lab/UniMove/.

</details>


### [50] [A Comparative Study of Feature Selection in Tsetlin Machines](https://arxiv.org/abs/2508.06991)
*Vojtech Halenka,Ole-Christoffer Granmo,Lei Jiao,Per-Arne Andersen*

Main category: cs.LG

TL;DR: 论文研究了特征选择（FS）在Tsetlin机器（TM）中的应用，评估了多种FS方法，包括经典方法和后解释方法，并提出了一种新的基于TM内部评分器的FS方法。结果显示TM内部评分器性能优异且计算成本低。


<details>
  <summary>Details</summary>
Motivation: TM缺乏特征重要性评估工具，因此需要研究如何将FS技术应用于TM以提高其解释性和性能。

Method: 研究采用了多种FS技术，包括经典过滤和嵌入方法，以及后解释方法（如SHAP和LIME），并提出了一种新的基于TM内部评分器的FS方法。

Result: TM内部评分器性能优异，能够揭示特征交互模式，且计算成本低。

Conclusion: 该研究为TM中的FS建立了首个全面基线，并为开发专门针对TM的解释性技术奠定了基础。

Abstract: Feature Selection (FS) is crucial for improving model interpretability,
reducing complexity, and sometimes for enhancing accuracy. The recently
introduced Tsetlin machine (TM) offers interpretable clause-based learning, but
lacks established tools for estimating feature importance. In this paper, we
adapt and evaluate a range of FS techniques for TMs, including classical filter
and embedded methods as well as post-hoc explanation methods originally
developed for neural networks (e.g., SHAP and LIME) and a novel family of
embedded scorers derived from TM clause weights and Tsetlin automaton (TA)
states. We benchmark all methods across 12 datasets, using evaluation
protocols, like Remove and Retrain (ROAR) strategy and Remove and Debias
(ROAD), to assess causal impact. Our results show that TM-internal scorers not
only perform competitively but also exploit the interpretability of clauses to
reveal interacting feature patterns. Simpler TM-specific scorers achieve
similar accuracy retention at a fraction of the computational cost. This study
establishes the first comprehensive baseline for FS in TM and paves the way for
developing specialized TM-specific interpretability techniques.

</details>


### [51] [Conformal Set-based Human-AI Complementarity with Multiple Experts](https://arxiv.org/abs/2508.06997)
*Helbert Paat,Guohao Shen*

Main category: cs.LG

TL;DR: 论文研究了如何从多个专家中选择适合特定实例的专家，利用保形预测集提升分类性能，提出了一种贪心算法，效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单一专家场景，而实际中多个专家可能对特定实例更有帮助，因此需要研究如何选择最优专家子集。

Method: 提出了一种贪心算法，利用保形预测集选择适合特定实例的专家子集。

Result: 在CIFAR-10H和ImageNet-16H数据集上的模拟实验表明，该算法能选出接近最优的专家子集，提升分类性能。

Conclusion: 多专家场景下，基于保形预测集的贪心算法能有效提升分类性能，优于传统方法。

Abstract: Decision support systems are designed to assist human experts in
classification tasks by providing conformal prediction sets derived from a
pre-trained model. This human-AI collaboration has demonstrated enhanced
classification performance compared to using either the model or the expert
independently. In this study, we focus on the selection of instance-specific
experts from a pool of multiple human experts, contrasting it with existing
research that typically focuses on single-expert scenarios. We characterize the
conditions under which multiple experts can benefit from the conformal sets.
With the insight that only certain experts may be relevant for each instance,
we explore the problem of subset selection and introduce a greedy algorithm
that utilizes conformal sets to identify the subset of expert predictions that
will be used in classifying an instance. This approach is shown to yield better
performance compared to naive methods for human subset selection. Based on real
expert predictions from the CIFAR-10H and ImageNet-16H datasets, our simulation
study indicates that our proposed greedy algorithm achieves near-optimal
subsets, resulting in improved classification performance among multiple
experts.

</details>


### [52] [From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving](https://arxiv.org/abs/2508.07029)
*Antonio Guillen-Perez*

Main category: cs.LG

TL;DR: 论文提出了一种基于离线强化学习的驾驶策略学习方法，显著提升了长期驾驶的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在线数据收集在自动驾驶中不切实际且不安全，而行为克隆（BC）策略在闭环执行中容易出错。

Method: 开发了基于Transformer的BC基线模型，并应用离线强化学习算法CQL，结合精心设计的奖励函数。

Result: 在Waymo数据集上，CQL策略的成功率提高了3.2倍，碰撞率降低了7.4倍。

Conclusion: 离线强化学习是从静态专家数据中学习鲁棒驾驶策略的关键。

Abstract: Learning robust driving policies from large-scale, real-world datasets is a
central challenge in autonomous driving, as online data collection is often
unsafe and impractical. While Behavioral Cloning (BC) offers a straightforward
approach to imitation learning, policies trained with BC are notoriously
brittle and suffer from compounding errors in closed-loop execution. This work
presents a comprehensive pipeline and a comparative study to address this
limitation. We first develop a series of increasingly sophisticated BC
baselines, culminating in a Transformer-based model that operates on a
structured, entity-centric state representation. While this model achieves low
imitation loss, we show that it still fails in long-horizon simulations. We
then demonstrate that by applying a state-of-the-art Offline Reinforcement
Learning algorithm, Conservative Q-Learning (CQL), to the same data and
architecture, we can learn a significantly more robust policy. Using a
carefully engineered reward function, the CQL agent learns a conservative value
function that enables it to recover from minor errors and avoid
out-of-distribution states. In a large-scale evaluation on 1,000 unseen
scenarios from the Waymo Open Motion Dataset, our final CQL agent achieves a
3.2x higher success rate and a 7.4x lower collision rate than the strongest BC
baseline, proving that an offline RL approach is critical for learning robust,
long-horizon driving policies from static expert data.

</details>


### [53] [A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling](https://arxiv.org/abs/2508.07032)
*Tiantian He,Keyue Jiang,An Zhao,Anna Schroder,Elinor Thompson,Sonja Soskic,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.LG

TL;DR: 提出了一种阶段感知的混合专家框架（IGND-MoE），用于建模神经退行性疾病的动态进展，结合图神经扩散和局部反应模块。


<details>
  <summary>Details</summary>
Motivation: 传统模型假设病理机制固定，无法捕捉疾病不同阶段的复杂机制变化，且缺乏纵向数据支持。

Method: 采用时间依赖的专家权重建模阶段特异性机制，结合迭代双优化方法估计时间位置，并引入非均匀图神经扩散模型（IGND）和局部神经反应模块。

Result: 模型动态整合时空组件，揭示早期图相关过程主导，后期其他物理过程更重要的临床见解。

Conclusion: IGND-MoE为理解疾病进展提供了新视角，支持阶段特异性机制的研究。

Abstract: The long-term progression of neurodegenerative diseases is commonly
conceptualized as a spatiotemporal diffusion process that consists of a graph
diffusion process across the structural brain connectome and a localized
reaction process within brain regions. However, modeling this progression
remains challenging due to 1) the scarcity of longitudinal data obtained
through irregular and infrequent subject visits and 2) the complex interplay of
pathological mechanisms across brain regions and disease stages, where
traditional models assume fixed mechanisms throughout disease progression. To
address these limitations, we propose a novel stage-aware Mixture of Experts
(MoE) framework that explicitly models how different contributing mechanisms
dominate at different disease stages through time-dependent expert
weighting.Data-wise, we utilize an iterative dual optimization method to
properly estimate the temporal position of individual observations,
constructing a co hort-level progression trajectory from irregular snapshots.
Model-wise, we enhance the spatial component with an inhomogeneous graph neural
diffusion model (IGND) that allows diffusivity to vary based on node states and
time, providing more flexible representations of brain networks. We also
introduce a localized neural reaction module to capture complex dynamics beyond
standard processes.The resulting IGND-MoE model dynamically integrates these
components across temporal states, offering a principled way to understand how
stage-specific pathological mechanisms contribute to progression. The
stage-wise weights yield novel clinical insights that align with literature,
suggesting that graph-related processes are more influential at early stages,
while other unknown physical processes become dominant later on.

</details>


### [54] [Differentiable Adaptive Kalman Filtering via Optimal Transport](https://arxiv.org/abs/2508.07037)
*Yangguang He,Wenhao Li,Minzhe Li,Juan Zhang,Xiangfeng Wang,Bo Jin*

Main category: cs.LG

TL;DR: OTAKNet是一种在线解决方案，用于解决学习型自适应卡尔曼滤波中的噪声统计漂移问题，通过最优传输实现无需标签或重新训练的在线适应。


<details>
  <summary>Details</summary>
Motivation: 现实环境中未观测到的噪声统计漂移会导致学习型方法性能下降，需要一种在线适应方法。

Method: OTAKNet通过一步预测测量似然将状态估计与漂移联系起来，并利用最优传输的几何感知成本和稳定梯度实现在线适应。

Result: 在合成和真实NCLT数据集上，OTAKNet表现优于传统模型和离线学习型方法，尤其在训练数据有限时。

Conclusion: OTAKNet为噪声统计漂移问题提供了有效的在线解决方案，适用于实际部署场景。

Abstract: Learning-based filtering has demonstrated strong performance in non-linear
dynamical systems, particularly when the statistics of noise are unknown.
However, in real-world deployments, environmental factors, such as changing
wind conditions or electromagnetic interference, can induce unobserved
noise-statistics drift, leading to substantial degradation of learning-based
methods. To address this challenge, we propose OTAKNet, the first online
solution to noise-statistics drift within learning-based adaptive Kalman
filtering. Unlike existing learning-based methods that perform offline
fine-tuning using batch pointwise matching over entire trajectories, OTAKNet
establishes a connection between the state estimate and the drift via one-step
predictive measurement likelihood, and addresses it using optimal transport.
This leverages OT's geometry - aware cost and stable gradients to enable fully
online adaptation without ground truth labels or retraining. We compare OTAKNet
against classical model-based adaptive Kalman filtering and offline
learning-based filtering. The performance is demonstrated on both synthetic and
real-world NCLT datasets, particularly under limited training data.

</details>


### [55] [Membership and Memorization in LLM Knowledge Distillation](https://arxiv.org/abs/2508.07054)
*Ziqi Zhang,Ali Shahin Shamsabadi,Hanxiao Lu,Yifeng Cai,Hamed Haddadi*

Main category: cs.LG

TL;DR: 论文研究了知识蒸馏（KD）技术在大型语言模型（LLMs）中的隐私风险，发现所有现有KD方法都会将教师的成员资格和记忆隐私风险传递给学生，但风险程度因技术而异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的高计算需求促使知识蒸馏技术的应用，但学生在继承教师知识时可能同时继承隐私风险，这一问题尚未被系统研究。

Method: 通过指令调优设置，涵盖七个NLP任务、三种教师模型家族（GPT-2、LLAMA-2、OPT）及不同规模的学生模型，分析了六种LLM KD技术的隐私风险。

Result: 所有KD方法均存在隐私风险，但程度因技术而异；记忆与成员资格隐私风险之间存在显著不一致；不同模块的隐私风险差异较大。

Conclusion: 研究揭示了KD技术在隐私风险方面的局限性，为未来设计更安全的KD方法提供了方向。

Abstract: Recent advances in Knowledge Distillation (KD) aim to mitigate the high
computational demands of Large Language Models (LLMs) by transferring knowledge
from a large ''teacher'' to a smaller ''student'' model. However, students may
inherit the teacher's privacy when the teacher is trained on private data. In
this work, we systematically characterize and investigate membership and
memorization privacy risks inherent in six LLM KD techniques. Using
instruction-tuning settings that span seven NLP tasks, together with three
teacher model families (GPT-2, LLAMA-2, and OPT), and various size student
models, we demonstrate that all existing LLM KD approaches carry membership and
memorization privacy risks from the teacher to its students. However, the
extent of privacy risks varies across different KD techniques. We
systematically analyse how key LLM KD components (KD objective functions,
student training data and NLP tasks) impact such privacy risks. We also
demonstrate a significant disagreement between memorization and membership
privacy risks of LLM KD techniques. Finally, we characterize per-block privacy
risk and demonstrate that the privacy risk varies across different blocks by a
large margin.

</details>


### [56] [Surgical Knowledge Rewrite in Compact LLMs: An 'Unlearn-then-Learn' Strategy with ($IA^3$) for Localized Factual Modulation and Catastrophic Forgetting Mitigation](https://arxiv.org/abs/2508.07075)
*Stanley Ngugi*

Main category: cs.LG

TL;DR: 论文提出了一种“先遗忘后学习”策略，通过参数高效微调技术（PEFT）和$IA^3$方法，解决大语言模型（LLMs）动态知识更新中的冲突问题，显著提高了新知识的准确性和旧知识的遗忘率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在动态知识更新时，尤其是新信息与旧知识冲突时，存在难以接受新知识和严重遗忘无关知识的问题。

Method: 采用“先遗忘后学习”策略，结合$IA^3$技术，通过电路定位阶段精准定位冲突知识的内部编码组件。

Result: 实验表明，该方法在调整新知识时达到98.50%的准确率，同时旧知识的遗忘率为96.00%，显著优于直接微调方法。

Conclusion: 该方法实现了精准、局部化和安全的知识管理，为大语言模型的知识更新提供了新思路。

Abstract: Large Language Models (LLMs) struggle with dynamic knowledge updates,
especially when new information conflicts with deeply embedded facts. Such
conflicting factual edits often lead to two critical issues: resistance to
adopting the new fact and severe catastrophic forgetting of unrelated
knowledge. This paper introduces and evaluates a novel "unlearn-then-learn"
strategy for precise knowledge editing in LLMs, leveraging the
parameter-efficient fine-tuning (PEFT) technique, Infused Adapter by Inhibiting
and Amplifying Inner Activations ($IA^3$). Crucially, this two-stage approach
is powered by an initial circuit localization phase that identifies and targets
the specific internal components responsible for encoding the conflicting fact.
Through a rigorous experimental methodology on
microsoft/Phi-3-mini-4k-instruct, we demonstrate that this mechanistically
informed two-stage approach achieves near-perfect accuracy (98.50%) for the
new, modulated fact while simultaneously effectively suppressing the original
conflicting fact (96.00% forget rate). Critically, our strategy exhibits
unprecedented localization (72.00% F_control accuracy), dramatically mitigating
catastrophic forgetting observed in direct fine-tuning approaches (which showed
as low as ~20% F_control accuracy), a direct benefit of our targeted
interpretability-guided intervention. Furthermore, qualitative analysis reveals
a nuanced mechanism of "soft forgetting," where original knowledge is
suppressed from default retrieval but remains latent and conditionally
accessible, enhancing model safety and control. These findings represent a
significant advancement towards precise, localized, and safe knowledge
management in compact LLMs.

</details>


### [57] [Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework](https://arxiv.org/abs/2508.07085)
*N Harshit,K Mounvik*

Main category: cs.LG

TL;DR: 提出了一种结合Transformer和Autoencoder的混合框架，用于在线检测概念漂移，并通过Trust Score方法提高检测敏感性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 概念漂移（数据分布的渐变或突变）会显著降低模型性能，现有检测方法多为被动且对早期检测不敏感。

Method: 使用Transformer和Autoencoder建模复杂时序动态，结合Trust Score方法（包括统计和重构指标、预测不确定性、规则违反及分类器误差趋势）。

Result: 在航空乘客数据集中，该方法比基线方法更早、更敏感地检测到漂移，并减少了错误率和逻辑违反。

Conclusion: 开发了一个可靠的框架，用于实时监控概念漂移。

Abstract: In applied machine learning, concept drift, which is either gradual or abrupt
changes in data distribution, can significantly reduce model performance.
Typical detection methods,such as statistical tests or reconstruction-based
models,are generally reactive and not very sensitive to early detection. Our
study proposes a hybrid framework consisting of Transformers and Autoencoders
to model complex temporal dynamics and provide online drift detection. We
create a distinct Trust Score methodology, which includes signals on (1)
statistical and reconstruction-based drift metrics, more specifically, PSI,
JSD, Transformer-AE error, (2) prediction uncertainty, (3) rules violations,
and (4) trend of classifier error aligned with the combined metrics defined by
the Trust Score. Using a time sequenced airline passenger data set with
synthetic drift, our proposed model allows for a better detection of drift
using as a whole and at different detection thresholds for both sensitivity and
interpretability compared to baseline methods and provides a strong pipeline
for drift detection in real time for applied machine learning. We evaluated
performance using a time-sequenced airline passenger dataset having the
gradually injected stimulus of drift in expectations,e.g. permuted ticket
prices in later batches, broken into 10 time segments [1].In the data, our
results support that the Transformation-Autoencoder detected drift earlier and
with more sensitivity than the autoencoders commonly used in the literature,
and provided improved modeling over more error rates and logical violations.
Therefore, a robust framework was developed to reliably monitor concept drift.

</details>


### [58] [Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria](https://arxiv.org/abs/2508.07102)
*Yang Cao,Yubin Chen,Zhao Song,Jiahao Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Second-Order MeanFlow的新方法，通过引入平均加速度场扩展了MeanFlow框架，证明了其可行性和高效性，并提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 生成建模在无模拟范式（如Flow Matching）中取得了显著进展，但需要进一步扩展以支持更丰富的动力学和高效采样。

Method: 通过引入平均加速度场，提出Second-Order MeanFlow，并证明其满足一致性条件，支持单步采样和高效损失函数。

Result: 证明了Second-Order MeanFlow在$\mathsf{TC}^0$类中可实现，并展示了注意力操作的高效近似方法。

Conclusion: 为高阶流匹配模型提供了理论基础，结合了丰富的动力学和实际采样效率。

Abstract: Generative modelling has seen significant advances through simulation-free
paradigms such as Flow Matching, and in particular, the MeanFlow framework,
which replaces instantaneous velocity fields with average velocities to enable
efficient single-step sampling. In this work, we introduce a theoretical study
on Second-Order MeanFlow, a novel extension that incorporates average
acceleration fields into the MeanFlow objective. We first establish the
feasibility of our approach by proving that the average acceleration satisfies
a generalized consistency condition analogous to first-order MeanFlow, thereby
supporting stable, one-step sampling and tractable loss functions. We then
characterize its expressivity via circuit complexity analysis, showing that
under mild assumptions, the Second-Order MeanFlow sampling process can be
implemented by uniform threshold circuits within the $\mathsf{TC}^0$ class.
Finally, we derive provably efficient criteria for scalable implementation by
leveraging fast approximate attention computations: we prove that attention
operations within the Second-Order MeanFlow architecture can be approximated to
within $1/\mathrm{poly}(n)$ error in time $n^{2+o(1)}$. Together, these results
lay the theoretical foundation for high-order flow matching models that combine
rich dynamics with practical sampling efficiency.

</details>


### [59] [BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation](https://arxiv.org/abs/2508.07106)
*Yiran Huang,Amirhossein Nouranizadeh,Christine Ahrends,Mengjia Xu*

Main category: cs.LG

TL;DR: 提出了一种名为BrainATCL的无监督自适应时序脑连接学习框架，用于动态fMRI数据的时空建模，解决了传统GNN难以捕捉长时序依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 动态fMRI数据中的功能连接变化与行为和神经精神疾病相关，但传统GNN难以捕捉长时序依赖，需要新的建模方法。

Method: 采用自适应回溯窗口策略，结合GINE-Mamba2骨干网络和脑结构功能属性，学习动态功能连接的时空表征。

Result: 在功能连接预测和年龄估计任务中表现优异，具有强泛化能力，包括跨会话预测场景。

Conclusion: BrainATCL为动态脑功能连接建模提供了有效工具，具有潜在的应用价值。

Abstract: Functional Magnetic Resonance Imaging (fMRI) is an imaging technique widely
used to study human brain activity. fMRI signals in areas across the brain
transiently synchronise and desynchronise their activity in a highly structured
manner, even when an individual is at rest. These functional connectivity
dynamics may be related to behaviour and neuropsychiatric disease. To model
these dynamics, temporal brain connectivity representations are essential, as
they reflect evolving interactions between brain regions and provide insight
into transient neural states and network reconfigurations. However,
conventional graph neural networks (GNNs) often struggle to capture long-range
temporal dependencies in dynamic fMRI data. To address this challenge, we
propose BrainATCL, an unsupervised, nonparametric framework for adaptive
temporal brain connectivity learning, enabling functional link prediction and
age estimation. Our method dynamically adjusts the lookback window for each
snapshot based on the rate of newly added edges. Graph sequences are
subsequently encoded using a GINE-Mamba2 backbone to learn spatial-temporal
representations of dynamic functional connectivity in resting-state fMRI data
of 1,000 participants from the Human Connectome Project. To further improve
spatial modeling, we incorporate brain structure and function-informed edge
attributes, i.e., the left/right hemispheric identity and subnetwork membership
of brain regions, enabling the model to capture biologically meaningful
topological patterns. We evaluate our BrainATCL on two tasks: functional link
prediction and age estimation. The experimental results demonstrate superior
performance and strong generalization, including in cross-session prediction
scenarios.

</details>


### [60] [Approaching Maximal Information Extraction in Low-Signal Regimes via Multiple Instance Learning](https://arxiv.org/abs/2508.07114)
*Atakan Azakli,Bernd Stelzer*

Main category: cs.LG

TL;DR: 提出了一种新的机器学习方法，通过多实例学习（MIL）提高假设检验中参数预测的精度和判别力，并系统减少预测误差。


<details>
  <summary>Details</summary>
Motivation: 解决现有分类器在极端情况下难以准确预测的问题，并探索MIL比单实例方法更具预测力的数学原因。

Method: 利用MIL模型，分析其在不同实例数量下的缩放行为，并以SMEFT的Wilson系数约束为例进行验证。

Result: 在特定条件下，可能从数据集中提取理论上的最大Fisher信息。

Conclusion: MIL方法在提高预测精度和信息提取方面具有潜力，尤其在复杂物理问题中表现优越。

Abstract: In this work, we propose a new machine learning (ML) methodology to obtain
more precise predictions for some parameters of interest in a given hypotheses
testing problem. Our proposed method also allows ML models to have more
discriminative power in cases where it is extremely challenging for
state-of-the-art classifiers to have any level of accurate predictions. This
method can also allow us to systematically decrease the error from ML models in
their predictions. In this paper, we provide a mathematical motivation why
Multiple Instance Learning (MIL) would have more predictive power over their
single-instance counterparts. We support our theoretical claims by analyzing
the behavior of the MIL models through their scaling behaviors with respect to
the number of instances on which the model makes predictions. As a concrete
application, we constrain Wilson coefficients of the Standard Model Effective
Field Theory (SMEFT) using kinematic information from subatomic particle
collision events at the Large Hadron Collider (LHC). We show that under certain
circumstances, it might be possible to extract the theoretical maximum Fisher
Information latent in a dataset.

</details>


### [61] [From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context](https://arxiv.org/abs/2508.07117)
*Peyman Baghershahi,Gregoire Fournier,Pranav Nyati,Sourav Medya*

Main category: cs.LG

TL;DR: LOGIC是一个轻量级框架，利用大型语言模型（LLM）为GNN预测生成可解释的解释，通过将GNN嵌入投影到LLM空间并结合混合提示，提升解释的忠实性和可读性。


<details>
  <summary>Details</summary>
Motivation: 现有GNN解释方法在处理富含自然语言的节点属性时难以生成细粒度的解释，LOGIC旨在解决这一问题。

Method: LOGIC将GNN节点嵌入投影到LLM空间，结合软提示和文本输入构建混合提示，使LLM能推理GNN内部表示并生成自然语言解释。

Result: 在四个真实TAG数据集上的实验表明，LOGIC在忠实性和稀疏性之间取得平衡，并显著提升人类中心指标（如洞察力）。

Conclusion: LOGIC为基于LLM的图学习可解释性开辟了新方向，通过将GNN内部表示与人类推理对齐。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning over
structured data, including text-attributed graphs, which are common in domains
such as citation networks, social platforms, and knowledge graphs. GNNs are not
inherently interpretable and thus, many explanation methods have been proposed.
However, existing explanation methods often struggle to generate interpretable,
fine-grained rationales, especially when node attributes include rich natural
language. In this work, we introduce LOGIC, a lightweight, post-hoc framework
that uses large language models (LLMs) to generate faithful and interpretable
explanations for GNN predictions. LOGIC projects GNN node embeddings into the
LLM embedding space and constructs hybrid prompts that interleave soft prompts
with textual inputs from the graph structure. This enables the LLM to reason
about GNN internal representations and produce natural language explanations
along with concise explanation subgraphs. Our experiments across four
real-world TAG datasets demonstrate that LOGIC achieves a favorable trade-off
between fidelity and sparsity, while significantly improving human-centric
metrics such as insightfulness. LOGIC sets a new direction for LLM-based
explainability in graph learning by aligning GNN internals with human
reasoning.

</details>


### [62] [Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks](https://arxiv.org/abs/2508.07122)
*Zhihao Xue,Yun Zi,Nia Qi,Ming Gong,Yujun Zou*

Main category: cs.LG

TL;DR: 提出一种基于时空图神经网络的性能预测算法，用于分布式后端系统的多级服务调用结构性能波动预测。


<details>
  <summary>Details</summary>
Motivation: 解决分布式后端系统中多级服务调用结构的性能波动预测挑战。

Method: 将系统状态抽象为图结构序列，结合运行时特征和调用关系，构建时空建模框架，使用图卷积网络和高门控循环网络提取依赖信息和动态演化，引入时间编码机制。

Result: 实验表明，模型在MAE、RMSE和R2等关键指标上优于现有方法，且在负载和结构变化下保持鲁棒性。

Conclusion: 模型在后台服务性能管理任务中具有实际应用潜力。

Abstract: This paper proposes a spatiotemporal graph neural network-based performance
prediction algorithm to address the challenge of forecasting performance
fluctuations in distributed backend systems with multi-level service call
structures. The method abstracts system states at different time slices into a
sequence of graph structures. It integrates the runtime features of service
nodes with the invocation relationships among services to construct a unified
spatiotemporal modeling framework. The model first applies a graph
convolutional network to extract high-order dependency information from the
service topology. Then it uses a gated recurrent network to capture the dynamic
evolution of performance metrics over time. A time encoding mechanism is also
introduced to enhance the model's ability to represent non-stationary temporal
sequences. The architecture is trained in an end-to-end manner, optimizing the
multi-layer nested structure to achieve high-precision regression of future
service performance metrics. To validate the effectiveness of the proposed
method, a large-scale public cluster dataset is used. A series of
multi-dimensional experiments are designed, including variations in time
windows and concurrent load levels. These experiments comprehensively evaluate
the model's predictive performance and stability. The experimental results show
that the proposed model outperforms existing representative methods across key
metrics such as MAE, RMSE, and R2. It maintains strong robustness under varying
load intensities and structural complexities. These results demonstrate the
model's practical potential for backend service performance management tasks.

</details>


### [63] [Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning](https://arxiv.org/abs/2508.07126)
*Zhengran Ji,Boyuan Chen*

Main category: cs.LG

TL;DR: Pref-GUIDE框架将实时标量反馈转化为偏好数据，提升奖励模型学习效果，优于传统标量反馈方法。


<details>
  <summary>Details</summary>
Motivation: 在在线强化学习中，标量反馈存在噪声和不一致问题，限制了奖励模型的准确性和泛化能力。

Method: Pref-GUIDE通过短窗口行为比较和过滤模糊反馈（Individual），以及用户群体投票形成共识偏好（Voting），改进奖励模型。

Result: 在三个挑战性环境中，Pref-GUIDE显著优于标量反馈基线，投票变体甚至超过专家设计的密集奖励。

Conclusion: Pref-GUIDE通过结构化偏好和群体反馈，为在线强化学习中利用人类输入提供了可扩展且原则性的方法。

Abstract: Training reinforcement learning agents with human feedback is crucial when
task objectives are difficult to specify through dense reward functions. While
prior methods rely on offline trajectory comparisons to elicit human
preferences, such data is unavailable in online learning scenarios where agents
must adapt on the fly. Recent approaches address this by collecting real-time
scalar feedback to guide agent behavior and train reward models for continued
learning after human feedback becomes unavailable. However, scalar feedback is
often noisy and inconsistent, limiting the accuracy and generalization of
learned rewards. We propose Pref-GUIDE, a framework that transforms real-time
scalar feedback into preference-based data to improve reward model learning for
continual policy training. Pref-GUIDE Individual mitigates temporal
inconsistency by comparing agent behaviors within short windows and filtering
ambiguous feedback. Pref-GUIDE Voting further enhances robustness by
aggregating reward models across a population of users to form consensus
preferences. Across three challenging environments, Pref-GUIDE significantly
outperforms scalar-feedback baselines, with the voting variant exceeding even
expert-designed dense rewards. By reframing scalar feedback as structured
preferences with population feedback, Pref-GUIDE offers a scalable and
principled approach for harnessing human input in online reinforcement
learning.

</details>


### [64] [How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction?](https://arxiv.org/abs/2508.07127)
*Niranjana Arun Menon,Iqra Farooq,Yulong Li,Sara Ahmed,Yutong Xie,Muhammad Awais,Imran Razzak*

Main category: cs.LG

TL;DR: 论文探讨了利用微调的大型语言模型（LLMs）预测心血管疾病（CVD）及其相关SNPs的潜力，通过分析基因组数据，展示了LLMs在早期检测和个性化医疗中的应用前景。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病预测因多因素病因和高维噪声数据而具有挑战性，LLMs在生物序列分析中的成功应用激发了其在CVD预测中的潜力。

Method: 研究通过微调LLMs，利用高通量基因组数据，以链式思维（CoT）推理任务形式预测疾病标签和临床推论。

Result: LLMs能够从基因组数据中学习潜在生物学关系，为CVD的早期检测和风险评估提供支持。

Conclusion: LLMs在心血管疾病预测和个性化医疗中展现出重要潜力。

Abstract: Cardiovascular disease (CVD) prediction remains a tremendous challenge due to
its multifactorial etiology and global burden of morbidity and mortality.
Despite the growing availability of genomic and electrophysiological data,
extracting biologically meaningful insights from such high-dimensional, noisy,
and sparsely annotated datasets remains a non-trivial task. Recently, LLMs has
been applied effectively to predict structural variations in biological
sequences. In this work, we explore the potential of fine-tuned LLMs to predict
cardiac diseases and SNPs potentially leading to CVD risk using genetic markers
derived from high-throughput genomic profiling. We investigate the effect of
genetic patterns associated with cardiac conditions and evaluate how LLMs can
learn latent biological relationships from structured and semi-structured
genomic data obtained by mapping genetic aspects that are inherited from the
family tree. By framing the problem as a Chain of Thought (CoT) reasoning task,
the models are prompted to generate disease labels and articulate informed
clinical deductions across diverse patient profiles and phenotypes. The
findings highlight the promise of LLMs in contributing to early detection, risk
assessment, and ultimately, the advancement of personalized medicine in cardiac
care.

</details>


### [65] [A Globally Optimal Analytic Solution for Semi-Nonnegative Matrix Factorization with Nonnegative or Mixed Inputs](https://arxiv.org/abs/2508.07134)
*Lu Chenggang*

Main category: cs.LG

TL;DR: 提出一种新的半非负矩阵分解（semi-NMF）方法，通过正交分解实现全局最优解，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有半-NMF算法多为迭代、非凸且易陷入局部最优，需改进。

Method: 基于输入数据的散布矩阵，通过正交分解得到全局最优解。

Result: 在Frobenius范数下达到全局最小重构误差，实验验证优于现有方法。

Conclusion: 新方法提供理论保证和实际优势，为矩阵分解优化提供新视角。

Abstract: Semi-Nonnegative Matrix Factorization (semi-NMF) extends classical
Nonnegative Matrix Factorization (NMF) by allowing the basis matrix to contain
both positive and negative entries, making it suitable for decomposing data
with mixed signs. However, most existing semi-NMF algorithms are iterative,
non-convex, and prone to local minima. In this paper, we propose a novel method
that yields a globally optimal solution to the semi-NMF problem under the
Frobenius norm, through an orthogonal decomposition derived from the scatter
matrix of the input data. We rigorously prove that our solution attains the
global minimum of the reconstruction error. Furthermore, we demonstrate that
when the input matrix is nonnegative, our method often achieves lower
reconstruction error than standard NMF algorithms, although unfortunately the
basis matrix may not satisfy nonnegativity. In particular, in low-rank cases
such as rank 1 or 2, our solution reduces exactly to a nonnegative
factorization, recovering the NMF structure. We validate our approach through
experiments on both synthetic data and the UCI Wine dataset, showing that our
method consistently outperforms existing NMF and semi-NMF methods in terms of
reconstruction accuracy. These results confirm that our globally optimal,
non-iterative formulation offers both theoretical guarantees and empirical
advantages, providing a new perspective on matrix factorization in optimization
and data analysis.

</details>


### [66] [A Stable and Principled Loss Function for Direct Language Model Alignment](https://arxiv.org/abs/2508.07137)
*Yuandong Tan*

Main category: cs.LG

TL;DR: 论文提出了一种新的损失函数，解决了Direct Preference Optimization (DPO) 中因对数差无限最大化导致的不稳定问题。


<details>
  <summary>Details</summary>
Motivation: DPO的损失函数与其理论推导不一致，可能导致训练不稳定和奖励黑客行为。

Method: 从RLHF最优性条件直接推导出新损失函数，针对对数差设定有限值而非最大化。

Result: 新方法避免了DPO中的大梯度问题，提升了稳定性，并在实验中显著优于DPO基线。

Conclusion: 提出的损失函数更稳定且有效，在模型对齐中表现优异。

Abstract: The alignment of large language models (LLMs) with human preferences is
commonly achieved through Reinforcement Learning from Human Feedback (RLHF).
Direct Preference Optimization (DPO) simplified this paradigm by establishing a
direct mapping between the optimal policy and a reward function, eliminating
the need for an explicit reward model. However, we argue that the DPO loss
function is theoretically misaligned with its own derivation, as it promotes
the indefinite maximization of a logits difference, which can lead to training
instability and reward hacking. In this paper, we propose a novel loss function
derived directly from the RLHF optimality condition. Our proposed loss targets
a specific, finite value for the logits difference, which is dictated by the
underlying reward, rather than its maximization. We provide a theoretical
analysis, including a gradient-based comparison, to demonstrate that our method
avoids the large gradients that plague DPO when the probability of dispreferred
responses approaches zero. This inherent stability prevents reward hacking and
leads to more effective alignment. We validate our approach by fine-tuning a
Qwen2.5-7B model, showing significant win-rate improvements over a standard DPO
baseline and achieving competitive performance against larger models like
Llama-3.1-8B.

</details>


### [67] [What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains](https://arxiv.org/abs/2508.07208)
*Chanakya Ekbote,Marco Bondaschi,Nived Rajaraman,Jason D. Lee,Michael Gastpar,Ashok Vardhan Makkuva,Paul Pu Liang*

Main category: cs.LG

TL;DR: 本文证明了双层单头Transformer可以表示任何k阶马尔可夫过程，填补了Transformer深度与马尔可夫阶数关系的理论空白。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer深度与ICL能力的关系，尤其是双层单头结构是否能表示高阶马尔可夫过程。

Method: 理论分析双层单头Transformer的表示能力，并通过简化的一阶马尔可夫链模型研究其学习动态。

Result: 证明双层单头Transformer可表示任何k阶条件k-gram，揭示了浅层架构在结构化序列任务中的强大ICL能力。

Conclusion: 研究深化了对Transformer ICL的理解，展示了浅层架构在复杂任务中的潜力。

Abstract: In-context learning (ICL) is a hallmark capability of transformers, through
which trained models learn to adapt to new tasks by leveraging information from
the input context. Prior work has shown that ICL emerges in transformers due to
the presence of special circuits called induction heads. Given the equivalence
between induction heads and conditional k-grams, a recent line of work modeling
sequential inputs as Markov processes has revealed the fundamental impact of
model depth on its ICL capabilities: while a two-layer transformer can
efficiently represent a conditional 1-gram model, its single-layer counterpart
cannot solve the task unless it is exponentially large. However, for higher
order Markov sources, the best known constructions require at least three
layers (each with a single attention head) - leaving open the question: can a
two-layer single-head transformer represent any kth-order Markov process? In
this paper, we precisely address this and theoretically show that a two-layer
transformer with one head per layer can indeed represent any conditional
k-gram. Thus, our result provides the tightest known characterization of the
interplay between transformer depth and Markov order for ICL. Building on this,
we further analyze the learning dynamics of our two-layer construction,
focusing on a simplified variant for first-order Markov chains, illustrating
how effective in-context representations emerge during training. Together,
these results deepen our current understanding of transformer-based ICL and
illustrate how even shallow architectures can surprisingly exhibit strong ICL
capabilities on structured sequence modeling tasks.

</details>


### [68] [Neural Bridge Processes](https://arxiv.org/abs/2508.07220)
*Jian Xu,Yican Liu,Qibin Zhao,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: 论文提出了一种名为Neural Bridge Processes (NBPs)的新方法，用于建模随机函数，通过动态锚定输入x的扩散轨迹，显著提升了性能与理论一致性。


<details>
  <summary>Details</summary>
Motivation: 传统方法如高斯过程（GPs）在处理大规模数据时存在可扩展性问题，且假设高斯性限制了其适用性；神经过程（NPs）虽更灵活，但难以捕捉复杂多模态分布；神经扩散过程（NDPs）虽增强表达能力，但输入耦合弱且存在语义不匹配问题。

Method: NBP通过将前向核显式依赖于输入x，强制约束扩散路径严格终止于监督目标，从而提供更强的梯度信号并保证终点一致性。

Result: 在合成数据、EEG信号回归和图像回归任务中，NBP显著优于基线方法。

Conclusion: NBP通过DDPM风格的桥采样，有效提升了结构化预测任务的性能与理论一致性。

Abstract: Learning stochastic functions from partially observed context-target pairs is
a fundamental problem in probabilistic modeling. Traditional models like
Gaussian Processes (GPs) face scalability issues with large datasets and assume
Gaussianity, limiting their applicability. While Neural Processes (NPs) offer
more flexibility, they struggle with capturing complex, multi-modal target
distributions. Neural Diffusion Processes (NDPs) enhance expressivity through a
learned diffusion process but rely solely on conditional signals in the
denoising network, resulting in weak input coupling from an unconditional
forward process and semantic mismatch at the diffusion endpoint. In this work,
we propose Neural Bridge Processes (NBPs), a novel method for modeling
stochastic functions where inputs x act as dynamic anchors for the entire
diffusion trajectory. By reformulating the forward kernel to explicitly depend
on x, NBP enforces a constrained path that strictly terminates at the
supervised target. This approach not only provides stronger gradient signals
but also guarantees endpoint coherence. We validate NBPs on synthetic data, EEG
signal regression and image regression tasks, achieving substantial
improvements over baselines. These results underscore the effectiveness of
DDPM-style bridge sampling in enhancing both performance and theoretical
consistency for structured prediction tasks.

</details>


### [69] [LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference](https://arxiv.org/abs/2508.07221)
*Po-Han Lee,Yu-Cheng Lin,Chan-Tung Ku,Chan Hsu,Pei-Cing Huang,Ping-Hsun Wu,Yihuang Kang*

Main category: cs.LG

TL;DR: 论文提出了一种基于大型语言模型（LLM）的代理方法，用于自动发现混杂变量和进行亚组分析，以提升因果机器学习的鲁棒性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 观测数据中的未测量混杂和结构偏差导致个体化治疗效果估计困难，现有因果机器学习方法在复杂环境中效果有限，且依赖领域专家带来高成本。

Method: 通过LLM代理模拟领域专家，自动发现混杂变量和进行亚组分析，减少人工依赖并保持可解释性。

Result: 在真实医疗数据集上的实验表明，该方法通过缩小置信区间和发现未识别的混杂偏差，提升了治疗效果估计的鲁棒性。

Conclusion: LLM代理为可扩展、可信且语义感知的因果推断提供了新方向。

Abstract: Estimating individualized treatment effects from observational data presents
a persistent challenge due to unmeasured confounding and structural bias.
Causal Machine Learning (causal ML) methods, such as causal trees and doubly
robust estimators, provide tools for estimating conditional average treatment
effects. These methods have limited effectiveness in complex real-world
environments due to the presence of latent confounders or those described in
unstructured formats. Moreover, reliance on domain experts for confounder
identification and rule interpretation introduces high annotation cost and
scalability concerns. In this work, we proposed Large Language Model-based
agents for automated confounder discovery and subgroup analysis that integrate
agents into the causal ML pipeline to simulate domain expertise. Our framework
systematically performs subgroup identification and confounding structure
discovery by leveraging the reasoning capabilities of LLM-based agents, which
reduces human dependency while preserving interpretability. Experiments on
real-world medical datasets show that our proposed approach enhances treatment
effect estimation robustness by narrowing confidence intervals and uncovering
unrecognized confounding biases. Our findings suggest that LLM-based agents
offer a promising path toward scalable, trustworthy, and semantically aware
causal inference.

</details>


### [70] [EDGE: A Theoretical Framework for Misconception-Aware Adaptive Learning](https://arxiv.org/abs/2508.07224)
*Ananda Prakash Verma*

Main category: cs.LG

TL;DR: EDGE是一个通用的、能识别误解的自适应学习框架，包含评估、诊断、生成和练习四个阶段，结合了心理测量学、认知诊断和对比性题目生成等技术。


<details>
  <summary>Details</summary>
Motivation: 解决传统自适应学习系统无法有效识别和纠正学习者误解的问题。

Method: 通过IRT/贝叶斯状态空间模型评估能力，从干扰项模式和反应时间诊断误解，生成对比性题目，并基于索引策略安排练习。

Result: 提出了EdgeScore指标，证明了其单调性和Lipschitz连续性，并推导出在温和假设下接近最优的索引策略。

Conclusion: EDGE框架理论上能更高效地减少学习者的误解，但需进一步实证验证。

Abstract: We present EDGE, a general-purpose, misconception-aware adaptive learning
framework composed of four stages: Evaluate (ability and state estimation),
Diagnose (posterior infer-ence of misconceptions), Generate (counterfactual
item synthesis), and Exercise (index-based retrieval scheduling). EDGE unifies
psychometrics (IRT/Bayesian state space models), cog-nitive diagnostics
(misconception discovery from distractor patterns and response latencies),
contrastive item generation (minimal perturbations that invalidate learner
shortcuts while pre-serving psychometric validity), and principled scheduling
(a restless bandit approximation to spaced retrieval). We formalize a composite
readiness metric, EdgeScore, prove its monotonicity and Lipschitz continuity,
and derive an index policy that is near-optimal under mild assumptions on
forgetting and learning gains. We further establish conditions under which
counterfactual items provably reduce the posterior probability of a targeted
misconception faster than standard practice. The paper focuses on theory and
implementable pseudocode; empirical study is left to future work.

</details>


### [71] [Causal Negative Sampling via Diffusion Model for Out-of-Distribution Recommendation](https://arxiv.org/abs/2508.07243)
*Chu Zhao,Eneng Yang,Yizhou Dang,Jianzhe Zhao,Guibing Guo,Xingwei Wang*

Main category: cs.LG

TL;DR: 提出了一种名为CNSDiff的新方法，通过扩散过程在潜在空间合成负样本，避免预定义候选池的偏差，并加入因果正则化项以提高OOD推荐性能。


<details>
  <summary>Details</summary>
Motivation: 启发式负采样可能因环境混杂因素（如曝光或流行度偏差）引入虚假硬负样本，影响模型的泛化能力。

Method: CNSDiff通过条件扩散过程在潜在空间合成负样本，并加入因果正则化项以减少混杂因素的影响。

Result: 在四种分布偏移场景下，CNSDiff平均性能提升13.96%，优于现有基线方法。

Conclusion: CNSDiff有效减少了虚假硬负样本，提升了OOD推荐任务的鲁棒性和泛化能力。

Abstract: Heuristic negative sampling enhances recommendation performance by selecting
negative samples of varying hardness levels from predefined candidate pools to
guide the model toward learning more accurate decision boundaries. However, our
empirical and theoretical analyses reveal that unobserved environmental
confounders (e.g., exposure or popularity biases) in candidate pools may cause
heuristic sampling methods to introduce false hard negatives (FHNS). These
misleading samples can encourage the model to learn spurious correlations
induced by such confounders, ultimately compromising its generalization ability
under distribution shifts. To address this issue, we propose a novel method
named Causal Negative Sampling via Diffusion (CNSDiff). By synthesizing
negative samples in the latent space via a conditional diffusion process,
CNSDiff avoids the bias introduced by predefined candidate pools and thus
reduces the likelihood of generating FHNS. Moreover, it incorporates a causal
regularization term to explicitly mitigate the influence of environmental
confounders during the negative sampling process, leading to robust negatives
that promote out-of-distribution (OOD) generalization. Comprehensive
experiments under four representative distribution shift scenarios demonstrate
that CNSDiff achieves an average improvement of 13.96% across all evaluation
metrics compared to state-of-the-art baselines, verifying its effectiveness and
robustness in OOD recommendation tasks.

</details>


### [72] [Policy Newton methods for Distortion Riskmetrics](https://arxiv.org/abs/2508.07249)
*Soumen Pachal,Mizhaan Prajit Maniyar,Prashanth L. A*

Main category: cs.LG

TL;DR: 论文研究了强化学习中的风险敏感控制问题，提出了一种基于失真风险度量的策略优化方法，并通过立方正则化牛顿算法实现收敛到二阶稳定点。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注风险中性目标或风险敏感目标的一阶收敛，本文旨在填补风险敏感目标二阶收敛的空白。

Method: 使用似然比方法推导策略Hessian定理，提出基于样本轨迹的Hessian估计器，并设计立方正则化牛顿算法。

Result: 算法收敛到二阶稳定点，样本复杂度为O(ε^{-3.5})，实验验证了理论结果。

Conclusion: 本文首次实现了风险敏感目标的二阶收敛，为相关领域提供了新思路。

Abstract: We consider the problem of risk-sensitive control in a reinforcement learning
(RL) framework. In particular, we aim to find a risk-optimal policy by
maximizing the distortion riskmetric (DRM) of the discounted reward in a finite
horizon Markov decision process (MDP). DRMs are a rich class of risk measures
that include several well-known risk measures as special cases. We derive a
policy Hessian theorem for the DRM objective using the likelihood ratio method.
Using this result, we propose a natural DRM Hessian estimator from sample
trajectories of the underlying MDP. Next, we present a cubic-regularized policy
Newton algorithm for solving this problem in an on-policy RL setting using
estimates of the DRM gradient and Hessian. Our proposed algorithm is shown to
converge to an $\epsilon$-second-order stationary point ($\epsilon$-SOSP) of
the DRM objective, and this guarantee ensures the escaping of saddle points.
The sample complexity of our algorithms to find an $ \epsilon$-SOSP is
$\mathcal{O}(\epsilon^{-3.5})$. Our experiments validate the theoretical
findings. To the best of our knowledge, our is the first work to present
convergence to an $\epsilon$-SOSP of a risk-sensitive objective, while existing
works in the literature have either shown convergence to a first-order
stationary point of a risk-sensitive objective, or a SOSP of a risk-neutral
one.

</details>


### [73] [PySeizure: A single machine learning classifier framework to detect seizures in diverse datasets](https://arxiv.org/abs/2508.07253)
*Bartlomiej Chybowski,Shima Abdullateef,Hollan Haule,Alfredo Gonzalez-Sulser,Javier Escudero*

Main category: cs.LG

TL;DR: 论文提出了一种开源机器学习框架，用于跨不同临床数据集的可靠癫痫发作检测，通过自动预处理和多数投票机制提升鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前癫痫诊断依赖耗时的人工EEG分析，现有机器学习方法受限于数据集特定优化，缺乏实际适用性和可重复性。

Method: 框架包含自动预处理管道和多数投票机制，训练和评估模型在不同数据集上的性能和跨数据集泛化能力。

Result: 模型在数据集内表现优异（AUC 0.904和0.864），跨数据集泛化能力显著（AUC 0.615和0.762），轻微后处理进一步提升性能。

Conclusion: 该框架为临床可行的、数据集无关的癫痫检测系统奠定了基础，具有广泛应用的潜力，可作为专家诊断的补充。

Abstract: Reliable seizure detection is critical for diagnosing and managing epilepsy,
yet clinical workflows remain dependent on time-consuming manual EEG
interpretation. While machine learning has shown promise, existing approaches
often rely on dataset-specific optimisations, limiting their real-world
applicability and reproducibility. Here, we introduce an innovative,
open-source machine-learning framework that enables robust and generalisable
seizure detection across varied clinical datasets. We evaluate our approach on
two publicly available EEG datasets that differ in patient populations and
electrode configurations. To enhance robustness, the framework incorporates an
automated pre-processing pipeline to standardise data and a majority voting
mechanism, in which multiple models independently assess each second of EEG
before reaching a final decision. We train, tune, and evaluate models within
each dataset, assessing their cross-dataset transferability. Our models achieve
high within-dataset performance (AUC 0.904+/-0.059 for CHB-MIT and
0.864+/-0.060 for TUSZ) and demonstrate strong generalisation across datasets
despite differences in EEG setups and populations (AUC 0.615+/-0.039 for models
trained on CHB-MIT and tested on TUSZ and 0.762+/-0.175 in the reverse case)
without any post-processing. Furthermore, a mild post-processing improved the
within-dataset results to 0.913+/-0.064 and 0.867+/-0.058 and cross-dataset
results to 0.619+/-0.036 and 0.768+/-0.172. These results underscore the
potential of, and essential considerations for, deploying our framework in
diverse clinical settings. By making our methodology fully reproducible, we
provide a foundation for advancing clinically viable, dataset-agnostic seizure
detection systems. This approach has the potential for widespread adoption,
complementing rather than replacing expert interpretation, and accelerating
clinical integration.

</details>


### [74] [Revisiting Data Attribution for Influence Functions](https://arxiv.org/abs/2508.07297)
*Hongbo Zhu,Angelo Cangelosi*

Main category: cs.LG

TL;DR: 本文综述了影响函数在深度学习中的数据归因能力，探讨其理论基础、高效逆Hessian向量积估计算法进展，并评估其在数据归因和错误标签检测中的有效性。


<details>
  <summary>Details</summary>
Motivation: 理解训练数据如何影响模型预测对机器学习可解释性、数据调试和模型问责至关重要。影响函数提供了一种高效的一阶近似方法，无需重新训练即可估计数据点对模型参数和预测的影响。

Method: 本文回顾了影响函数的理论基础，并讨论了高效逆Hessian向量积估计算法的最新进展。通过实验评估了影响函数在数据归因和错误标签检测中的表现。

Result: 影响函数在数据归因和错误标签检测中表现出有效性，但仍面临大规模实际应用中的挑战。

Conclusion: 影响函数在深度学习中有巨大潜力，但需进一步研究以解决实际应用中的问题。

Abstract: The goal of data attribution is to trace the model's predictions through the
learning algorithm and back to its training data. thereby identifying the most
influential training samples and understanding how the model's behavior leads
to particular predictions. Understanding how individual training examples
influence a model's predictions is fundamental for machine learning
interpretability, data debugging, and model accountability. Influence
functions, originating from robust statistics, offer an efficient, first-order
approximation to estimate the impact of marginally upweighting or removing a
data point on a model's learned parameters and its subsequent predictions,
without the need for expensive retraining. This paper comprehensively reviews
the data attribution capability of influence functions in deep learning. We
discuss their theoretical foundations, recent algorithmic advances for
efficient inverse-Hessian-vector product estimation, and evaluate their
effectiveness for data attribution and mislabel detection. Finally,
highlighting current challenges and promising directions for unleashing the
huge potential of influence functions in large-scale, real-world deep learning
scenarios.

</details>


### [75] [When Is Prior Knowledge Helpful? Exploring the Evaluation and Selection of Unsupervised Pretext Tasks from a Neuro-Symbolic Perspective](https://arxiv.org/abs/2508.07299)
*Lin-Han Jia,Si-Yu Han,Wen-Chao Hu,Jie-Jing Shao,Wen-Da Wei,Zhi Zhou,Lan-Zhe Guo,Yu-Feng Li*

Main category: cs.LG

TL;DR: 论文提出了一种理论框架，将神经符号学习（Nesy）与半/自监督学习（SSL）统一起来，并通过理论分析揭示了预任务对目标性能的影响因素。


<details>
  <summary>Details</summary>
Motivation: 当前无监督预任务的选择缺乏理论依据，论文旨在通过理论分析解决这一问题。

Method: 基于理论分析，提出了知识可学习性、可靠性和完整性的评估方法，并开发了一种预测预任务有效性的方法。

Result: 实验验证了预测性能与实际性能之间的高相关性，证明了理论和评估方法的有效性。

Conclusion: 论文为无监督预任务的选择提供了理论支持，改变了当前依赖启发式方法的现状。

Abstract: Neuro-symbolic (Nesy) learning improves the target task performance of models
by enabling them to satisfy knowledge, while semi/self-supervised learning
(SSL) improves the target task performance by designing unsupervised pretext
tasks for unlabeled data to make models satisfy corresponding assumptions. We
extend the Nesy theory based on reliable knowledge to the scenario of
unreliable knowledge (i.e., assumptions), thereby unifying the theoretical
frameworks of SSL and Nesy. Through rigorous theoretical analysis, we
demonstrate that, in theory, the impact of pretext tasks on target performance
hinges on three factors: knowledge learnability with respect to the model,
knowledge reliability with respect to the data, and knowledge completeness with
respect to the target. We further propose schemes to operationalize these
theoretical metrics, and thereby develop a method that can predict the
effectiveness of pretext tasks in advance. This will change the current status
quo in practical applications, where the selections of unsupervised tasks are
heuristic-based rather than theory-based, and it is difficult to evaluate the
rationality of unsupervised pretext task selection before testing the model on
the target task. In experiments, we verify a high correlation between the
predicted performance-estimated using minimal data-and the actual performance
achieved after large-scale semi-supervised or self-supervised learning, thus
confirming the validity of the theory and the effectiveness of the evaluation
method.

</details>


### [76] [Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative](https://arxiv.org/abs/2508.07329)
*Tuo Zhang,Ning Li,Xin Yuan,Wenchao Xu,Quan Chen,Song Guo,Haijun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于Hessian感知量化（HAQ）和CPU-GPU协同推理的高效MoE边缘部署方案，解决了量化精度和内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在多模态任务中的突破性进展，如何在资源受限的边缘设备上高效部署成为关键挑战。MoE架构通过稀疏激活提升模型容量，但在实际部署中面临量化精度下降和内存限制问题。

Method: 采用Hessian感知量化（HAQ）实现激活和权重的8位联合量化，并设计专家级协同卸载与推理机制，结合专家激活路径统计，优化CPU-GPU间的模块调度。

Result: 在OPT系列和Mixtral 8*7B等主流大模型上验证，低比特量化模型的推理精度接近全精度模型，GPU内存使用减少约60%，推理延迟显著降低。

Conclusion: 该方法显著提升了MoE模型在边缘设备上的部署效率和性能，为资源受限环境下的高效推理提供了可行方案。

Abstract: With the breakthrough progress of large language models (LLMs) in natural
language processing and multimodal tasks, efficiently deploying them on
resource-constrained edge devices has become a critical challenge. The Mixture
of Experts (MoE) architecture enhances model capacity through sparse
activation, but faces two major difficulties in practical deployment: (1) The
presence of numerous outliers in activation distributions leads to severe
degradation in quantization accuracy for both activations and weights,
significantly impairing inference performance; (2) Under limited memory,
efficient offloading and collaborative inference of expert modules struggle to
balance latency and throughput. To address these issues, this paper proposes an
efficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ)
and CPU-GPU collaborative inference. First, by introducing smoothed Hessian
matrix quantization, we achieve joint 8-bit quantization of activations and
weights, which significantly alleviates the accuracy loss caused by outliers
while ensuring efficient implementation on mainstream hardware. Second, we
design an expert-level collaborative offloading and inference mechanism, which,
combined with expert activation path statistics, enables efficient deployment
and scheduling of expert modules between CPU and GPU, greatly reducing memory
footprint and inference latency. Extensive experiments validate the
effectiveness of our method on mainstream large models such as the OPT series
and Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of
the low-bit quantized model approaches that of the full-precision model, while
GPU memory usage is reduced by about 60%, and inference latency is
significantly improved.

</details>


### [77] [Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants](https://arxiv.org/abs/2508.07333)
*Yuhao Liu,Rui Hu,Yu Chen,Longbo Huang*

Main category: cs.LG

TL;DR: 本文研究了基于随机插值的ODE数值实现有限时间收敛性，提出了两种数值积分器的误差界，并优化了计算效率。


<details>
  <summary>Details</summary>
Motivation: 随机插值在生成建模中潜力巨大，但数值实现的有限时间收敛性缺乏严格分析。

Method: 分析了前向欧拉法和Heun法的有限时间误差界，并优化了随机插值的迭代复杂度。

Result: 建立了总变差距离下的误差界，并通过数值实验验证了理论结果。

Conclusion: 研究为随机插值的数值实现提供了理论支持，优化了计算效率。

Abstract: Stochastic interpolants offer a robust framework for continuously
transforming samples between arbitrary data distributions, holding significant
promise for generative modeling. Despite their potential, rigorous finite-time
convergence guarantees for practical numerical schemes remain largely
unexplored. In this work, we address the finite-time convergence analysis of
numerical implementations for ordinary differential equations (ODEs) derived
from stochastic interpolants. Specifically, we establish novel finite-time
error bounds in total variation distance for two widely used numerical
integrators: the first-order forward Euler method and the second-order Heun's
method. Furthermore, our analysis on the iteration complexity of specific
stochastic interpolant constructions provides optimized schedules to enhance
computational efficiency. Our theoretical findings are corroborated by
numerical experiments, which validate the derived error bounds and complexity
analyses.

</details>


### [78] [ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis](https://arxiv.org/abs/2508.07345)
*Samiha Afaf Neha,Abir Ahammed Bhuiyan,Md. Ishrak Khan*

Main category: cs.LG

TL;DR: 论文提出ProteoKnight，一种基于图像的编码方法，用于噬菌体病毒蛋白（PVP）分类，并通过蒙特卡洛Dropout评估预测不确定性，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 噬菌体病毒蛋白（PVP）的准确预测对基因组研究至关重要，但现有编码方法存在空间信息丢失问题。

Method: ProteoKnight改进DNA-Walk算法，结合像素颜色和步长调整，利用预训练CNN分类，并通过MCD评估不确定性。

Result: 二元分类准确率达90.8%，多分类仍有改进空间；不确定性分析显示预测置信度受蛋白类别和序列长度影响。

Conclusion: ProteoKnight克服了FCGR的空间信息丢失问题，提供高精度PVP预测，并能识别低置信度预测。

Abstract: \textbf{Introduction:} Accurate prediction of Phage Virion Proteins (PVP) is
essential for genomic studies due to their crucial role as structural elements
in bacteriophages. Computational tools, particularly machine learning, have
emerged for annotating phage protein sequences from high-throughput sequencing.
However, effective annotation requires specialized sequence encodings. Our
paper introduces ProteoKnight, a new image-based encoding method that addresses
spatial constraints in existing techniques, yielding competitive performance in
PVP classification using pre-trained convolutional neural networks.
Additionally, our study evaluates prediction uncertainty in binary PVP
classification through Monte Carlo Dropout (MCD). \textbf{Methods:}
ProteoKnight adapts the classical DNA-Walk algorithm for protein sequences,
incorporating pixel colors and adjusting walk distances to capture intricate
protein features. Encoded sequences were classified using multiple pre-trained
CNNs. Variance and entropy measures assessed prediction uncertainty across
proteins of various classes and lengths. \textbf{Results:} Our experiments
achieved 90.8% accuracy in binary classification, comparable to
state-of-the-art methods. Multi-class classification accuracy remains
suboptimal. Our uncertainty analysis unveils variability in prediction
confidence influenced by protein class and sequence length.
\textbf{Conclusions:} Our study surpasses frequency chaos game representation
(FCGR) by introducing novel image encoding that mitigates spatial information
loss limitations. Our classification technique yields accurate and robust PVP
predictions while identifying low-confidence predictions.

</details>


### [79] [Intrinsic training dynamics of deep neural networks](https://arxiv.org/abs/2508.07370)
*Sibylle Marcotte,Gabriel Peyré,Rémi Gribonval*

Main category: cs.LG

TL;DR: 论文研究了高维参数空间中梯度流能否简化为低维结构，提出了一个基于核包含的简单判据，并应用于ReLU网络和线性网络，证明了在某些初始化条件下可以实现维度缩减。


<details>
  <summary>Details</summary>
Motivation: 理解深度学习中的梯度训练是否可以通过低维结构捕捉，即所谓的隐式偏差，是理论上的核心挑战。

Method: 通过研究高维变量θ的梯度流是否隐含低维变量z=φ(θ)的梯度流，提出了一种基于核包含的判据，并应用于ReLU网络和线性网络。

Result: 证明了对于任意初始化的ReLU网络，可以将其梯度流重写为仅依赖于z和初始化的低维动态；对于线性网络，放宽的平衡初始化是唯一能确保这种性质的条件。

Conclusion: 论文为梯度流的低维动态提供了理论支持，并扩展了平衡初始化的适用范围。

Abstract: A fundamental challenge in the theory of deep learning is to understand
whether gradient-based training in high-dimensional parameter spaces can be
captured by simpler, lower-dimensional structures, leading to so-called
implicit bias. As a stepping stone, we study when a gradient flow on a
high-dimensional variable $\theta$ implies an intrinsic gradient flow on a
lower-dimensional variable $z = \phi(\theta)$, for an architecture-related
function $\phi$. We express a so-called intrinsic dynamic property and show how
it is related to the study of conservation laws associated with the
factorization $\phi$. This leads to a simple criterion based on the inclusion
of kernels of linear maps which yields a necessary condition for this property
to hold. We then apply our theory to general ReLU networks of arbitrary depth
and show that, for any initialization, it is possible to rewrite the flow as an
intrinsic dynamic in a lower dimension that depends only on $z$ and the
initialization, when $\phi$ is the so-called path-lifting. In the case of
linear networks with $\phi$ the product of weight matrices, so-called balanced
initializations are also known to enable such a dimensionality reduction; we
generalize this result to a broader class of {\em relaxed balanced}
initializations, showing that, in certain configurations, these are the
\emph{only} initializations that ensure the intrinsic dynamic property.
Finally, for the linear neural ODE associated with the limit of infinitely deep
linear networks, with relaxed balanced initialization, we explicitly express
the corresponding intrinsic dynamics.

</details>


### [80] [Tight Bounds for Schrödinger Potential Estimation in Unpaired Image-to-Image Translation Problems](https://arxiv.org/abs/2508.07392)
*Nikita Puchkin,Denis Suchkov,Alexey Naumov,Denis Belomestny*

Main category: cs.LG

TL;DR: 论文研究了基于Schrödinger桥和随机最优控制理论的生成模型和无配对图像转换方法，通过Ornstein-Uhlenbeck过程估计Schrödinger势，并推导了经验风险最小化器的泛化能力界限。


<details>
  <summary>Details</summary>
Motivation: 解决在仅能获取初始和目标分布独立同分布样本的情况下，如何优化生成模型和无配对图像转换的问题。

Method: 采用随机最优控制方法，选择Ornstein-Uhlenbeck过程作为参考，估计Schrödinger势，并通过Kullback-Leibler散度定义风险函数。

Result: 在包括高斯混合的Schrödinger势类中，推导了经验风险最小化器的紧致泛化界限，并在有利场景下接近快速收敛速率。

Conclusion: 通过数值实验验证了方法的有效性，展示了其在生成模型和图像转换中的潜力。

Abstract: Modern methods of generative modelling and unpaired image-to-image
translation based on Schr\"odinger bridges and stochastic optimal control
theory aim to transform an initial density to a target one in an optimal way.
In the present paper, we assume that we only have access to i.i.d. samples from
initial and final distributions. This makes our setup suitable for both
generative modelling and unpaired image-to-image translation. Relying on the
stochastic optimal control approach, we choose an Ornstein-Uhlenbeck process as
the reference one and estimate the corresponding Schr\"odinger potential.
Introducing a risk function as the Kullback-Leibler divergence between
couplings, we derive tight bounds on generalization ability of an empirical
risk minimizer in a class of Schr\"odinger potentials including Gaussian
mixtures. Thanks to the mixing properties of the Ornstein-Uhlenbeck process, we
almost achieve fast rates of convergence up to some logarithmic factors in
favourable scenarios. We also illustrate performance of the suggested approach
with numerical experiments.

</details>


### [81] [Parity Requires Unified Input Dependence and Negative Eigenvalues in SSMs](https://arxiv.org/abs/2508.07395)
*Behnoush Khavari,Mehran Shakerinava,Jayesh Khullar,Jerry Huang,François Rivest,Siamak Ravanbakhsh,Sarath Chandar*

Main category: cs.LG

TL;DR: 研究表明，LRNN模型（如S4D、Mamba和DeltaNet）因时间不变转移矩阵或受限特征值范围而缺乏状态跟踪能力。输入依赖的转移矩阵（特别是复杂或非三角矩阵）被提出以提升SSM性能。尽管现有理论表明输入独立和非负SSM无法解决简单状态跟踪任务（如奇偶校验），但未探讨多层SSM中结合这两种类型是否有效。本文研究发现，即使结合这两种类型，对角转移矩阵的SSM仍无法解决奇偶校验问题，表明递归层需同时具备输入依赖性和负特征值。实验通过结合S4D和Mamba层的SSM模型验证了这一结论。


<details>
  <summary>Details</summary>
Motivation: 探讨多层SSM中结合输入独立和非负SSM是否能解决状态跟踪问题（如奇偶校验），以弥补现有LRNN模型的不足。

Method: 研究对角转移矩阵的SSM在多层结合输入独立和非负SSM时的表现，并通过实验分析结合S4D和Mamba层的SSM模型。

Result: 研究发现，即使结合输入独立和非负SSM，对角转移矩阵的SSM仍无法解决奇偶校验问题。

Conclusion: 递归层需同时具备输入依赖性和负特征值才能有效解决状态跟踪任务。

Abstract: Recent work has shown that LRNN models such as S4D, Mamba, and DeltaNet lack
state-tracking capability due to either time-invariant transition matrices or
restricted eigenvalue ranges. To address this, input-dependent transition
matrices, particularly those that are complex or non-triangular, have been
proposed to enhance SSM performance on such tasks. While existing theorems
demonstrate that both input-independent and non-negative SSMs are incapable of
solving simple state-tracking tasks, such as parity, regardless of depth, they
do not explore whether combining these two types in a multilayer SSM could
help. We investigate this question for efficient SSMs with diagonal transition
matrices and show that such combinations still fail to solve parity. This
implies that a recurrence layer must both be input-dependent and include
negative eigenvalues. Our experiments support this conclusion by analyzing an
SSM model that combines S4D and Mamba layers.

</details>


### [82] [Efficient Reward Identification In Max Entropy Reinforcement Learning with Sparsity and Rank Priors](https://arxiv.org/abs/2508.07400)
*Mohamad Louai Shehab,Alperen Tercan,Necmiye Ozay*

Main category: cs.LG

TL;DR: 论文研究了从最优策略或最大熵强化学习演示中恢复时变奖励函数的问题，提出了两种奖励先验假设，并转化为稀疏化和秩最小化问题，设计了高效算法。


<details>
  <summary>Details</summary>
Motivation: 奖励函数恢复问题在缺乏额外假设时高度不适定，但实际应用中奖励通常具有稀疏性，因此研究如何利用先验信息高效恢复奖励函数。

Method: 提出两种奖励先验假设：1）奖励多为常数且变化少；2）奖励可表示为少量特征函数的线性组合。将问题转化为稀疏化和秩最小化问题，并设计多项式时间算法。

Result: 通过稀疏化和秩最小化方法，实现了高效的奖励函数恢复算法，并通过示例验证了恢复奖励的准确性和泛化性。

Conclusion: 论文证明了在特定先验假设下，奖励函数恢复问题可通过优化方法高效解决，且恢复结果具有实际应用价值。

Abstract: In this paper, we consider the problem of recovering time-varying reward
functions from either optimal policies or demonstrations coming from a max
entropy reinforcement learning problem. This problem is highly ill-posed
without additional assumptions on the underlying rewards. However, in many
applications, the rewards are indeed parsimonious, and some prior information
is available. We consider two such priors on the rewards: 1) rewards are mostly
constant and they change infrequently, 2) rewards can be represented by a
linear combination of a small number of feature functions. We first show that
the reward identification problem with the former prior can be recast as a
sparsification problem subject to linear constraints. Moreover, we give a
polynomial-time algorithm that solves this sparsification problem exactly.
Then, we show that identifying rewards representable with the minimum number of
features can be recast as a rank minimization problem subject to linear
constraints, for which convex relaxations of rank can be invoked. In both
cases, these observations lead to efficient optimization-based reward
identification algorithms. Several examples are given to demonstrate the
accuracy of the recovered rewards as well as their generalizability.

</details>


### [83] [Lightning Prediction under Uncertainty: DeepLight with Hazy Loss](https://arxiv.org/abs/2508.07428)
*Md Sultanul Arifin,Abu Nowshed Sakib,Yeasir Rayhan,Tanzima Hashem*

Main category: cs.LG

TL;DR: DeepLight是一种新型深度学习架构，用于预测闪电发生，通过多源气象数据和双编码器架构提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 闪电对人身安全和经济发展构成重大威胁，现有预测模型在动态空间上下文捕捉和不确定性处理上存在不足。

Method: 利用雷达反射率、云属性和历史闪电数据，通过双编码器架构和多分支卷积技术动态捕捉空间相关性，并使用Hazy Loss函数处理不确定性。

Result: 实验表明，DeepLight在公平威胁评分（ETS）上比现有方法提高了18%-30%。

Conclusion: DeepLight为闪电预测提供了一种高效且鲁棒的解决方案。

Abstract: Lightning, a common feature of severe meteorological conditions, poses
significant risks, from direct human injuries to substantial economic losses.
These risks are further exacerbated by climate change. Early and accurate
prediction of lightning would enable preventive measures to safeguard people,
protect property, and minimize economic losses. In this paper, we present
DeepLight, a novel deep learning architecture for predicting lightning
occurrences. Existing prediction models face several critical limitations: they
often struggle to capture the dynamic spatial context and inherent uncertainty
of lightning events, underutilize key observational data, such as radar
reflectivity and cloud properties, and rely heavily on Numerical Weather
Prediction (NWP) systems, which are both computationally expensive and highly
sensitive to parameter settings. To overcome these challenges, DeepLight
leverages multi-source meteorological data, including radar reflectivity, cloud
properties, and historical lightning occurrences through a dual-encoder
architecture. By employing multi-branch convolution techniques, it dynamically
captures spatial correlations across varying extents. Furthermore, its novel
Hazy Loss function explicitly addresses the spatio-temporal uncertainty of
lightning by penalizing deviations based on proximity to true events, enabling
the model to better learn patterns amidst randomness. Extensive experiments
show that DeepLight improves the Equitable Threat Score (ETS) by 18%-30% over
state-of-the-art methods, establishing it as a robust solution for lightning
prediction.

</details>


### [84] [Unsupervised operator learning approach for dissipative equations via Onsager principle](https://arxiv.org/abs/2508.07440)
*Zhipeng Chang,Zhenye Wen,Xiaofei Zhao*

Main category: cs.LG

TL;DR: 提出了一种名为DOOL的无监督框架，用于解决耗散方程，无需标记数据，通过直接最小化Onsager变分原理定义的Rayleighian函数进行训练。


<details>
  <summary>Details</summary>
Motivation: 现有算子学习方法依赖高保真模拟数据的监督训练，计算成本高。

Method: 基于Onsager变分原理，DOOL通过最小化Rayleighian函数训练深度算子网络，无需标记数据，并采用时空解耦策略提高效率。

Result: 数值实验验证了DOOL的有效性，与监督方法DeepONet和MIONet相比表现更优。

Conclusion: DOOL是一种高效的无监督方法，适用于耗散方程，并可扩展至不直接遵循Onsager变分原理的二阶波动模型。

Abstract: Existing operator learning methods rely on supervised training with
high-fidelity simulation data, introducing significant computational cost. In
this work, we propose the deep Onsager operator learning (DOOL) method, a novel
unsupervised framework for solving dissipative equations. Rooted in the Onsager
variational principle (OVP), DOOL trains a deep operator network by directly
minimizing the OVP-defined Rayleighian functional, requiring no labeled data,
and then proceeds in time explicitly through conservation/change laws for the
solution. Another key innovation here lies in the spatiotemporal decoupling
strategy: the operator's trunk network processes spatial coordinates
exclusively, thereby enhancing training efficiency, while integrated external
time stepping enables temporal extrapolation. Numerical experiments on typical
dissipative equations validate the effectiveness of the DOOL method, and
systematic comparisons with supervised DeepONet and MIONet demonstrate its
enhanced performance. Extensions are made to cover the second-order wave models
with dissipation that do not directly follow OVP.

</details>


### [85] [Stackelberg Coupling of Online Representation Learning and Reinforcement Learning](https://arxiv.org/abs/2508.07452)
*Fernando Martinez,Tao Li,Yingdong Lu,Juntao Chen*

Main category: cs.LG

TL;DR: SCORER框架通过博弈论动态改进深度强化学习中感知与控制的交互，提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏奖励信号下特征学习效率低的问题，避免复杂辅助目标或解耦设计。

Method: 提出SCORER框架，将感知与控制建模为Stackelberg博弈，采用双时间尺度算法逼近均衡。

Result: 在标准DQN任务中，SCORER提高了样本效率和最终性能。

Conclusion: 通过结构化感知与控制交互，无需复杂设计即可实现性能提升。

Abstract: Integrated, end-to-end learning of representations and policies remains a
cornerstone of deep reinforcement learning (RL). However, to address the
challenge of learning effective features from a sparse reward signal, recent
trends have shifted towards adding complex auxiliary objectives or fully
decoupling the two processes, often at the cost of increased design complexity.
This work proposes an alternative to both decoupling and naive end-to-end
learning, arguing that performance can be significantly improved by structuring
the interaction between distinct perception and control networks with a
principled, game-theoretic dynamic. We formalize this dynamic by introducing
the Stackelberg Coupled Representation and Reinforcement Learning (SCORER)
framework, which models the interaction between perception and control as a
Stackelberg game. The perception network (leader) strategically learns features
to benefit the control network (follower), whose own objective is to minimize
its Bellman error. We approximate the game's equilibrium with a practical
two-timescale algorithm. Applied to standard DQN variants on benchmark tasks,
SCORER improves sample efficiency and final performance. Our results show that
performance gains can be achieved through principled algorithmic design of the
perception-control dynamic, without requiring complex auxiliary objectives or
architectures.

</details>


### [86] [Towards Unveiling Predictive Uncertainty Vulnerabilities in the Context of the Right to Be Forgotten](https://arxiv.org/abs/2508.07458)
*Wei Qian,Chenxu Zhao,Yangyi Li,Wenqian Ye,Mengdi Huai*

Main category: cs.LG

TL;DR: 本文提出了一种针对预测不确定性的恶意遗忘攻击，填补了该领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 随着机器遗忘需求的增加，现有方法未考虑预测不确定性在恶意攻击下的脆弱性。

Method: 设计了新的优化框架，包括黑盒场景实验。

Result: 攻击比传统标签误分类攻击更有效，现有防御措施无效。

Conclusion: 首次揭示了预测不确定性在恶意遗忘攻击中的风险。

Abstract: Currently, various uncertainty quantification methods have been proposed to
provide certainty and probability estimates for deep learning models' label
predictions. Meanwhile, with the growing demand for the right to be forgotten,
machine unlearning has been extensively studied as a means to remove the impact
of requested sensitive data from a pre-trained model without retraining the
model from scratch. However, the vulnerabilities of such generated predictive
uncertainties with regard to dedicated malicious unlearning attacks remain
unexplored. To bridge this gap, for the first time, we propose a new class of
malicious unlearning attacks against predictive uncertainties, where the
adversary aims to cause the desired manipulations of specific predictive
uncertainty results. We also design novel optimization frameworks for our
attacks and conduct extensive experiments, including black-box scenarios.
Notably, our extensive experiments show that our attacks are more effective in
manipulating predictive uncertainties than traditional attacks that focus on
label misclassifications, and existing defenses against conventional attacks
are ineffective against our attacks.

</details>


### [87] [MOTGNN: Interpretable Graph Neural Networks for Multi-Omics Disease Classification](https://arxiv.org/abs/2508.07465)
*Tiantian Yang,Zhiqian Chen*

Main category: cs.LG

TL;DR: MOTGNN是一种新型的多组学数据集成框架，通过XGBoost和GNN实现高效分类，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多组学数据的高维性和复杂性对疾病预测建模提出了挑战，需要一种既能提高准确性又能保持可解释性的方法。

Method: MOTGNN结合XGBoost构建组学特异性图，使用GNN进行层次表示学习，并通过深度前馈网络实现跨组学集成。

Result: 在三个真实疾病数据集上，MOTGNN的准确率、ROC-AUC和F1分数比现有方法高5-10%，且对类别不平衡具有鲁棒性。

Conclusion: MOTGNN在多组学疾病建模中显著提升了预测准确性和可解释性，具有广泛应用潜力。

Abstract: Integrating multi-omics data, such as DNA methylation, mRNA expression, and
microRNA (miRNA) expression, offers a comprehensive view of the biological
mechanisms underlying disease. However, the high dimensionality and complex
interactions among omics layers present major challenges for predictive
modeling. We propose Multi-Omics integration with Tree-generated Graph Neural
Network (MOTGNN), a novel and interpretable framework for binary disease
classification. MOTGNN employs eXtreme Gradient Boosting (XGBoost) to perform
omics-specific supervised graph construction, followed by modality-specific
Graph Neural Networks (GNNs) for hierarchical representation learning, and a
deep feedforward network for cross-omics integration. On three real-world
disease datasets, MOTGNN outperforms state-of-the-art baselines by 5-10% in
accuracy, ROC-AUC, and F1-score, and remains robust to severe class imbalance
(e.g., 87.2% vs. 33.4% F1 on imbalanced data). The model maintains
computational efficiency through sparse graphs (2.1-2.8 edges per node) and
provides built-in interpretability, revealing both top-ranked biomarkers and
the relative contributions of each omics modality. These results highlight
MOTGNN's potential to improve both predictive accuracy and interpretability in
multi-omics disease modeling.

</details>


### [88] [Online Convex Optimization with Heavy Tails: Old Algorithms, New Regrets, and Applications](https://arxiv.org/abs/2508.07473)
*Zijian Liu*

Main category: cs.LG

TL;DR: 该论文研究了在线凸优化（OCO）中梯度估计具有重尾分布时的性能，证明了经典算法（如在线梯度下降）无需修改即可在重尾条件下实现最优遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注梯度方差有限的情况，但对梯度估计具有重尾分布（即仅有限p阶中心矩）时的OCO问题研究较少。

Method: 分析了经典OCO算法（如在线梯度下降）在重尾条件下的性能，未对算法做任何修改。

Result: 在标准有界域假设下，证明了这些算法在重尾条件下仍能实现完全最优的遗憾界，且无需额外操作（如梯度裁剪）。

Conclusion: 结果表明，重尾条件下的OCO问题可通过经典算法有效解决，无需额外操作，并拓展了非光滑非凸优化及乐观算法的应用。

Abstract: In Online Convex Optimization (OCO), when the stochastic gradient has a
finite variance, many algorithms provably work and guarantee a sublinear
regret. However, limited results are known if the gradient estimate has a heavy
tail, i.e., the stochastic gradient only admits a finite $\mathsf{p}$-th
central moment for some $\mathsf{p}\in\left(1,2\right]$. Motivated by it, this
work examines different old algorithms for OCO (e.g., Online Gradient Descent)
in the more challenging heavy-tailed setting. Under the standard bounded domain
assumption, we establish new regrets for these classical methods without any
algorithmic modification. Remarkably, these regret bounds are fully optimal in
all parameters (can be achieved even without knowing $\mathsf{p}$), suggesting
that OCO with heavy tails can be solved effectively without any extra operation
(e.g., gradient clipping). Our new results have several applications. A
particularly interesting one is the first provable convergence result for
nonsmooth nonconvex optimization under heavy-tailed noise without gradient
clipping. Furthermore, we explore broader settings (e.g., smooth OCO) and
extend our ideas to optimistic algorithms to handle different cases
simultaneously.

</details>


### [89] [N-BEATS-MOE: N-BEATS with a Mixture-of-Experts Layer for Heterogeneous Time Series Forecasting](https://arxiv.org/abs/2508.07490)
*Ricardo Matos,Luis Roque,Vitor Cerqueira*

Main category: cs.LG

TL;DR: N-BEATS-MOE是N-BEATS的扩展，通过引入混合专家（MoE）层和动态块加权策略，提升了时间序列预测的适应性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法如N-BEATS在时间序列预测中表现优异，但仍有提升空间，尤其是在适应不同时间序列特性和增强可解释性方面。

Method: N-BEATS-MOE结合了MoE层和动态块加权策略，通过门控网络选择最相关的专家块，优化模型对不同时间序列的适应性。

Result: 在12个基准数据集上的实验表明，N-BEATS-MOE在异质性时间序列上表现尤为突出，优于其他方法。

Conclusion: N-BEATS-MOE通过MoE层和动态加权策略，显著提升了预测性能和可解释性，适用于复杂时间序列任务。

Abstract: Deep learning approaches are increasingly relevant for time series
forecasting tasks. Methods such as N-BEATS, which is built on stacks of
multilayer perceptrons (MLPs) blocks, have achieved state-of-the-art results on
benchmark datasets and competitions. N-BEATS is also more interpretable
relative to other deep learning approaches, as it decomposes forecasts into
different time series components, such as trend and seasonality. In this work,
we present N-BEATS-MOE, an extension of N-BEATS based on a Mixture-of-Experts
(MoE) layer. N-BEATS-MOE employs a dynamic block weighting strategy based on a
gating network which allows the model to better adapt to the characteristics of
each time series. We also hypothesize that the gating mechanism provides
additional interpretability by identifying which expert is most relevant for
each series. We evaluate our method across 12 benchmark datasets against
several approaches, achieving consistent improvements on several datasets,
especially those composed of heterogeneous time series.

</details>


### [90] [Enhancing Privacy in Decentralized Min-Max Optimization: A Differentially Private Approach](https://arxiv.org/abs/2508.07505)
*Yueyang Quan,Chang Wang,Shengjie Zhai,Minghong Fang,Zhuqing Liu*

Main category: cs.LG

TL;DR: 论文提出了一种名为DPMixSGD的隐私保护算法，用于解决非凸分散式最小-最大优化问题，通过差分隐私技术保护数据隐私，同时确保收敛性能。


<details>
  <summary>Details</summary>
Motivation: 分散式最小-最大优化中，模型更新的共享可能导致敏感数据泄露，差分隐私虽能保护隐私，但噪声可能影响收敛性能。

Method: 基于STORM算法，提出DPMixSGD，通过添加噪声保护隐私，并理论证明噪声不影响收敛。

Result: 实验验证了算法的有效性，噪声未显著影响收敛，同时确保了隐私保护。

Conclusion: DPMixSGD在隐私保护和收敛性能之间取得了平衡，适用于非凸分散式最小-最大优化问题。

Abstract: Decentralized min-max optimization allows multi-agent systems to
collaboratively solve global min-max optimization problems by facilitating the
exchange of model updates among neighboring agents, eliminating the need for a
central server. However, sharing model updates in such systems carry a risk of
exposing sensitive data to inference attacks, raising significant privacy
concerns. To mitigate these privacy risks, differential privacy (DP) has become
a widely adopted technique for safeguarding individual data. Despite its
advantages, implementing DP in decentralized min-max optimization poses
challenges, as the added noise can hinder convergence, particularly in
non-convex scenarios with complex agent interactions in min-max optimization
problems. In this work, we propose an algorithm called DPMixSGD (Differential
Private Minmax Hybrid Stochastic Gradient Descent), a novel privacy-preserving
algorithm specifically designed for non-convex decentralized min-max
optimization. Our method builds on the state-of-the-art STORM-based algorithm,
one of the fastest decentralized min-max solutions. We rigorously prove that
the noise added to local gradients does not significantly compromise
convergence performance, and we provide theoretical bounds to ensure privacy
guarantees. To validate our theoretical findings, we conduct extensive
experiments across various tasks and models, demonstrating the effectiveness of
our approach.

</details>


### [91] [FairDRL-ST: Disentangled Representation Learning for Fair Spatio-Temporal Mobility Prediction](https://arxiv.org/abs/2508.07518)
*Sichen Zhao,Wei Shao,Jeffrey Chan,Ziqi Xu,Flora Salim*

Main category: cs.LG

TL;DR: 论文提出了一种基于解耦表示学习的框架FairDRL-ST，用于解决时空预测中的公平性问题，特别关注移动需求预测。


<details>
  <summary>Details</summary>
Motivation: 随着时空神经网络在城市计算中的广泛应用，其预测偏差可能加剧社会经济不平等，因此需要关注公平性。

Method: 通过对抗学习和解耦表示学习，框架分离敏感信息属性，以无监督方式实现公平性。

Result: 在真实城市移动数据集上验证，该框架能缩小公平性差距，同时保持预测性能。

Conclusion: FairDRL-ST在公平性和性能间取得平衡，为AI在公共服务中的伦理部署提供支持。

Abstract: As deep spatio-temporal neural networks are increasingly utilised in urban
computing contexts, the deployment of such methods can have a direct impact on
users of critical urban infrastructure, such as public transport, emergency
services, and traffic management systems. While many spatio-temporal methods
focus on improving accuracy, fairness has recently gained attention due to
growing evidence that biased predictions in spatio-temporal applications can
disproportionately disadvantage certain demographic or geographic groups,
thereby reinforcing existing socioeconomic inequalities and undermining the
ethical deployment of AI in public services. In this paper, we propose a novel
framework, FairDRL-ST, based on disentangled representation learning, to
address fairness concerns in spatio-temporal prediction, with a particular
focus on mobility demand forecasting. By leveraging adversarial learning and
disentangled representation learning, our framework learns to separate
attributes that contain sensitive information. Unlike existing methods that
enforce fairness through supervised learning, which may lead to
overcompensation and degraded performance, our framework achieves fairness in
an unsupervised manner with minimal performance loss. We apply our framework to
real-world urban mobility datasets and demonstrate its ability to close
fairness gaps while delivering competitive predictive performance compared to
state-of-the-art fairness-aware methods.

</details>


### [92] [Physics-Informed Multimodal Bearing Fault Classification under Variable Operating Conditions using Transfer Learning](https://arxiv.org/abs/2508.07536)
*Tasfiq E. Alam,Md Manjurul Ahsan,Shivakumar Raman*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的多模态CNN模型，结合振动和电机电流信号，通过物理特征提取分支和损失函数提升轴承故障分类的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决变工况下轴承故障分类的准确性和可解释性问题，应对领域偏移对模型性能的影响。

Method: 采用多模态CNN架构，融合振动和电流信号，引入物理特征提取分支和物理信息损失函数，评估三种迁移学习策略。

Result: 在Paderborn和KAIST数据集上表现优异，最高准确率达98%，显著优于非物理信息基线。

Conclusion: 结合领域知识与数据驱动学习，实现了鲁棒、可解释且通用的故障诊断框架。

Abstract: Accurate and interpretable bearing fault classification is critical for
ensuring the reliability of rotating machinery, particularly under variable
operating conditions where domain shifts can significantly degrade model
performance. This study proposes a physics-informed multimodal convolutional
neural network (CNN) with a late fusion architecture, integrating vibration and
motor current signals alongside a dedicated physics-based feature extraction
branch. The model incorporates a novel physics-informed loss function that
penalizes physically implausible predictions based on characteristic bearing
fault frequencies - Ball Pass Frequency Outer (BPFO) and Ball Pass Frequency
Inner (BPFI) - derived from bearing geometry and shaft speed. Comprehensive
experiments on the Paderborn University dataset demonstrate that the proposed
physics-informed approach consistently outperforms a non-physics-informed
baseline, achieving higher accuracy, reduced false classifications, and
improved robustness across multiple data splits. To address performance
degradation under unseen operating conditions, three transfer learning (TL)
strategies - Target-Specific Fine-Tuning (TSFT), Layer-Wise Adaptation Strategy
(LAS), and Hybrid Feature Reuse (HFR) - are evaluated. Results show that LAS
yields the best generalization, with additional performance gains when combined
with physics-informed modeling. Validation on the KAIST bearing dataset
confirms the framework's cross-dataset applicability, achieving up to 98
percent accuracy. Statistical hypothesis testing further verifies significant
improvements (p < 0.01) in classification performance. The proposed framework
demonstrates the potential of integrating domain knowledge with data-driven
learning to achieve robust, interpretable, and generalizable fault diagnosis
for real-world industrial applications.

</details>


### [93] [Uncertainty-Driven Reliability: Selective Prediction and Trustworthy Deployment in Modern Machine Learning](https://arxiv.org/abs/2508.07556)
*Stephan Rabanser*

Main category: cs.LG

TL;DR: 该论文研究了如何利用不确定性估计提升机器学习在高风险领域的可靠性和安全性，重点探讨了选择性预测方法。通过利用模型训练轨迹中的不确定性信号，提出了一种轻量级、后验的弃权方法，并在隐私保护条件下验证了其鲁棒性。此外，论文还分析了选择性分类误差来源，并提出了防御对抗性攻击的方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习在高风险领域的应用需要更高的可靠性和安全性，不确定性估计是关键。论文旨在通过选择性预测方法提升模型的信任度，并解决隐私保护与不确定性质量之间的权衡问题。

Method: 1. 利用模型训练轨迹中的不确定性信号，提出一种轻量级的后验弃权方法；2. 在差分隐私条件下验证方法的鲁棒性；3. 分解选择性分类误差来源；4. 设计防御对抗性攻击的方法。

Result: 1. 提出的方法在选择性预测任务中达到最优性能，且适用于差分隐私场景；2. 明确了选择性分类误差的五种来源；3. 设计了针对对抗性攻击的防御机制。

Conclusion: 论文通过改进、评估和保护不确定性估计，推动了可靠机器学习的发展，使模型不仅能做出准确预测，还能在不确定时主动弃权。

Abstract: Machine learning (ML) systems are increasingly deployed in high-stakes
domains where reliability is paramount. This thesis investigates how
uncertainty estimation can enhance the safety and trustworthiness of ML,
focusing on selective prediction -- where models abstain when confidence is
low.
  We first show that a model's training trajectory contains rich uncertainty
signals that can be exploited without altering its architecture or loss. By
ensembling predictions from intermediate checkpoints, we propose a lightweight,
post-hoc abstention method that works across tasks, avoids the cost of deep
ensembles, and achieves state-of-the-art selective prediction performance.
Crucially, this approach is fully compatible with differential privacy (DP),
allowing us to study how privacy noise affects uncertainty quality. We find
that while many methods degrade under DP, our trajectory-based approach remains
robust, and we introduce a framework for isolating the privacy-uncertainty
trade-off. Next, we then develop a finite-sample decomposition of the selective
classification gap -- the deviation from the oracle accuracy-coverage curve --
identifying five interpretable error sources and clarifying which interventions
can close the gap. This explains why calibration alone cannot fix ranking
errors, motivating methods that improve uncertainty ordering. Finally, we show
that uncertainty signals can be adversarially manipulated to hide errors or
deny service while maintaining high accuracy, and we design defenses combining
calibration audits with verifiable inference.
  Together, these contributions advance reliable ML by improving, evaluating,
and safeguarding uncertainty estimation, enabling models that not only make
accurate predictions -- but also know when to say "I do not know".

</details>


### [94] [Towards Theoretical Understanding of Transformer Test-Time Computing: Investigation on In-Context Linear Regression](https://arxiv.org/abs/2508.07571)
*Xingwu Chen,Miao Lu,Beining Wu,Difan Zou*

Main category: cs.LG

TL;DR: 论文通过引入随机性和采样，将实际语言模型推理与理论分析结合，重点研究了上下文线性回归，分析了常见推理技术的效果。


<details>
  <summary>Details</summary>
Motivation: 弥合实际语言模型推理与理论分析之间的差距，探索随机性和采样对模型性能的影响。

Method: 采用噪声注入和二元系数采样模拟语言模型解码，分析上下文线性回归中的推理技术。

Result: 理论和实证分析表明，该方法为理解实际语言模型推理行为提供了新视角。

Conclusion: 研究框架和分析方法为语言模型推理行为的深入理解提供了潜力。

Abstract: Using more test-time computation during language model inference, such as
generating more intermediate thoughts or sampling multiple candidate answers,
has proven effective in significantly improving model performance. This paper
takes an initial step toward bridging the gap between practical language model
inference and theoretical transformer analysis by incorporating randomness and
sampling. We focus on in-context linear regression with continuous/binary
coefficients, where our framework simulates language model decoding through
noise injection and binary coefficient sampling. Through this framework, we
provide detailed analyses of widely adopted inference techniques. Supported by
empirical results, our theoretical framework and analysis demonstrate the
potential for offering new insights into understanding inference behaviors in
real-world language models.

</details>


### [95] [When and how can inexact generative models still sample from the data manifold?](https://arxiv.org/abs/2508.07581)
*Nisha Chandramoorthy,Adriaan de Clercq*

Main category: cs.LG

TL;DR: 论文研究了生成模型中学习误差导致样本沿数据分布支撑移动而非偏离的现象，揭示了其动力学机制，并提出了支持鲁棒性的条件。


<details>
  <summary>Details</summary>
Motivation: 探讨生成模型中学习误差对样本分布支撑的鲁棒性现象，理解其动力学机制。

Method: 采用动力学系统方法分析生成过程，通过扰动分析揭示概率流的变化，并研究Lyapunov向量与数据流形切空间的关联。

Result: 发现学习误差仅导致预测密度在数据流形上变化，证明了生成过程动力学条件对支撑鲁棒性的重要性。

Conclusion: 研究为生成模型提供了理论保证，适用于多种模型和目标分布，扩展了现有理论框架。

Abstract: A curious phenomenon observed in some dynamical generative models is the
following: despite learning errors in the score function or the drift vector
field, the generated samples appear to shift \emph{along} the support of the
data distribution but not \emph{away} from it. In this work, we investigate
this phenomenon of \emph{robustness of the support} by taking a dynamical
systems approach on the generating stochastic/deterministic process. Our
perturbation analysis of the probability flow reveals that infinitesimal
learning errors cause the predicted density to be different from the target
density only on the data manifold for a wide class of generative models.
Further, what is the dynamical mechanism that leads to the robustness of the
support? We show that the alignment of the top Lyapunov vectors (most sensitive
infinitesimal perturbation directions) with the tangent spaces along the
boundary of the data manifold leads to robustness and prove a sufficient
condition on the dynamics of the generating process to achieve this alignment.
Moreover, the alignment condition is efficient to compute and, in practice, for
robust generative models, automatically leads to accurate estimates of the
tangent bundle of the data manifold. Using a finite-time linear perturbation
analysis on samples paths as well as probability flows, our work complements
and extends existing works on obtaining theoretical guarantees for generative
models from a stochastic analysis, statistical learning and uncertainty
quantification points of view. Our results apply across different dynamical
generative models, such as conditional flow-matching and score-based generative
models, and for different target distributions that may or may not satisfy the
manifold hypothesis.

</details>


### [96] [Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization](https://arxiv.org/abs/2508.07629)
*Zhenpeng Su,Leiyu Pan,Xue Bai,Dening Liu,Guanting Dong,Jiaming Huang,Wenping Hu,Guorui Zhou*

Main category: cs.LG

TL;DR: Klear-Reasoner是一个具有长推理能力的模型，通过详细的工作流程和优化方法在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前社区中高性能推理模型的复现存在问题，主要由于训练细节披露不完整。本文旨在提供完整的训练流程分析和优化方法。

Method: 包括数据准备、长链思维监督微调（long CoT SFT）和强化学习（RL），并提出Gradient-Preserving clipping Policy Optimization（GPPO）解决RL中的关键问题。

Result: Klear-Reasoner在数学和编程任务中表现卓越，AIME 2024得分90.5%，AIME 2025得分83.2%，LiveCodeBench V5得分66.0%，V6得分58.1%。

Conclusion: 高质量数据和小样本训练更有效，GPPO提升了模型的探索能力和学习效率。

Abstract: We present Klear-Reasoner, a model with long reasoning capabilities that
demonstrates careful deliberation during problem solving, achieving outstanding
performance across multiple benchmarks. Although there are already many
excellent works related to inference models in the current community, there are
still many problems with reproducing high-performance inference models due to
incomplete disclosure of training details. This report provides an in-depth
analysis of the reasoning model, covering the entire post-training workflow
from data preparation and long Chain-of-Thought supervised fine-tuning (long
CoT SFT) to reinforcement learning (RL), along with detailed ablation studies
for each experimental component. For SFT data, our experiments show that a
small number of high-quality data sources are more effective than a large
number of diverse data sources, and that difficult samples can achieve better
results without accuracy filtering. In addition, we investigate two key issues
with current clipping mechanisms in RL: Clipping suppresses critical
exploration signals and ignores suboptimal trajectories. To address these
challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO)
that gently backpropagates gradients from clipped tokens. GPPO not only
enhances the model's exploration capacity but also improves its efficiency in
learning from negative samples. Klear-Reasoner exhibits exceptional reasoning
abilities in mathematics and programming, scoring 90.5\% on AIME 2024, 83.2\%
on AIME 2025, 66.0\% on LiveCodeBench V5 and 58.1\% on LiveCodeBench V6.

</details>


### [97] [Efficient Approximate Posterior Sampling with Annealed Langevin Monte Carlo](https://arxiv.org/abs/2508.07631)
*Advait Parulekar,Litu Rout,Karthikeyan Shanmugam,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: 论文研究了基于分数的生成模型中后验采样的问题，提出了一种在多项式时间内近似采样的方法，确保样本与测量和先验一致。


<details>
  <summary>Details</summary>
Motivation: 尽管后验采样在KL散度下通常是难解的，但实际应用中许多算法表现良好。本文旨在探索一种更通用的“倾斜”问题，即在最小假设下实现近似采样。

Method: 通过将后验采样视为“倾斜”问题，提出了一种方法，能够从噪声先验的KL散度接近分布中采样，同时与真实后验在Fisher散度上接近。

Result: 证明了在多项式时间内可以实现近似后验采样，确保样本既符合测量又符合先验。

Conclusion: 这是首次在多项式时间内实现近似后验采样的正式结果，为相关任务提供了理论支持。

Abstract: We study the problem of posterior sampling in the context of score based
generative models. We have a trained score network for a prior $p(x)$, a
measurement model $p(y|x)$, and are tasked with sampling from the posterior
$p(x|y)$. Prior work has shown this to be intractable in KL (in the worst case)
under well-accepted computational hardness assumptions. Despite this, popular
algorithms for tasks such as image super-resolution, stylization, and
reconstruction enjoy empirical success. Rather than establishing distributional
assumptions or restricted settings under which exact posterior sampling is
tractable, we view this as a more general "tilting" problem of biasing a
distribution towards a measurement. Under minimal assumptions, we show that one
can tractably sample from a distribution that is simultaneously close to the
posterior of a noised prior in KL divergence and the true posterior in Fisher
divergence. Intuitively, this combination ensures that the resulting sample is
consistent with both the measurement and the prior. To the best of our
knowledge these are the first formal results for (approximate) posterior
sampling in polynomial time.

</details>


### [98] [Attribution Explanations for Deep Neural Networks: A Theoretical Perspective](https://arxiv.org/abs/2508.07636)
*Huiqi Deng,Hongbin Pei,Quanshi Zhang,Mengnan Du*

Main category: cs.LG

TL;DR: 论文探讨了深度神经网络（DNNs）的归因解释方法，分析了其忠实性问题及三大核心挑战，并总结了理论进展的三个关键方向。


<details>
  <summary>Details</summary>
Motivation: 归因解释方法在解释DNNs时存在忠实性问题，影响其可靠性和实用性。论文旨在解决这一问题，并推动理论发展。

Method: 通过理论统一、理论基础和理论评估三个方向，系统分析和比较现有归因方法。

Result: 总结了理论进展，揭示了归因方法的共性与差异，并提出了未来研究方向。

Conclusion: 论文为归因方法的理论理解、方法选择和新方法设计提供了指导，并指出了未来研究的开放性问题。

Abstract: Attribution explanation is a typical approach for explaining deep neural
networks (DNNs), inferring an importance or contribution score for each input
variable to the final output. In recent years, numerous attribution methods
have been developed to explain DNNs. However, a persistent concern remains
unresolved, i.e., whether and which attribution methods faithfully reflect the
actual contribution of input variables to the decision-making process. The
faithfulness issue undermines the reliability and practical utility of
attribution explanations. We argue that these concerns stem from three core
challenges. First, difficulties arise in comparing attribution methods due to
their unstructured heterogeneity, differences in heuristics, formulations, and
implementations that lack a unified organization. Second, most methods lack
solid theoretical underpinnings, with their rationales remaining absent,
ambiguous, or unverified. Third, empirically evaluating faithfulness is
challenging without ground truth. Recent theoretical advances provide a
promising way to tackle these challenges, attracting increasing attention. We
summarize these developments, with emphasis on three key directions: (i)
Theoretical unification, which uncovers commonalities and differences among
methods, enabling systematic comparisons; (ii) Theoretical rationale,
clarifying the foundations of existing methods; (iii) Theoretical evaluation,
rigorously proving whether methods satisfy faithfulness principles. Beyond a
comprehensive review, we provide insights into how these studies help deepen
theoretical understanding, inform method selection, and inspire new attribution
methods. We conclude with a discussion of promising open problems for further
work.

</details>


### [99] [Extracting Complex Topology from Multivariate Functional Approximation: Contours, Jacobi Sets, and Ridge-Valley Graphs](https://arxiv.org/abs/2508.07637)
*Guanqun Ma,David Lenz,Hanqi Guo,Tom Peterka,Bei Wang*

Main category: cs.LG

TL;DR: 提出了一种直接从连续隐式模型（MFA）中提取复杂拓扑特征（如轮廓、雅可比集和脊谷图）的框架，无需离散化。


<details>
  <summary>Details</summary>
Motivation: 连续隐式模型（如MFA）为科学数据的存储、传输和分析提供了新视角，但缺乏直接提取拓扑特征的方法。

Method: 基于MFA模型，直接提取拓扑特征，支持函数值和高阶导数查询。

Result: 实现了从连续隐式模型中直接提取复杂拓扑特征，适用于任何支持相关查询的连续隐式模型。

Conclusion: 为连续隐式模型的拓扑数据分析和可视化奠定了基础。

Abstract: Implicit continuous models, such as functional models and implicit neural
networks, are an increasingly popular method for replacing discrete data
representations with continuous, high-order, and differentiable surrogates.
These models offer new perspectives on the storage, transfer, and analysis of
scientific data. In this paper, we introduce the first framework to directly
extract complex topological features -- contours, Jacobi sets, and ridge-valley
graphs -- from a type of continuous implicit model known as multivariate
functional approximation (MFA). MFA replaces discrete data with continuous
piecewise smooth functions. Given an MFA model as the input, our approach
enables direct extraction of complex topological features from the model,
without reverting to a discrete representation of the model. Our work is easily
generalizable to any continuous implicit model that supports the queries of
function values and high-order derivatives. Our work establishes the building
blocks for performing topological data analysis and visualization on implicit
continuous models.

</details>


### [100] [Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals](https://arxiv.org/abs/2508.07638)
*Jia Zhang,Yao Liu,Chen-Xi Zhang,Yi Liu,Yi-Xuan Jin,Lan-Zhe Guo,Yu-Feng Li*

Main category: cs.LG

TL;DR: 论文提出了一种基于数据选择的方法DMPO，通过量化偏好冲突（PD）选择高共识数据，显著提升了LLM对齐效果和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如DPO）在细粒度偏好数据中难以处理噪声和冲突，需要更有效的数据选择策略。

Method: 提出DMPO目标，利用PD项量化偏好冲突，并基于此设计数据选择原则，选择高共识数据用于训练。

Result: 在UltraFeedback数据集上，方法相对标准偏好和聚合偏好信号分别提升10%以上，同时提高训练效率。

Conclusion: 通过数据选择策略，DMPO有效解决了细粒度偏好信号中的冲突问题，为LLM对齐提供了新思路。

Abstract: Aligning Large Language Models (LLMs) with diverse human values requires
moving beyond a single holistic "better-than" preference criterion. While
collecting fine-grained, aspect-specific preference data is more reliable and
scalable, existing methods like Direct Preference Optimization (DPO) struggle
with the severe noise and conflicts inherent in such aggregated datasets. In
this paper, we tackle this challenge from a data-centric perspective. We first
derive the Direct Multi-Preference Optimization (DMPO) objective, and uncover a
key Preference Divergence (PD) term that quantifies inter-aspect preference
conflicts. Instead of using this term for direct optimization, we leverage it
to formulate a novel, theoretically-grounded data selection principle. Our
principle advocates for selecting a subset of high-consensus data-identified by
the most negative PD values-for efficient DPO training. We prove the optimality
of this strategy by analyzing the loss bounds of the DMPO objective in the
selection problem. To operationalize our approach, we introduce practical
methods of PD term estimation and length bias mitigation, thereby proposing our
PD selection method. Evaluation on the UltraFeedback dataset with three varying
conflict levels shows that our simple yet effective strategy achieves over 10%
relative improvement against both the standard holistic preference and a
stronger oracle using aggregated preference signals, all while boosting
training efficiency and obviating the need for intractable holistic preference
annotating, unlocking the potential of robust LLM alignment via fine-grained
preference signals.

</details>


### [101] [Multi-Turn Jailbreaks Are Simpler Than They Seem](https://arxiv.org/abs/2508.07646)
*Xiaoxue Yang,Jaeha Lee,Anna-Katharina Dick,Jasper Timm,Fei Xie,Diogo Cruz*

Main category: cs.LG

TL;DR: 多轮越狱攻击在大型语言模型（LLMs）中的成功率超过70%，研究发现其复杂性与单轮攻击重复采样相当，且攻击成功率在相似模型间相关。


<details>
  <summary>Details</summary>
Motivation: 研究多轮越狱攻击的漏洞，挑战其复杂性认知，并为AI安全评估和抗越狱系统设计提供依据。

Method: 使用StrongREJECT基准对GPT-4、Claude和Gemini等先进模型进行多轮越狱攻击的实证分析。

Result: 多轮攻击成功率与单轮攻击重复采样相当，攻击成功率在相似模型间相关，推理能力越强的模型越易受攻击。

Conclusion: 研究揭示了多轮越狱攻击的简化本质，为AI安全评估和系统设计提供了重要启示。

Abstract: While defenses against single-turn jailbreak attacks on Large Language Models
(LLMs) have improved significantly, multi-turn jailbreaks remain a persistent
vulnerability, often achieving success rates exceeding 70% against models
optimized for single-turn protection. This work presents an empirical analysis
of automated multi-turn jailbreak attacks across state-of-the-art models
including GPT-4, Claude, and Gemini variants, using the StrongREJECT benchmark.
Our findings challenge the perceived sophistication of multi-turn attacks: when
accounting for the attacker's ability to learn from how models refuse harmful
requests, multi-turn jailbreaking approaches are approximately equivalent to
simply resampling single-turn attacks multiple times. Moreover, attack success
is correlated among similar models, making it easier to jailbreak newly
released ones. Additionally, for reasoning models, we find surprisingly that
higher reasoning effort often leads to higher attack success rates. Our results
have important implications for AI safety evaluation and the design of
jailbreak-resistant systems. We release the source code at
https://github.com/diogo-cruz/multi_turn_simpler

</details>


### [102] [Discovering Spatial Correlations between Earth Observations in Global Atmospheric State Estimation by using Adaptive Graph Structure Learning](https://arxiv.org/abs/2508.07659)
*Hyeon-Ju Jeon,Jeon-Ho Kang,In-Hyuk Kwon,O-Joun Lee*

Main category: cs.LG

TL;DR: 研究旨在通过时空图神经网络（STGNN）改进全球大气状态估计的预测准确性，解决传统数值天气预报（NWP）系统中动态空间相关性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统NWP系统在固定网格点上预测大气状态，但地球观测数据的位置不固定，导致动态空间相关性复杂。

Method: 采用带结构学习的STGNN，通过自适应节点度数和空间距离调节边采样，避免信息丢失和过平滑问题。

Result: 在东亚真实数据上验证，该方法在高变异性区域优于现有STGNN模型。

Conclusion: 提出的方法有效解决了动态空间相关性挑战，提升了预测性能。

Abstract: This study aims to discover spatial correlations between Earth observations
and atmospheric states to improve the forecasting accuracy of global
atmospheric state estimation, which are usually conducted using conventional
numerical weather prediction (NWP) systems and is the beginning of weather
forecasting. NWP systems predict future atmospheric states at fixed locations,
which are called NWP grid points, by analyzing previous atmospheric states and
newly acquired Earth observations without fixed locations. Thus, surrounding
meteorological context and the changing locations of the observations make
spatial correlations between atmospheric states and observations over time. To
handle complicated spatial correlations, which change dynamically, we employ
spatiotemporal graph neural networks (STGNNs) with structure learning. However,
structure learning has an inherent limitation that this can cause structural
information loss and over-smoothing problem by generating excessive edges. To
solve this problem, we regulate edge sampling by adaptively determining node
degrees and considering the spatial distances between NWP grid points and
observations. We validated the effectiveness of the proposed method by using
real-world atmospheric state and observation data from East Asia. Even in areas
with high atmospheric variability, the proposed method outperformed existing
STGNN models with and without structure learning.

</details>


### [103] [GLiClass: Generalist Lightweight Model for Sequence Classification Tasks](https://arxiv.org/abs/2508.07662)
*Ihor Stepanov,Mykhailo Shtopko,Dmytro Vodianytskyi,Oleksandr Lukashov,Alexander Yavorskyi,Mykyta Yaroshenko*

Main category: cs.LG

TL;DR: GLiClass是一种新方法，通过改进GLiNER架构用于序列分类任务，兼顾高效性和灵活性，适用于零样本和小样本学习场景。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统需处理大量数据，早期分类错误会传播到下游任务，同时分类需求可能动态变化，需要具备零样本能力的模型。现有方法（如生成式LLM、交叉编码器和嵌入方法）各有不足。

Method: 提出GLiClass方法，改进GLiNER架构用于序列分类，并结合近端策略优化（PPO）进行多标签文本分类训练。

Result: GLiClass在准确性和效率上与嵌入方法相当，同时保持零样本和小样本学习的灵活性。

Conclusion: GLiClass为解决分类任务中的效率、灵活性和动态需求问题提供了有效方案。

Abstract: Classification is one of the most widespread tasks in AI applications,
serving often as the first step in filtering, sorting, and categorizing data.
Since modern AI systems must handle large volumes of input data and early
pipeline stages can propagate errors downstream, achieving high efficiency and
accuracy is critical. Moreover, classification requirements can change
dynamically based on user needs, necessitating models with strong zero-shot
capabilities. While generative LLMs have become mainstream for zero-shot
classification due to their versatility, they suffer from inconsistent
instruction following and computational inefficiency. Cross-encoders, commonly
used as rerankers in RAG pipelines, face a different bottleneck: they must
process text-label pairs sequentially, significantly reducing efficiency with
large label sets. Embedding-based approaches offer good efficiency but struggle
with complex scenarios involving logical and semantic constraints. We propose
GLiClass, a novel method that adapts the GLiNER architecture for sequence
classification tasks. Our approach achieves strong accuracy and efficiency
comparable to embedding-based methods, while maintaining the flexibility needed
for zero-shot and few-shot learning scenarios. Additionally, we adapted
proximal policy optimization (PPO) for multi-label text classification,
enabling training classifiers in data-sparse conditions or from human feedback.

</details>


### [104] [AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting](https://arxiv.org/abs/2508.07668)
*Hyobin Park,Jinwook Jung,Minseok Seo,Hyunsoo Choi,Deukjae Cho,Sekil Park,Dong-Geol Choi*

Main category: cs.LG

TL;DR: 提出AIS-LLM框架，结合时间序列AIS数据与大型语言模型，实现船舶轨迹预测、异常检测和碰撞风险评估的多任务集成。


<details>
  <summary>Details</summary>
Motivation: 现有方法多独立处理海事任务，难以全面考虑复杂海事情况，需一种集成解决方案。

Method: AIS-LLM框架包含时间序列编码器、LLM提示编码器、跨模态对齐模块和多任务解码器，实现端到端多任务处理。

Result: 实验表明AIS-LLM在各项任务中优于现有方法，并能生成综合海事情况简报。

Conclusion: AIS-LLM为智能高效的海事交通管理提供了潜力。

Abstract: With the increase in maritime traffic and the mandatory implementation of the
Automatic Identification System (AIS), the importance and diversity of maritime
traffic analysis tasks based on AIS data, such as vessel trajectory prediction,
anomaly detection, and collision risk assessment, is rapidly growing. However,
existing approaches tend to address these tasks individually, making it
difficult to holistically consider complex maritime situations. To address this
limitation, we propose a novel framework, AIS-LLM, which integrates time-series
AIS data with a large language model (LLM). AIS-LLM consists of a Time-Series
Encoder for processing AIS sequences, an LLM-based Prompt Encoder, a
Cross-Modality Alignment Module for semantic alignment between time-series data
and textual prompts, and an LLM-based Multi-Task Decoder. This architecture
enables the simultaneous execution of three key tasks: trajectory prediction,
anomaly detection, and risk assessment of vessel collisions within a single
end-to-end system. Experimental results demonstrate that AIS-LLM outperforms
existing methods across individual tasks, validating its effectiveness.
Furthermore, by integratively analyzing task outputs to generate situation
summaries and briefings, AIS-LLM presents the potential for more intelligent
and efficient maritime traffic management.

</details>


### [105] [Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation](https://arxiv.org/abs/2508.07675)
*Xutong Liu,Baran Atalar,Xiangxiang Dai,Jinhang Zuo,Siwei Wang,John C. S. Lui,Wei Chen,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 论文提出了一种基于学习的原则性框架，用于解决语义缓存中未知查询和成本分布下的缓存淘汰问题，并开发了高效的算法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的高推理成本带来了可扩展性和可持续性挑战，传统缓存方法无法处理语义相似性，现有语义缓存方法缺乏理论基础且无法适应不确定性。

Method: 提出了一个基于学习的框架，包括离线优化和在线学习两种问题形式，并开发了具有理论保证的高效算法。

Result: 在合成数据集上的实验表明，所提算法的性能优于或与基线方法相当。

Conclusion: 该框架为语义缓存提供了一种理论支持且适应性强的方法，解决了现有方法的局限性。

Abstract: Large Language Models (LLMs) are revolutionizing how users interact with
information systems, yet their high inference cost poses serious scalability
and sustainability challenges. Caching inference responses, allowing them to be
retrieved without another forward pass through the LLM, has emerged as one
possible solution. Traditional exact-match caching, however, overlooks the
semantic similarity between queries, leading to unnecessary recomputation.
Semantic caching addresses this by retrieving responses based on semantic
similarity, but introduces a fundamentally different cache eviction problem:
one must account for mismatch costs between incoming queries and cached
responses. Moreover, key system parameters, such as query arrival probabilities
and serving costs, are often unknown and must be learned over time. Existing
semantic caching methods are largely ad-hoc, lacking theoretical foundations
and unable to adapt to real-world uncertainty. In this paper, we present a
principled, learning-based framework for semantic cache eviction under unknown
query and cost distributions. We formulate both offline optimization and online
learning variants of the problem, and develop provably efficient algorithms
with state-of-the-art guarantees. We also evaluate our framework on a synthetic
dataset, showing that our proposed algorithms perform matching or superior
performance compared with baselines.

</details>


### [106] [MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation](https://arxiv.org/abs/2508.07681)
*Yooseok Lim,ByoungJun Jeon,Seong-A Park,Jisoo Lee,Sae Won Choi,Chang Wook Jeong,Ho-Geol Ryu,Hongyeol Lee,Hyun-Lim Yang*

Main category: cs.LG

TL;DR: MORE-CLEAR框架利用多模态离线强化学习和大型语言模型（LLM）提升脓毒症管理的状态表示和治疗效果。


<details>
  <summary>Details</summary>
Motivation: 脓毒症早期检测和管理至关重要，现有方法依赖结构化数据且缺乏对患者状态的全面理解。

Method: 结合预训练LLM提取临床笔记的语义信息，通过门控融合和跨模态注意力动态整合多模态数据。

Result: 在公开和私有数据集上验证，MORE-CLEAR显著提高生存率和策略性能。

Conclusion: 该框架首次将LLM用于多模态离线强化学习，有望通过更全面的患者状态理解优化脓毒症治疗。

Abstract: Sepsis, a life-threatening inflammatory response to infection, causes organ
dysfunction, making early detection and optimal management critical. Previous
reinforcement learning (RL) approaches to sepsis management rely primarily on
structured data, such as lab results or vital signs, and on a dearth of a
comprehensive understanding of the patient's condition. In this work, we
propose a Multimodal Offline REinforcement learning for Clinical notes
Leveraged Enhanced stAte Representation (MORE-CLEAR) framework for sepsis
control in intensive care units. MORE-CLEAR employs pre-trained large-scale
language models (LLMs) to facilitate the extraction of rich semantic
representations from clinical notes, preserving clinical context and improving
patient state representation. Gated fusion and cross-modal attention allow
dynamic weight adjustment in the context of time and the effective integration
of multimodal data. Extensive cross-validation using two public (MIMIC-III and
MIMIC-IV) and one private dataset demonstrates that MORE-CLEAR significantly
improves estimated survival rate and policy performance compared to
single-modal RL approaches. To our knowledge, this is the first to leverage LLM
capabilities within a multimodal offline RL for better state representation in
medical applications. This approach can potentially expedite the treatment and
management of sepsis by enabling reinforcement learning models to propose
enhanced actions based on a more comprehensive understanding of patient
conditions.

</details>


### [107] [Semantic-Enhanced Time-Series Forecasting via Large Language Models](https://arxiv.org/abs/2508.07697)
*Hao Liu,Chun Yang,Zhang xiaoxing,Xiaobin Zhu*

Main category: cs.LG

TL;DR: 提出了一种语义增强的大型语言模型（SE-LLM），通过探索时间序列的周期性和异常特征，增强语义表示，并改进LLM在时间序列分析中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究在时间序列预测中仅关注令牌级模态对齐，未能弥合语言知识结构与时间序列数据模式之间的模态差距，限制了语义表示能力。

Method: 提出SE-LLM，将时间序列的周期性和异常特征嵌入语义空间以增强令牌嵌入；同时设计插件模块，在自注意力中建模长短期依赖关系。

Result: 实验表明SE-LLM在性能上优于现有方法，同时通过冻结LLM和降低序列维度减少了计算消耗。

Conclusion: SE-LLM有效提升了LLM在时间序列分析中的表现，解决了现有方法的局限性。

Abstract: Time series forecasting plays a significant role in finance, energy,
meteorology, and IoT applications. Recent studies have leveraged the
generalization capabilities of large language models (LLMs) to adapt to time
series forecasting, achieving promising performance. However, existing studies
focus on token-level modal alignment, instead of bridging the intrinsic
modality gap between linguistic knowledge structures and time series data
patterns, greatly limiting the semantic representation. To address this issue,
we propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent
periodicity and anomalous characteristics of time series to embed into the
semantic space to enhance the token embedding. This process enhances the
interpretability of tokens for LLMs, thereby activating the potential of LLMs
for temporal sequence analysis. Moreover, existing Transformer-based LLMs excel
at capturing long-range dependencies but are weak at modeling short-term
anomalies in time-series data. Hence, we propose a plugin module embedded
within self-attention that models long-term and short-term dependencies to
effectively adapt LLMs to time-series analysis. Our approach freezes the LLM
and reduces the sequence dimensionality of tokens, greatly reducing
computational consumption. Experiments demonstrate the superiority performance
of our SE-LLM against the state-of-the-art (SOTA) methods.

</details>


### [108] [Energy Consumption in Parallel Neural Network Training](https://arxiv.org/abs/2508.07706)
*Philipp Huber,David Li,Juan Pedro Gutiérrez Hermosillo Muriedas,Deifilia Kieckhefen,Markus Götz,Achim Streit,Charlotte Debus*

Main category: cs.LG

TL;DR: 论文研究了数据并行训练中GPU数量、全局批次大小和本地批次大小对预测性能、训练时间和能耗的影响，发现能耗与GPU小时数近似线性相关，但不同模型和硬件间差异显著。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络训练对计算资源需求的增加，能耗问题日益突出，而并行化的能耗影响常被忽视。

Method: 通过数据并行训练ResNet50和FourCastNet模型，分析GPU数量、批次大小等参数对性能、时间和能耗的影响。

Result: 能耗与GPU小时数近似线性相关，但不同模型和硬件间的能耗差异显著，且受样本数量和梯度更新频率影响。

Conclusion: 研究揭示了神经网络训练扩展与能耗的复杂关系，为可持续AI研究提供了参考。

Abstract: The increasing demand for computational resources of training neural networks
leads to a concerning growth in energy consumption. While parallelization has
enabled upscaling model and dataset sizes and accelerated training, its impact
on energy consumption is often overlooked. To close this research gap, we
conducted scaling experiments for data-parallel training of two models,
ResNet50 and FourCastNet, and evaluated the impact of parallelization
parameters, i.e., GPU count, global batch size, and local batch size, on
predictive performance, training time, and energy consumption. We show that
energy consumption scales approximately linearly with the consumed resources,
i.e., GPU hours; however, the respective scaling factor differs substantially
between distinct model trainings and hardware, and is systematically influenced
by the number of samples and gradient updates per GPU hour. Our results shed
light on the complex interplay of scaling up neural network training and can
inform future developments towards more sustainable AI research.

</details>


### [109] [Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer](https://arxiv.org/abs/2508.07710)
*Jingya Wang,Xin Deng,Wenjie Wei,Dehao Zhang,Shuai Wang,Qian Sun,Jieyuan Zhang,Hanwen Liu,Ning Xie,Malu Zhang*

Main category: cs.LG

TL;DR: 提出了一种无需训练的高性能ANN-to-SNN转换框架，用于Transformer架构，通过MBE神经元有效逼近非线性操作，实现高效部署。


<details>
  <summary>Details</summary>
Motivation: 现有ANN-to-SNN转换方法在Transformer架构中处理非线性操作时存在不足，且需额外微调，限制了效率。

Method: 引入多基指数衰减（MBE）神经元，采用指数衰减策略和多基编码方法，无需修改预训练ANN权重。

Result: 在多种任务和Transformer架构上实现近乎无损的转换精度，且延迟显著降低。

Conclusion: 为Spiking Transformer的高效和可扩展部署提供了可行方案。

Abstract: Leveraging the event-driven paradigm, Spiking Neural Networks (SNNs) offer a
promising approach for constructing energy-efficient Transformer architectures.
Compared to directly trained Spiking Transformers, ANN-to-SNN conversion
methods bypass the high training costs. However, existing methods still suffer
from notable limitations, failing to effectively handle nonlinear operations in
Transformer architectures and requiring additional fine-tuning processes for
pre-trained ANNs. To address these issues, we propose a high-performance and
training-free ANN-to-SNN conversion framework tailored for Transformer
architectures. Specifically, we introduce a Multi-basis Exponential Decay (MBE)
neuron, which employs an exponential decay strategy and multi-basis encoding
method to efficiently approximate various nonlinear operations. It removes the
requirement for weight modifications in pre-trained ANNs. Extensive experiments
across diverse tasks (CV, NLU, NLG) and mainstream Transformer architectures
(ViT, RoBERTa, GPT-2) demonstrate that our method achieves near-lossless
conversion accuracy with significantly lower latency. This provides a promising
pathway for the efficient and scalable deployment of Spiking Transformers in
real-world applications.

</details>


### [110] [Detecting Mislabeled and Corrupted Data via Pointwise Mutual Information](https://arxiv.org/abs/2508.07713)
*Jinghan Yang,Jiayu Weng*

Main category: cs.LG

TL;DR: 提出了一种基于互信息的数据选择框架，用于处理混合噪声场景，通过量化输入与标签的统计依赖关系筛选低质量样本。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络会记忆损坏的标签，而现实数据集常受标签噪声和输入噪声影响，数据质量对模型性能至关重要。

Method: 计算每个样本对整体互信息的点贡献，贡献较低的样本可能为噪声或错误标签。

Result: 在MNIST数据集上验证，该方法能有效过滤低质量样本，标签噪声下训练高互信息样本可使分类准确率提升15%。

Conclusion: 该方法对良性输入修改具有鲁棒性，能保留语义有效数据并过滤真正损坏的样本。

Abstract: Deep neural networks can memorize corrupted labels, making data quality
critical for model performance, yet real-world datasets are frequently
compromised by both label noise and input noise. This paper proposes a mutual
information-based framework for data selection under hybrid noise scenarios
that quantifies statistical dependencies between inputs and labels. We compute
each sample's pointwise contribution to the overall mutual information and find
that lower contributions indicate noisy or mislabeled instances. Empirical
validation on MNIST with different synthetic noise settings demonstrates that
the method effectively filters low-quality samples. Under label corruption,
training on high-MI samples improves classification accuracy by up to 15\%
compared to random sampling. Furthermore, the method exhibits robustness to
benign input modifications, preserving semantically valid data while filtering
truly corrupted samples.

</details>


### [111] [Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning](https://arxiv.org/abs/2508.07738)
*Jialu Zhou,Dianxi Shi,Shaowu Yang,Xinyu Wei,Mingyue Yang,Leqian Li,Mengzhu Wang,Chunping Qiu*

Main category: cs.LG

TL;DR: 论文提出了一种名为TRGE的方法，通过动态扩展预训练模型并引入两级路由机制，解决了多领域持续学习中的灾难性遗忘和前瞻性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 多领域持续学习面临任务类别和分布变化的双重异质性挑战，现有方法难以同时解决灾难性遗忘和前瞻性遗忘。

Method: TRGE方法动态扩展CLIP模型，为每个任务分配专家组，并通过两级路由机制（组内路由和基于任务标识符的组间路由）优化任务协作。同时利用MLLM生成任务描述以识别任务标识符。

Result: 实验表明，TRGE在多种设置下优于其他先进方法，且训练参数更少。

Conclusion: TRGE通过动态扩展和两级路由机制，有效解决了多领域持续学习中的遗忘问题，提升了模型性能。

Abstract: Multi-Domain Continual Learning (MDCL) acquires knowledge from sequential
tasks with shifting class sets and distribution. Despite the
Parameter-Efficient Fine-Tuning (PEFT) methods can adapt for this dual
heterogeneity, they still suffer from catastrophic forgetting and forward
forgetting. To address these challenges, we propose a Two-Level Routing Grouped
Mixture-of-Experts (TRGE) method. Firstly, TRGE dynamically expands the
pre-trained CLIP model, assigning specific expert group for each task to
mitigate catastrophic forgetting. With the number of experts continually grows
in this process, TRGE maintains the static experts count within the group and
introduces the intra-group router to alleviate routing overfitting caused by
the increasing routing complexity. Meanwhile, we design an inter-group routing
policy based on task identifiers and task prototype distance, which dynamically
selects relevant expert groups and combines their outputs to enhance inter-task
collaboration. Secondly, to get the correct task identifiers, we leverage
Multimodal Large Language Models (MLLMs) which own powerful multimodal
comprehension capabilities to generate semantic task descriptions and recognize
the correct task identifier. Finally, to mitigate forward forgetting, we
dynamically fuse outputs for unseen samples from the frozen CLIP model and TRGE
adapter based on training progress, leveraging both pre-trained and learned
knowledge. Through extensive experiments across various settings, our method
outperforms other advanced methods with fewer trainable parameters.

</details>


### [112] [A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory](https://arxiv.org/abs/2508.07746)
*Fengdi Che*

Main category: cs.LG

TL;DR: 该论文综述了离线强化学习的理论洞察及其对算法设计的实际意义，探讨了理论条件、反例及解决方案。


<details>
  <summary>Details</summary>
Motivation: 弥合离线强化学习理论与实际算法设计之间的差距，为研究者提供理论指导。

Method: 通过分析理论条件（如函数表示和数据覆盖假设）、反例及解决方案，探讨离线RL的可行性与局限性。

Result: 揭示了离线RL的固有挑战及解决条件，强调了在条件不满足时需寻找新方法。

Conclusion: 理论条件不仅为证明提供基础，也揭示了算法局限性，推动未来研究突破。

Abstract: Offline reinforcement learning (RL) aims to optimize the return given a fixed
dataset of agent trajectories without additional interactions with the
environment. While algorithm development has progressed rapidly, significant
theoretical advances have also been made in understanding the fundamental
challenges of offline RL. However, bridging these theoretical insights with
practical algorithm design remains an ongoing challenge. In this survey, we
explore key intuitions derived from theoretical work and their implications for
offline RL algorithms.
  We begin by listing the conditions needed for the proofs, including function
representation and data coverage assumptions. Function representation
conditions tell us what to expect for generalization, and data coverage
assumptions describe the quality requirement of the data. We then examine
counterexamples, where offline RL is not solvable without an impractically
large amount of data. These cases highlight what cannot be achieved for all
algorithms and the inherent hardness of offline RL. Building on techniques to
mitigate these challenges, we discuss the conditions that are sufficient for
offline RL. These conditions are not merely assumptions for theoretical proofs,
but they also reveal the limitations of these algorithms and remind us to
search for novel solutions when the conditions cannot be satisfied.

</details>


### [113] [Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment](https://arxiv.org/abs/2508.07750)
*Haowen Wang,Yun Yue,Zhiling Ye,Shuowen Zhang,Lei Fan,Jiaxin Liang,Jiadi Jiang,Cheng Wei,Jingyuan Deng,Xudong Han,Ji Li,Chunxiao Guo,Peng Wei,Jian Wang,Jinjie Gu*

Main category: cs.LG

TL;DR: GRAO（Group Relative Alignment Optimization）是一种结合SFT和RL优势的统一框架，通过多样本生成、组内相对优势加权和参考感知参数更新，显著提升了语言模型的对齐能力。


<details>
  <summary>Details</summary>
Motivation: 解决SFT和RL在语言模型对齐中的局限性：SFT受限于离线策略轨迹，RL则样本效率低且依赖高质量基础模型。

Method: 提出GRAO框架，包括多样本生成策略、组内相对优势加权损失和参考感知参数更新。

Result: 在复杂对齐任务中，GRAO比SFT、DPO、PPO和GRPO基线分别提升了57.70%、17.65%、7.95%和5.18%。

Conclusion: GRAO提供了理论支持的高效对齐框架，为语言模型能力进化提供了实证依据。

Abstract: Alignment methodologies have emerged as a critical pathway for enhancing
language model alignment capabilities. While SFT (supervised fine-tuning)
accelerates convergence through direct token-level loss intervention, its
efficacy is constrained by offline policy trajectory. In contrast,
RL(reinforcement learning) facilitates exploratory policy optimization, but
suffers from low sample efficiency and stringent dependency on high-quality
base models. To address these dual challenges, we propose GRAO (Group Relative
Alignment Optimization), a unified framework that synergizes the respective
strengths of SFT and RL through three key innovations: 1) A multi-sample
generation strategy enabling comparative quality assessment via reward
feedback; 2) A novel Group Direct Alignment Loss formulation leveraging
intra-group relative advantage weighting; 3) Reference-aware parameter updates
guided by pairwise preference dynamics. Our theoretical analysis establishes
GRAO's convergence guarantees and sample efficiency advantages over
conventional approaches. Comprehensive evaluations across complex human
alignment tasks demonstrate GRAO's superior performance, achieving
57.70\%,17.65\% 7.95\% and 5.18\% relative improvements over SFT, DPO, PPO and
GRPO baselines respectively. This work provides both a theoretically grounded
alignment framework and empirical evidence for efficient capability evolution
in language models.

</details>


### [114] [Sparse Probabilistic Graph Circuits](https://arxiv.org/abs/2508.07763)
*Martin Rektoris,Milan Papež,Václav Šmídl,Tomáš Pevný*

Main category: cs.LG

TL;DR: 论文提出了一种稀疏概率图电路（SPGCs），解决了传统深度生成模型（DGMs）在概率推断上的不可解问题，并通过稀疏表示将复杂度从O(n²)降至O(n+m)，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度生成模型（DGMs）在概率推断上不可解，而现有的概率图电路（PGCs）虽然解决了这一问题，但其复杂度为O(n²)，限制了在大规模稀疏图上的应用。

Method: 提出稀疏概率图电路（SPGCs），直接操作稀疏图表示，将复杂度降至O(n + m)。

Result: 在药物设计任务中，SPGCs保持了精确推断能力，提升了内存效率和推断速度，性能与不可解的DGMs相当。

Conclusion: SPGCs是一种高效的、可解的生成模型，适用于稀疏图场景，具有实际应用价值。

Abstract: Deep generative models (DGMs) for graphs achieve impressively high expressive
power thanks to very efficient and scalable neural networks. However, these
networks contain non-linearities that prevent analytical computation of many
standard probabilistic inference queries, i.e., these DGMs are considered
\emph{intractable}. While recently proposed Probabilistic Graph Circuits (PGCs)
address this issue by enabling \emph{tractable} probabilistic inference, they
operate on dense graph representations with $\mathcal{O}(n^2)$ complexity for
graphs with $n$ nodes and \emph{$m$ edges}. To address this scalability issue,
we introduce Sparse PGCs, a new class of tractable generative models that
operate directly on sparse graph representation, reducing the complexity to
$\mathcal{O}(n + m)$, which is particularly beneficial for $m \ll n^2$. In the
context of de novo drug design, we empirically demonstrate that SPGCs retain
exact inference capabilities, improve memory efficiency and inference speed,
and match the performance of intractable DGMs in key metrics.

</details>


### [115] [Pareto Multi-Objective Alignment for Language Models](https://arxiv.org/abs/2508.07768)
*Qiang He,Setareh Maghsudi*

Main category: cs.LG

TL;DR: 论文提出了一种名为PAMA的高效多目标对齐算法，用于解决大型语言模型（LLMs）在多目标优化（MOA）中的问题，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 当前基于RLHF的对齐方法仅优化单一奖励函数，导致LLMs行为僵化，无法适应多样的人类偏好，限制了其在实际场景中的适应性。

Method: 提出PAMA算法，将多目标RLHF转化为凸优化问题，提供封闭解，将复杂度从O(n^2*d)降至O(n)。

Result: 实验证明PAMA在125M至7B参数的模型中表现优异，能高效收敛到Pareto稳定点。

Conclusion: PAMA为MOA问题提供了高效且理论可靠的解决方案，推动了LLMs在多样化人类价值观下的实际应用。

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications that require careful balancing of multiple, often conflicting,
objectives, such as informativeness versus conciseness, or helpfulness versus
creativity. However, current alignment methods, primarily based on RLHF,
optimize LLMs toward a single reward function, resulting in rigid behavior that
fails to capture the complexity and diversity of human preferences. This
limitation hinders the adaptability of LLMs to practical scenarios, making
multi-objective alignment (MOA) a critical yet underexplored area. To bridge
this gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and
computationally efficient algorithm designed explicitly for MOA in LLMs. In
contrast to computationally prohibitive multi-objective optimization (MOO)
methods, PAMA transforms multi-objective RLHF into a convex optimization with a
closed-form solution, significantly enhancing scalability. Traditional MOO
approaches suffer from prohibitive O(n^2*d) complexity, where d represents the
number of model parameters, typically in the billions for LLMs, rendering
direct optimization infeasible. PAMA reduces this complexity to O(n) where n is
the number of objectives, enabling optimization to be completed within
milliseconds. We provide theoretical guarantees that PAMA converges to a Pareto
stationary point, where no objective can be improved without degrading at least
one other. Extensive experiments across language models ranging from 125M to 7B
parameters demonstrate PAMA's robust and effective MOA capabilities, aligning
with its theoretical advantages. PAMA provides a highly efficient solution to
the MOA problem that was previously considered intractable, offering a
practical and theoretically grounded approach to aligning LLMs with diverse
human values, paving the way for versatile and adaptable real-world AI
deployments.

</details>


### [116] [Topological Feature Compression for Molecular Graph Neural Networks](https://arxiv.org/abs/2508.07807)
*Rahul Khorana*

Main category: cs.LG

TL;DR: 提出了一种新型图神经网络架构，结合高阶拓扑信号与标准分子特征，平衡预测准确性、可解释性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 分子表示学习虽取得进展，但提取通用化学见解并平衡准确性、可解释性和计算效率仍是挑战。

Method: 引入结合高阶拓扑信号的GNN架构，捕捉全局几何信息，同时保持计算效率和可解释性。

Result: 在多个基准测试中表现优异，准确性和鲁棒性均领先。

Conclusion: 该架构在分子表示学习中实现了高性能，代码已开源。

Abstract: Recent advances in molecular representation learning have produced highly
effective encodings of molecules for numerous cheminformatics and
bioinformatics tasks. However, extracting general chemical insight while
balancing predictive accuracy, interpretability, and computational efficiency
remains a major challenge. In this work, we introduce a novel Graph Neural
Network (GNN) architecture that combines compressed higher-order topological
signals with standard molecular features. Our approach captures global
geometric information while preserving computational tractability and
human-interpretable structure. We evaluate our model across a range of
benchmarks, from small-molecule datasets to complex material datasets, and
demonstrate superior performance using a parameter-efficient architecture. We
achieve the best performing results in both accuracy and robustness across
almost all benchmarks. We open source all code \footnote{All code and results
can be found on Github https://github.com/rahulkhorana/TFC-PACT-Net}.

</details>


### [117] [EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning](https://arxiv.org/abs/2508.07809)
*Huanyu Liu,Jia Li,Chang Yu,Taozhi Chen,Yihong Dong,Lecheng Wang,Hu XiaoLong,Ge Li*

Main category: cs.LG

TL;DR: EvoCoT是一种基于两阶段思维链优化的自进化课程学习框架，用于解决强化学习中稀疏奖励问题，提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，当大语言模型在难题上的表现不佳时，奖励变得稀疏，限制了学习效率和探索能力。现有方法依赖更强的模型或过滤难题，缺乏可扩展性或限制了推理能力的提升。

Method: EvoCoT通过自生成和验证思维链轨迹，逐步缩短轨迹以扩展探索空间，使模型能在稀疏奖励下稳定学习未解决的难题。

Result: 实验表明，EvoCoT能帮助模型解决未解决的问题，提升推理能力，且无需外部思维链监督，兼容多种强化学习微调方法。

Conclusion: EvoCoT为提升大语言模型的推理能力提供了一种有效且可扩展的解决方案。

Abstract: Reinforcement learning with verifiable reward (RLVR) has become a promising
paradigm for post-training large language models (LLMs) to improve their
reasoning capability. However, when the rollout accuracy is low on hard
problems, the reward becomes sparse, limiting learning efficiency and causing
exploration bottlenecks. Existing approaches either rely on stronger LLMs for
distillation or filter out difficult problems, which limits scalability or
restricts reasoning improvement through exploration.
  We propose EvoCoT, a self-evolving curriculum learning framework based on
two-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the
exploration space by self-generating and verifying CoT trajectories, then
gradually shortens them to expand the space in a controlled way. This enables
LLMs to stably learn from initially unsolved hard problems under sparse
rewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,
and Llama. Experiments show that EvoCoT enables LLMs to solve previously
unsolved problems, improves reasoning capability without external CoT
supervision, and is compatible with various RL fine-tuning methods. We release
the source code to support future research.

</details>


### [118] [Learning Satellite Attitude Dynamics with Physics-Informed Normalising Flow](https://arxiv.org/abs/2508.07841)
*Carlo Cena,Mauro Martini,Marcello Chiaberge*

Main category: cs.LG

TL;DR: 论文研究了将物理信息神经网络（PINNs）引入航天器姿态动力学学习的优势，与纯数据驱动方法相比，PINNs显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 在物理模型不完整或计算成本高的情况下，纯数据驱动方法泛化性和稳定性不足，因此探索PINNs的潜力。

Method: 使用Real NVP神经网络架构和自注意力机制，结合Basilisk模拟器生成的数据，对比纯数据驱动和物理信息两种训练策略。

Result: 物理信息方法将最佳架构的平均相对误差降低了27.08%，在MPC框架中表现更优，控制精度和鲁棒性提升达42.86%。

Conclusion: PINNs显著提升了航天器姿态控制的性能，尤其在MPC框架中表现突出，为复杂系统控制提供了更优解决方案。

Abstract: Attitude control is a fundamental aspect of spacecraft operations. Model
Predictive Control (MPC) has emerged as a powerful strategy for these tasks,
relying on accurate models of the system dynamics to optimize control actions
over a prediction horizon. In scenarios where physics models are incomplete,
difficult to derive, or computationally expensive, machine learning offers a
flexible alternative by learning the system behavior directly from data.
However, purely data-driven models often struggle with generalization and
stability, especially when applied to inputs outside their training domain. To
address these limitations, we investigate the benefits of incorporating
Physics-Informed Neural Networks (PINNs) into the learning of spacecraft
attitude dynamics, comparing their performance with that of purely data-driven
approaches. Using a Real-valued Non-Volume Preserving (Real NVP) neural network
architecture with a self-attention mechanism, we trained several models on
simulated data generated with the Basilisk simulator. Two training strategies
were considered: a purely data-driven baseline and a physics-informed variant
to improve robustness and stability. Our results demonstrate that the inclusion
of physics-based information significantly enhances the performance in terms of
the mean relative error of the best architectures found by 27.08%. These
advantages are particularly evident when the learned models are integrated into
an MPC framework, where PINN-based models consistently outperform their purely
data-driven counterparts in terms of control accuracy and robustness, yielding
improvements of up to 42.86% in performance stability error and increased
robustness-to-noise.

</details>


### [119] [Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant](https://arxiv.org/abs/2508.07887)
*Sabrina Namazova,Alessandra Brondetta,Younes Strittmatter,Matthew Nassar,Sebastian Musslick*

Main category: cs.LG

TL;DR: 论文探讨了模拟器在科学实践中的重要性，特别是行为科学中的参与者模拟器Centaur，但其生成行为与人类数据存在系统性差异。


<details>
  <summary>Details</summary>
Motivation: 模拟器如AlphaFold在自然科学中取得巨大成功，行为科学需要类似可靠的参与者模拟器以加速实验设计和假设测试。

Method: 评估Centaur（基于160个实验数据微调的LLM）作为参与者模拟器的核心标准，重点关注其生成行为。

Result: Centaur预测准确性高，但生成行为与人类数据存在系统性差异。

Conclusion: Centaur虽在预测人类行为上迈出重要一步，但尚未达到可靠参与者模拟器或准确认知模型的标准。

Abstract: Simulators have revolutionized scientific practice across the natural
sciences. By generating data that reliably approximate real-world phenomena,
they enable scientists to accelerate hypothesis testing and optimize
experimental designs. This is perhaps best illustrated by AlphaFold, a
Nobel-prize winning simulator in chemistry that predicts protein structures
from amino acid sequences, enabling rapid prototyping of molecular
interactions, drug targets, and protein functions. In the behavioral sciences,
a reliable participant simulator - a system capable of producing human-like
behavior across cognitive tasks - would represent a similarly transformative
advance. Recently, Binz et al. introduced Centaur, a large language model (LLM)
fine-tuned on human data from 160 experiments, proposing its use not only as a
model of cognition but also as a participant simulator for "in silico
prototyping of experimental studies", e.g., to advance automated cognitive
science. Here, we review the core criteria for a participant simulator and
assess how well Centaur meets them. Although Centaur demonstrates strong
predictive accuracy, its generative behavior - a critical criterion for a
participant simulator - systematically diverges from human data. This suggests
that, while Centaur is a significant step toward predicting human behavior, it
does not yet meet the standards of a reliable participant simulator or an
accurate model of cognition.

</details>


### [120] [Score Augmentation for Diffusion Models](https://arxiv.org/abs/2508.07926)
*Liang Hou,Yuan Gao,Boyuan Jiang,Xin Tao,Qi Yan,Renjie Liao,Pengfei Wan,Di Zhang,Kun Gai*

Main category: cs.LG

TL;DR: 本文提出了一种名为ScoreAug的新型数据增强框架，专门针对扩散模型在数据有限情况下的过拟合问题，通过噪声数据变换和等变学习目标显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成建模中取得了显著成功，但在数据有限的情况下存在过拟合问题，需要一种专门的数据增强方法来解决。

Method: 提出ScoreAug框架，对噪声数据进行变换，并要求去噪器预测原始目标的增强，实现等变学习目标。

Result: 在多个基准测试中验证了ScoreAug的有效性，显著提升了性能并缓解了过拟合问题，同时避免了数据泄漏。

Conclusion: ScoreAug不仅有效解决了扩散模型的过拟合问题，还能与传统数据增强技术结合，进一步提升性能。

Abstract: Diffusion models have achieved remarkable success in generative modeling.
However, this study confirms the existence of overfitting in diffusion model
training, particularly in data-limited regimes. To address this challenge, we
propose Score Augmentation (ScoreAug), a novel data augmentation framework
specifically designed for diffusion models. Unlike conventional augmentation
approaches that operate on clean data, ScoreAug applies transformations to
noisy data, aligning with the inherent denoising mechanism of diffusion.
Crucially, ScoreAug further requires the denoiser to predict the augmentation
of the original target. This design establishes an equivariant learning
objective, enabling the denoiser to learn scores across varied denoising
spaces, thereby realizing what we term score augmentation. We also
theoretically analyze the relationship between scores in different spaces under
general transformations. In experiments, we extensively validate ScoreAug on
multiple benchmarks including CIFAR-10, FFHQ, AFHQv2, and ImageNet, with
results demonstrating significant performance improvements over baselines.
Notably, ScoreAug effectively mitigates overfitting across diverse scenarios,
such as varying data scales and model capacities, while exhibiting stable
convergence properties. Another advantage of ScoreAug over standard data
augmentation lies in its ability to circumvent data leakage issues under
certain conditions. Furthermore, we show that ScoreAug can be synergistically
combined with traditional data augmentation techniques to achieve additional
performance gains.

</details>


### [121] [Adaptive Fine-Tuning via Pattern Specialization for Deep Time Series Forecasting](https://arxiv.org/abs/2508.07927)
*Amal Saadallah,Abdulaziz Al-Ademi*

Main category: cs.LG

TL;DR: 提出了一种通过模型适应和选择提升DNN在非平稳时间序列预测中性能的新框架。


<details>
  <summary>Details</summary>
Motivation: 非平稳环境中时间序列预测的挑战在于模式随时间变化，传统方法难以适应。

Method: 离线训练基础DNN，分割验证集以聚类主导模式，针对每个聚类微调模型，推理时匹配最近模式并部署对应模型，集成概念漂移检测。

Result: 框架适用于多种DNN架构，在GluonTS库中实现了显著性能提升。

Conclusion: 该框架通过模式适应和动态选择有效提升了非平稳时间序列预测的准确性。

Abstract: Time series forecasting poses significant challenges in non-stationary
environments where underlying patterns evolve over time. In this work, we
propose a novel framework that enhances deep neural network (DNN) performance
by leveraging specialized model adaptation and selection. Initially, a base DNN
is trained offline on historical time series data. A reserved validation subset
is then segmented to extract and cluster the most dominant patterns within the
series, thereby identifying distinct regimes. For each identified cluster, the
base DNN is fine-tuned to produce a specialized version that captures unique
pattern characteristics. At inference, the most recent input is matched against
the cluster centroids, and the corresponding fine-tuned version is deployed
based on the closest similarity measure. Additionally, our approach integrates
a concept drift detection mechanism to identify and adapt to emerging patterns
caused by non-stationary behavior. The proposed framework is generalizable
across various DNN architectures and has demonstrated significant performance
gains on both traditional DNNs and recent advanced architectures implemented in
the GluonTS library.

</details>


### [122] [Shapley-Inspired Feature Weighting in $k$-means with No Additional Hyperparameters](https://arxiv.org/abs/2508.07952)
*Richard J. Fawley,Renato Cordeiro de Amorim*

Main category: cs.LG

TL;DR: SHARK是一种基于Shapley值的特征加权聚类算法，无需额外参数调整，通过分解k均值目标函数为Shapley值，高效计算特征重要性。


<details>
  <summary>Details</summary>
Motivation: 高维或噪声数据中，传统聚类算法假设所有特征贡献均等，导致性能下降。现有特征加权方法需额外参数调整。

Method: 利用Shapley值量化特征相关性，将k均值目标分解为各特征Shapley值之和，迭代调整特征权重。

Result: 实验表明，SHARK在合成和真实数据上优于现有方法，尤其在噪声场景中表现稳健且准确。

Conclusion: SHARK为无监督特征相关性提供了理论支持，并在实际应用中展现出优越性能。

Abstract: Clustering algorithms often assume all features contribute equally to the
data structure, an assumption that usually fails in high-dimensional or noisy
settings. Feature weighting methods can address this, but most require
additional parameter tuning. We propose SHARK (Shapley Reweighted $k$-means), a
feature-weighted clustering algorithm motivated by the use of Shapley values
from cooperative game theory to quantify feature relevance, which requires no
additional parameters beyond those in $k$-means. We prove that the $k$-means
objective can be decomposed into a sum of per-feature Shapley values, providing
an axiomatic foundation for unsupervised feature relevance and reducing Shapley
computation from exponential to polynomial time. SHARK iteratively re-weights
features by the inverse of their Shapley contribution, emphasising informative
dimensions and down-weighting irrelevant ones. Experiments on synthetic and
real-world data sets show that SHARK consistently matches or outperforms
existing methods, achieving superior robustness and accuracy, particularly in
scenarios where noise may be present. Software:
https://github.com/rickfawley/shark.

</details>


### [123] [WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer](https://arxiv.org/abs/2508.07970)
*Junyu Wu,Weiming Chang,Xiaotao Liu,Guanyou He,Tingfeng Xian,Haoqiang Hong,Boqi Chen,Haotao Tian,Tao Yang,Yunsheng Shi,Feng Lin,Ting Yao*

Main category: cs.LG

TL;DR: WeChat-YATT是一个针对RLHF训练框架的改进方案，解决了现有系统在扩展性和动态资源分配上的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF训练框架在复杂多模态工作流和动态负载适应方面存在局限性，尤其是控制器扩展性和资源分配效率问题。

Method: 提出WeChat-YATT框架，采用并行控制器编程模型和动态资源分配策略。

Result: 实验表明，WeChat-YATT在吞吐量上显著优于现有框架，并已成功应用于微信产品。

Conclusion: WeChat-YATT是一个高效、可扩展的RLHF训练框架，适用于大规模实际应用。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent
paradigm for training large language models and multimodal systems. Despite
notable advances enabled by existing RLHF training frameworks, significant
challenges remain in scaling to complex multimodal workflows and adapting to
dynamic workloads. In particular, current systems often encounter limitations
related to controller scalability when managing large models, as well as
inefficiencies in orchestrating intricate RLHF pipelines, especially in
scenarios that require dynamic sampling and resource allocation. In this paper,
we introduce WeChat-YATT (Yet Another Transformer Trainer in WeChat), a simple,
scalable, and balanced RLHF training framework specifically designed to address
these challenges. WeChat-YATT features a parallel controller programming model
that enables flexible and efficient orchestration of complex RLHF workflows,
effectively mitigating the bottlenecks associated with centralized controller
architectures and facilitating scalability in large-scale data scenarios. In
addition, we propose a dynamic placement schema that adaptively partitions
computational resources and schedules workloads, thereby significantly reducing
hardware idle time and improving GPU utilization under variable training
conditions. We evaluate WeChat-YATT across a range of experimental scenarios,
demonstrating that it achieves substantial improvements in throughput compared
to state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been
successfully deployed to train models supporting WeChat product features for a
large-scale user base, underscoring its effectiveness and robustness in
real-world applications.

</details>


### [124] [A Physics-informed Deep Operator for Real-Time Freeway Traffic State Estimation](https://arxiv.org/abs/2508.08002)
*Hongxin Yu,Yibing Wang,Fengyue Jin,Meng Zhang,Anni Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息深度算子网络（PI-DeepONet）的实时高速公路交通状态估计方法，扩展了原始架构并显著提升了估计精度。


<details>
  <summary>Details</summary>
Motivation: 交通状态估计（TSE）分为模型驱动、数据驱动和模型-数据双驱动三类，但现有方法难以兼顾准确性和实时性。本文旨在结合物理模型和数据驱动方法的优势，提出更高效的TSE方法。

Method: 扩展了PI-DeepONet架构，支持2-D数据输入、引入非线性扩展层、注意力机制和MIMO机制，并设计了自适应识别交通流模型参数的神经网络。

Result: 在NGSIM短高速公路和中国大规模城市快速路的实验中，该方法在流量和平均速度的估计精度上优于四种基线方法。

Conclusion: 基于扩展PI-DeepONet的TSE方法显著提升了估计精度，验证了其在实时交通状态估计中的有效性。

Abstract: Traffic state estimation (TSE) falls methodologically into three categories:
model-driven, data-driven, and model-data dual-driven. Model-driven TSE relies
on macroscopic traffic flow models originated from hydrodynamics. Data-driven
TSE leverages historical sensing data and employs statistical models or machine
learning methods to infer traffic state. Model-data dual-driven traffic state
estimation attempts to harness the strengths of both aspects to achieve more
accurate TSE. From the perspective of mathematical operator theory, TSE can be
viewed as a type of operator that maps available measurements of inerested
traffic state into unmeasured traffic state variables in real time. For the
first time this paper proposes to study real-time freeway TSE in the idea of
physics-informed deep operator network (PI-DeepONet), which is an
operator-oriented architecture embedding traffic flow models based on deep
neural networks. The paper has developed an extended architecture from the
original PI-DeepONet. The extended architecture is featured with: (1) the
acceptance of 2-D data input so as to support CNN-based computations; (2) the
introduction of a nonlinear expansion layer, an attention mechanism, and a MIMO
mechanism; (3) dedicated neural network design for adaptive identification of
traffic flow model parameters. A traffic state estimator built on the basis of
this extended PI-DeepONet architecture was evaluated with respect to a short
freeway stretch of NGSIM and a large-scale urban expressway in China, along
with other four baseline TSE methods. The evaluation results demonstrated that
this novel TSE method outperformed the baseline methods with high-precision
estimation results of flow and mean speed.

</details>


### [125] [Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP](https://arxiv.org/abs/2508.08005)
*Xiang Li,Shanshan Wang,Chenglong Xiao*

Main category: cs.LG

TL;DR: 提出了一种基于学习的框架，结合传统机器学习和图神经网络，用于解决最大团问题的算法选择问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏针对最大团问题的算法选择方法，且无单一算法在所有实例中表现最佳。

Method: 构建标记数据集，评估四种传统分类器，并开发双通道模型GAT-MLP（结合图注意力网络和多层感知机）。

Result: 随机森林表现稳定，GAT-MLP模型在所有指标上均表现优异。

Conclusion: 双通道架构和图神经网络在组合算法选择中具有潜力。

Abstract: Extensive experiments and prior studies show that no single maximum clique
algorithm consistently performs best across all instances, highlighting the
importance of selecting suitable algorithms based on instance features. Through
an extensive analysis of relevant studies, it is found that there is a lack of
research work concerning algorithm selection oriented toward the Maximum Clique
Problem (MCP). In this work, we propose a learning-based framework that
integrates both traditional machine learning and graph neural networks to
address this gap. We construct a labeled dataset by running four exact MCP
algorithms on a diverse collection of graph instances, accompanied by
structural and global statistical features extracted from each graph. We first
evaluate four conventional classifiers: Support Vector Machine (SVM), Random
Forest (RF), Decision Tree (DT), and K-Nearest Neighbors (KNN), across multiple
dataset variants. Experimental results show that RF consistently shows strong
performance across metrics and dataset variants, making it a reliable baseline.
In addition, feature importance analysis indicates that connectivity and
topological structure are strong predictors of algorithm performance. Building
on these findings, we develop a dual-channel model named GAT-MLP, which
combines a Graph Attention Network (GAT) for local structural encoding with a
Multilayer Perceptron (MLP) for global feature modeling. The GAT-MLP model
shows strong and consistent performance across all metrics. Our results
highlight the effectiveness of dual-channel architectures and the promise of
graph neural networks in combinatorial algorithm selection.

</details>


### [126] [Communication-Efficient Zero-Order and First-Order Federated Learning Methods over Wireless Networks](https://arxiv.org/abs/2508.08013)
*Mohamad Assaad,Zeinab Nehme,Merouane Debbah*

Main category: cs.LG

TL;DR: 论文提出了两种通信高效的联邦学习方法，通过减少通信开销和利用信道信息，解决了联邦学习中的通信瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在边缘设备上协作训练模型时，通信开销大，受限于无线系统的容量。本文旨在减少通信负担并提高效率。

Method: 1. 使用零阶优化技术和两点梯度估计器；2. 采用一阶梯度计算策略。两种方法均利用信道信息，避免额外获取信道状态信息（CSI）。

Result: 论文为两种方法提供了严格的分析框架，推导了收敛保证和性能界限。

Conclusion: 提出的方法有效减少了FL的通信开销，同时利用了信道信息，适用于异步设备场景。

Abstract: Federated Learning (FL) is an emerging learning framework that enables edge
devices to collaboratively train ML models without sharing their local data. FL
faces, however, a significant challenge due to the high amount of information
that must be exchanged between the devices and the aggregator in the training
phase, which can exceed the limited capacity of wireless systems. In this
paper, two communication-efficient FL methods are considered where
communication overhead is reduced by communicating scalar values instead of
long vectors and by allowing high number of users to send information
simultaneously. The first approach employs a zero-order optimization technique
with two-point gradient estimator, while the second involves a first-order
gradient computation strategy. The novelty lies in leveraging channel
information in the learning algorithms, eliminating hence the need for
additional resources to acquire channel state information (CSI) and to remove
its impact, as well as in considering asynchronous devices. We provide a
rigorous analytical framework for the two methods, deriving convergence
guarantees and establishing appropriate performance bounds.

</details>


### [127] [Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles](https://arxiv.org/abs/2508.08034)
*Roksana Yahyaabadi,Ghazal Farhani,Taufiq Rahman,Soodeh Nikan,Abdullah Jirjees,Fadi Araji*

Main category: cs.LG

TL;DR: 提出了一种基于数据驱动的方法，结合传统机器学习和深度神经网络，用于预测内燃机、电动车和混合动力车的瞬时和累计功耗。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖专业仪器或固定物理模型，难以大规模实际应用，因此需要一种更实用的解决方案。

Method: 使用动力系统动态特征集，结合传统机器学习和深度神经网络（如Transformer和LSTM）进行功耗预测。

Result: 内燃机瞬时误差低至10^-3，累计误差低于3%；电动车和混合动力车累计误差分别低于4.1%和2.1%。

Conclusion: 该方法在不同车型上均有效，但电动车和混合动力车数据变异性更高，需更鲁棒的模型。

Abstract: Accurate power consumption prediction is crucial for improving efficiency and
reducing environmental impact, yet traditional methods relying on specialized
instruments or rigid physical models are impractical for large-scale,
real-world deployment. This study introduces a scalable data-driven method
using powertrain dynamic feature sets and both traditional machine learning and
deep neural networks to estimate instantaneous and cumulative power consumption
in internal combustion engine (ICE), electric vehicle (EV), and hybrid electric
vehicle (HEV) platforms. ICE models achieved high instantaneous accuracy with
mean absolute error and root mean squared error on the order of $10^{-3}$, and
cumulative errors under 3%. Transformer and long short-term memory models
performed best for EVs and HEVs, with cumulative errors below 4.1% and 2.1%,
respectively. Results confirm the approach's effectiveness across vehicles and
models. Uncertainty analysis revealed greater variability in EV and HEV
datasets than ICE, due to complex power management, emphasizing the need for
robust models for advanced powertrains.

</details>


### [128] [BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models](https://arxiv.org/abs/2508.08040)
*Maozhen Zhang,Mengnan Zhao,Bo Wang*

Main category: cs.LG

TL;DR: 论文提出了一种针对多模态对比模型中基于提示的联邦学习的后门攻击方法BadPromptFL，攻击成功率高且隐蔽性强。


<details>
  <summary>Details</summary>
Motivation: 探索基于提示的联邦学习在安全方面的潜在风险，填补了相关研究的空白。

Method: 通过联合优化本地后门触发器和提示嵌入，将中毒提示注入全局聚合过程。

Result: 攻击成功率超过90%，且在多种数据集和聚合协议下验证了其有效性和隐蔽性。

Conclusion: BadPromptFL揭示了基于提示的联邦学习在实际部署中的脆弱性，引发了对安全性的关注。

Abstract: Prompt-based tuning has emerged as a lightweight alternative to full
fine-tuning in large vision-language models, enabling efficient adaptation via
learned contextual prompts. This paradigm has recently been extended to
federated learning settings (e.g., PromptFL), where clients collaboratively
train prompts under data privacy constraints. However, the security
implications of prompt-based aggregation in federated multimodal learning
remain largely unexplored, leaving a critical attack surface unaddressed. In
this paper, we introduce \textbf{BadPromptFL}, the first backdoor attack
targeting prompt-based federated learning in multimodal contrastive models. In
BadPromptFL, compromised clients jointly optimize local backdoor triggers and
prompt embeddings, injecting poisoned prompts into the global aggregation
process. These prompts are then propagated to benign clients, enabling
universal backdoor activation at inference without modifying model parameters.
Leveraging the contextual learning behavior of CLIP-style architectures,
BadPromptFL achieves high attack success rates (e.g., \(>90\%\)) with minimal
visibility and limited client participation. Extensive experiments across
multiple datasets and aggregation protocols validate the effectiveness,
stealth, and generalizability of our attack, raising critical concerns about
the robustness of prompt-based federated learning in real-world deployments.

</details>


### [129] [On Understanding of the Dynamics of Model Capacity in Continual Learning](https://arxiv.org/abs/2508.08052)
*Supriyo Chakraborty,Krishnan Raghavan*

Main category: cs.LG

TL;DR: 论文提出了一种称为CLEMC的模型，用于描述持续学习中稳定性与可塑性平衡的动态行为，并通过理论和实验证明这种平衡是非静态的。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中的稳定性与可塑性难题，研究神经网络在不同任务分布下的表现能力。

Method: 提出CLEMC模型，建立差分方程描述神经网络、任务数据和优化过程的动态交互。

Result: 实验表明，无论网络架构或优化方法如何，任务分布变化都会降低网络对新任务的表示能力。

Conclusion: CLEMC揭示了持续学习中稳定性与可塑性平衡的非静态特性，为未来研究提供了理论基础。

Abstract: The stability-plasticity dilemma, closely related to a neural network's (NN)
capacity-its ability to represent tasks-is a fundamental challenge in continual
learning (CL). Within this context, we introduce CL's effective model capacity
(CLEMC) that characterizes the dynamic behavior of the stability-plasticity
balance point. We develop a difference equation to model the evolution of the
interplay between the NN, task data, and optimization procedure. We then
leverage CLEMC to demonstrate that the effective capacity-and, by extension,
the stability-plasticity balance point is inherently non-stationary. We show
that regardless of the NN architecture or optimization method, a NN's ability
to represent new tasks diminishes when incoming task distributions differ from
previous ones. We conduct extensive experiments to support our theoretical
findings, spanning a range of architectures-from small feedforward network and
convolutional networks to medium-sized graph neural networks and
transformer-based large language models with millions of parameters.

</details>


### [130] [C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction](https://arxiv.org/abs/2508.08071)
*Yunqing Li,Zixiang Tang,Jiaying Zhuang,Zhenyu Yang,Farhad Ameri,Jianbang Zhang*

Main category: cs.LG

TL;DR: 论文介绍了PMGraph基准数据集和C-MAG架构，用于提升供应链中制造商与产品链接预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉复杂的制造商能力和多模态数据，需要更高效的供应链链接方法。

Method: 提出C-MAG两阶段架构：先对齐聚合文本和视觉属性，再通过多尺度消息传递增强链接预测。

Result: C-MAG在噪声环境中保持预测性能，提升了链接准确性。

Conclusion: PMGraph和C-MAG为供应链管理提供了实用工具和方法。

Abstract: Connecting an ever-expanding catalogue of products with suitable
manufacturers and suppliers is critical for resilient, efficient global supply
chains, yet traditional methods struggle to capture complex capabilities,
certifications, geographic constraints, and rich multimodal data of real-world
manufacturer profiles. To address these gaps, we introduce PMGraph, a public
benchmark of bipartite and heterogeneous multimodal supply-chain graphs linking
8,888 manufacturers, over 70k products, more than 110k manufacturer-product
edges, and over 29k product images. Building on this benchmark, we propose the
Cascade Multimodal Attributed Graph C-MAG, a two-stage architecture that first
aligns and aggregates textual and visual attributes into intermediate group
embeddings, then propagates them through a manufacturer-product hetero-graph
via multiscale message passing to enhance link prediction accuracy. C-MAG also
provides practical guidelines for modality-aware fusion, preserving predictive
performance in noisy, real-world settings.

</details>


### [131] [ELF: Efficient Logic Synthesis by Pruning Redundancy in Refactoring](https://arxiv.org/abs/2508.08073)
*Dimitris Tsaras,Xing Li,Lei Chen,Zhiyao Xie,Mingxuan Yuan*

Main category: cs.LG

TL;DR: 通过分类器预剪枝无效切割，提升逻辑优化速度3.9倍。


<details>
  <summary>Details</summary>
Motivation: 传统逻辑优化算子计算成本高，且98%的切割尝试失败。

Method: 利用分类器预剪枝无效切割，避免不必要的重合成操作。

Result: 在EPFL基准套件和10个大型工业设计上，速度提升3.9倍。

Conclusion: 该方法显著降低了计算成本，提升了逻辑优化效率。

Abstract: In electronic design automation, logic optimization operators play a crucial
role in minimizing the gate count of logic circuits. However, their computation
demands are high. Operators such as refactor conventionally form iterative cuts
for each node, striving for a more compact representation - a task which often
fails 98% on average. Prior research has sought to mitigate computational cost
through parallelization. In contrast, our approach leverages a classifier to
prune unsuccessful cuts preemptively, thus eliminating unnecessary resynthesis
operations. Experiments on the refactor operator using the EPFL benchmark suite
and 10 large industrial designs demonstrate that this technique can speedup
logic optimization by 3.9x on average compared with the state-of-the-art ABC
implementation.

</details>


### [132] [Symbolic Quantile Regression for the Interpretable Prediction of Conditional Quantiles](https://arxiv.org/abs/2508.08080)
*Cas Oude Hoekstra,Floris den Hengst*

Main category: cs.LG

TL;DR: 论文提出了一种符号分位数回归（SQR）方法，用于预测条件分位数并保持模型的可解释性，优于透明模型且与黑盒模型性能相当。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归（SR）方法仅能估计目标变量的平均值，无法分析变量在其他分位数（如中位数或极值）上的关系，而这对高风险领域至关重要。

Method: 引入符号分位数回归（SQR），通过符号回归预测条件分位数。

Result: SQR在性能上优于透明模型，与黑盒基线相当，同时保持了可解释性。

Conclusion: SQR适用于预测条件分位数，并能帮助理解不同分位数下特征的影响。

Abstract: Symbolic Regression (SR) is a well-established framework for generating
interpretable or white-box predictive models. Although SR has been successfully
applied to create interpretable estimates of the average of the outcome, it is
currently not well understood how it can be used to estimate the relationship
between variables at other points in the distribution of the target variable.
Such estimates of e.g. the median or an extreme value provide a fuller picture
of how predictive variables affect the outcome and are necessary in
high-stakes, safety-critical application domains. This study introduces
Symbolic Quantile Regression (SQR), an approach to predict conditional
quantiles with SR. In an extensive evaluation, we find that SQR outperforms
transparent models and performs comparably to a strong black-box baseline
without compromising transparency. We also show how SQR can be used to explain
differences in the target distribution by comparing models that predict extreme
and central outcomes in an airline fuel usage case study. We conclude that SQR
is suitable for predicting conditional quantiles and understanding interesting
feature influences at varying quantiles.

</details>


### [133] [Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation](https://arxiv.org/abs/2508.08087)
*Amir Ali Panahi,Daniel Luder,Billy Wu,Gregory Offer,Dirk Uwe Sauer,Weihan Li*

Main category: cs.LG

TL;DR: 论文提出了一种参数嵌入的傅里叶神经算子（PE-FNO），用于锂离子电池的高保真数字孪生模型，其在速度和精度上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 实现锂离子电池数字孪生模型的高物理保真度和亚毫秒级速度需求。

Method: 比较了DeepONets、FNOs和新提出的PE-FNO在单粒子模型（SPM）上的表现，训练数据涵盖多种电流类型和全SOC范围。

Result: PE-FNO在动态负载下表现优异，误差低于1%，速度比传统方法快200倍，并在参数估计任务中展示了高精度。

Conclusion: PE-FNO为实时电池管理和大规模推断提供了高速度、高保真度的解决方案，优于传统神经网络替代模型。

Abstract: Reliable digital twins of lithium-ion batteries must achieve high physical
fidelity with sub-millisecond speed. In this work, we benchmark three
operator-learning surrogates for the Single Particle Model (SPM): Deep Operator
Networks (DeepONets), Fourier Neural Operators (FNOs) and a newly proposed
parameter-embedded Fourier Neural Operator (PE-FNO), which conditions each
spectral layer on particle radius and solid-phase diffusivity. Models are
trained on simulated trajectories spanning four current families (constant,
triangular, pulse-train, and Gaussian-random-field) and a full range of
State-of-Charge (SOC) (0 % to 100 %). DeepONet accurately replicates
constant-current behaviour but struggles with more dynamic loads. The basic FNO
maintains mesh invariance and keeps concentration errors below 1 %, with
voltage mean-absolute errors under 1.7 mV across all load types. Introducing
parameter embedding marginally increases error, but enables generalisation to
varying radii and diffusivities. PE-FNO executes approximately 200 times faster
than a 16-thread SPM solver. Consequently, PE-FNO's capabilities in inverse
tasks are explored in a parameter estimation task with Bayesian optimisation,
recovering anode and cathode diffusivities with 1.14 % and 8.4 % mean absolute
percentage error, respectively, and 0.5918 percentage points higher error in
comparison with classical methods. These results pave the way for neural
operators to meet the accuracy, speed and parametric flexibility demands of
real-time battery management, design-of-experiments and large-scale inference.
PE-FNO outperforms conventional neural surrogates, offering a practical path
towards high-speed and high-fidelity electrochemical digital twins.

</details>


### [134] [Grid2Guide: A* Enabled Small Language Model for Indoor Navigation](https://arxiv.org/abs/2508.08100)
*Md. Wasiul Haque,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: Grid2Guide结合A*算法和小型语言模型（SLM），生成清晰的自然语言导航指令，为复杂室内环境提供轻量级、无需基础设施的导航解决方案。


<details>
  <summary>Details</summary>
Motivation: 复杂室内环境中，缺乏外部定位信号和专用基础设施时，可靠的室内导航仍具挑战性。

Method: 通过二进制占用矩阵生成最优路径，并用SLM将路径转换为自然语言指令。

Result: 实验证明该方法能生成准确、及时的导航指导。

Conclusion: Grid2Guide是一种轻量级、无需基础设施的实时室内导航解决方案。

Abstract: Reliable indoor navigation remains a significant challenge in complex
environments, particularly where external positioning signals and dedicated
infrastructures are unavailable. This research presents Grid2Guide, a hybrid
navigation framework that combines the A* search algorithm with a Small
Language Model (SLM) to generate clear, human-readable route instructions. The
framework first conducts a binary occupancy matrix from a given indoor map.
Using this matrix, the A* algorithm computes the optimal path between origin
and destination, producing concise textual navigation steps. These steps are
then transformed into natural language instructions by the SLM, enhancing
interpretability for end users. Experimental evaluations across various indoor
scenarios demonstrate the method's effectiveness in producing accurate and
timely navigation guidance. The results validate the proposed approach as a
lightweight, infrastructure-free solution for real-time indoor navigation
support.

</details>


### [135] [Vision-Based Localization and LLM-based Navigation for Indoor Environments](https://arxiv.org/abs/2508.08120)
*Keyan Rahimi,Md. Wasiul Haque,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: 该研究提出了一种结合视觉定位和大型语言模型（LLM）导航的室内导航方法，利用智能手机摄像头和公开的平面图实现高精度导航。


<details>
  <summary>Details</summary>
Motivation: 室内导航因缺乏GPS信号和建筑复杂性而具有挑战性，研究旨在提供一种无需基础设施的解决方案。

Method: 使用ResNet-50卷积神经网络进行视觉定位，结合LLM（如ChatGPT）生成导航指令。

Result: 定位准确率达96%，导航指令平均准确率为75%，但在零样本推理和响应时间上存在局限。

Conclusion: 该方法展示了利用现有设备和公开数据实现高效室内导航的潜力，适用于医院、机场等场景。

Abstract: Indoor navigation remains a complex challenge due to the absence of reliable
GPS signals and the architectural intricacies of large enclosed environments.
This study presents an indoor localization and navigation approach that
integrates vision-based localization with large language model (LLM)-based
navigation. The localization system utilizes a ResNet-50 convolutional neural
network fine-tuned through a two-stage process to identify the user's position
using smartphone camera input. To complement localization, the navigation
module employs an LLM, guided by a carefully crafted system prompt, to
interpret preprocessed floor plan images and generate step-by-step directions.
Experimental evaluation was conducted in a realistic office corridor with
repetitive features and limited visibility to test localization robustness. The
model achieved high confidence and an accuracy of 96% across all tested
waypoints, even under constrained viewing conditions and short-duration
queries. Navigation tests using ChatGPT on real building floor maps yielded an
average instruction accuracy of 75%, with observed limitations in zero-shot
reasoning and inference time. This research demonstrates the potential for
scalable, infrastructure-free indoor navigation using off-the-shelf cameras and
publicly available floor plans, particularly in resource-constrained settings
like hospitals, airports, and educational institutions.

</details>


### [136] [MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing](https://arxiv.org/abs/2508.08122)
*Mingrong Lin,Ke Deng,Zhengyang Wu,Zetao Zheng,Jie Li*

Main category: cs.LG

TL;DR: 论文提出了一种基于时间变分自编码器的知识追踪模型memoryKT，通过模拟记忆的三阶段过程（编码、存储、检索）和个性化遗忘模式，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型通常依赖单一、无差异的遗忘机制，忽略了记忆的其他过程及个性化遗忘模式，限制了模型的性能和可解释性。

Method: 提出memoryKT模型，通过时间变分自编码器模拟记忆动态过程，包括学习知识记忆特征分布、重构练习反馈，并在时间工作流中嵌入个性化遗忘模块。

Result: 在四个公开数据集上的实验表明，memoryKT显著优于现有基线模型。

Conclusion: memoryKT通过完整建模记忆的编码-存储-检索循环，增强了对个体差异的感知能力，为知识追踪提供了更优的解决方案。

Abstract: Knowledge Tracing (KT) is committed to capturing students' knowledge mastery
from their historical interactions. Simulating students' memory states is a
promising approach to enhance both the performance and interpretability of
knowledge tracing models. Memory consists of three fundamental processes:
encoding, storage, and retrieval. Although forgetting primarily manifests
during the storage stage, most existing studies rely on a single,
undifferentiated forgetting mechanism, overlooking other memory processes as
well as personalized forgetting patterns. To address this, this paper proposes
memoryKT, a knowledge tracing model based on a novel temporal variational
autoencoder. The model simulates memory dynamics through a three-stage process:
(i) Learning the distribution of students' knowledge memory features, (ii)
Reconstructing their exercise feedback, while (iii) Embedding a personalized
forgetting module within the temporal workflow to dynamically modulate memory
storage strength. This jointly models the complete encoding-storage-retrieval
cycle, significantly enhancing the model's perception capability for individual
differences. Extensive experiments on four public datasets demonstrate that our
proposed approach significantly outperforms state-of-the-art baselines.

</details>


### [137] [NeuroDx-LM: A Clinical Large-Scale Model for EEG-based Neurological Disorder Detection](https://arxiv.org/abs/2508.08124)
*Guanghao Jin,Yuan Liang,Yihan Ma,Jingpei Wu,Guoyang Liu*

Main category: cs.LG

TL;DR: NeuroDx-LM是一种新型大规模模型，专为基于EEG的神经障碍检测设计，通过选择性时频嵌入和渐进特征感知训练策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决EEG大规模模型在实际部署中面临的标记数据不足和临床性能不佳的问题。

Method: 采用选择性时频嵌入机制和两阶段渐进特征感知训练策略。

Result: 在CHB-MIT和Schizophrenia数据集上实现了最先进的检测性能。

Conclusion: NeuroDx-LM展示了EEG大规模模型在临床应用中的巨大潜力。

Abstract: Large-scale models pre-trained on Electroencephalography (EEG) have shown
promise in clinical applications such as neurological disorder detection.
However, the practical deployment of EEG-based large-scale models faces
critical challenges such as limited labeled EEG data and suboptimal performance
in clinical scenarios. To address these issues, we propose NeuroDx-LM, a novel
large-scale model specifically designed for detecting EEG-based neurological
disorders. Our key contributions include (i) a Selective Temporal-Frequency
Embedding mechanism that adaptively captures complex temporal and spectral
patterns in EEG signals; and (ii) a Progressive Feature-Aware Training strategy
that refines feature representation in a two-stage process. In the first stage,
our model learns the fundamental discriminative features of EEG activities; in
the second stage, the model further extracts more specialized fine-grained
features for accurate diagnostic performance. We evaluated NeuroDx-LM on the
CHB-MIT and Schizophrenia datasets, achieving state-of-the-art performance in
EEG-based seizure and schizophrenia detection, respectively. These results
demonstrate the great potential of EEG-based large-scale models to advance
clinical applicability. Our code is available at
https://github.com/LetItBe12345/NeuroDx-LM.

</details>


### [138] [OFAL: An Oracle-Free Active Learning Framework](https://arxiv.org/abs/2508.08126)
*Hadi Khorsand,Vahid Pourahmadi*

Main category: cs.LG

TL;DR: OFAL是一种无需标注数据的主动学习方案，通过神经网络不确定性生成信息性样本，提升模型准确性。


<details>
  <summary>Details</summary>
Motivation: 传统主动学习依赖标注数据，成本高且复杂，OFAL旨在无需标注数据的情况下提升学习效果。

Method: 1. 分离并量化不确定性，使用蒙特卡洛Dropout近似贝叶斯神经网络；2. 通过变分自编码器从置信样本生成不确定性样本；3. 与其他主动学习方法对比与整合。

Result: OFAL能够生成信息性样本，提升模型准确性。

Conclusion: OFAL为主动学习提供了一种无需标注数据的高效方案。

Abstract: In the active learning paradigm, using an oracle to label data has always
been a complex and expensive task, and with the emersion of large unlabeled
data pools, it would be highly beneficial If we could achieve better results
without relying on an oracle. This research introduces OFAL, an oracle-free
active learning scheme that utilizes neural network uncertainty. OFAL uses the
model's own uncertainty to transform highly confident unlabeled samples into
informative uncertain samples. First, we start with separating and quantifying
different parts of uncertainty and introduce Monte Carlo Dropouts as an
approximation of the Bayesian Neural Network model. Secondly, by adding a
variational autoencoder, we go on to generate new uncertain samples by stepping
toward the uncertain part of latent space starting from a confidence seed
sample. By generating these new informative samples, we can perform active
learning and enhance the model's accuracy. Lastly, we try to compare and
integrate our method with other widely used active learning sampling methods.

</details>


### [139] [MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation](https://arxiv.org/abs/2508.08137)
*Pravallika Abbineni,Saoud Aldowaish,Colin Liechty,Soroosh Noorzad,Ali Ghazizadeh,Morteza Fayazi*

Main category: cs.LG

TL;DR: MuaLLM是一种开源多模态大语言模型代理，用于电路设计辅助，结合混合检索增强生成框架和自适应向量数据库，显著提升文献分析和设计效率。


<details>
  <summary>Details</summary>
Motivation: 电路设计文献综述面临研究快速更新、数据表示不一致和设计目标复杂等挑战，需要高效工具辅助。

Method: 提出MuaLLM，采用Reason + Act工作流，支持多模态数据处理和动态检索，实现可扩展推理。

Result: MuaLLM在RAG-250和Reas-100数据集上分别达到90.1%召回率和86.8%准确率，成本降低10倍，速度提升1.6倍。

Conclusion: MuaLLM为电路设计提供高效、低成本的自动化辅助工具，克服传统方法的局限性。

Abstract: Conducting a comprehensive literature review is crucial for advancing circuit
design methodologies. However, the rapid influx of state-of-the-art research,
inconsistent data representation, and the complexity of optimizing circuit
design objectives make this task significantly challenging. In this paper, we
propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for
circuit design assistance that integrates a hybrid Retrieval-Augmented
Generation (RAG) framework with an adaptive vector database of circuit design
research papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason +
Act (ReAct) workflow for iterative reasoning, goal-setting, and multi-step
information retrieval. It functions as a question-answering design assistant,
capable of interpreting complex queries and providing reasoned responses
grounded in circuit literature. Its multimodal capabilities enable processing
of both textual and visual data, facilitating more efficient and comprehensive
analysis. The system dynamically adapts using intelligent search tools,
automated document retrieval from the internet, and real-time database updates.
Unlike conventional approaches constrained by model context limits, MuaLLM
decouples retrieval from inference, enabling scalable reasoning over
arbitrarily large corpora. At the maximum context length supported by standard
LLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining
the same accuracy. This allows rapid, no-human-in-the-loop database generation,
overcoming the bottleneck of simulation-based dataset creation for circuits. To
evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval
and citation performance, and Reasoning-100 (Reas-100), focused on multistep
reasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8%
accuracy on Reas-100.

</details>


### [140] [FairFLRep: Fairness aware fault localization and repair of Deep Neural Networks](https://arxiv.org/abs/2508.08151)
*Moses Openja,Paolo Arcaini,Foutse Khomh,Fuyuki Ishikawa*

Main category: cs.LG

TL;DR: FairFLRep是一种自动化的公平性感知故障定位与修复技术，用于识别和修正DNN分类器中可能导致偏见的神经元，提升公平性同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: DNN在高风险决策应用中可能放大数据偏见，导致不公平行为，现有方法难以有效识别和修正这些偏见。

Method: 通过调整与敏感属性相关的神经元权重，分析网络输入输出关系，修正导致预测质量差异的神经元。

Result: 在多个数据集和模型上，FairFLRep在提升公平性方面优于现有方法，且效率更高。

Conclusion: FairFLRep证明了在故障定位和修复阶段考虑公平性的重要性，是一种高效的偏见修正方法。

Abstract: Deep neural networks (DNNs) are being utilized in various aspects of our
daily lives, including high-stakes decision-making applications that impact
individuals. However, these systems reflect and amplify bias from the data used
during training and testing, potentially resulting in biased behavior and
inaccurate decisions. For instance, having different misclassification rates
between white and black sub-populations. However, effectively and efficiently
identifying and correcting biased behavior in DNNs is a challenge. This paper
introduces FairFLRep, an automated fairness-aware fault localization and repair
technique that identifies and corrects potentially bias-inducing neurons in DNN
classifiers. FairFLRep focuses on adjusting neuron weights associated with
sensitive attributes, such as race or gender, that contribute to unfair
decisions. By analyzing the input-output relationships within the network,
FairFLRep corrects neurons responsible for disparities in predictive quality
parity. We evaluate FairFLRep on four image classification datasets using two
DNN classifiers, and four tabular datasets with a DNN model. The results show
that FairFLRep consistently outperforms existing methods in improving fairness
while preserving accuracy. An ablation study confirms the importance of
considering fairness during both fault localization and repair stages. Our
findings also show that FairFLRep is more efficient than the baseline
approaches in repairing the network.

</details>


### [141] [Federated Learning for Epileptic Seizure Prediction Across Heterogeneous EEG Datasets](https://arxiv.org/abs/2508.08159)
*Cem Ata Baykara,Saurav Raj Pandey,Ali Burak Ünal,Harlin Lee,Mete Akgün*

Main category: cs.LG

TL;DR: 论文提出了一种基于联邦学习的癫痫发作预测方法，通过随机子集聚合策略解决数据异质性问题，显著提升了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于患者隐私法规和数据异质性（非独立同分布特性），开发跨临床站点的癫痫发作预测模型面临挑战。联邦学习提供了一种隐私保护框架，但标准方法在异质数据下可能产生偏差。

Method: 采用隐私保护的全局归一化和随机子集聚合策略，每个客户端每轮训练固定大小的随机数据子集，确保聚合时的平等贡献。

Result: 随机子集聚合显著提高了代表性不足客户端的性能（赫尔辛基数据集准确率提升至81.7%，NCH提升至68.7%），全局宏平均准确率达77.1%。

Conclusion: 研究表明，平衡的联邦学习方法在保护隐私的同时，能在异质多医院环境中构建更鲁棒和公平的癫痫发作预测系统。

Abstract: Developing accurate and generalizable epileptic seizure prediction models
from electroencephalography (EEG) data across multiple clinical sites is
hindered by patient privacy regulations and significant data heterogeneity
(non-IID characteristics). Federated Learning (FL) offers a privacy-preserving
framework for collaborative training, but standard aggregation methods like
Federated Averaging (FedAvg) can be biased by dominant datasets in
heterogeneous settings. This paper investigates FL for seizure prediction using
a single EEG channel across four diverse public datasets (Siena, CHB-MIT,
Helsinki, NCH), representing distinct patient populations (adult, pediatric,
neonate) and recording conditions. We implement privacy-preserving global
normalization and propose a Random Subset Aggregation strategy, where each
client trains on a fixed-size random subset of its data per round, ensuring
equal contribution during aggregation. Our results show that locally trained
models fail to generalize across sites, and standard weighted FedAvg yields
highly skewed performance (e.g., 89.0% accuracy on CHB-MIT but only 50.8% on
Helsinki and 50.6% on NCH). In contrast, Random Subset Aggregation
significantly improves performance on under-represented clients (accuracy
increases to 81.7% on Helsinki and 68.7% on NCH) and achieves a superior
macro-average accuracy of 77.1% and pooled accuracy of 80.0% across all sites,
demonstrating a more robust and fair global model. This work highlights the
potential of balanced FL approaches for building effective and generalizable
seizure prediction systems in realistic, heterogeneous multi-hospital
environments while respecting data privacy.

</details>


### [142] [Neural Logic Networks for Interpretable Classification](https://arxiv.org/abs/2508.08172)
*Vincent Perreault,Katsumi Inoue,Richard Labib,Alain Hertz*

Main category: cs.LG

TL;DR: 论文提出了一种可解释的神经网络（Neural Logic Networks），通过引入NOT操作和偏置项，结合逻辑与概率建模，改进了布尔网络的发现方法，并在医学分类任务中展示了其价值。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络虽然分类性能强，但缺乏可解释性。Neural Logic Networks通过逻辑机制（AND、OR操作）提供可解释性，但需要进一步扩展以处理未观测数据和更复杂的逻辑关系。

Method: 论文扩展了Neural Logic Networks，引入NOT操作和偏置项，提出了一种因子化的IF-THEN规则结构，并改进了学习算法。

Result: 该方法在布尔网络发现任务中达到最优性能，并在医学分类任务中学习到相关且可解释的规则。

Conclusion: 通过逻辑与概率建模的结合，论文提出的方法在保持高性能的同时增强了模型的可解释性，特别适用于需要透明决策的领域（如医学）。

Abstract: Traditional neural networks have an impressive classification performance,
but what they learn cannot be inspected, verified or extracted. Neural Logic
Networks on the other hand have an interpretable structure that enables them to
learn a logical mechanism relating the inputs and outputs with AND and OR
operations. We generalize these networks with NOT operations and biases that
take into account unobserved data and develop a rigorous logical and
probabilistic modeling in terms of concept combinations to motivate their use.
We also propose a novel factorized IF-THEN rule structure for the model as well
as a modified learning algorithm. Our method improves the state-of-the-art in
Boolean networks discovery and is able to learn relevant, interpretable rules
in tabular classification, notably on an example from the medical field where
interpretability has tangible value.

</details>


### [143] [Cross-Subject and Cross-Montage EEG Transfer Learning via Individual Tangent Space Alignment and Spatial-Riemannian Feature Fusion](https://arxiv.org/abs/2508.08216)
*Nicole Lai-Tan,Xiao Gu,Marios G. Philiastides,Fani Deligianni*

Main category: cs.LG

TL;DR: 论文提出了一种名为ITSA的新方法，通过结合个体特定的对齐策略和混合架构，提升了脑机接口在跨个体音乐干预中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 脑机接口在个性化音乐干预中具有潜力，但个体间EEG信号的差异和运动伪影等问题限制了其泛化能力。

Method: 提出ITSA方法，结合个体特定的重新中心化、分布匹配和监督旋转对齐，并采用混合架构融合RCSP和黎曼几何。

Result: ITSA在跨个体验证中表现显著优于基线方法，并行融合架构效果最佳。

Conclusion: ITSA为脑机接口在康复干预中的泛化提供了有效解决方案，代码将公开。

Abstract: Personalised music-based interventions offer a powerful means of supporting
motor rehabilitation by dynamically tailoring auditory stimuli to provide
external timekeeping cues, modulate affective states, and stabilise gait
patterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise for
adapting these interventions across individuals. However, inter-subject
variability in EEG signals, further compounded by movement-induced artefacts
and motor planning differences, hinders the generalisability of BCIs and
results in lengthy calibration processes. We propose Individual Tangent Space
Alignment (ITSA), a novel pre-alignment strategy incorporating subject-specific
recentering, distribution matching, and supervised rotational alignment to
enhance cross-subject generalisation. Our hybrid architecture fuses Regularised
Common Spatial Patterns (RCSP) with Riemannian geometry in parallel and
sequential configurations, improving class separability while maintaining the
geometric structure of covariance matrices for robust statistical computation.
Using leave-one-subject-out cross-validation, `ITSA' demonstrates significant
performance improvements across subjects and conditions. The parallel fusion
approach shows the greatest enhancement over its sequential counterpart, with
robust performance maintained across varying data conditions and electrode
configurations. The code will be made publicly available at the time of
publication.

</details>


### [144] [Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning](https://arxiv.org/abs/2508.08221)
*Zihe Liu,Jiashun Liu,Yancheng He,Weixun Wang,Jiaheng Liu,Ling Pan,Xinyu Hu,Shaopan Xiong,Ju Huang,Jian Hu,Shengyi Huang,Siran Yang,Jiamang Wang,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: 本文系统回顾了LLM推理中的强化学习技术，通过统一框架下的实验分析，提出了选择技术的指南，并发现了一种简单的组合方法优于现有策略。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在LLM推理领域发展迅速，但缺乏标准化指南和对其机制的深入理解，导致实践中的混乱。

Method: 通过统一框架下的严格复现和孤立评估，分析不同技术的内部机制、适用场景和核心原则。

Result: 研究发现，两种技术的简单组合可以显著提升性能，优于现有策略。

Conclusion: 本文为LLM领域的强化学习提供了清晰的指南和可靠的技术选择路线图。

Abstract: Reinforcement learning for LLM reasoning has rapidly emerged as a prominent
research area, marked by a significant surge in related studies on both
algorithmic innovations and practical applications. Despite this progress,
several critical challenges remain, including the absence of standardized
guidelines for employing RL techniques and a fragmented understanding of their
underlying mechanisms. Additionally, inconsistent experimental settings,
variations in training data, and differences in model initialization have led
to conflicting conclusions, obscuring the key characteristics of these
techniques and creating confusion among practitioners when selecting
appropriate techniques. This paper systematically reviews widely adopted RL
techniques through rigorous reproductions and isolated evaluations within a
unified open-source framework. We analyze the internal mechanisms, applicable
scenarios, and core principles of each technique through fine-grained
experiments, including datasets of varying difficulty, model sizes, and
architectures. Based on these insights, we present clear guidelines for
selecting RL techniques tailored to specific setups, and provide a reliable
roadmap for practitioners navigating the RL for the LLM domain. Finally, we
reveal that a minimalist combination of two techniques can unlock the learning
capability of critic-free policies using vanilla PPO loss. The results
demonstrate that our simple combination consistently improves performance,
surpassing strategies like GRPO and DAPO.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [145] [Generative Bid Shading in Real-Time Bidding Advertising](https://arxiv.org/abs/2508.06550)
*Yinqiu Huang,Hao Ma,Wenshuai Chen,Shuli Wang,Yongqiang Zhang,Xue Wei,Yinhua Zhu,Haitao Wang,Xingxing Wang*

Main category: cs.GT

TL;DR: 论文提出了一种名为GBS的生成式投标调整方法，通过端到端生成模型和奖励偏好对齐系统，解决了现有投标调整方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有投标调整方法受限于单峰假设和离散化模型的依赖性问题，且存在样本选择偏差，导致预测和优化效果不佳。

Method: GBS包括两部分：(1) 端到端生成模型，逐步生成调整比例；(2) 奖励偏好对齐系统，结合CHNet优化短期和长期收益。

Result: 离线和在线A/B测试验证了GBS的有效性，并已在美团DSP平台部署，每日处理数十亿投标请求。

Conclusion: GBS通过生成模型和奖励优化，显著提升了投标调整的适应性和效果。

Abstract: Bid shading plays a crucial role in Real-Time Bidding~(RTB) by adaptively
adjusting the bid to avoid advertisers overspending. Existing mainstream
two-stage methods, which first model bid landscapes and then optimize surplus
using operations research techniques, are constrained by unimodal assumptions
that fail to adapt for non-convex surplus curves and are vulnerable to
cascading errors in sequential workflows. Additionally, existing discretization
models of continuous values ignore the dependence between discrete intervals,
reducing the model's error correction ability, while sample selection bias in
bidding scenarios presents further challenges for prediction. To address these
issues, this paper introduces Generative Bid Shading~(GBS), which comprises two
primary components: (1) an end-to-end generative model that utilizes an
autoregressive approach to generate shading ratios by stepwise residuals,
capturing complex value dependencies without relying on predefined priors; and
(2) a reward preference alignment system, which incorporates a channel-aware
hierarchical dynamic network~(CHNet) as the reward model to extract
fine-grained features, along with modules for surplus optimization and
exploration utility reward alignment, ultimately optimizing both short-term and
long-term surplus using group relative policy optimization~(GRPO). Extensive
experiments on both offline and online A/B tests validate GBS's effectiveness.
Moreover, GBS has been deployed on the Meituan DSP platform, serving billions
of bid requests daily.

</details>


### [146] [Algorithmic Delegated Choice: An Annotated Reading List](https://arxiv.org/abs/2508.06562)
*Mohammad T. Hajiaghayi,Suho Shin*

Main category: cs.GT

TL;DR: 综述了委托选择问题的经典和近期算法视角研究。


<details>
  <summary>Details</summary>
Motivation: 委托选择问题在经济学和计算机科学中具有长期重要性，需要系统梳理相关研究。

Method: 通过文献综述方法，分析经典和近期关于委托选择问题的论文。

Result: 提供了委托选择问题的全面概述，涵盖经济学和计算机科学的视角。

Conclusion: 委托选择问题研究具有跨学科价值，未来可进一步探索算法与经济学结合的方向。

Abstract: The problem of delegated choice has been of long interest in economics and
recently on computer science. We overview a list of papers on delegated choice
problem, from classic works to recent papers with algorithmic perspectives.

</details>


### [147] [Asymmetric Network Games: $α$-Potential Function and Learning](https://arxiv.org/abs/2508.06619)
*Kiran Rokade,Adit Jain,Francesca Parise,Vikram Krishnamurthy,Eva Tardos*

Main category: cs.GT

TL;DR: 该论文研究了网络游戏中的静态博弈，提出了α-势函数的概念，并证明了两种改进算法能收敛到2α-纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 现实中的网络通常是非对称且包含大量异质玩家，需要一种理论框架来分析此类网络游戏。

Method: 使用α-势博弈框架，在玩家行动集和效用函数的温和假设下，推导了α-势函数，并提出了两种改进算法。

Result: 证明了算法能收敛到2α-纳什均衡，且α与网络的最大非对称性相关。线性二次网络游戏中，α表现良好。

Conclusion: 通过数值实验验证了算法的收敛性和2α-纳什均衡的性质，为网络博弈提供了理论支持。

Abstract: In a network game, players interact over a network and the utility of each
player depends on his own action and on an aggregate of his neighbours'
actions. Many real world networks of interest are asymmetric and involve a
large number of heterogeneous players. This paper analyzes static network games
using the framework of $\alpha$-potential games. Under mild assumptions on the
action sets (compact intervals) and the utility functions (twice continuously
differentiable) of the players, we derive an expression for an inexact
potential function of the game, called the $\alpha$-potential function. Using
such a function, we show that modified versions of the sequential best-response
algorithm and the simultaneous gradient play algorithm achieve convergence of
players' actions to a $2\alpha$-Nash equilibrium. For linear-quadratic network
games, we show that $\alpha$ depends on the maximum asymmetry in the network
and is well-behaved for a wide range of networks of practical interest.
Further, we derive bounds on the social welfare of the $\alpha$-Nash
equilibrium corresponding to the maximum of the $\alpha$-potential function,
under suitable assumptions. We numerically illustrate the convergence of the
proposed algorithms and properties of the learned $2\alpha$-Nash equilibria.

</details>


### [148] [Convergence of Fast Policy Iteration in Markov Games and Robust MDPs](https://arxiv.org/abs/2508.06661)
*Keith Badger,Marek Petrik,Jefferson Huang*

Main category: cs.GT

TL;DR: 论文分析了Filar-Tolwinski（FT）算法的收敛性问题，并提出了一种改进算法RCPI，确保收敛且性能显著优于其他算法。


<details>
  <summary>Details</summary>
Motivation: 研究FT算法的收敛性问题，并提出一种更可靠的替代算法。

Method: 提出Residual Conditioned Policy Iteration（RCPI），基于FT算法但确保收敛。

Result: RCPI在数值实验中表现优于其他收敛算法多个数量级。

Conclusion: RCPI是一种高效且可靠的替代算法，解决了FT的收敛问题。

Abstract: Markov games and robust MDPs are closely related models that involve
computing a pair of saddle point policies. As part of the long-standing effort
to develop efficient algorithms for these models, the Filar-Tolwinski (FT)
algorithm has shown considerable promise. As our first contribution, we
demonstrate that FT may fail to converge to a saddle point and may loop
indefinitely, even in small games. This observation contradicts the proof of
FT's convergence to a saddle point in the original paper. As our second
contribution, we propose Residual Conditioned Policy Iteration (RCPI). RCPI
builds on FT, but is guaranteed to converge to a saddle point. Our numerical
results show that RCPI outperforms other convergent algorithms by several
orders of magnitude.

</details>


### [149] [Emergence of Cooperation and Commitment in Optional Prisoner's Dilemma](https://arxiv.org/abs/2508.06702)
*Zhao Song,The Anh Han*

Main category: cs.GT

TL;DR: 研究探讨了在自愿参与的囚徒困境游戏中，承诺机制对合作行为的影响，并比较了两种制度激励的效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了玩家自愿退出的自由，限制了承诺机制在现实场景中的应用。

Method: 采用两阶段博弈模型，分析承诺接受和合作行为，并引入严格和灵活的制度激励进行比较。

Result: 自愿参与提高了承诺接受率但未能促进合作，灵活激励在特定情况下能提升社会福利。

Conclusion: 仅依赖自愿参与和承诺不足以解决社会困境，需设计合理的制度激励以促进合作和社会福利。

Abstract: Commitment is a well-established mechanism for fostering cooperation in human
society and multi-agent systems. However, existing research has predominantly
focused on the commitment that neglects the freedom of players to abstain from
an interaction, limiting their applicability to many real-world scenarios where
participation is often voluntary. In this paper, we present a two-stage game
model to investigate the evolution of commitment-based behaviours and
cooperation within the framework of the optional Prisoner's Dilemma game. In
the pre-game stage, players decide whether to accept a mutual commitment. Once
in the game, they choose among cooperation, defection, or exiting, depending on
the formation of a pre-game commitment. We find that optional participation
boosts commitment acceptance but fails to foster cooperation, leading instead
to widespread exit behaviour. To address this, we then introduce and compare
two institutional incentive approaches: i) a strict one (STRICT-COM) that
rewards only committed players who cooperate in the game, and ii) a flexible
one (FLEXIBLE-COM) that rewards any committed players who do not defect in the
game. The results reveal that, while the strict approach is demonstrably better
for promoting cooperation as the flexible rule creates a loophole for an
opportunistic exit after committing, the flexible rule offers an efficient
alternative for enhancing social welfare when such opportunistic behaviour
results in a high gain. This study highlights the limitations of relying solely
on voluntary participation and commitment to resolving social dilemmas,
emphasising the importance of well-designed institutional incentives to promote
cooperation and social welfare effectively.

</details>


### [150] [When Competition Helps: Achieving Optimal Traffic Flow with Multiple Autonomous Planners](https://arxiv.org/abs/2508.07145)
*Ivan Geffner,Erez Karpas,Moshe Tennenholtz*

Main category: cs.GT

TL;DR: 研究表明，竞争是实现交通路由最优化的关键因素，而非障碍。


<details>
  <summary>Details</summary>
Motivation: 探讨自动驾驶车辆是否能通过中央规划消除自私路由的低效问题，同时考虑个体合理性和多代理竞争。

Method: 设计了一种路由机制，从经典的Pigou网络出发，利用竞争实现最优分配。

Result: 竞争是实现最优交通分配的必要条件。

Conclusion: 竞争是优化交通路由的关键，而非阻碍。

Abstract: The inefficiency of selfish routing in congested networks is a classical
problem in algorithmic game theory, often captured by the Price of Anarchy
(i.e., the ratio between the social cost of decentralized decisions and that of
a centrally optimized solution.) With the advent of autonomous vehicles,
capable of receiving and executing centrally assigned routes, it is natural to
ask whether their deployment can eliminate this inefficiency. At first glance,
a central authority could simply compute an optimal traffic assignment and
instruct each vehicle to follow its assigned path. However, this vision
overlooks critical challenges: routes must be individually rational (no vehicle
has an incentive to deviate), and in practice, multiple planning agents (e.g.,
different companies) may coexist and compete. Surprisingly, we show that such
competition is not merely an obstacle but a necessary ingredient for achieving
optimal outcomes. In this work, we design a routing mechanism that embraces
competition and converges to an optimal assignment, starting from the classical
Pigou network as a foundational case.

</details>


### [151] [Maximizing Social Welfare with Side Payments](https://arxiv.org/abs/2508.07147)
*Ivan Geffner,Caspar Oesterheld,Vincent Conitzer*

Main category: cs.GT

TL;DR: 论文研究了允许玩家在行动前预先承诺结果相关转移的正常形式博弈，提出了一种分阶段承诺协议以避免效率损失。


<details>
  <summary>Details</summary>
Motivation: 解决Jackson和Wilkie指出的无限制同时承诺可能导致博弈效率下降的问题。

Method: 引入分阶段承诺协议，玩家只能以小额、有上限的增量在多轮中承诺转移，且需全体同意才能继续。

Result: 证明该协议能从任何有限博弈的非退化纳什均衡出发，实现所有严格帕累托改进的福利最大化支付配置。

Conclusion: 分阶段和有界承诺恢复了侧支付的效率潜力，避免了效率损失。

Abstract: We examine normal-form games in which players may \emph{pre-commit} to
outcome-contingent transfers before choosing their actions. In the one-shot
version of this model, Jackson and Wilkie showed that side contracting can
backfire: even a game with a Pareto-optimal Nash equilibrium can devolve into
inefficient equilibria once unbounded, simultaneous commitments are allowed.
The root cause is a prisoner's dilemma effect, where each player can exploit
her commitment power to reshape the equilibrium in her favor, harming overall
welfare.
  To circumvent this problem we introduce a \emph{staged-commitment} protocol.
Players may pledge transfers only in small, capped increments over multiple
rounds, and the phase continues only with unanimous consent. We prove that,
starting from any finite game $\Gamma$ with a non-degenerate Nash equilibrium
$\vec{\sigma}$, this protocol implements every welfare-maximizing payoff
profile that \emph{strictly} Pareto-improves $\vec{\sigma}$. Thus, gradual and
bounded commitments restore the full efficiency potential of side payments
while avoiding the inefficiencies identified by Jackson and Wilkie.

</details>


### [152] [Last-Iterate Convergence in Adaptive Regret Minimization for Approximate Extensive-Form Perfect Equilibrium](https://arxiv.org/abs/2508.07699)
*Hang Ren,Xiaozhen Sun,Tianzi Ma,Jiajia Zhang,Xuan Wang*

Main category: cs.GT

TL;DR: 提出了一种自适应后悔最小化算法（RTCFR）来计算近似EFPE，解决了现有EFPE算法的高计算成本和固定扰动问题。


<details>
  <summary>Details</summary>
Motivation: Nash均衡在非完美信息扩展形式博弈中无法保证非均衡分支的最优策略，EFPE通过扰动改进但现有算法存在计算成本高和固定扰动问题。

Method: 采用奖励转换反事实后悔最小化（RTCFR）解决扰动博弈，并引入信息集Nash均衡（ISNE）动态调整扰动。

Result: 理论分析证实收敛到EFPE，实验显示在NE和EFPE任务中显著优于现有算法。

Conclusion: RTCFR算法高效且适应性强的EFPE求解方法，解决了现有算法的局限性。

Abstract: The Nash Equilibrium (NE) assumes rational play in imperfect-information
Extensive-Form Games (EFGs) but fails to ensure optimal strategies for
off-equilibrium branches of the game tree, potentially leading to suboptimal
outcomes in practical settings. To address this, the Extensive-Form Perfect
Equilibrium (EFPE), a refinement of NE, introduces controlled perturbations to
model potential player errors. However, existing EFPE-finding algorithms, which
typically rely on average strategy convergence and fixed perturbations, face
significant limitations: computing average strategies incurs high computational
costs and approximation errors, while fixed perturbations create a trade-off
between NE approximation accuracy and the convergence rate of NE refinements.
  To tackle these challenges, we propose an efficient adaptive regret
minimization algorithm for computing approximate EFPE, achieving last-iterate
convergence in two-player zero-sum EFGs. Our approach introduces Reward
Transformation Counterfactual Regret Minimization (RTCFR) to solve perturbed
games and defines a novel metric, the Information Set Nash Equilibrium (ISNE),
to dynamically adjust perturbations. Theoretical analysis confirms convergence
to EFPE, and experimental results demonstrate that our method significantly
outperforms state-of-the-art algorithms in both NE and EFPE-finding tasks.

</details>


### [153] [Truthful Two-Obnoxious-Facility Location Games with Optional Preferences and Minimum Distance Constraint](https://arxiv.org/abs/2508.08036)
*Xiaojia Han,Wenjing Liu,Qizhi Fang*

Main category: cs.GT

TL;DR: 研究了在最小距离约束下的两厌恶设施选址问题，提出了确定性和随机性策略证明机制，并给出了近似比和下限。


<details>
  <summary>Details</summary>
Motivation: 解决代理人在报告私人位置时的真实性问题，同时最大化社会效用。

Method: 针对d=0和一般情况，分别提出确定性和随机性策略证明机制。

Result: 确定性机制近似比≤4（d=0）和≤8（一般），随机性机制近似比≤2（d=0）和≤4（一般）。下限分别为2和14/13。

Conclusion: 提出的机制在保证真实性的同时，有效优化了社会效用。

Abstract: In this paper, we study a truthful two-obnoxious-facility location problem,
in which each agent has a private location in [0, 1] and a public optional
preference over two obnoxious facilities, and there is a minimum distance
constraint d between the two facilities. Each agent wants to be as far away as
possible from the facilities that affect her, and the utility of each agent is
the total distance from her to these facilities. The goal is to decide how to
place the facilities in [0, 1] so as to incentivize agents to report their
private locations truthfully as well as maximize the social utility. First, we
consider the special setting where d = 0, that is, the two facilities can be
located at any point in [0, 1]. We propose a deterministic strategyproof
mechanism with approximation ratio of at most 4 and a randomized strategyproof
mechanism with approximation ratio of at most 2, respectively. Then we study
the general setting. We propose a deterministic strategyproof mechanism with
approximation ratio of at most 8 and a randomized strategyproof mechanism with
approximation ratio of at most 4, respectively. Furthermore, we provide lower
bounds of 2 and 14/13 on the approximation ratio for any deterministic and any
randomized strategyproof mechanism, respectively.

</details>


### [154] [Constrained Distributed Heterogeneous Two-Facility Location Problems with Max-Variant Cost](https://arxiv.org/abs/2508.08045)
*Xinru Xu,Wenjing Liu,Qizhi Fang*

Main category: cs.GT

TL;DR: 研究了分布式异构双设施选址问题，约束条件下设计策略证明机制以优化社会目标。


<details>
  <summary>Details</summary>
Motivation: 解决在候选位置有限且分组约束下，如何激励代理真实报告位置并优化社会目标的问题。

Method: 提出分布式机制，分组选择代表候选位置，分析确定性策略证明机制的失真界限。

Result: 在四种社会目标下，获得了恒定的失真上下界。

Conclusion: 证明了分布式机制在约束条件下能有效优化社会目标。

Abstract: We study a constrained distributed heterogeneous two-facility location
problem, where a set of agents with private locations on the real line are
divided into disjoint groups. The constraint means that the facilities can only
be built in a given multiset of candidate locations and at most one facility
can be built at each candidate location. Given the locations of the two
facilities, the cost of an agent is the distance from her location to the
farthest facility (referred to as max-variant). Our goal is to design
strategyproof distributed mechanisms that can incentivize all agents to
truthfully report their locations and approximately optimize some social
objective. A distributed mechanism consists of two steps: for each group, the
mechanism chooses two candidate locations as the representatives of the group
based only on the locations reported by agents therein; then, it outputs two
facility locations among all the representatives. We focus on a class of
deterministic strategyproof distributed mechanisms and analyze upper and lower
bounds on the distortion under the Average-of-Average cost (average of the
average individual cost of agents in each group), the Max-of-Max cost (maximum
individual cost among all agents), the Average-of-Max cost (average of the
maximum individual cost among all agents in each group) and the Max-of-Average
cost (maximum of the average individual cost of all agents in each group).
Under four social objectives, we obtain constant upper and lower distortion
bounds.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [155] [Omni Geometry Representation Learning vs Large Language Models for Geospatial Entity Resolution](https://arxiv.org/abs/2508.06584)
*Kalana Wijegunarathna,Kristin Stock,Christopher B. Jones*

Main category: cs.DB

TL;DR: 论文提出了一种名为Omni的地理空间实体解析模型，通过全几何编码器处理多种几何形状，并结合预训练语言模型提升匹配效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多样化几何形状的地理实体时存在信息丢失问题，缺乏统一的神经网络嵌入技术。

Method: Omni模型采用全几何编码器处理点、线、多边形等几何形状，并结合基于Transformer的预训练语言模型分析文本属性。

Result: Omni在现有数据集上F1分数提升12%，且实验表明大型语言模型（LLMs）在实体解析任务中表现竞争力。

Conclusion: Omni模型显著提升了地理空间实体解析的准确性，同时LLMs展示了潜在的应用前景。

Abstract: The development, integration, and maintenance of geospatial databases rely
heavily on efficient and accurate matching procedures of Geospatial Entity
Resolution (ER). While resolution of points-of-interest (POIs) has been widely
addressed, resolution of entities with diverse geometries has been largely
overlooked. This is partly due to the lack of a uniform technique for embedding
heterogeneous geometries seamlessly into a neural network framework. Existing
neural approaches simplify complex geometries to a single point, resulting in
significant loss of spatial information. To address this limitation, we propose
Omni, a geospatial ER model featuring an omni-geometry encoder. This encoder is
capable of embedding point, line, polyline, polygon, and multi-polygon
geometries, enabling the model to capture the complex geospatial intricacies of
the places being compared. Furthermore, Omni leverages transformer-based
pre-trained language models over individual textual attributes of place records
in an Attribute Affinity mechanism. The model is rigorously tested on existing
point-only datasets and a new diverse-geometry geospatial ER dataset. Omni
produces up to 12% (F1) improvement over existing methods.
  Furthermore, we test the potential of Large Language Models (LLMs) to conduct
geospatial ER, experimenting with prompting strategies and learning scenarios,
comparing the results of pre-trained language model-based methods with LLMs.
Results indicate that LLMs show competitive results.

</details>


### [156] [Metadata Management for AI-Augmented Data Workflows](https://arxiv.org/abs/2508.06814)
*Jinjin Zhao,Sanjay Krishnan*

Main category: cs.DB

TL;DR: TableVault是一个用于人-AI协作数据创建的元数据治理框架，支持透明性和可重复性。


<details>
  <summary>Details</summary>
Motivation: AI增强的数据工作流带来复杂的治理挑战，需要解决异构工具、动态执行模式和不透明模型决策的问题。

Method: TableVault框架记录数据摄取事件、跟踪操作状态、链接执行参数到数据起源，并提供标准化元数据层。

Result: 通过文档分类案例研究，TableVault展示了其在部分可观察环境中仍能保持详细的数据沿袭和操作上下文。

Conclusion: TableVault结合数据库保证和AI设计，支持混合人-模型管道的透明性和可重复性。

Abstract: AI-augmented data workflows introduce complex governance challenges, as both
human and model-driven processes generate, transform, and consume data
artifacts. These workflows blend heterogeneous tools, dynamic execution
patterns, and opaque model decisions, making comprehensive metadata capture
difficult. In this work, we present TableVault, a metadata governance framework
designed for human-AI collaborative data creation. TableVault records ingestion
events, traces operation status, links execution parameters to their data
origins, and exposes a standardized metadata layer. By combining
database-inspired guarantees with AI-oriented design, such as declarative
operation builders and lineage-aware references, TableVault supports
transparency and reproducibility across mixed human-model pipelines. Through a
document classification case study, we demonstrate how TableVault preserves
detailed lineage and operational context, enabling robust metadata management,
even in partially observable execution environments.

</details>


### [157] [Balancing Privacy and Efficiency: Music Information Retrieval via Additive Homomorphic Encryption](https://arxiv.org/abs/2508.07044)
*William Zerong Wang,Dongfang Zhao*

Main category: cs.DB

TL;DR: 论文提出了一种基于加法同态加密（AHE）的音乐嵌入向量隐私保护方法，解决了传统加密和全同态加密（FHE）在音乐数据隐私保护中的不足。


<details>
  <summary>Details</summary>
Motivation: 音乐数据的时序性和多模态特性使其向量嵌入容易被模型学习或滥用，传统保护方法如版权许可和数字水印效果有限，需要更强的加密手段。

Method: 通过分析音乐信息检索系统的威胁模型，提出了一种基于AHE的向量相似性搜索方法，利用音乐嵌入的内积实现隐私保护。

Result: 实验证明该方法在真实MP3文件上高效且实用，性能优于FHE方案。

Conclusion: AHE为音乐嵌入的隐私保护提供了一种可行的解决方案，平衡了安全性和计算效率。

Abstract: In the era of generative AI, ensuring the privacy of music data presents
unique challenges: unlike static artworks such as images, music data is
inherently temporal and multimodal, and it is sampled, transformed, and remixed
at an unprecedented scale. These characteristics make its core vector
embeddings, i.e, the numerical representations of the music, highly susceptible
to being learned, misused, or even stolen by models without accessing the
original audio files. Traditional methods like copyright licensing and digital
watermarking offer limited protection for these abstract mathematical
representations, thus necessitating a stronger, e.g., cryptographic, approach
to safeguarding the embeddings themselves. Standard encryption schemes, such as
AES, render data unintelligible for computation, making such searches
impossible. While Fully Homomorphic Encryption (FHE) provides a plausible
solution by allowing arbitrary computations on ciphertexts, its substantial
performance overhead remains impractical for large-scale vector similarity
searches. Given this trade-off, we propose a more practical approach using
Additive Homomorphic Encryption (AHE) for vector similarity search. The primary
contributions of this paper are threefold: we analyze threat models unique to
music information retrieval systems; we provide a theoretical analysis and
propose an efficient AHE-based solution through inner products of music
embeddings to deliver privacy-preserving similarity search; and finally, we
demonstrate the efficiency and practicality of the proposed approach through
empirical evaluation and comparison to FHE schemes on real-world MP3 files.

</details>


### [158] [SQL-Exchange: Transforming SQL Queries Across Domains](https://arxiv.org/abs/2508.07087)
*Mohammadreza Daviran,Brian Lin,Davood Rafiei*

Main category: cs.DB

TL;DR: SQL-Exchange是一个框架，用于在不同数据库模式之间映射SQL查询，保留源查询结构的同时调整领域特定元素以匹配目标模式。研究探讨了这种映射的可行性和益处，并评估了其对文本到SQL系统上下文学习性能的影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决不同数据库模式间SQL查询的映射问题，以提升文本到SQL系统的性能。

Method: 提出SQL-Exchange框架，通过保留源查询结构并调整领域元素，实现跨模式查询映射。通过多模型和基准数据集评估其有效性。

Result: SQL-Exchange在多种模式和查询类型中表现有效，映射查询作为上下文示例显著提升了文本到SQL性能。

Conclusion: SQL-Exchange为跨模式SQL查询映射提供了有效解决方案，并验证了其在提升下游任务性能中的价值。

Abstract: We introduce SQL-Exchange, a framework for mapping SQL queries across
different database schemas by preserving the source query structure while
adapting domain-specific elements to align with the target schema. We
investigate the conditions under which such mappings are feasible and
beneficial, and examine their impact on enhancing the in-context learning
performance of text-to-SQL systems as a downstream task. Our comprehensive
evaluation across multiple model families and benchmark datasets--assessing
structural alignment with source queries, execution validity on target
databases, and semantic correctness--demonstrates that SQL-Exchange is
effective across a wide range of schemas and query types. Our results further
show that using mapped queries as in-context examples consistently improves
text-to-SQL performance over using queries from the source schema.

</details>


### [159] [Accelerating High-Dimensional Nearest Neighbor Search with Dynamic Query Preference](https://arxiv.org/abs/2508.07218)
*Yunjun Gao,Ruijie Zhao,Zhonggen Li,Baihua Zheng,Yifan Zhu,Zhaoqing Chen*

Main category: cs.DB

TL;DR: 提出了一种名为DQF的双层索引查询框架，通过动态搜索策略和决策树优化高频查询，显著提升了搜索速度，同时保持高召回率。


<details>
  <summary>Details</summary>
Motivation: 现有图基ANNS方法假设查询分布均匀，但实际场景中查询频率不均，需优化高频查询效率。

Method: 采用双层索引结构（热索引和全索引）和基于决策树的动态搜索策略，区分管理高频和低频查询。

Result: 在四个真实数据集上，DQF比现有算法快2.0-5.7倍，召回率保持95%，且无需重建索引。

Conclusion: DQF在动态查询分布场景中高效实用，显著提升了ANNS性能。

Abstract: Approximate Nearest Neighbor Search (ANNS) is a crucial operation in
databases and artificial intelligence. Current graph-based ANNS methods, such
as HNSW and NSG, have shown remarkable performance but are designed under the
assumption of a uniform query distribution. However, in practical scenarios,
user preferences and query temporal dynamics lead to some queries being
searched for more frequently than others. To fully utilize these
characteristics, we propose DQF, a novel Dual-Index Query Framework. This
framework comprises a dual-layer index structure and a dynamic search strategy
based on a decision tree. The dual-layer index structure comprises a hot index
for high-frequency nodes and a full index for the entire dataset, allowing for
the separate management of hot and cold queries. Furthermore, we propose a
dynamic search strategy that employs a decision tree to adapt to the specific
characteristics of each query. The decision tree evaluates whether a query is
of the high-frequency type to detect the opportunities for early termination on
the dual-layer, avoiding unnecessary searches in the full index. Experimental
results on four real-world datasets demonstrate that the Dual-Index Query
Framework achieves a significant speedup of 2.0-5.7x over state-of-the-art
algorithms while maintaining a 95% recall rate. Importantly, it does not
require full index reconstruction when query distributions change, underscoring
its efficiency and practicality in dynamic query distribution scenarios.

</details>


### [160] [RNA-KG v2.0: An RNA-centered Knowledge Graph with Properties](https://arxiv.org/abs/2508.07427)
*Emanuele Cavalleri,Paolo Perlasca,Marco Mesiti*

Main category: cs.DB

TL;DR: RNA-KG v2.0是一个集成了约1亿条手动整理交互的知识图谱，支持RNA分子分类、新交互预测及隐藏模式发现。


<details>
  <summary>Details</summary>
Motivation: 整合公开数据源中的编码和非编码RNA分子交互，为RNA研究提供更全面的资源。

Method: 从91个开放数据仓库和本体中整合交互数据，标准化属性描述上下文，并丰富节点属性。

Result: 构建了包含详细上下文信息的RNA-KG v2.0，支持高级查询和上下文感知的链接预测。

Conclusion: RNA-KG v2.0为RNA研究提供了更强大的工具，支持复杂查询和下游应用。

Abstract: RNA-KG is a recently developed knowledge graph that integrates the
interactions involving coding and non-coding RNA molecules extracted from
public data sources. It can be used to support the classification of new
molecules, identify new interactions through the use of link prediction
methods, and reveal hidden patterns among the represented entities. In this
paper, we propose RNA-KG v2.0, a new release of RNA-KG that integrates around
100M manually curated interactions sourced from 91 linked open data
repositories and ontologies. Relationships are characterized by standardized
properties that capture the specific context (e.g., cell line, tissue,
pathological state) in which they have been identified. In addition, the nodes
are enriched with detailed attributes, such as descriptions, synonyms, and
molecular sequences sourced from platforms such as OBO ontologies, NCBI
repositories, RNAcentral, and Ensembl. The enhanced repository enables the
expression of advanced queries that take into account the context in which the
experiments were conducted. It also supports downstream applications in RNA
research, including "context-aware" link prediction techniques that combine
both topological and semantic information.

</details>


### [161] [A Benchmark for Databases with Varying Value Lengths](https://arxiv.org/abs/2508.07551)
*Danushka Liyanage,Shubham Pandey,Joshua Goldstein,Michael Cahill,Akon Dey,Alan Fekete,Uwe Röhm*

Main category: cs.DB

TL;DR: 论文提出了一种新的基准测试方法，通过扩展YCSB基准以模拟记录动态增长，评估了三种DBMS在处理变长数据时的性能差异。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试未考虑记录长度的动态变化，而实际应用中变长数据可能导致性能异常，因此需要更真实的测试方法。

Method: 扩展YCSB基准，引入“extend”操作模拟记录增长，测试MongoDB和MariaDB（InnoDB、MyRocks）的性能。

Result: 实验显示不同存储引擎在处理变长数据时性能差异显著，揭示了动态数据对DBMS性能的影响。

Conclusion: 研究强调了动态基准测试的重要性，为实践和科研提供了更真实的性能评估工具。

Abstract: The performance of database management systems (DBMS) is traditionally
evaluated using benchmarks that focus on workloads with (almost) fixed record
lengths. However, some real-world workloads in key/value stores, document
databases, and graph databases exhibit significant variability in value
lengths, which can lead to performance anomalies, particularly when popular
records grow disproportionately large. Existing benchmarks fail to account for
this variability, leaving an important aspect of DBMS behavior underexplored.
  In this paper, we address this gap by extending the Yahoo! Cloud Serving
Benchmark (YCSB) to include an "extend" operation, which appends data to record
fields, simulating the growth of values over time. Using this modified
benchmark, we have measured the performance of three popular DBMS backends:
MongoDB, MariaDB with the InnoDB storage engine, and MariaDB with the MyRocks
storage engine. Our experiments alternate between extending values and
executing query workloads, revealing significant performance differences driven
by storage engine design and their handling of variable-sized values.
  Our key contribution is the introduction of a novel benchmarking approach to
evaluate the impact of growing value sizes and isolate the effect of querying
data with a distribution of data sizes from any cost associated with accessing
data after a history of updates. This highlights the need for more
representative benchmarks that capture the dynamic nature of real-world
workloads, providing valuable guidance for both practitioners and researchers.

</details>


### [162] [MLego: Interactive and Scalable Topic Exploration Through Model Reuse](https://arxiv.org/abs/2508.07654)
*Fei Ye,Jiapan Liu,Yinan Jing,Zhenying He,Weirao Wang,X. Sean Wang*

Main category: cs.DB

TL;DR: MLego是一个支持实时主题建模分析的交互式查询框架，通过模型物化和重用，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统主题建模技术（如LDA）计算成本高，难以满足实时数据分析需求。

Method: MLego利用物化主题模型的高效合并，结合分层计划搜索和查询重排序技术，实现实时交互。

Result: 实验表明，MLego在保持高质量主题建模结果的同时，显著降低了计算成本。

Conclusion: MLego填补了可扩展主题建模与交互式数据分析之间的空白，为实时查询驱动的探索提供了新方法。

Abstract: With massive texts on social media, users and analysts often rely on topic
modeling techniques to quickly extract key themes and gain insights.
Traditional topic modeling techniques, such as Latent Dirichlet Allocation
(LDA), provide valuable insights but are computationally expensive, making them
impractical for real-time data analysis. Although recent advances in
distributed training and fast sampling methods have improved efficiency,
real-time topic exploration remains a significant challenge. In this paper, we
present MLego, an interactive query framework designed to support real-time
topic modeling analysis by leveraging model materialization and reuse. Instead
of retraining models from scratch, MLego efficiently merges materialized topic
models to construct approximate results at interactive speeds. To further
enhance efficiency, we introduce a hierarchical plan search strategy for single
queries and an optimized query reordering technique for batch queries. We
integrate MLego into a visual analytics prototype system, enabling users to
explore large-scale textual datasets through interactive queries. Extensive
experiments demonstrate that MLego significantly reduces computation costs
while maintaining high-quality topic modeling results. MLego enhances existing
visual analytics approaches, which primarily focus on user-driven topic
modeling, by enabling real-time, query-driven exploration. This complements
traditional methods and bridges the gap between scalable topic modeling and
interactive data analysis.

</details>


### [163] [TQL: Towards Type-Driven Data Discovery](https://arxiv.org/abs/2508.08054)
*Andrew Kang,Sainyam Galhotra*

Main category: cs.DB

TL;DR: 论文提出了一种以用户需求为中心的查询语言TQL，结合编程语言研究成果，通过类型系统支持下游转换上下文。


<details>
  <summary>Details</summary>
Motivation: 现有数据发现查询语言过于系统驱动，忽视用户需求，需重新以用户为中心设计语言。

Method: 提出TQL语言，定义其语法、语义及评估模型，并实现原型，对比现有语言。

Result: TQL在表达能力和实际应用中优于现有数据发现语言。

Conclusion: TQL通过类型系统和用户中心设计，提升了数据发现查询的灵活性和实用性。

Abstract: Existing query languages for data discovery exhibit system-driven designs
that emphasize database features and functionality over user needs. We propose
a re-prioritization of the client through an introduction of a language-driven
approach to data discovery systems that can leverage powerful results from
programming languages research. In this paper, we describe TQL, a flexible and
practical query language which incorporates a type-like system to encompass
downstream transformation-context in its discovery queries. The syntax and
semantics of TQL (including the underlying evaluation model), are formally
defined, and a sketch of its implementation is also provided. Additionally, we
provide comparisons to existing languages for data retrieval and data discovery
to examine the advantages of TQL's expanded expressive power in real-life
settings.

</details>


### [164] [Towards General-Purpose Data Discovery: A Programming Languages Approach](https://arxiv.org/abs/2508.08074)
*Andrew Kang,Yashnil Saha,Sainyam Galhotra*

Main category: cs.DB

TL;DR: 论文提出了一种名为TQL的领域特定语言，用于高效数据发现，并基于编程语言研究成果设计了其语法和语义。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习和数据科学应用需要高效的数据发现工具，但缺乏通用的形式化语言和实现方法。

Method: 提出了TQL语言，并通过代数模型ImpRAT对其核心语言进行了形式化描述，实现了一个模块化的原型系统。

Result: TQL语言能够有效支持通用数据发现查询，并通过ImpRAT模型提供了理论基础。

Conclusion: TQL为数据发现领域提供了一种表达力强且实用的解决方案，具有潜在的应用价值。

Abstract: Efficient and effective data discovery is critical for many modern
applications in machine learning and data science. One major bottleneck to the
development of a general-purpose data discovery tool is the absence of an
expressive formal language, and corresponding implementation, for
characterizing and solving generic discovery queries. To this end, we present
TQL, a domain-specific language for data discovery well-designed to leverage
and exploit the results of programming languages research in both its syntax
and semantics. In this paper, we fully and formally characterize the core
language through an algebraic model, Imperative Relational Algebra with Types
(ImpRAT), and implement a modular proof-of-concept system prototype.

</details>


### [165] [Heterogeneity in Entity Matching: A Survey and Experimental Analysis](https://arxiv.org/abs/2508.08076)
*Mohammad Hossein Moslemi,Amir Mousavi,Behshid Behkamal,Mostafa Milani*

Main category: cs.DB

TL;DR: 该论文综述了异构实体匹配（HEM）的挑战，提出了一种分类法，并探讨了其与FAIR原则的联系，同时评估了现有方法的局限性及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 异构数据集在结构、格式、语义等方面的差异为实体匹配（EM）带来巨大挑战，需要系统化的理解和解决方案。

Method: 提出分类法区分表示和语义异构性，结合FAIR原则分析挑战与策略，并评估现有方法的鲁棒性。

Result: 发现现有方法在语义异构性下的局限性，提出未来研究方向如多模态匹配、人机协作等。

Conclusion: HEM研究需进一步整合新技术（如大语言模型）并关注公平性，以应对复杂异构场景。

Abstract: Entity matching (EM) is a fundamental task in data integration and analytics,
essential for identifying records that refer to the same real-world entity
across diverse sources. In practice, datasets often differ widely in structure,
format, schema, and semantics, creating substantial challenges for EM. We refer
to this setting as Heterogeneous EM (HEM). This survey offers a unified
perspective on HEM by introducing a taxonomy, grounded in prior work, that
distinguishes two primary categories -- representation and semantic
heterogeneity -- and their subtypes. The taxonomy provides a systematic lens
for understanding how variations in data form and meaning shape the complexity
of matching tasks. We then connect this framework to the FAIR principles --
Findability, Accessibility, Interoperability, and Reusability -- demonstrating
how they both reveal the challenges of HEM and suggest strategies for
mitigating them. Building on this foundation, we critically review recent EM
methods, examining their ability to address different heterogeneity types, and
conduct targeted experiments on state-of-the-art models to evaluate their
robustness and adaptability under semantic heterogeneity. Our analysis uncovers
persistent limitations in current approaches and points to promising directions
for future research, including multimodal matching, human-in-the-loop
workflows, deeper integration with large language models and knowledge graphs,
and fairness-aware evaluation in heterogeneous settings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [166] [Solving Pasur Using GPU-Accelerated Counterfactual Regret Minimization](https://arxiv.org/abs/2508.06559)
*Sina Baghal*

Main category: cs.AI

TL;DR: 本文介绍了一个基于CUDA加速的Pasur纸牌游戏计算框架，通过高效内存管理和CFR算法计算近纳什均衡，解决了游戏规则复杂和游戏树庞大的问题。


<details>
  <summary>Details</summary>
Motivation: Pasur游戏规则复杂且游戏树庞大，传统方法难以高效处理，因此需要一种新的计算框架来解决这些问题。

Method: 使用PyTorch CUDA张量处理规则复杂性，将游戏树分解为实际游戏状态和继承分数，采用逐轮反向训练策略降低计算复杂度。

Result: 构建了包含超过10^9个节点的完整游戏树，并通过大规模自对弈估计了每副牌的公平价值。

Conclusion: 该框架可扩展到其他多轮分解的强化学习算法，如回合制策略游戏或金融市场中的顺序交易决策。

Abstract: Pasur is a fishing card game played over six rounds and is played similarly
to games such as Cassino and Scopa, and Bastra. This paper introduces a
CUDA-accelerated computational framework for simulating Pasur, emphasizing
efficient memory management. We use our framework to compute near-Nash
equilibria via Counterfactual Regret Minimization (CFR), a well-known algorithm
for solving large imperfect-information games.
  Solving Pasur presents unique challenges due to its intricate rules and the
large size of its game tree. We handle rule complexity using PyTorch CUDA
tensors and to address the memory-intensive nature of the game, we decompose
the game tree into two key components: (1) actual game states, and (2)
inherited scores from previous rounds. We construct the Full Game Tree by
pairing card states with accumulated scores in the Unfolding Process. This
design reduces memory overhead by storing only essential strategy values and
node connections. To further manage computational complexity, we apply a
round-by-round backward training strategy, starting from the final round and
recursively propagating average utilities to earlier stages. Our approach
constructs the complete game tree, which on average consists of over $10^9$
nodes. We provide detailed implementation snippets.
  After computing a near-Nash equilibrium strategy, we train a tree-based model
to predict these strategies for use during gameplay. We then estimate the fair
value of each deck through large-scale self-play between equilibrium strategies
by simulating, for instance, 10,000 games per matchup, executed in parallel
using GPU acceleration.
  Similar frameworks can be extended to other reinforcement learning algorithms
where the action tree naturally decomposes into multiple rounds such as
turn-based strategy games or sequential trading decisions in financial markets.

</details>


### [167] [Operationalizing Serendipity: Multi-Agent AI Workflows for Enhanced Materials Characterization with Theory-in-the-Loop](https://arxiv.org/abs/2508.06569)
*Lance Yao,Suman Samantray,Ayana Ghosh,Kevin Roccapriore,Libor Kovarik,Sarah Allec,Maxim Ziatdinov*

Main category: cs.AI

TL;DR: SciLink是一个开源的多智能体AI框架，旨在通过自动化链接实验观察、新颖性评估和理论模拟，促进材料研究中的意外发现。


<details>
  <summary>Details</summary>
Motivation: 现代自主实验室虽然高效，但可能忽略意外发现，而科学史表明这些发现往往开启新领域。SciLink旨在填补这一空白。

Method: 采用混合AI策略，结合机器学习模型定量分析实验数据，大型语言模型处理高级推理，将原始数据转化为可验证的科学主张，并根据文献评估新颖性。

Result: SciLink在多种研究场景中表现出色，包括原子分辨率和超光谱数据处理，并能整合专家指导和提出后续实验。

Conclusion: SciLink不仅提高效率，还系统性地促进意外发现，弥合自动化实验与开放科学探索之间的鸿沟。

Abstract: The history of science is punctuated by serendipitous discoveries, where
unexpected observations, rather than targeted hypotheses, opened new fields of
inquiry. While modern autonomous laboratories excel at accelerating hypothesis
testing, their optimization for efficiency risks overlooking these crucial,
unplanned findings. To address this gap, we introduce SciLink, an open-source,
multi-agent artificial intelligence framework designed to operationalize
serendipity in materials research by creating a direct, automated link between
experimental observation, novelty assessment, and theoretical simulations. The
framework employs a hybrid AI strategy where specialized machine learning
models perform quantitative analysis of experimental data, while large language
models handle higher-level reasoning. These agents autonomously convert raw
data from materials characterization techniques into falsifiable scientific
claims, which are then quantitatively scored for novelty against the published
literature. We demonstrate the framework's versatility across diverse research
scenarios, showcasing its application to atomic-resolution and hyperspectral
data, its capacity to integrate real-time human expert guidance, and its
ability to close the research loop by proposing targeted follow-up experiments.
By systematically analyzing all observations and contextualizing them, SciLink
provides a practical framework for AI-driven materials research that not only
enhances efficiency but also actively cultivates an environment ripe for
serendipitous discoveries, thereby bridging the gap between automated
experimentation and open-ended scientific exploration.

</details>


### [168] [IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model](https://arxiv.org/abs/2508.06571)
*Anqing Jiang,Yu Gao,Yiru Wang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun,Shichen Tang,Lijuan Zhu,Jinhao Chai,Jijun Wang,Zichong Gu,Hao Jiang,Li Sun*

Main category: cs.AI

TL;DR: 本文提出IRL-VLA框架，通过逆强化学习构建轻量级奖励世界模型，结合VLA架构和PPO强化学习，提升自动驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在开环模仿学习中表现受限，闭环训练依赖高保真仿真，存在领域差距和计算效率问题。

Method: 三阶段方法：1) 预训练VLA策略；2) 构建轻量级奖励世界模型；3) 通过PPO优化规划性能。

Result: 在NAVSIM v2和CVPR2025挑战赛中取得领先成绩。

Conclusion: IRL-VLA框架为闭环自动驾驶研究提供了高效解决方案。

Abstract: Vision-Language-Action (VLA) models have demonstrated potential in autonomous
driving. However, two critical challenges hinder their development: (1)
Existing VLA architectures are typically based on imitation learning in
open-loop setup which tends to capture the recorded behaviors in the dataset,
leading to suboptimal and constrained performance, (2) Close-loop training
relies heavily on high-fidelity sensor simulation, where domain gaps and
computational inefficiencies pose significant barriers. In this paper, we
introduce IRL-VLA, a novel close-loop Reinforcement Learning via
\textbf{I}nverse \textbf{R}einforcement \textbf{L}earning reward world model
with a self-built VLA approach. Our framework proceeds in a three-stage
paradigm: In the first stage, we propose a VLA architecture and pretrain the
VLA policy via imitation learning. In the second stage, we construct a
lightweight reward world model via inverse reinforcement learning to enable
efficient close-loop reward computation. To further enhance planning
performance, finally, we design specialized reward world model guidence
reinforcement learning via PPO(Proximal Policy Optimization) to effectively
balance the safety incidents, comfortable driving, and traffic efficiency. Our
approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving
benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that
our framework will accelerate VLA research in close-loop autonomous driving.

</details>


### [169] [CountQA: How Well Do MLLMs Count in the Wild?](https://arxiv.org/abs/2508.06585)
*Jayant Sravan Tamarapalli,Rynaa Grover,Nilay Pande,Sahiti Yerramilli*

Main category: cs.AI

TL;DR: 多模态大语言模型（MLLMs）在视觉场景理解上表现流畅，但缺乏对象计数的基本认知能力。CountQA是一个新基准，用于评估和改进这一缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能全面测试MLLMs在复杂场景中的对象计数能力，限制了其实际应用的可靠性。

Method: 引入CountQA基准，包含1,500多个问题-答案对，涵盖高密度、杂乱和遮挡的真实世界图像，评估15种主流MLLMs。

Result: 表现最佳的模型准确率仅为42.9%，且随着对象数量增加性能下降。

Conclusion: CountQA为改进MLLMs的计数能力提供了基准，推动其向更全面、数值化和空间感知的方向发展。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in
understanding visual scenes, yet they exhibit a critical lack in a fundamental
cognitive skill: object counting. This blind spot severely limits their
reliability in real-world applications. To date, this capability has been
largely unevaluated in complex scenarios, as existing benchmarks either feature
sparse object densities or are confined to specific visual domains, failing to
test models under realistic conditions. Addressing this gap, we introduce
CountQA, a challenging new benchmark designed to probe this deficiency.
Comprising over 1,500 question-answer pairs, CountQA features real-world images
with high object density, clutter, and occlusion. We investigate this weakness
by evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the
top-performing model achieves a mere 42.9% accuracy, with performance declining
as object counts rise. By providing a dedicated benchmark to diagnose and
rectify this core weakness, CountQA paves the way for a new generation of MLLMs
that are not only descriptively fluent but also numerically grounded and
spatially aware. We will open-source the dataset and code upon paper acceptance
to foster further research.

</details>


### [170] [Formal Concept Analysis: a Structural Framework for Variability Extraction and Analysis](https://arxiv.org/abs/2508.06668)
*Jessie Galasso*

Main category: cs.AI

TL;DR: 本文探讨了形式概念分析（FCA）在变异性分析中的关键属性及其应用。


<details>
  <summary>Details</summary>
Motivation: FCA在知识表示和发现中具有潜力，但其数学基础文献使其在变异性任务中的应用不够直观。本文旨在填补这一空白。

Method: 通过筛选FCA框架中对变异性分析至关重要的属性，并解释其在概念结构中的应用。

Result: 明确了FCA中可用于变异性分析的关键属性及其解释方法。

Conclusion: FCA的属性可用于有效分析和解释变异性信息，为相关任务提供支持。

Abstract: Formal Concept Analysis (FCA) is a mathematical framework for knowledge
representation and discovery. It performs a hierarchical clustering over a set
of objects described by attributes, resulting in conceptual structures in which
objects are organized depending on the attributes they share. These conceptual
structures naturally highlight commonalities and variabilities among similar
objects by categorizing them into groups which are then arranged by similarity,
making it particularly appropriate for variability extraction and analysis.
Despite the potential of FCA, determining which of its properties can be
leveraged for variability-related tasks (and how) is not always
straightforward, partly due to the mathematical orientation of its foundational
literature. This paper attempts to bridge part of this gap by gathering a
selection of properties of the framework which are essential to variability
analysis, and how they can be used to interpret diverse variability information
within the resulting conceptual structures.

</details>


### [171] [Zero-Shot Cellular Trajectory Map Matching](https://arxiv.org/abs/2508.06674)
*Weijie Shi,Yue Cui,Hao Chen,Jiaming Li,Mengze Li,Jia Zhu,Jiajie Xu,Xiaofang Zhou*

Main category: cs.AI

TL;DR: 提出了一种基于像素的轨迹校准辅助方法，用于零样本细胞轨迹地图匹配（CTMM），通过迁移地理空间知识校准轨迹，并在道路网络层面引导路径查找。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖ID特征和区域特定数据，限制了在未探索区域的适应性，因此需要一种无需额外训练的高精度CTMM方法。

Method: 结合高斯混合模型的VAE提取场景自适应特征，设计时空感知模块捕获序列特征和位置不确定性，并采用约束路径查找算法重建道路ID序列。

Result: 实验表明，该方法在零样本CTMM中性能优于现有方法16.8%。

Conclusion: 该方法通过迁移知识和优化路径查找，显著提升了零样本CTMM的准确性和适应性。

Abstract: Cellular Trajectory Map-Matching (CTMM) aims to align cellular location
sequences to road networks, which is a necessary preprocessing in
location-based services on web platforms like Google Maps, including navigation
and route optimization. Current approaches mainly rely on ID-based features and
region-specific data to learn correlations between cell towers and roads,
limiting their adaptability to unexplored areas. To enable high-accuracy CTMM
without additional training in target regions, Zero-shot CTMM requires to
extract not only region-adaptive features, but also sequential and location
uncertainty to alleviate positioning errors in cellular data. In this paper, we
propose a pixel-based trajectory calibration assistant for zero-shot CTMM,
which takes advantage of transferable geospatial knowledge to calibrate
pixelated trajectory, and then guide the path-finding process at the road
network level. To enhance knowledge sharing across similar regions, a Gaussian
mixture model is incorporated into VAE, enabling the identification of
scenario-adaptive experts through soft clustering. To mitigate high positioning
errors, a spatial-temporal awareness module is designed to capture sequential
features and location uncertainty, thereby facilitating the inference of
approximate user positions. Finally, a constrained path-finding algorithm is
employed to reconstruct the road ID sequence, ensuring topological validity
within the road network. This process is guided by the calibrated trajectory
while optimizing for the shortest feasible path, thus minimizing unnecessary
detours. Extensive experiments demonstrate that our model outperforms existing
methods in zero-shot CTMM by 16.8\%.

</details>


### [172] [Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets](https://arxiv.org/abs/2508.06706)
*Jaikrishna Manojkumar Patil,Nathaniel Lee,Al Mehdi Saadat Chowdhury,YooJung Choi,Paulo Shakarian*

Main category: cs.AI

TL;DR: 论文提出了一种基于规则上下文和概率电路的知识图谱补全方法，显著减少规则数量并保持高性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法需要大量规则才能达到高性能，但规则过多会降低可解释性。

Method: 从训练数据中发现规则上下文，并利用概率电路学习规则上下文的概率分布，以快速达到完整规则集的性能。

Result: 规则数量减少70-96%，性能优于基线31倍，保留91%的峰值性能。

Conclusion: 该方法在8个标准数据集上验证有效，为基于规则的推理提供了新思路。

Abstract: Rule-based methods for knowledge graph completion provide explainable results
but often require a significantly large number of rules to achieve competitive
performance. This can hinder explainability due to overwhelmingly large rule
sets. We discover rule contexts (meaningful subsets of rules that work
together) from training data and use learned probability distribution (i.e.
probabilistic circuits) over these rule contexts to more rapidly achieve
performance of the full rule set. Our approach achieves a 70-96% reduction in
number of rules used while outperforming baseline by up to 31$\times$ when
using equivalent minimal number of rules and preserves 91% of peak baseline
performance even when comparing our minimal rule sets against baseline's full
rule sets. We show that our framework is grounded in well-known semantics of
probabilistic logic, does not require independence assumptions, and that our
tractable inference procedure provides both approximate lower bounds and exact
probability of a given query. The efficacy of our method is validated by
empirical studies on 8 standard benchmark datasets where we show competitive
performance by using only a fraction of the rules required by AnyBURL's
standard inference method, the current state-of-the-art for rule-based
knowledge graph completion. This work may have further implications for general
probabilistic reasoning over learned sets of rules.

</details>


### [173] [GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning](https://arxiv.org/abs/2508.06716)
*Blair Johnson,Clayton Kerce,Faramarz Fekri*

Main category: cs.AI

TL;DR: GLIDR是一种可微分的规则学习方法，通过更灵活的语法和消息传递推理算法，提升了知识图谱任务中的规则学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有链式规则结构的局限性影响了性能和可解释性，GLIDR旨在解决这一问题。

Method: GLIDR采用可微分消息传递推理算法，支持分支和循环等复杂规则结构。

Result: GLIDR在知识图谱补全任务中显著优于现有方法，且对训练数据噪声具有高鲁棒性。

Conclusion: GLIDR不仅性能优越，还能与深度神经网络结合，适用于多模态数据规则学习。

Abstract: Differentiable inductive logic programming (ILP) techniques have proven
effective at finding approximate rule-based solutions to link prediction and
node classification problems on knowledge graphs; however, the common
assumption of chain-like rule structure can hamper the performance and
interpretability of existing approaches. We introduce GLIDR, a differentiable
rule learning method that models the inference of logic rules with more
expressive syntax than previous methods. GLIDR uses a differentiable message
passing inference algorithm that generalizes previous chain-like rule learning
methods to allow rules with features like branches and cycles. GLIDR has a
simple and expressive rule search space which is parameterized by a limit on
the maximum number of free variables that may be included in a rule. Explicit
logic rules can be extracted from the weights of a GLIDR model for use with
symbolic solvers. We demonstrate that GLIDR can significantly outperform
existing rule learning methods on knowledge graph completion tasks and even
compete with embedding methods despite the inherent disadvantage of being a
structure-only prediction method. We show that rules extracted from GLIDR
retain significant predictive performance, and that GLIDR is highly robust to
training data noise. Finally, we demonstrate that GLIDR can be chained with
deep neural networks and optimized end-to-end for rule learning on arbitrary
data modalities.

</details>


### [174] [MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA](https://arxiv.org/abs/2508.07022)
*Shengtao Wen,Haodong Chen,Yadong Wang,Zhongying Pan,Xiang Chen,Yu Tian,Bo Qian,Dong Liang,Sheng-Jun Huang*

Main category: cs.AI

TL;DR: MultiMedEdit是首个针对临床多模态任务的知识编辑（KE）基准，揭示了当前方法在复杂临床工作流中的局限性，并为未来开发提供了基础。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑研究多关注文本领域，而多模态医学场景中KE的研究较少，需要结合视觉推理以支持临床决策。

Method: 提出MultiMedEdit基准，涵盖理解和推理任务，定义三维度量（可靠性、通用性和局部性），支持跨范式比较。

Result: 实验表明当前方法在泛化和长尾推理上表现不佳，尤其在复杂临床工作流中。效率分析揭示了实际部署中的权衡。

Conclusion: MultiMedEdit揭示了当前方法的不足，为未来开发临床鲁棒的KE技术奠定了基础。

Abstract: Knowledge editing (KE) provides a scalable approach for updating factual
knowledge in large language models without full retraining. While previous
studies have demonstrated effectiveness in general domains and medical QA
tasks, little attention has been paid to KE in multimodal medical scenarios.
Unlike text-only settings, medical KE demands integrating updated knowledge
with visual reasoning to support safe and interpretable clinical decisions. To
address this gap, we propose MultiMedEdit, the first benchmark tailored to
evaluating KE in clinical multimodal tasks. Our framework spans both
understanding and reasoning task types, defines a three-dimensional metric
suite (reliability, generality, and locality), and supports cross-paradigm
comparisons across general and domain-specific models. We conduct extensive
experiments under single-editing and lifelong-editing settings. Results suggest
that current methods struggle with generalization and long-tail reasoning,
particularly in complex clinical workflows. We further present an efficiency
analysis (e.g., edit latency, memory footprint), revealing practical trade-offs
in real-world deployment across KE paradigms. Overall, MultiMedEdit not only
reveals the limitations of current approaches but also provides a solid
foundation for developing clinically robust knowledge editing techniques in the
future.

</details>


### [175] [ParBalans: Parallel Multi-Armed Bandits-based Adaptive Large Neighborhood Search](https://arxiv.org/abs/2508.06736)
*Alican Yilmaz,Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: 本文研究了Balans的并行化能力，提出ParBalans扩展，结合求解器和算法级并行，显著提升MIP问题求解性能。


<details>
  <summary>Details</summary>
Motivation: 由于MIP问题的组合性质，计算资源需求高，并行化成为加速求解的关键策略。Balans的并行潜力尚未充分探索。

Method: 提出ParBalans，结合求解器级和算法级并行，扩展Balans的并行能力。

Result: 实验表明，ParBalans在复杂MIP实例上表现优异，尤其在某些基准测试中优于商业求解器Gurobi。

Conclusion: ParBalans通过并行化显著提升MIP求解性能，为大规模优化问题提供了高效解决方案。

Abstract: Solving Mixed-Integer Programming (MIP) problems often requires substantial
computational resources due to their combinatorial nature. Parallelization has
emerged as a critical strategy to accelerate solution times and enhance
scalability to tackle large, complex instances. This paper investigates the
parallelization capabilities of Balans, a recently proposed multi-armed
bandits-based adaptive large neighborhood search for MIPs. While Balans's
modular architecture inherently supports parallel exploration of diverse
parameter configurations, this potential has not been thoroughly examined. To
address this gap, we introduce ParBalans, an extension that leverages both
solver-level and algorithmic-level parallelism to improve performance on
challenging MIP instances. Our experimental results demonstrate that ParBalans
exhibits competitive performance compared to the state-of-the-art commercial
solver Gurobi, particularly on hard optimization benchmarks.

</details>


### [176] [Topology Generation of UAV Covert Communication Networks: A Graph Diffusion Approach with Incentive Mechanism](https://arxiv.org/abs/2508.06746)
*Xin Tang,Qian Chen,Fengshun Li,Youchun Gong,Yinqiu Liu,Wen Tian,Shaowen Qin,Xiaohuan Li*

Main category: cs.AI

TL;DR: 本文提出了一种结合图扩散策略优化（GDPO）和Stackelberg博弈（SG）激励机制的无人机网络框架，以解决动态移动性和隐蔽通信的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着无人机网络在敏感应用中的需求增长，确保可靠连接和隐蔽通信变得至关重要，但动态移动性和暴露风险带来了重大挑战。

Method: 采用GDPO方法生成稀疏但连接良好的拓扑结构，并结合SG激励机制引导无人机选择支持协作和隐蔽通信的行为。

Result: 实验验证了框架在模型收敛性、拓扑生成质量和隐蔽通信性能提升方面的有效性。

Conclusion: 该框架为无人机网络提供了动态适应性和隐蔽通信的解决方案。

Abstract: With the growing demand for Uncrewed Aerial Vehicle (UAV) networks in
sensitive applications, such as urban monitoring, emergency response, and
secure sensing, ensuring reliable connectivity and covert communication has
become increasingly vital. However, dynamic mobility and exposure risks pose
significant challenges. To tackle these challenges, this paper proposes a
self-organizing UAV network framework combining Graph Diffusion-based Policy
Optimization (GDPO) with a Stackelberg Game (SG)-based incentive mechanism. The
GDPO method uses generative AI to dynamically generate sparse but
well-connected topologies, enabling flexible adaptation to changing node
distributions and Ground User (GU) demands. Meanwhile, the Stackelberg Game
(SG)-based incentive mechanism guides self-interested UAVs to choose relay
behaviors and neighbor links that support cooperation and enhance covert
communication. Extensive experiments are conducted to validate the
effectiveness of the proposed framework in terms of model convergence, topology
generation quality, and enhancement of covert communication performance.

</details>


### [177] [Pushing the Envelope of LLM Inference on AI-PC](https://arxiv.org/abs/2508.06753)
*Evangelos Georganas,Dhiraj Kalamkar,Alexander Heinecke*

Main category: cs.AI

TL;DR: 论文提出了一种针对超低比特（1/1.58/2-bit）LLM模型的优化微内核设计，显著提升了推理效率，适用于资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 超低比特LLM模型在资源受限环境（如边缘设备和AI PC）中具有潜力，但现有推理运行时的计算效率尚未充分探索。

Method: 设计并实现了针对现代CPU优化的1-bit和2-bit微内核，集成到PyTorch-TPP框架中，进行端到端推理测试。

Result: 优化后的运行时在2-bit模型上比当前SOTA运行时bitnet.cpp快2.2倍，比16-bit模型推理快7倍。

Conclusion: 优化后的运行时推动了超低比特LLM模型在AI PC和边缘设备上的高效部署。

Abstract: The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the
perplexity and end-task performance of their full-precision counterparts using
the same model size, is ushering in a new era of LLM inference for
resource-constrained environments such as edge devices and AI PCs. While these
quantization advances promise models that are more cost-effective in terms of
latency, memory, throughput, and energy consumption, the computational
efficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp)
used to deploy them remains underexplored. In this work, we take a bottom-up
approach: we first design and implement 1-bit and 2-bit microkernels optimized
for modern CPUs, achieving peak computational efficiency across a variety of
CPU platforms. We integrate these microkernels into a state-of-the-art LLM
inference framework, namely PyTorch-TPP, and present end-to-end inference
results with 2-bit models that outperform the current SOTA runtime bitnet.cpp
by up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model
inference. Our optimized runtime advances the state of LLM inference on AI PCs
and edge devices, paving the way for efficient deployment of ultra-low-bit LLM
models.

</details>


### [178] [Efficient and Reliable Hitting-Set Computations for the Implicit Hitting Set Approach](https://arxiv.org/abs/2508.07015)
*Hannes Ihalainen,Dieter Vandesande,André Schidler,Jeremias Berg,Bart Bogaerts,Matti Järvisalo*

Main category: cs.AI

TL;DR: 本文探讨了隐式命中集（IHS）框架中替代优化技术的可行性，比较了伪布尔推理和随机局部搜索方法，并评估了其在0-1整数规划中的实际表现。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索IHS框架中替代优化技术的可行性，以解决传统整数规划方法可能存在的数值不稳定性和效率问题。

Method: 方法包括使用伪布尔（PB）推理和随机局部搜索作为替代优化技术，并与商业整数规划求解器进行比较。

Result: 结果表明，虽然商业整数规划求解器效率最高，但PB推理在数值稳定性上更优，并能提供计算正确性证明。

Conclusion: 结论指出，PB推理在IHS框架中具有竞争力，尤其在需要数值稳定性和正确性证明的场景中。

Abstract: The implicit hitting set (IHS) approach offers a general framework for
solving computationally hard combinatorial optimization problems declaratively.
IHS iterates between a decision oracle used for extracting sources of
inconsistency and an optimizer for computing so-called hitting sets (HSs) over
the accumulated sources of inconsistency. While the decision oracle is
language-specific, the optimizers is usually instantiated through integer
programming.
  We explore alternative algorithmic techniques for hitting set optimization
based on different ways of employing pseudo-Boolean (PB) reasoning as well as
stochastic local search. We extensively evaluate the practical feasibility of
the alternatives in particular in the context of pseudo-Boolean (0-1 IP)
optimization as one of the most recent instantiations of IHS. Highlighting a
trade-off between efficiency and reliability, while a commercial IP solver
turns out to remain the most effective way to instantiate HS computations, it
can cause correctness issues due to numerical instability; in fact, we show
that exact HS computations instantiated via PB reasoning can be made
competitive with a numerically exact IP solver. Furthermore, the use of PB
reasoning as a basis for HS computations allows for obtaining certificates for
the correctness of IHS computations, generally applicable to any IHS
instantiation in which reasoning in the declarative language at hand can be
captured in the PB-based proof format we employ.

</details>


### [179] [A Fuzzy Logic Prompting Framework for Large Language Models in Adaptive and Uncertain Tasks](https://arxiv.org/abs/2508.06754)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: 提出一种模块化提示框架，支持更安全、更自适应地使用大语言模型（LLMs）于动态、用户中心的任务。


<details>
  <summary>Details</summary>
Motivation: 基于人类学习理论（如最近发展区ZPD），旨在提升LLMs在动态任务中的适应性和安全性。

Method: 结合自然语言边界提示与控制模式，采用模糊支架逻辑和适应规则，无需微调或外部协调。

Result: 在模拟智能辅导环境中，框架显著提升支架质量、适应性和教学对齐，优于标准提示基线。

Conclusion: 该框架不仅适用于教育领域，还可扩展至其他交互密集型任务，为不确定或动态场景提供可解释、目标对齐的LLM行为设计方法。

Abstract: We introduce a modular prompting framework that supports safer and more
adaptive use of large language models (LLMs) across dynamic, user-centered
tasks. Grounded in human learning theory, particularly the Zone of Proximal
Development (ZPD), our method combines a natural language boundary prompt with
a control schema encoded with fuzzy scaffolding logic and adaptation rules.
This architecture enables LLMs to modulate behavior in response to user state
without requiring fine-tuning or external orchestration. In a simulated
intelligent tutoring setting, the framework improves scaffolding quality,
adaptivity, and instructional alignment across multiple models, outperforming
standard prompting baselines. Evaluation is conducted using rubric-based LLM
graders at scale. While initially developed for education, the framework has
shown promise in other interaction-heavy domains, such as procedural content
generation for games. Designed for safe deployment, it provides a reusable
methodology for structuring interpretable, goal-aligned LLM behavior in
uncertain or evolving contexts.

</details>


### [180] [Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation](https://arxiv.org/abs/2508.06823)
*Xuan Zhao,Jun Tao*

Main category: cs.AI

TL;DR: 提出了一种基于自然语言交互的框架，用于优化体数据探索的视角选择，结合CLIP Score和强化学习提升导航效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 体数据探索对科学数据分析至关重要，但缺乏领域知识或3D导航经验的用户难以选择最佳视角。

Method: 将体数据块编码以区分结构，利用CLIP Score提供语义信息，并通过强化学习框架搜索符合用户意图的视角。

Result: 自动化视角选择提高了导航效率，并增强了复杂科学现象的可解释性。

Conclusion: 该框架通过自然语言交互和语义引导，显著提升了体数据探索的效率和用户体验。

Abstract: Exploring volumetric data is crucial for interpreting scientific datasets.
However, selecting optimal viewpoints for effective navigation can be
challenging, particularly for users without extensive domain expertise or
familiarity with 3D navigation. In this paper, we propose a novel framework
that leverages natural language interaction to enhance volumetric data
exploration. Our approach encodes volumetric blocks to capture and
differentiate underlying structures. It further incorporates a CLIP Score
mechanism, which provides semantic information to the blocks to guide
navigation. The navigation is empowered by a reinforcement learning framework
that leverage these semantic cues to efficiently search for and identify
desired viewpoints that align with the user's intent. The selected viewpoints
are evaluated using CLIP Score to ensure that they best reflect the user
queries. By automating viewpoint selection, our method improves the efficiency
of volumetric data navigation and enhances the interpretability of complex
scientific phenomena.

</details>


### [181] [Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges](https://arxiv.org/abs/2508.06832)
*Haifeng Li,Wang Guo,Haiyang Wu,Mengwei Wu,Jipeng Zhang,Qing Zhu,Yu Liu,Xin Huang,Chao Tao*

Main category: cs.AI

TL;DR: 本文提出从视觉为中心转向语言为中心的遥感图像解释范式，借鉴全球工作空间理论，将大语言模型（LLMs）作为认知中枢，整合感知、任务、知识和行动空间，实现统一理解、推理和决策。


<details>
  <summary>Details</summary>
Motivation: 现有视觉为中心的模型在多模态推理、语义抽象和交互决策方面存在局限，缺乏统一的理论框架解释语言在认知中的作用。

Method: 提出语言为中心的框架，将LLMs作为认知中枢，探讨其在遥感解释中的潜力，总结技术挑战，并构建全球工作空间驱动的解释机制。

Result: 语言为中心的解决方案能够应对多模态表示、知识关联、推理和决策等核心挑战。

Conclusion: 为下一代遥感解释系统提供概念基础，并建立认知驱动智能地理空间分析的路线图。

Abstract: The mainstream paradigm of remote sensing image interpretation has long been
dominated by vision-centered models, which rely on visual features for semantic
understanding. However, these models face inherent limitations in handling
multi-modal reasoning, semantic abstraction, and interactive decision-making.
While recent advances have introduced Large Language Models (LLMs) into remote
sensing workflows, existing studies primarily focus on downstream applications,
lacking a unified theoretical framework that explains the cognitive role of
language. This review advocates a paradigm shift from vision-centered to
language-centered remote sensing interpretation. Drawing inspiration from the
Global Workspace Theory (GWT) of human cognition, We propose a
language-centered framework for remote sensing interpretation that treats LLMs
as the cognitive central hub integrating perceptual, task, knowledge and action
spaces to enable unified understanding, reasoning, and decision-making. We
first explore the potential of LLMs as the central cognitive component in
remote sensing interpretation, and then summarize core technical challenges,
including unified multimodal representation, knowledge association, and
reasoning and decision-making. Furthermore, we construct a global
workspace-driven interpretation mechanism and review how language-centered
solutions address each challenge. Finally, we outline future research
directions from four perspectives: adaptive alignment of multimodal data, task
understanding under dynamic knowledge constraints, trustworthy reasoning, and
autonomous interaction. This work aims to provide a conceptual foundation for
the next generation of remote sensing interpretation systems and establish a
roadmap toward cognition-driven intelligent geospatial analysis.

</details>


### [182] [Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.06836)
*Xutong Zhao,Yaqi Xie*

Main category: cs.AI

TL;DR: 论文提出了一种多级优势信用分配方法（MACA），用于解决多智能体强化学习中的信用分配问题，通过注意力框架识别智能体关系并构建多级优势函数。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）中的信用分配是一个关键挑战，需要评估每个智能体对共享奖励的贡献。由于任务的多样性，信用分配可能涉及不同级别的协作，现有方法难以处理多级共存的情况。

Method: 提出MACA方法，通过多级优势公式进行显式反事实推理，捕捉个体、联合和相关动作的贡献。采用注意力框架识别智能体关系，并构建多级优势函数指导策略学习。

Result: 在Starcraft v1和v2任务上的实验表明，MACA在复杂信用分配场景中表现优异。

Conclusion: MACA通过多级优势信用分配，有效解决了多智能体协作中的信用分配问题，适用于多级协作场景。

Abstract: Cooperative multi-agent reinforcement learning (MARL) aims to coordinate
multiple agents to achieve a common goal. A key challenge in MARL is credit
assignment, which involves assessing each agent's contribution to the shared
reward. Given the diversity of tasks, agents may perform different types of
coordination, with rewards attributed to diverse and often overlapping agent
subsets. In this work, we formalize the credit assignment level as the number
of agents cooperating to obtain a reward, and address scenarios with multiple
coexisting levels. We introduce a multi-level advantage formulation that
performs explicit counterfactual reasoning to infer credits across distinct
levels. Our method, Multi-level Advantage Credit Assignment (MACA), captures
agent contributions at multiple levels by integrating advantage functions that
reason about individual, joint, and correlated actions. Utilizing an
attention-based framework, MACA identifies correlated agent relationships and
constructs multi-level advantages to guide policy learning. Comprehensive
experiments on challenging Starcraft v1\&v2 tasks demonstrate MACA's superior
performance, underscoring its efficacy in complex credit assignment scenarios.

</details>


### [183] [MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams](https://arxiv.org/abs/2508.06851)
*Pengfei Zhou,Xiaopeng Peng,Fanrui Zhang,Zhaopan Xu,Jiaxin Ai,Yansheng Qiu,Chuanhao Li,Zhen Li,Ming Li,Yukang Feng,Jianwen Sun,Haoquan Zhang,Zizhen Li,Xiaofeng Mao,Zekai Li,Wangbo Zhao,Kai Wang,Xiaojun Chang,Wenqi Shao,Yang You,Kaipeng Zhang*

Main category: cs.AI

TL;DR: MDK12-Bench是一个多学科大规模基准测试，用于评估多模态大语言模型（MLLMs）在多个维度的表现，包括难度、时间变化、上下文变化和知识驱动推理。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs的评测基准存在规模小、覆盖窄、知识无结构化等问题，无法全面评估模型能力。

Method: 构建了MDK12-Bench基准，包含141K实例和6,225个知识点，并提出动态评估框架和知识参考增强生成（KP-RAG）方法。

Result: 发现当前MLLMs在多个方面存在局限性，并提供了增强模型鲁棒性和可解释性的指导。

Conclusion: MDK12-Bench为MLLMs的全面评估提供了新工具，并推动了AI辅助教育的发展。

Abstract: Multimodal large language models (MLLMs), which integrate language and visual
cues for problem-solving, are crucial for advancing artificial general
intelligence (AGI). However, current benchmarks for measuring the intelligence
of MLLMs suffer from limited scale, narrow coverage, and unstructured
knowledge, offering only static and undifferentiated evaluations. To bridge
this gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark
built from real-world K-12 exams spanning six disciplines with 141K instances
and 6,225 knowledge points organized in a six-layer taxonomy. Covering five
question formats with difficulty and year annotations, it enables comprehensive
evaluation to capture the extent to which MLLMs perform over four dimensions:
1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts,
and 4) knowledge-driven reasoning. We propose a novel dynamic evaluation
framework that introduces unfamiliar visual, textual, and question form shifts
to challenge model generalization while improving benchmark objectivity and
longevity by mitigating data contamination. We further evaluate knowledge-point
reference-augmented generation (KP-RAG) to examine the role of knowledge in
problem-solving. Key findings reveal limitations in current MLLMs in multiple
aspects and provide guidance for enhancing model robustness, interpretability,
and AI-assisted education.

</details>


### [184] [MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction](https://arxiv.org/abs/2508.06859)
*Shuo Tang,Jian Xu,Jiadong Zhang,Yi Chen,Qizhao Jin,Lingdong Shen,Chenglin Liu,Shiming Xiang*

Main category: cs.AI

TL;DR: 论文提出了一种端到端的AI气象站系统MP-Bench，通过构建大规模多模态数据集和开发气象多模态大模型（MMLM），解决了现有系统依赖人工、数据对齐不足等问题。


<details>
  <summary>Details</summary>
Motivation: 当前严重天气预警系统依赖人工专家解读，存在主观性和操作负担。AI技术的发展为端到端自动化预测提供了可能，但面临数据稀缺、数据对齐不足等挑战。

Method: 构建了MP-Bench数据集（421,363对气象数据与文本），并开发了MMLM模型，包含三个自适应融合模块，处理4D气象数据的时空依赖性。

Result: 实验表明MMLM在多种任务中表现优异，验证了其在严重天气预测中的有效性。

Conclusion: MMLM为自动化AI气象预测系统迈出了关键一步，代码和数据集将公开。

Abstract: Timely and accurate severe weather warnings are critical for disaster
mitigation. However, current forecasting systems remain heavily reliant on
manual expert interpretation, introducing subjectivity and significant
operational burdens. With the rapid development of AI technologies, the
end-to-end "AI weather station" is gradually emerging as a new trend in
predicting severe weather events. Three core challenges impede the development
of end-to-end AI severe weather system: (1) scarcity of severe weather event
samples; (2) imperfect alignment between high-dimensional meteorological data
and textual warnings; (3) existing multimodal language models are unable to
handle high-dimensional meteorological data and struggle to fully capture the
complex dependencies across temporal sequences, vertical pressure levels, and
spatial dimensions. To address these challenges, we introduce MP-Bench, the
first large-scale temporal multimodal dataset for severe weather events
prediction, comprising 421,363 pairs of raw multi-year meteorological data and
corresponding text caption, covering a wide range of severe weather scenarios
across China. On top of this dataset, we develop a meteorology multimodal large
model (MMLM) that directly ingests 4D meteorological inputs. In addition, it is
designed to accommodate the unique characteristics of 4D meteorological data
flow, incorporating three plug-and-play adaptive fusion modules that enable
dynamic feature extraction and integration across temporal sequences, vertical
pressure layers, and spatial dimensions. Extensive experiments on MP-Bench
demonstrate that MMLM performs exceptionally well across multiple tasks,
highlighting its effectiveness in severe weather understanding and marking a
key step toward realizing automated, AI-driven weather forecasting systems. Our
source code and dataset will be made publicly available.

</details>


### [185] [Pushdown Reward Machines for Reinforcement Learning](https://arxiv.org/abs/2508.06894)
*Giovanni Varricchione,Toryn Q. Klassen,Natasha Alechina,Mehdi Dastani,Brian Logan,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 论文提出了一种基于确定性下推自动机的奖励机器（pdRMs），扩展了传统奖励机器（RMs）的表达能力，能够识别和奖励确定性上下文无关语言表示的行为。


<details>
  <summary>Details</summary>
Motivation: 传统奖励机器（RMs）只能处理正则语言表示的行为，限制了其在复杂任务中的应用。pdRMs的提出旨在扩展表达能力，支持更复杂的时序行为。

Method: 提出了两种基于pdRM的策略：一种可以访问整个堆栈，另一种只能访问堆栈顶部的k个符号。并提供了检查两种策略在给定环境下是否达到相同最优奖励的方法。

Result: 理论分析表明pdRMs的表达能力更强，并提供了学习问题的空间复杂度结果。实验证明pdRMs可以训练代理执行确定性上下文无关语言表示的任务。

Conclusion: pdRMs显著扩展了奖励机器的表达能力，为复杂时序行为的强化学习提供了新工具。

Abstract: Reward machines (RMs) are automata structures that encode (non-Markovian)
reward functions for reinforcement learning (RL). RMs can reward any behaviour
representable in regular languages and, when paired with RL algorithms that
exploit RM structure, have been shown to significantly improve sample
efficiency in many domains. In this work, we present pushdown reward machines
(pdRMs), an extension of reward machines based on deterministic pushdown
automata. pdRMs can recognize and reward temporally extended behaviours
representable in deterministic context-free languages, making them more
expressive than reward machines. We introduce two variants of pdRM-based
policies, one which has access to the entire stack of the pdRM, and one which
can only access the top $k$ symbols (for a given constant $k$) of the stack. We
propose a procedure to check when the two kinds of policies (for a given
environment, pdRM, and constant $k$) achieve the same optimal expected reward.
We then provide theoretical results establishing the expressive power of pdRMs,
and space complexity results about the proposed learning problems. Finally, we
provide experimental results showing how agents can be trained to perform tasks
representable in deterministic context-free languages using pdRMs.

</details>


### [186] [GDBA Revisited: Unleashing the Power of Guided Local Search for Distributed Constraint Optimization](https://arxiv.org/abs/2508.06899)
*Yanchen Deng,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 论文提出了一种名为DGLS的新方法，解决了GDBA在分布式约束优化问题中的性能问题，通过自适应约束违反条件、惩罚蒸发机制和同步方案显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: GDBA在分布式约束优化问题中表现不佳，主要由于过度激进的约束违反条件、无限制的惩罚积累和不协调的惩罚更新。

Method: 提出了分布式引导局部搜索（DGLS），包括自适应约束违反条件、惩罚蒸发机制和同步惩罚更新方案。

Result: DGLS在标准基准测试中表现优异，尤其在结构化问题上显著优于现有方法（3.77%--66.3%）。

Conclusion: DGLS通过改进GDBA的不足，显著提升了分布式约束优化问题的求解性能。

Abstract: Local search is an important class of incomplete algorithms for solving
Distributed Constraint Optimization Problems (DCOPs) but it often converges to
poor local optima. While GDBA provides a comprehensive rule set to escape
premature convergence, its empirical benefits remain marginal on general-valued
problems. In this work, we systematically examine GDBA and identify three
factors that potentially lead to its inferior performance, i.e.,
over-aggressive constraint violation conditions, unbounded penalty
accumulation, and uncoordinated penalty updates. To address these issues, we
propose Distributed Guided Local Search (DGLS), a novel GLS framework for DCOPs
that incorporates an adaptive violation condition to selectively penalize
constraints with high cost, a penalty evaporation mechanism to control the
magnitude of penalization, and a synchronization scheme for coordinated penalty
updates. We theoretically show that the penalty values are bounded, and agents
play a potential game in our DGLS. Our extensive empirical results on various
standard benchmarks demonstrate the great superiority of DGLS over
state-of-the-art baselines. Particularly, compared to Damped Max-sum with high
damping factors (e.g., 0.7 or 0.9), our DGLS achieves competitive performance
on general-valued problems, and outperforms it by significant margins
(\textbf{3.77\%--66.3\%}) on structured problems in terms of anytime results.

</details>


### [187] [Automated Formalization via Conceptual Retrieval-Augmented LLMs](https://arxiv.org/abs/2508.06931)
*Wangyue Lu,Lun Du,Sirui Li,Ke Weng,Haozhe Sun,Hengyu Liu,Minghe Yu,Tiancheng Zhang,Ge Yu*

Main category: cs.AI

TL;DR: CRAMF是一个概念驱动的检索增强数学形式化框架，通过检索核心数学概念的形式定义，提升基于LLM的自动形式化效果。


<details>
  <summary>Details</summary>
Motivation: 交互式定理证明器（ITP）需要手动形式化，工作量大且需要专业知识。自动形式化面临模型幻觉和语义鸿沟的挑战。

Method: CRAMF通过构建概念定义知识库、上下文查询增强和双通道混合检索策略，解决数学概念的多态性和检索精度问题。

Result: 在多个基准测试中，CRAMF显著提升了翻译准确率，最高达62.1%，平均提升29.9%。

Conclusion: CRAMF为自动形式化提供了有效解决方案，显著提升了基于LLM的自动形式化性能。

Abstract: Interactive theorem provers (ITPs) require manual formalization, which is
labor-intensive and demands expert knowledge. While automated formalization
offers a potential solution, it faces two major challenges: model hallucination
(e.g., undefined predicates, symbol misuse, and version incompatibility) and
the semantic gap caused by ambiguous or missing premises in natural language
descriptions. To address these issues, we propose CRAMF, a Concept-driven
Retrieval-Augmented Mathematical Formalization framework. CRAMF enhances
LLM-based autoformalization by retrieving formal definitions of core
mathematical concepts, providing contextual grounding during code generation.
However, applying retrieval-augmented generation (RAG) in this setting is
non-trivial due to the lack of structured knowledge bases, the polymorphic
nature of mathematical concepts, and the high precision required in formal
retrieval. We introduce a framework for automatically constructing a
concept-definition knowledge base from Mathlib4, the standard mathematical
library for the Lean 4 theorem prover, indexing over 26,000 formal definitions
and 1,000+ core mathematical concepts. To address conceptual polymorphism, we
propose contextual query augmentation with domain- and application-level
signals. In addition, we design a dual-channel hybrid retrieval strategy with
reranking to ensure accurate and relevant definition retrieval. Experiments on
miniF2F, ProofNet, and our newly proposed AdvancedMath benchmark show that
CRAMF can be seamlessly integrated into LLM-based autoformalizers, yielding
consistent improvements in translation accuracy, achieving up to 62.1% and an
average of 29.9% relative improvement.

</details>


### [188] [Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction](https://arxiv.org/abs/2508.06939)
*Hiba Najjar,Deepak Pathak,Marlon Nuske,Andreas Dengel*

Main category: cs.AI

TL;DR: 该研究利用Transformer模型的可解释性，通过自注意力机制分析多模态学习网络，用于子田级作物产量预测，并比较了不同特征和模态归因方法的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态学习在农业中具有潜力，但模型复杂性常导致可解释性不足。本研究旨在通过Transformer模型提升多模态学习的可解释性。

Method: 采用自注意力机制，使用Attention Rollout（AR）和Generic Attention（GA）两种方法估计特征归因，并提出Weighted Modality Activation（WMA）评估模态归因。

Result: Transformer模型在子田和田块级别的R2分数分别比卷积和循环网络高0.10和0.04。AR在时间归因上表现更稳健。

Conclusion: Transformer模型在多模态学习中表现优异，AR方法提供了更可靠的时间归因，模态归因方法揭示了不同模式的差异。

Abstract: Multimodal learning enables various machine learning tasks to benefit from
diverse data sources, effectively mimicking the interplay of different factors
in real-world applications, particularly in agriculture. While the
heterogeneous nature of involved data modalities may necessitate the design of
complex architectures, the model interpretability is often overlooked. In this
study, we leverage the intrinsic explainability of Transformer-based models to
explain multimodal learning networks, focusing on the task of crop yield
prediction at the subfield level. The large datasets used cover various crops,
regions, and years, and include four different input modalities: multispectral
satellite and weather time series, terrain elevation maps and soil properties.
Based on the self-attention mechanism, we estimate feature attributions using
two methods, namely the Attention Rollout (AR) and Generic Attention (GA), and
evaluate their performance against Shapley-based model-agnostic estimations,
Shapley Value Sampling (SVS). Additionally, we propose the Weighted Modality
Activation (WMA) method to assess modality attributions and compare it with SVS
attributions. Our findings indicate that Transformer-based models outperform
other architectures, specifically convolutional and recurrent networks,
achieving R2 scores that are higher by 0.10 and 0.04 at the subfield and field
levels, respectively. AR is shown to provide more robust and reliable temporal
attributions, as confirmed through qualitative and quantitative evaluation,
compared to GA and SVS values. Information about crop phenology stages was
leveraged to interpret the explanation results in the light of established
agronomic knowledge. Furthermore, modality attributions revealed varying
patterns across the two methods compared.[...]

</details>


### [189] [Large Language Models Do Not Simulate Human Psychology](https://arxiv.org/abs/2508.06950)
*Sarah Schröder,Thekla Morgenroth,Ulrike Kuhl,Valerie Vaquet,Benjamin Paaßen*

Main category: cs.AI

TL;DR: 论文警告不要用LLMs（如ChatGPT）模拟人类心理学研究，因其与人类反应存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否能替代人类参与者进行心理学研究。

Method: 通过概念论证和实证证据（如微调后的CENTAUR模型）分析LLMs与人类反应的差异。

Result: LLMs对微小语义变化的反应与人类不一致，且不同模型间差异显著。

Conclusion: LLMs不能模拟人类心理学，需在每次新应用中验证其可靠性。

Abstract: Large Language Models (LLMs),such as ChatGPT, are increasingly used in
research, ranging from simple writing assistance to complex data annotation
tasks. Recently, some research has suggested that LLMs may even be able to
simulate human psychology and can, hence, replace human participants in
psychological studies. We caution against this approach. We provide conceptual
arguments against the hypothesis that LLMs simulate human psychology. We then
present empiric evidence illustrating our arguments by demonstrating that
slight changes to wording that correspond to large changes in meaning lead to
notable discrepancies between LLMs' and human responses, even for the recent
CENTAUR model that was specifically fine-tuned on psychological responses.
Additionally, different LLMs show very different responses to novel items,
further illustrating their lack of reliability. We conclude that LLMs do not
simulate human psychology and recommend that psychological researchers should
treat LLMs as useful but fundamentally unreliable tools that need to be
validated against human responses for every new application.

</details>


### [190] [DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery](https://arxiv.org/abs/2508.06960)
*Keyu Li,Mohan Jiang,Dayuan Fu,Yunze Wu,Xiangkun Hu,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 论文提出了DatasetResearch基准，评估AI代理在发现和合成数据集方面的能力，揭示了当前技术与完美数据集发现之间的巨大差距。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，数据可用性成为AI发展的瓶颈，论文旨在解决AI代理能否自主发现符合用户需求的数据集的问题。

Method: 引入DatasetResearch基准，包含208个真实需求任务，采用三维评估框架分析AI代理的表现。

Result: 结果显示，即使是先进的深度研究系统在挑战性子集上仅得22%分，暴露了当前能力的不足。

Conclusion: 论文为数据集发现代理建立了首个严格基准，为下一代自改进AI系统奠定了基础。

Abstract: The rapid advancement of large language models has fundamentally shifted the
bottleneck in AI development from computational power to data availability-with
countless valuable datasets remaining hidden across specialized repositories,
research appendices, and domain platforms. As reasoning capabilities and deep
research methodologies continue to evolve, a critical question emerges: can AI
agents transcend conventional search to systematically discover any dataset
that meets specific user requirements, enabling truly autonomous demand-driven
data curation? We introduce DatasetResearch, the first comprehensive benchmark
evaluating AI agents' ability to discover and synthesize datasets from 208
real-world demands across knowledge-intensive and reasoning-intensive tasks.
Our tri-dimensional evaluation framework reveals a stark reality: even advanced
deep research systems achieve only 22% score on our challenging
DatasetResearch-pro subset, exposing the vast gap between current capabilities
and perfect dataset discovery. Our analysis uncovers a fundamental
dichotomy-search agents excel at knowledge tasks through retrieval breadth,
while synthesis agents dominate reasoning challenges via structured
generation-yet both catastrophically fail on "corner cases" outside existing
distributions. These findings establish the first rigorous baseline for dataset
discovery agents and illuminate the path toward AI systems capable of finding
any dataset in the digital universe. Our benchmark and comprehensive analysis
provide the foundation for the next generation of self-improving AI systems and
are publicly available at https://github.com/GAIR-NLP/DatasetResearch.

</details>


### [191] [MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair](https://arxiv.org/abs/2508.06963)
*Changqing Li,Tianlin Li,Xiaohan Zhang,Aishan Liu,Li Pan*

Main category: cs.AI

TL;DR: MASteer是一个基于表示工程的端到端框架，用于修复大型语言模型（LLMs）的可信性问题，通过自动生成多样化样本和自适应策略，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有修复方法（如SFT和RLHF）成本高且速度慢，而提示工程缺乏鲁棒性和可扩展性。表示工程提供了一种轻量级替代方案，但依赖手动样本和固定策略，限制了自动化和适应性。

Method: MASteer结合AutoTester（多智能体系统生成多样化样本）和AutoRepairer（构建自适应策略），实现自动化、上下文感知的策略选择。

Result: 在标准及定制任务中，MASteer性能优于基线方法，LLaMA-3.1-8B-Chat和Qwen-3-8B-Chat分别提升15.36%和4.21%，同时保持模型通用能力。

Conclusion: MASteer展示了强大的鲁棒性、泛化能力和实用价值，为高效、可扩展的可信性修复提供了解决方案。

Abstract: Large Language Models (LLMs) face persistent and evolving trustworthiness
issues, motivating developers to seek automated and flexible repair methods
that enable convenient deployment across diverse scenarios. Existing repair
methods like supervised fine-tuning (SFT) and reinforcement learning with human
feedback (RLHF) are costly and slow, while prompt engineering lacks robustness
and scalability. Representation engineering, which steers model behavior by
injecting targeted concept vectors during inference, offers a lightweight,
training-free alternative. However, current approaches depend on manually
crafted samples and fixed steering strategies, limiting automation and
adaptability. To overcome these challenges, we propose MASteer, the first
end-to-end framework for trustworthiness repair in LLMs based on representation
engineering. MASteer integrates two core components: AutoTester, a multi-agent
system that generates diverse, high-quality steer samples tailored to developer
needs; and AutoRepairer, which constructs adaptive steering strategies with
anchor vectors for automated, context-aware strategy selection during
inference. Experiments on standard and customized trustworthiness tasks show
MASteer consistently outperforms baselines, improving metrics by 15.36% on
LLaMA-3.1-8B-Chat and 4.21% on Qwen-3-8B-Chat, while maintaining general model
capabilities. MASteer demonstrates strong robustness, generalization, and
practical value for scalable, efficient trustworthiness repair.

</details>


### [192] [DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning](https://arxiv.org/abs/2508.06972)
*Dan Ivanov,Tristan Freiberg,Haruna Isah*

Main category: cs.AI

TL;DR: DSperse是一个模块化框架，用于分布式机器学习推理，通过战略性的加密验证实现高效和灵活性。


<details>
  <summary>Details</summary>
Motivation: 解决分布式零知识机器学习中全模型电路化的高成本和僵化问题，提供针对性的验证策略。

Method: 通过验证选择的子计算（“切片”），支持部分或全部推理管道的验证，并通过审计、复制或经济激励确保全局一致性。

Result: 在多种证明系统下评估，展示了内存使用、运行时间和电路行为的表现，支持灵活验证策略。

Conclusion: DSperse通过灵活的验证边界设计，实现了可扩展的、针对性的验证策略，适应多样化部署需求。

Abstract: DSperse is a modular framework for distributed machine learning inference
with strategic cryptographic verification. Operating within the emerging
paradigm of distributed zero-knowledge machine learning, DSperse avoids the
high cost and rigidity of full-model circuitization by enabling targeted
verification of strategically chosen subcomputations. These verifiable
segments, or "slices", may cover part or all of the inference pipeline, with
global consistency enforced through audit, replication, or economic incentives.
This architecture supports a pragmatic form of trust minimization, localizing
zero-knowledge proofs to the components where they provide the greatest value.
We evaluate DSperse using multiple proving systems and report empirical results
on memory usage, runtime, and circuit behavior under sliced and unsliced
configurations. By allowing proof boundaries to align flexibly with the model's
logical structure, DSperse supports scalable, targeted verification strategies
suited to diverse deployment needs.

</details>


### [193] [Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model](https://arxiv.org/abs/2508.06980)
*Aswin Paul,Moein Khajehnejad,Forough Habibollahi,Brett J. Kagan,Adeel Razi*

Main category: cs.AI

TL;DR: 论文提出了一种基于主动推理的框架，用于建模具身代理的决策过程，通过实验验证了生物神经元网络的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的快速发展，理解自主代理的有目的行为基础对开发安全高效系统至关重要。生物神经元网络可能提供更高效率和可解释性。

Method: 使用基于实验的生成模型，在模拟游戏环境中模拟决策过程，并借鉴生物神经元的实验设置。

Result: 结果表明代理能够学习，揭示了基于记忆的学习和预测规划在智能决策中的作用。

Conclusion: 该工作为可解释AI领域提供了生物基础和可扩展的方法，有助于理解代理的有目的行为。

Abstract: With recent and rapid advancements in artificial intelligence (AI),
understanding the foundation of purposeful behaviour in autonomous agents is
crucial for developing safe and efficient systems. While artificial neural
networks have dominated the path to AI, recent studies are exploring the
potential of biologically based systems, such as networks of living biological
neuronal networks. Along with promises of high power and data efficiency, these
systems may also inform more explainable and biologically plausible models. In
this work, we propose a framework rooted in active inference, a general theory
of behaviour, to model decision-making in embodied agents. Using
experiment-informed generative models, we simulate decision-making processes in
a simulated game-play environment, mirroring experimental setups that use
biological neurons. Our results demonstrate learning in these agents, providing
insights into the role of memory-based learning and predictive planning in
intelligent decision-making. This work contributes to the growing field of
explainable AI by offering a biologically grounded and scalable approach to
understanding purposeful behaviour in agents.

</details>


### [194] [K-Dense Analyst: Towards Fully Automated Scientific Analysis](https://arxiv.org/abs/2508.07043)
*Orion Li,Vinayak Agarwal,Summer Zhou,Ashwin Gopinath,Timothy Kassis*

Main category: cs.AI

TL;DR: K-Dense Analyst是一个分层多代理系统，通过双循环架构实现自主生物信息学分析，性能超越当前最强语言模型。


<details>
  <summary>Details</summary>
Motivation: 现代生物信息学分析的复杂性导致数据生成与科学洞察之间存在巨大鸿沟，现有语言模型在真实分析流程中表现有限。

Method: 采用分层多代理系统，结合规划与验证执行，将复杂目标分解为可执行任务。

Result: 在BixBench基准测试中，K-Dense Analyst准确率达29.2%，比GPT-5高6.3个百分点。

Conclusion: 自主科学推理需要专门构建的系统，而不仅仅是增强的语言模型。

Abstract: The complexity of modern bioinformatics analysis has created a critical gap
between data generation and developing scientific insights. While large
language models (LLMs) have shown promise in scientific reasoning, they remain
fundamentally limited when dealing with real-world analytical workflows that
demand iterative computation, tool integration and rigorous validation. We
introduce K-Dense Analyst, a hierarchical multi-agent system that achieves
autonomous bioinformatics analysis through a dual-loop architecture. K-Dense
Analyst, part of the broader K-Dense platform, couples planning with validated
execution using specialized agents to decompose complex objectives into
executable, verifiable tasks within secure computational environments. On
BixBench, a comprehensive benchmark for open-ended biological analysis, K-Dense
Analyst achieves 29.2% accuracy, surpassing the best-performing language model
(GPT-5) by 6.3 percentage points, representing nearly 27% improvement over what
is widely considered the most powerful LLM available. Remarkably, K-Dense
Analyst achieves this performance using Gemini 2.5 Pro, which attains only
18.3% accuracy when used directly, demonstrating that our architectural
innovations unlock capabilities far beyond the underlying model's baseline
performance. Our insights demonstrate that autonomous scientific reasoning
requires more than enhanced language models, it demands purpose-built systems
that can bridge the gap between high-level scientific objectives and low-level
computational execution. These results represent a significant advance toward
fully autonomous computational biologists capable of accelerating discovery
across the life sciences.

</details>


### [195] [Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach](https://arxiv.org/abs/2508.07063)
*Naseem Machlovi,Maryam Saleki,Innocent Ababio,Ruhul Amin*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在内容审核中的局限性，并提出了一种新的框架SafePhi，其在性能上优于现有基准模型。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在日常生活中的普及，对更安全、可靠的内容审核需求增加，但LLMs在道德推理和偏见检测方面仍存在不足。

Method: 开发了一个基于SOTA模型的实验框架，评估LLMs在情感和冒犯行为识别中的表现，并引入统一基准数据集和微调模型SafePhi。

Result: SafePhi在Macro F1得分上达到0.89，优于OpenAI Moderator（0.77）和Llama Guard（0.74）。

Conclusion: 研究强调了LLMs在关键领域的不足，需结合更多异构数据和人工干预以提升模型的鲁棒性和可解释性。

Abstract: As AI systems become more integrated into daily life, the need for safer and
more reliable moderation has never been greater. Large Language Models (LLMs)
have demonstrated remarkable capabilities, surpassing earlier models in
complexity and performance. Their evaluation across diverse tasks has
consistently showcased their potential, enabling the development of adaptive
and personalized agents. However, despite these advancements, LLMs remain prone
to errors, particularly in areas requiring nuanced moral reasoning. They
struggle with detecting implicit hate, offensive language, and gender biases
due to the subjective and context-dependent nature of these issues. Moreover,
their reliance on training data can inadvertently reinforce societal biases,
leading to inconsistencies and ethical concerns in their outputs. To explore
the limitations of LLMs in this role, we developed an experimental framework
based on state-of-the-art (SOTA) models to assess human emotions and offensive
behaviors. The framework introduces a unified benchmark dataset encompassing 49
distinct categories spanning the wide spectrum of human emotions, offensive and
hateful text, and gender and racial biases. Furthermore, we introduced SafePhi,
a QLoRA fine-tuned version of Phi-4, adapting diverse ethical contexts and
outperforming benchmark moderators by achieving a Macro F1 score of 0.89, where
OpenAI Moderator and Llama Guard score 0.77 and 0.74, respectively. This
research also highlights the critical domains where LLM moderators consistently
underperformed, pressing the need to incorporate more heterogeneous and
representative data with human-in-the-loop, for better model robustness and
explainability.

</details>


### [196] [Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention](https://arxiv.org/abs/2508.07107)
*Timothy Oluwapelumi Adeyemi,Nadiah Fahad AlOtaibi*

Main category: cs.AI

TL;DR: 提出了一种反馈驱动的决策支持系统（DSS），通过闭环架构实现模型的持续优化，提升学生成绩预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型多为静态，无法适应新数据（如干预后的学生表现），限制了预测的时效性和准确性。

Method: 采用LightGBM回归器结合增量式重训练，通过Flask网页界面实时交互，并集成SHAP提升模型可解释性。

Result: 实验显示重训练后RMSE降低10.7%，且对干预学生的预测分数持续上调。

Conclusion: 该框架将静态预测器转化为自优化系统，推动教育分析向以人为本、数据驱动和响应式AI发展。

Abstract: Accurate prediction of student performance is essential for timely academic
intervention. However, most machine learning models in education are static and
cannot adapt when new data, such as post-intervention outcomes, become
available. To address this limitation, we propose a Feedback-Driven Decision
Support System (DSS) with a closed-loop architecture that enables continuous
model refinement. The system integrates a LightGBM-based regressor with
incremental retraining, allowing educators to input updated student results,
which automatically trigger model updates. This adaptive mechanism improves
prediction accuracy by learning from real-world academic progress. The platform
features a Flask-based web interface for real-time interaction and incorporates
SHAP for explainability, ensuring transparency. Experimental results show a
10.7\% reduction in RMSE after retraining, with consistent upward adjustments
in predicted scores for intervened students. By transforming static predictors
into self-improving systems, our approach advances educational analytics toward
human-centered, data-driven, and responsive AI. The framework is designed for
integration into LMS and institutional dashboards.

</details>


### [197] [Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables](https://arxiv.org/abs/2508.07186)
*Amit Dhanda*

Main category: cs.AI

TL;DR: 提出了一种基于LLM的多代理框架，用于跨维度总结结构化企业数据，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统表格到文本模型缺乏跨层次结构和上下文感知差异的推理能力，而这对业务报告至关重要。

Method: 采用多代理管道，包括数据切片、差异检测、上下文构建和基于LLM的生成。

Result: 框架在数据忠实度（83%）、显著变化覆盖率和决策关键见解相关性（4.4/5）上表现优异。

Conclusion: 该方法在复杂业务场景（如收入与销量权衡）中表现突出，显著提升了总结质量。

Abstract: We propose a novel framework for summarizing structured enterprise data
across multiple dimensions using large language model (LLM)-based agents.
Traditional table-to-text models often lack the capacity to reason across
hierarchical structures and context-aware deltas, which are essential in
business reporting tasks. Our method introduces a multi-agent pipeline that
extracts, analyzes, and summarizes multi-dimensional data using agents for
slicing, variance detection, context construction, and LLM-based generation.
Our results show that the proposed framework outperforms traditional
approaches, achieving 83\% faithfulness to underlying data, superior coverage
of significant changes, and high relevance scores (4.4/5) for decision-critical
insights. The improvements are especially pronounced in categories involving
subtle trade-offs, such as increased revenue due to price changes amid
declining unit volumes, which competing methods either overlook or address with
limited specificity. We evaluate the framework on Kaggle datasets and
demonstrate significant improvements in faithfulness, relevance, and insight
quality over baseline table summarization approaches.

</details>


### [198] [EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning](https://arxiv.org/abs/2508.07292)
*Yi Tang,Kaini Wang,Yang Chen,Guangquan Zhou*

Main category: cs.AI

TL;DR: EndoAgent是一种基于双内存设计的AI代理，用于内窥镜图像诊断，通过短期行动跟踪和长期经验学习提升推理能力，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于大规模预训练的方法在多任务协调和复杂临床流程处理上表现不足，AI代理在内窥镜领域的潜力尚未充分挖掘。

Method: 提出EndoAgent，采用双内存设计，结合短期行动跟踪和长期经验学习，集成专家工具库，支持多样化临床任务。

Result: 在EndoAgentBench基准测试中，EndoAgent优于通用和医学多模态模型，展现出强大的灵活性和推理能力。

Conclusion: EndoAgent为内窥镜诊断提供了一种高效、灵活的AI解决方案，具有广泛的应用潜力。

Abstract: Developing general artificial intelligence (AI) systems to support endoscopic
image diagnosis is an emerging research priority. Existing methods based on
large-scale pretraining often lack unified coordination across tasks and
struggle to handle the multi-step processes required in complex clinical
workflows. While AI agents have shown promise in flexible instruction parsing
and tool integration across domains, their potential in endoscopy remains
underexplored. To address this gap, we propose EndoAgent, the first
memory-guided agent for vision-to-decision endoscopic analysis that integrates
iterative reasoning with adaptive tool selection and collaboration. Built on a
dual-memory design, it enables sophisticated decision-making by ensuring
logical coherence through short-term action tracking and progressively
enhancing reasoning acuity through long-term experiential learning. To support
diverse clinical tasks, EndoAgent integrates a suite of expert-designed tools
within a unified reasoning loop. We further introduce EndoAgentBench, a
benchmark of 5,709 visual question-answer pairs that assess visual
understanding and language generation capabilities in realistic scenarios.
Extensive experiments show that EndoAgent consistently outperforms both general
and medical multimodal models, exhibiting its strong flexibility and reasoning
capabilities.

</details>


### [199] [Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape](https://arxiv.org/abs/2508.07334)
*Quan Shi,Wang Xi,Zenghui Ding,Jianqing Gao,Xianjun Yang*

Main category: cs.AI

TL;DR: 论文形式化大语言模型为概率图灵机，证明幻觉现象不可避免，并提出两种解决方案：检索增强生成（RAG）和持续学习。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLMs）幻觉现象的核心障碍，为其可靠部署提供理论基础。

Method: 构建“计算必要性层次结构”，证明幻觉在不可计算性和信息理论边界上的必然性，并提出RAG和持续学习作为解决方案。

Result: 证明了RAG通过“计算跳跃”绝对逃脱幻觉，持续学习通过“内部化预言”机制实现。

Conclusion: 论文为LLMs的可靠部署提供了形式化理论和实践路径。

Abstract: The illusion phenomenon of large language models (LLMs) is the core obstacle
to their reliable deployment. This article formalizes the large language model
as a probabilistic Turing machine by constructing a "computational necessity
hierarchy", and for the first time proves the illusions are inevitable on
diagonalization, incomputability, and information theory boundaries supported
by the new "learner pump lemma". However, we propose two "escape routes": one
is to model Retrieval Enhanced Generations (RAGs) as oracle machines, proving
their absolute escape through "computational jumps", providing the first formal
theory for the effectiveness of RAGs; The second is to formalize continuous
learning as an "internalized oracle" mechanism and implement this path through
a novel neural game theory framework.Finally, this article proposes a

</details>


### [200] [Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach](https://arxiv.org/abs/2508.07353)
*Rubing Chen,Jiaxin Wu,Jian Wang,Xulu Zhang,Wenqi Fan,Chenghua Lin,Xiao-Yong Wei,Qing Li*

Main category: cs.AI

TL;DR: 论文提出了一种基于全面性和紧凑性原则的迭代基准框架Comp-Comp，用于构建领域特定的大型语言模型（LLM）评测基准，并通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有领域特定基准主要依赖规模法则，但语料库和问答集设计对领域特定LLM的精确率和召回率的影响尚未研究。

Method: 提出Comp-Comp框架，结合全面性（语义召回）和紧凑性（精确率），指导语料库和问答集构建，并通过案例研究验证。

Result: 在知名大学案例中构建了XUBench，证明了Comp-Comp框架的有效性和可扩展性。

Conclusion: Comp-Comp框架为跨领域基准构建提供了新思路，表明规模法则并非总是最优原则。

Abstract: Numerous benchmarks have been built to evaluate the domain-specific abilities
of large language models (LLMs), highlighting the need for effective and
efficient benchmark construction. Existing domain-specific benchmarks primarily
focus on the scaling law, relying on massive corpora for supervised fine-tuning
or generating extensive question sets for broad coverage. However, the impact
of corpus and question-answer (QA) set design on the precision and recall of
domain-specific LLMs remains unexplored. In this paper, we address this gap and
demonstrate that the scaling law is not always the optimal principle for
benchmark construction in specific domains. Instead, we propose Comp-Comp, an
iterative benchmarking framework based on a comprehensiveness-compactness
principle. Here, comprehensiveness ensures semantic recall of the domain, while
compactness enhances precision, guiding both corpus and QA set construction. To
validate our framework, we conducted a case study in a well-renowned
university, resulting in the creation of XUBench, a large-scale and
comprehensive closed-domain benchmark. Although we use the academic domain as
the case in this work, our Comp-Comp framework is designed to be extensible
beyond academia, providing valuable insights for benchmark construction across
various domains.

</details>


### [201] [Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning](https://arxiv.org/abs/2508.07382)
*He Kong,Die Hu,Jingguo Ge,Liangxiong Li,Hui Li,Tong Li*

Main category: cs.AI

TL;DR: Pentest-R1是一个通过两阶段强化学习优化LLM在渗透测试中推理能力的框架，显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在渗透测试中存在错误处理差、推理效率低和无法自主完成复杂任务的问题，需要改进。

Method: 采用两阶段强化学习：离线RL学习基础攻击逻辑，在线RL在CTF环境中微调，学习自我纠错和自适应策略。

Result: 在AutoPenBench上达到24.2%成功率，Cybench上15.0%成功率，性能接近顶级专有模型。

Conclusion: 两阶段训练协同是Pentest-R1成功的关键，为开源LLM设定了新标杆。

Abstract: Automating penetration testing is crucial for enhancing cybersecurity, yet
current Large Language Models (LLMs) face significant limitations in this
domain, including poor error handling, inefficient reasoning, and an inability
to perform complex end-to-end tasks autonomously. To address these challenges,
we introduce Pentest-R1, a novel framework designed to optimize LLM reasoning
capabilities for this task through a two-stage reinforcement learning pipeline.
We first construct a dataset of over 500 real-world, multi-step walkthroughs,
which Pentest-R1 leverages for offline reinforcement learning (RL) to instill
foundational attack logic. Subsequently, the LLM is fine-tuned via online RL in
an interactive Capture The Flag (CTF) environment, where it learns directly
from environmental feedback to develop robust error self-correction and
adaptive strategies. Our extensive experiments on the Cybench and AutoPenBench
benchmarks demonstrate the framework's effectiveness. On AutoPenBench,
Pentest-R1 achieves a 24.2\% success rate, surpassing most state-of-the-art
models and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a
15.0\% success rate in unguided tasks, establishing a new state-of-the-art for
open-source LLMs and matching the performance of top proprietary models.
Ablation studies confirm that the synergy of both training stages is critical
to its success.

</details>


### [202] [Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding](https://arxiv.org/abs/2508.07388)
*Zhaoyu Chen,Hongnan Lin,Yongwei Nie,Fei Ma,Xuemiao Xu,Fei Yu,Chengjiang Long*

Main category: cs.AI

TL;DR: 论文提出Invert4TVG框架，通过三个反转任务提升视频片段定位的准确性和语义理解，无需额外数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度优化时间IoU指标，牺牲了语义动作理解，影响TVG的鲁棒性。

Method: 利用三个反转任务（动词补全、动作识别、视频描述）结合强化学习框架，平衡定位和语义优化。

Result: 实验显示方法在Charades-STA上R1@0.7指标提升7.1%，优于现有技术。

Conclusion: 通过反转任务增强语义理解，显著提高了定位准确性的上限。

Abstract: Temporal Video Grounding (TVG) seeks to localize video segments matching a
given textual query. Current methods, while optimizing for high temporal
Intersection-over-Union (IoU), often overfit to this metric, compromising
semantic action understanding in the video and query, a critical factor for
robust TVG. To address this, we introduce Inversion Tasks for TVG (Invert4TVG),
a novel framework that enhances both localization accuracy and action
understanding without additional data. Our approach leverages three inversion
tasks derived from existing TVG annotations: (1) Verb Completion, predicting
masked action verbs in queries from video segments; (2) Action Recognition,
identifying query-described actions; and (3) Video Description, generating
descriptions of video segments that explicitly embed query-relevant actions.
These tasks, integrated with TVG via a reinforcement learning framework with
well-designed reward functions, ensure balanced optimization of localization
and semantics. Experiments show our method outperforms state-of-the-art
approaches, achieving a 7.1\% improvement in R1@0.7 on Charades-STA for a 3B
model compared to Time-R1. By inverting TVG to derive query-related actions
from segments, our approach strengthens semantic understanding, significantly
raising the ceiling of localization accuracy.

</details>


### [203] [Generative AI for Strategic Plan Development](https://arxiv.org/abs/2508.07405)
*Jesse Ponnock*

Main category: cs.AI

TL;DR: 论文探讨了如何利用生成式人工智能（GAI）和大型语言模型（LLMs）为政府组织开发战略计划，并评估了BERTopic和NMF在主题建模中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着GAI和LLMs的突破，许多专业服务开始通过AI增强，本文旨在探索其在政府战略计划中的应用。

Method: 使用BERTopic和NMF对政府问责办公室（GAO）的大量报告进行主题建模，生成与战略计划中“愿景要素”相似的主题。

Result: 两种方法均能生成与100%的愿景要素相似的主题，其中BERTopic表现更优，超过一半的主题达到“中等”或“强”相关性。

Conclusion: GAI在战略计划开发中具有潜力，未来将聚焦于概念的实际应用和模型中其他模块的可行性。

Abstract: Given recent breakthroughs in Generative Artificial Intelligence (GAI) and
Large Language Models (LLMs), more and more professional services are being
augmented through Artificial Intelligence (AI), which once seemed impossible to
automate. This paper presents a modular model for leveraging GAI in developing
strategic plans for large scale government organizations and evaluates leading
machine learning techniques in their application towards one of the identified
modules. Specifically, the performance of BERTopic and Non-negative Matrix
Factorization (NMF) are evaluated in their ability to use topic modeling to
generate themes representative of Vision Elements within a strategic plan. To
accomplish this, BERTopic and NMF models are trained using a large volume of
reports from the Government Accountability Office (GAO). The generated topics
from each model are then scored for similarity against the Vision Elements of a
published strategic plan and the results are compared. Our results show that
these techniques are capable of generating themes similar to 100% of the
elements being evaluated against. Further, we conclude that BERTopic performs
best in this application with more than half of its correlated topics achieving
a "medium" or "strong" correlation. A capability of GAI-enabled strategic plan
development impacts a multi-billion dollar industry and assists the federal
government in overcoming regulatory requirements which are crucial to the
public good. Further work will focus on the operationalization of the concept
proven in this study as well as viability of the remaining modules in the
proposed model for GAI-generated strategic plans.

</details>


### [204] [A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems](https://arxiv.org/abs/2508.07407)
*Jinyuan Fang,Yanwen Peng,Xi Zhang,Yingxu Wang,Xinhao Yi,Guibin Zhang,Yi Xu,Bin Wu,Siwei Liu,Zihao Li,Zhaochun Ren,Nikos Aletras,Xi Wang,Han Zhou,Zaiqiao Meng*

Main category: cs.AI

TL;DR: 本文综述了自进化AI代理系统的技术，提出了统一框架并探讨了领域专用策略及伦理问题。


<details>
  <summary>Details</summary>
Motivation: 现有代理系统依赖静态配置，无法适应动态环境，需自进化技术提升适应性。

Method: 提出统一框架，系统回顾自进化技术，并分析领域专用策略及伦理评估。

Result: 总结了自进化代理系统的关键组件、技术及领域应用，强调其适应性和可靠性。

Conclusion: 为开发更自适应、自主的代理系统奠定基础，推动终身学习代理的发展。

Abstract: Recent advances in large language models have sparked growing interest in AI
agents capable of solving complex, real-world tasks. However, most existing
agent systems rely on manually crafted configurations that remain static after
deployment, limiting their ability to adapt to dynamic and evolving
environments. To this end, recent research has explored agent evolution
techniques that aim to automatically enhance agent systems based on interaction
data and environmental feedback. This emerging direction lays the foundation
for self-evolving AI agents, which bridge the static capabilities of foundation
models with the continuous adaptability required by lifelong agentic systems.
In this survey, we provide a comprehensive review of existing techniques for
self-evolving agentic systems. Specifically, we first introduce a unified
conceptual framework that abstracts the feedback loop underlying the design of
self-evolving agentic systems. The framework highlights four key components:
System Inputs, Agent System, Environment, and Optimisers, serving as a
foundation for understanding and comparing different strategies. Based on this
framework, we systematically review a wide range of self-evolving techniques
that target different components of the agent system. We also investigate
domain-specific evolution strategies developed for specialised fields such as
biomedicine, programming, and finance, where optimisation objectives are
tightly coupled with domain constraints. In addition, we provide a dedicated
discussion on the evaluation, safety, and ethical considerations for
self-evolving agentic systems, which are critical to ensuring their
effectiveness and reliability. This survey aims to provide researchers and
practitioners with a systematic understanding of self-evolving AI agents,
laying the foundation for the development of more adaptive, autonomous, and
lifelong agentic systems.

</details>


### [205] [Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs](https://arxiv.org/abs/2508.07466)
*Dom Huh,Prasant Mohapatra*

Main category: cs.AI

TL;DR: 论文提出了一种多智能体大语言模型（LLM）的系统框架，通过整合多智能体决策算法，优化了LLM的协作能力。


<details>
  <summary>Details</summary>
Motivation: 语言是协作和推理的基础，建立共同语言有助于清晰沟通和多智能体协调。本文旨在扩展LLM在多智能体环境中的能力。

Method: 提出系统框架，包括高级提示工程、记忆架构设计、多模态信息处理和微调对齐策略。

Result: 在经典游戏场景中通过消融实验验证了设计选择的有效性。

Conclusion: 整合多智能体决策算法可显著提升LLM的协作能力，为多智能体系统提供新思路。

Abstract: Language is a ubiquitous tool that is foundational to reasoning and
collaboration, ranging from everyday interactions to sophisticated
problem-solving tasks. The establishment of a common language can serve as a
powerful asset in ensuring clear communication and understanding amongst
agents, facilitating desired coordination and strategies. In this work, we
extend the capabilities of large language models (LLMs) by integrating them
with advancements in multi-agent decision-making algorithms. We propose a
systematic framework for the design of multi-agentic large language models
(LLMs), focusing on key integration practices. These include advanced prompt
engineering techniques, the development of effective memory architectures,
multi-modal information processing, and alignment strategies through
fine-tuning algorithms. We evaluate these design choices through extensive
ablation studies on classic game settings with significant underlying social
dilemmas and game-theoretic considerations.

</details>


### [206] [CP-Agent: Agentic Constraint Programming](https://arxiv.org/abs/2508.07468)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 提出了一种基于纯代理策略的新方法，利用ReAct原则的Python编码代理，成功解决了CP-Bench基准集中的所有101个问题。


<details>
  <summary>Details</summary>
Motivation: 将自然语言问题描述转化为形式化约束模型是约束编程中的核心挑战，现有固定流程方法在多数基准问题上表现不佳。

Method: 采用无固定流程的纯代理策略，基于ReAct原则开发通用Python编码代理，通过精心设计的项目提示注入领域知识。

Result: 该方法成功解决了CP-Bench基准集中的所有101个问题。

Conclusion: 约束建模任务需要通用编码工具与提示编码的领域知识结合，而非专用代理架构或预定义流程。

Abstract: Translating natural language problem descriptions into formal constraint
models remains a fundamental challenge in constraint programming, requiring
deep expertise in both the problem domain and modeling frameworks. Previous
approaches to automating this translation have employed fixed workflows with
predetermined modeling steps, failing on a significant number of benchmark
problems. We present a new approach using a pure agentic strategy without any
fixed pipeline. We developed a general-purpose Python coding agent based on the
ReAct (Reason and Act) principle, utilizing a persistent IPython kernel for
stateful code execution and iterative development. Rather than embedding
constraint programming logic into the agent architecture, domain-specific
expertise is injected solely through a carefully crafted project prompt. The
agent combines this prompt-encoded knowledge with access to file operations and
code execution tools, enabling it to test hypotheses, debug failures, and
verify solutions dynamically. Implemented in just a few hundred lines of code,
this architecture successfully solves all 101 problems of the CP-Bench
constraint programming benchmark set. The results suggest that constraint
modeling tasks require the combination of general coding tools and domain
expertise encoded in prompts, rather than specialized agent architectures or
predefined workflows.

</details>


### [207] [Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy](https://arxiv.org/abs/2508.07485)
*Alexander Duffy,Samuel J Paech,Ishana Shastri,Elizabeth Karpinski,Baptiste Alloui-Cros,Tyler Marques,Matthew Lyle Olson*

Main category: cs.AI

TL;DR: 提出了一种无需微调或专门训练即可让本地大型语言模型（LLM）玩完整版《外交》游戏的评估工具。


<details>
  <summary>Details</summary>
Motivation: 解决《外交》游戏状态复杂和信息密度高的问题，使其无需依赖前沿LLM或微调即可研究。

Method: 通过数据驱动的迭代优化文本游戏状态表示，开发工具支持假设测试和统计分析。

Result: 24B模型无需微调即可可靠完成比赛，大型模型表现最佳，小型模型也能胜任。

Conclusion: 该工具使LLM的战略推理评估民主化，揭示了这些能力如何从广泛使用的LLM中自然涌现。

Abstract: We present the first evaluation harness that enables any out-of-the-box,
local, Large Language Models (LLMs) to play full-press Diplomacy without
fine-tuning or specialized training. Previous work required frontier LLMs, or
fine-tuning, due to the high complexity and information density of Diplomacy's
game state. Combined with the high variance of matches, these factors made
Diplomacy prohibitive for study. In this work, we used data-driven iteration to
optimize a textual game state representation such that a 24B model can reliably
complete matches without any fine tuning. We develop tooling to facilitate
hypothesis testing and statistical analysis, and we present case studies on
persuasion, aggressive playstyles, and performance across a range of models. We
conduct a variety of experiments across many popular LLMs, finding the larger
models perform the best, but the smaller models still play adequately. We also
introduce Critical State Analysis: an experimental protocol for rapidly
iterating and analyzing key moments in a game at depth. Our harness
democratizes the evaluation of strategic reasoning in LLMs by eliminating the
need for fine-tuning, and it provides insights into how these capabilities
emerge naturally from widely used LLMs. Our code is available in the supplement
and will be open sourced.

</details>


### [208] [MCPToolBench++: A Large Scale AI Agent Model Context Protocol MCP Tool Use Benchmark](https://arxiv.org/abs/2508.07575)
*Shiqing Fan,Xichen Ding,Liang Zhang,Linjian Mo*

Main category: cs.AI

TL;DR: 论文提出MCPToolBench++，一个用于评估LLMs调用MCP工具性能的大规模多领域基准，解决了现有评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法缺乏全面数据集和统一标准，且MCP工具的成功率和格式多样性增加了评估难度。

Method: 构建基于4000多个MCP服务器的多领域基准，包含单步和多步工具调用数据集。

Result: 评估了具有代理能力的SOTA LLMs，并报告了结果。

Conclusion: MCPToolBench++为评估LLMs在MCP工具调用中的性能提供了有效解决方案。

Abstract: LLMs' capabilities are enhanced by using function calls to integrate various
data sources or API results into the context window. Typical tools include
search, web crawlers, maps, financial data, file systems, and browser usage,
etc. Integrating these data sources or functions requires a standardized
method. The Model Context Protocol (MCP) provides a standardized way to supply
context to LLMs. However, the evaluation of LLMs and AI Agents' MCP tool use
abilities suffer from several issues. First, there's a lack of comprehensive
datasets or benchmarks to evaluate various MCP tools. Second, the diverse
formats of response from MCP tool call execution further increase the
difficulty of evaluation. Additionally, unlike existing tool-use benchmarks
with high success rates in functions like programming and math functions, the
success rate of real-world MCP tool is not guaranteed and varies across
different MCP servers. Furthermore, the LLMs' context window also limits the
number of available tools that can be called in a single run, because the
textual descriptions of tool and the parameters have long token length for an
LLM to process all at once. To help address the challenges of evaluating LLMs'
performance on calling MCP tools, we propose MCPToolBench++, a large-scale,
multi-domain AI Agent tool use benchmark. As of July 2025, this benchmark is
build upon marketplace of over 4k MCP servers from more than 40 categories,
collected from the MCP marketplaces and GitHub communities. The datasets
consist of both single-step and multi-step tool calls across different
categories. We evaluated SOTA LLMs with agentic abilities on this benchmark and
reported the results.

</details>


### [209] [Optimization of Private Semantic Communication Performance: An Uncooperative Covert Communication Method](https://arxiv.org/abs/2508.07586)
*Wenjing Zhang,Ye Hu,Tao Luo,Zhilong Zhang,Mingzhe Chen*

Main category: cs.AI

TL;DR: 提出了一种新颖的隐蔽语义通信框架，通过友好干扰器和时间槽优化策略保护语义信息传输，避免被攻击者窃取。


<details>
  <summary>Details</summary>
Motivation: 解决语义通信中信息被攻击者窃取的问题，提升隐私和传输质量。

Method: 采用优先采样辅助的双延迟深度确定性策略梯度算法，联合优化语义信息和传输功率。

Result: 仿真结果显示，隐私和传输质量分别提升77.8%和14.3%。

Conclusion: 所提算法能有效提升隐蔽语义通信的安全性和性能。

Abstract: In this paper, a novel covert semantic communication framework is
investigated. Within this framework, a server extracts and transmits the
semantic information, i.e., the meaning of image data, to a user over several
time slots. An attacker seeks to detect and eavesdrop the semantic transmission
to acquire details of the original image. To avoid data meaning being
eavesdropped by an attacker, a friendly jammer is deployed to transmit jamming
signals to interfere the attacker so as to hide the transmitted semantic
information. Meanwhile, the server will strategically select time slots for
semantic information transmission. Due to limited energy, the jammer will not
communicate with the server and hence the server does not know the transmit
power of the jammer. Therefore, the server must jointly optimize the semantic
information transmitted at each time slot and the corresponding transmit power
to maximize the privacy and the semantic information transmission quality of
the user. To solve this problem, we propose a prioritised sampling assisted
twin delayed deep deterministic policy gradient algorithm to jointly determine
the transmitted semantic information and the transmit power per time slot
without the communications between the server and the jammer. Compared to
standard reinforcement learning methods, the propose method uses an additional
Q network to estimate Q values such that the agent can select the action with a
lower Q value from the two Q networks thus avoiding local optimal action
selection and estimation bias of Q values. Simulation results show that the
proposed algorithm can improve the privacy and the semantic information
transmission quality by up to 77.8% and 14.3% compared to the traditional
reinforcement learning methods.

</details>


### [210] [HGMF: A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol](https://arxiv.org/abs/2508.07602)
*Wenpeng Xing,Zhipeng Chen,Changting Lin,Meng Han*

Main category: cs.AI

TL;DR: HGMF是一种概率剪枝方法，通过分层高斯混合模型提高LLM工具选择的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在大规模、分层结构工具库中选择正确工具的挑战，避免低准确率和高计算成本。

Method: HGMF将查询和工具描述映射到统一语义空间，分两阶段使用高斯混合模型聚类和过滤工具。

Result: 实验证明HGMF显著提高了工具选择准确性并降低了推理延迟。

Conclusion: HGMF是一种可扩展且有效的方法，适用于大规模工具库。

Abstract: Invoking external tools enables Large Language Models (LLMs) to perform
complex, real-world tasks, yet selecting the correct tool from large,
hierarchically-structured libraries remains a significant challenge. The
limited context windows of LLMs and noise from irrelevant options often lead to
low selection accuracy and high computational costs. To address this, we
propose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic
pruning method for scalable tool invocation. HGMF first maps the user query and
all tool descriptions into a unified semantic space. The framework then
operates in two stages: it clusters servers using a Gaussian Mixture Model
(GMM) and filters them based on the query's likelihood. Subsequently, it
applies the same GMM-based clustering and filtering to the tools associated
with the selected servers. This hierarchical process produces a compact,
high-relevance candidate set, simplifying the final selection task for the LLM.
Experiments on a public dataset show that HGMF significantly improves tool
selection accuracy while reducing inference latency, confirming the framework's
scalability and effectiveness for large-scale tool libraries.

</details>


### [211] [ThinkTuning: Instilling Cognitive Reflections without Distillation](https://arxiv.org/abs/2508.07616)
*Aswin RRV,Jacob Dineen,Divij Handa,Md Nayem Uddin,Mihir Parmar,Chitta Baral,Ben Zhou*

Main category: cs.AI

TL;DR: 论文提出ThinkTuning方法，通过教师模型指导学生模型，提升其推理能力，实验显示性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习（RL）方法无法真正赋予模型新的推理能力，仅能挖掘已有行为。如何让不具备此类能力的模型发展出推理能力成为研究动机。

Method: 采用GRPO框架，通过教师模型提供反馈指导，模拟课堂教学中的问题-尝试-反馈-解决方案流程，逐步提升学生模型的推理能力。

Result: 实验表明，ThinkTuning在多个基准测试中平均提升3.85%，在MATH-500、AIME和GPQA-Diamond上分别提升2.08%、2.23%和3.99%。

Conclusion: ThinkTuning通过教师模型的隐式监督有效提升了学生模型的推理能力，为模型训练提供了新思路。

Abstract: Recent advances in test-time scaling have led to the emergence of thinking
LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL
drives this self-improvement paradigm, a recent study (Gandhi et al., 2025)
shows that RL alone does not truly instill these new reasoning abilities - it
merely draws out behaviors already present in the base models. This raises a
question: How can we train the models that don't exhibit such thinking behavior
to develop it in the first place? To this end, we propose ThinkTuning, a
GRPO-based interactive training approach where we augment the rollouts of a
student model with the guidance from a teacher model. A simple idea from
classroom practice inspires our method: a teacher poses a problem, lets the
student try an answer, then gives corrective feedback -- enough to point the
mind in the right direction and then show the solution. Each piece of feedback
reshapes the student's thoughts, leading them to arrive at the correct
solution. Similarly, we find that this type of implicit supervision through
feedback from a teacher model of the same size improves the reasoning
capabilities of the student model. In particular, on average, our method shows
a 3.85% improvement over zero-shot baselines across benchmarks, and on
MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements
over the vanilla-GRPO baseline. Source code is available at
https://github.com/3rdAT/ThinkTuning.

</details>


### [212] [Multimodal AI Systems for Enhanced Laying Hen Welfare Assessment and Productivity Optimization](https://arxiv.org/abs/2508.07628)
*Daniel Essien,Suresh Neethirajan*

Main category: cs.AI

TL;DR: 论文探讨了利用多模态AI技术替代传统主观、劳动密集型的家禽福利监测方法，提出了一种基于特征级融合策略的智能监测系统，并引入了新的评估工具和部署框架。


<details>
  <summary>Details</summary>
Motivation: 传统家禽福利监测方法受限于人工观察和单一传感器数据，无法全面反映现代养殖场中蛋鸡的多维度福利状况。多模态AI技术为解决这一问题提供了突破性方案。

Method: 采用多模态AI技术，整合视觉、声学、环境和生理数据流，提出特征级融合策略，并引入域转移评分（DTS）和数据可靠性指数（DRI）作为评估工具。

Result: 特征级融合策略在鲁棒性和性能之间取得了最佳平衡，并具有更好的可扩展性。同时，新提出的评估工具和部署框架解决了传感器脆弱性和跨农场泛化性等问题。

Conclusion: 该研究为从被动、单一模态监测转向主动、精准驱动的家禽福利系统奠定了基础，实现了生产效率与科学伦理的结合。

Abstract: The future of poultry production depends on a paradigm shift replacing
subjective, labor-intensive welfare checks with data-driven, intelligent
monitoring ecosystems. Traditional welfare assessments-limited by human
observation and single-sensor data-cannot fully capture the complex,
multidimensional nature of laying hen welfare in modern farms. Multimodal
Artificial Intelligence (AI) offers a breakthrough, integrating visual,
acoustic, environmental, and physiological data streams to reveal deeper
insights into avian welfare dynamics. This investigation highlights multimodal
As transformative potential, showing that intermediate (feature-level) fusion
strategies achieve the best balance between robustness and performance under
real-world poultry conditions, and offer greater scalability than early or late
fusion approaches. Key adoption barriers include sensor fragility in harsh farm
environments, high deployment costs, inconsistent behavioral definitions, and
limited cross-farm generalizability. To address these, we introduce two novel
evaluation tools - the Domain Transfer Score (DTS) to measure model
adaptability across diverse farm settings, and the Data Reliability Index (DRI)
to assess sensor data quality under operational constraints. We also propose a
modular, context-aware deployment framework designed for laying hen
environments, enabling scalable and practical integration of multimodal
sensing. This work lays the foundation for a transition from reactive, unimodal
monitoring to proactive, precision-driven welfare systems that unite
productivity with ethical, science based animal care.

</details>


### [213] [Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents](https://arxiv.org/abs/2508.07642)
*Tianyi Ma,Yue Zhang,Zehao Wang,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: SkillNav是一个模块化框架，通过引入基于技能的推理改进Transformer-based VLN代理，在R2R和GSA-R2R基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前VLN方法在复杂空间和时间推理场景中泛化能力不足，需要更结构化的解决方案。

Method: SkillNav将导航分解为可解释的原子技能，并使用VLM-based路由器动态选择最适合的代理。

Result: 在R2R基准测试中达到新SOTA，并在GSA-R2R中展示强泛化能力。

Conclusion: SkillNav通过模块化和技能分解显著提升了VLN任务的性能与泛化能力。

Abstract: Vision-and-Language Navigation (VLN) poses significant challenges in enabling
agents to interpret natural language instructions and navigate complex 3D
environments. While recent progress has been driven by large-scale pre-training
and data augmentation, current methods still struggle to generalize to unseen
scenarios, particularly when complex spatial and temporal reasoning is
required. In this work, we propose SkillNav, a modular framework that
introduces structured, skill-based reasoning into Transformer-based VLN agents.
Our method decomposes navigation into a set of interpretable atomic skills
(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each
handled by a specialized agent. We then introduce a novel zero-shot
Vision-Language Model (VLM)-based router, which dynamically selects the most
suitable agent at each time step by aligning sub-goals with visual observations
and historical actions. SkillNav achieves a new state-of-the-art performance on
the R2R benchmark and demonstrates strong generalization to the GSA-R2R
benchmark that includes novel instruction styles and unseen environments.

</details>


### [214] [Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation](https://arxiv.org/abs/2508.07649)
*Jie Li,Haoye Dong,Zhengyang Wu,Zetao Zheng,Mingrong Lin*

Main category: cs.AI

TL;DR: DiMuST模型通过解耦表示学习解决POI推荐中时空转换的冗余问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将时空转换分开建模，导致关键节点表示不一致，引入冗余信息，影响模型效果和可解释性。

Method: 提出DiMuST模型，采用解耦变分多图自动编码器（DAE），分离共享和私有分布，通过PoE机制融合共享特征，对比约束去噪私有特征。

Result: 在两个数据集上，DiMuST在多项指标上显著优于现有方法。

Conclusion: DiMuST成功解决了时空转换的冗余问题，提升了POI推荐的性能和可解释性。

Abstract: Next Point-of-Interest (POI) recommendation is a research hotspot in business
intelligence, where users' spatial-temporal transitions and social
relationships play key roles. However, most existing works model spatial and
temporal transitions separately, leading to misaligned representations of the
same spatial-temporal key nodes. This misalignment introduces redundant
information during fusion, increasing model uncertainty and reducing
interpretability. To address this issue, we propose DiMuST, a socially enhanced
POI recommendation model based on disentangled representation learning over
multiplex spatial-temporal transition graphs. The model employs a novel
Disentangled variational multiplex graph Auto-Encoder (DAE), which first
disentangles shared and private distributions using a multiplex
spatial-temporal graph strategy. It then fuses the shared features via a
Product of Experts (PoE) mechanism and denoises the private features through
contrastive constraints. The model effectively captures the spatial-temporal
transition representations of POIs while preserving the intrinsic correlation
of their spatial-temporal relationships. Experiments on two challenging
datasets demonstrate that our DiMuST significantly outperforms existing methods
across multiple metrics.

</details>


### [215] [1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning](https://arxiv.org/abs/2508.07667)
*Wenkai Li,Liwen Sun,Zhenxiang Guan,Xuhui Zhou,Maarten Sap*

Main category: cs.AI

TL;DR: 论文提出了一种多智能体框架，通过分解隐私推理任务减少单智能体信息负载，显著降低隐私信息泄露。


<details>
  <summary>Details</summary>
Motivation: 解决多源信息处理中上下文隐私保护的挑战，如会议摘要中公私信息的混合。

Method: 采用多智能体框架，将隐私推理分解为提取和分类等子任务，并通过迭代验证提高隐私规范遵守。

Result: 实验表明，最佳多智能体配置显著减少隐私泄露（ConfAIde 18%，PrivacyLens 19%），同时保持公共内容准确性。

Conclusion: 多智能体系统的信息流设计在LLM上下文隐私保护中具有潜力。

Abstract: Addressing contextual privacy concerns remains challenging in interactive
settings where large language models (LLMs) process information from multiple
sources (e.g., summarizing meetings with private and public information). We
introduce a multi-agent framework that decomposes privacy reasoning into
specialized subtasks (extraction, classification), reducing the information
load on any single agent while enabling iterative validation and more reliable
adherence to contextual privacy norms. To understand how privacy errors emerge
and propagate, we conduct a systematic ablation over information-flow
topologies, revealing when and why upstream detection mistakes cascade into
downstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with
several open-source and closed-sourced LLMs demonstrate that our best
multi-agent configuration substantially reduces private information leakage
(\textbf{18\%} on ConfAIde and \textbf{19\%} on PrivacyLens with GPT-4o) while
preserving the fidelity of public content, outperforming single-agent
baselines. These results highlight the promise of principled information-flow
design in multi-agent systems for contextual privacy with LLMs.

</details>


### [216] [EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration](https://arxiv.org/abs/2508.07671)
*Mohamed Rayan Barhdadi,Mehmet Tuncel,Erchin Serpedin,Hasan Kurban*

Main category: cs.AI

TL;DR: EMPATHIA是一个多智能体框架，旨在解决难民整合中的文化、情感和伦理问题，通过模块化设计（SEED、RISE、THRIVE）实现透明和可解释的决策。


<details>
  <summary>Details</summary>
Motivation: 当前AI方法在难民整合中仅关注就业等狭窄目标，忽视了文化、情感和伦理等长期成功的关键维度。

Method: 基于Kegan的建构发展理论，EMPATHIA分为三个模块：SEED（初始安置）、RISE（早期独立）和THRIVE（持续结果），使用情感、文化和伦理智能体进行透明决策。

Result: 在UN Kakuma数据集上验证，覆盖15,026名难民，实现87.4%的验证收敛，并在五个东道国提供可解释的评估。

Conclusion: EMPATHIA通过平衡文化、情感和伦理因素，支持人机协作，为多价值协调的AI驱动任务提供了通用框架。

Abstract: Current AI approaches to refugee integration optimize narrow objectives such
as employment and fail to capture the cultural, emotional, and ethical
dimensions critical for long-term success. We introduce EMPATHIA (Enriched
Multimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance),
a multi-agent framework addressing the central Creative AI question: how do we
preserve human dignity when machines participate in life-altering decisions?
Grounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes
integration into three modules: SEED (Socio-cultural Entry and Embedding
Decision) for initial placement, RISE (Rapid Integration and Self-sufficiency
Engine) for early independence, and THRIVE (Transcultural Harmony and
Resilience through Integrated Values and Engagement) for sustained outcomes.
SEED employs a selector-validator architecture with three specialized agents -
emotional, cultural, and ethical - that deliberate transparently to produce
interpretable recommendations. Experiments on the UN Kakuma dataset (15,026
individuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and
implementation on 6,359 working-age refugees (15+) with 150+ socioeconomic
variables achieved 87.4% validation convergence and explainable assessments
across five host countries. EMPATHIA's weighted integration of cultural,
emotional, and ethical factors balances competing value systems while
supporting practitioner-AI collaboration. By augmenting rather than replacing
human expertise, EMPATHIA provides a generalizable framework for AI-driven
allocation tasks where multiple values must be reconciled.

</details>


### [217] [Ethics2vec: aligning automatic agents and human preferences](https://arxiv.org/abs/2508.07673)
*Gianluca Bontempi*

Main category: cs.AI

TL;DR: 论文提出Ethics2Vec方法，通过向量化表示AI决策行为，以评估其与人类伦理价值的对齐。


<details>
  <summary>Details</summary>
Motivation: 解决AI行为中隐含的伦理价值难以量化与对齐的问题，尤其是在涉及不可比较的伦理考量时。

Method: 扩展Anything2vec方法，将AI决策策略映射为多元向量表示，用于比较与人类价值的对齐程度。

Result: 提出了Ethics2Vec方法，并在二元决策和自动驾驶控制场景中验证其可行性。

Conclusion: Ethics2Vec为AI伦理对齐提供了一种可量化的新途径，适用于复杂决策场景。

Abstract: Though intelligent agents are supposed to improve human experience (or make
it more efficient), it is hard from a human perspective to grasp the ethical
values which are explicitly or implicitly embedded in an agent behaviour. This
is the well-known problem of alignment, which refers to the challenge of
designing AI systems that align with human values, goals and preferences. This
problem is particularly challenging since most human ethical considerations
refer to \emph{incommensurable} (i.e. non-measurable and/or incomparable)
values and criteria. Consider, for instance, a medical agent prescribing a
treatment to a cancerous patient. How could it take into account (and/or weigh)
incommensurable aspects like the value of a human life and the cost of the
treatment? Now, the alignment between human and artificial values is possible
only if we define a common space where a metric can be defined and used. This
paper proposes to extend to ethics the conventional Anything2vec approach,
which has been successful in plenty of similar and hard-to-quantify domains
(ranging from natural language processing to recommendation systems and graph
analysis). This paper proposes a way to map an automatic agent decision-making
(or control law) strategy to a multivariate vector representation, which can be
used to compare and assess the alignment with human values. The Ethics2Vec
method is first introduced in the case of an automatic agent performing binary
decision-making. Then, a vectorisation of an automatic control law (like in the
case of a self-driving car) is discussed to show how the approach can be
extended to automatic control settings.

</details>


### [218] [Symmetry-Aware Transformer Training for Automated Planning](https://arxiv.org/abs/2508.07743)
*Markus Fritzsche,Elliot Gestrin,Jendrik Seipp*

Main category: cs.AI

TL;DR: 本文提出了一种新的对比学习目标，使Transformer能够感知对称性，从而弥补其在自动规划领域的局限性。


<details>
  <summary>Details</summary>
Motivation: Transformer在自动规划领域的应用受限，主要由于问题对称性导致的组合爆炸问题。

Method: 提出了一种对比学习目标，结合架构改进，使Transformer能够高效训练用于规划生成或启发式预测。

Result: 实验结果表明，对称感知训练有效解决了PlanGPT的局限性。

Conclusion: 对称感知训练显著提升了Transformer在自动规划中的表现。

Abstract: While transformers excel in many settings, their application in the field of
automated planning is limited. Prior work like PlanGPT, a state-of-the-art
decoder-only transformer, struggles with extrapolation from easy to hard
planning problems. This in turn stems from problem symmetries: planning tasks
can be represented with arbitrary variable names that carry no meaning beyond
being identifiers. This causes a combinatorial explosion of equivalent
representations that pure transformers cannot efficiently learn from. We
propose a novel contrastive learning objective to make transformers
symmetry-aware and thereby compensate for their lack of inductive bias.
Combining this with architectural improvements, we show that transformers can
be efficiently trained for either plan-generation or heuristic-prediction. Our
results across multiple planning domains demonstrate that our symmetry-aware
training effectively and efficiently addresses the limitations of PlanGPT.

</details>


### [219] [Best-Effort Policies for Robust Markov Decision Processes](https://arxiv.org/abs/2508.07790)
*Alessandro Abate,Thom Badings,Giuseppe De Giacomo,Francesco Fabiano*

Main category: cs.AI

TL;DR: 论文研究了鲁棒马尔可夫决策过程（RMDPs），提出了一种新的策略选择标准——最优鲁棒最佳努力（ORBE）策略，以在非完全对抗性概率下最大化期望回报。


<details>
  <summary>Details</summary>
Motivation: 在RMDPs中，虽然可以通过鲁棒值迭代高效计算最优鲁棒策略，但这些策略在非对抗性概率下的表现可能不同。因此，需要一种更精细的策略选择标准。

Method: 提出了ORBE策略的概念，结合了博弈论中的优势与最佳努力思想，要求策略在对抗性和非对抗性概率下均表现优异。并给出了计算ORBE策略的算法。

Result: 证明了ORBE策略的存在性，描述了其结构，并通过数值实验验证了方法的可行性。

Conclusion: ORBE策略为RMDPs提供了一种更优的策略选择标准，能够有效区分最优鲁棒策略在非对抗性环境中的表现。

Abstract: We study the common generalization of Markov decision processes (MDPs) with
sets of transition probabilities, known as robust MDPs (RMDPs). A standard goal
in RMDPs is to compute a policy that maximizes the expected return under an
adversarial choice of the transition probabilities. If the uncertainty in the
probabilities is independent between the states, known as s-rectangularity,
such optimal robust policies can be computed efficiently using robust value
iteration. However, there might still be multiple optimal robust policies,
which, while equivalent with respect to the worst-case, reflect different
expected returns under non-adversarial choices of the transition probabilities.
Hence, we propose a refined policy selection criterion for RMDPs, drawing
inspiration from the notions of dominance and best-effort in game theory.
Instead of seeking a policy that only maximizes the worst-case expected return,
we additionally require the policy to achieve a maximal expected return under
different (i.e., not fully adversarial) transition probabilities. We call such
a policy an optimal robust best-effort (ORBE) policy. We prove that ORBE
policies always exist, characterize their structure, and present an algorithm
to compute them with a small overhead compared to standard robust value
iteration. ORBE policies offer a principled tie-breaker among optimal robust
policies. Numerical experiments show the feasibility of our approach.

</details>


### [220] [KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations](https://arxiv.org/abs/2508.07834)
*Mubaris Nadeem,Johannes Zenkert,Lisa Bender,Christian Weber,Madjid Fathi*

Main category: cs.AI

TL;DR: 本文提出了一种基于知识图谱的创新知识管理方法，旨在为急救人员提供智能治疗建议，以优化紧急医疗响应。


<details>
  <summary>Details</summary>
Motivation: 全球救援需求迅速增长，急救人员需在短时间内提供个性化医疗并评估患者状况，但缺乏实时知识支持。

Method: 采用知识图谱作为核心知识表示，结合人工智能预识别技术，为急救人员提供智能推荐。

Result: 该方法能够为急救人员提供实时、智能的治疗建议，优化紧急医疗响应。

Conclusion: 知识图谱与人工智能的结合显著提升了急救效率，为紧急医疗提供了创新解决方案。

Abstract: Over the years, the need for rescue operations throughout the world has
increased rapidly. Demographic changes and the resulting risk of injury or
health disorders form the basis for emergency calls. In such scenarios, first
responders are in a rush to reach the patient in need, provide first aid, and
save lives. In these situations, they must be able to provide personalized and
optimized healthcare in the shortest possible time and estimate the patients
condition with the help of freshly recorded vital data in an emergency
situation. However, in such a timedependent situation, first responders and
medical experts cannot fully grasp their knowledge and need assistance and
recommendation for further medical treatments. To achieve this, on the spot
calculated, evaluated, and processed knowledge must be made available to
improve treatments by first responders. The Knowledge Graph presented in this
article as a central knowledge representation provides first responders with an
innovative knowledge management that enables intelligent treatment
recommendations with an artificial intelligence-based pre-recognition of the
situation.

</details>


### [221] [\(X\)-evolve: Solution space evolution powered by large language models](https://arxiv.org/abs/2508.07932)
*Yi Zhai,Zhiqiang Wei,Ruohan Li,Keyu Pan,Shuo Liu,Lu Zhang,Jianmin Ji,Wuyang Zhang,Yu Zhang,Yanyong Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为X-evolve的新方法，通过演化解空间而非单个解，显著减少了大语言模型（LLM）的调用成本，并在多个优化问题上取得了突破性成果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常演化单个解，导致LLM调用成本高，限制了复杂优化问题的解决效率。

Method: X-evolve通过生成可调程序定义解空间，利用基于分数的搜索算法高效探索参数化空间。

Result: 在三个优化问题上取得显著成果：cap set问题、信息论中的独立集问题以及在线装箱问题。

Conclusion: X-evolve通过演化解空间显著提升了搜索效率，解决了高维问题的计算难题。

Abstract: While combining large language models (LLMs) with evolutionary algorithms
(EAs) shows promise for solving complex optimization problems, current
approaches typically evolve individual solutions, often incurring high LLM call
costs. We introduce \(X\)-evolve, a paradigm-shifting method that instead
evolves solution spaces \(X\) (sets of individual solutions) - subsets of the
overall search space \(S\). In \(X\)-evolve, LLMs generate tunable programs
wherein certain code snippets, designated as parameters, define a tunable
solution space. A score-based search algorithm then efficiently explores this
parametrically defined space, guided by feedback from objective function
scores. This strategy enables broader and more efficient exploration, which can
potentially accelerate convergence at a much lower search cost, requiring up to
two orders of magnitude fewer LLM calls than prior leading methods. We
demonstrate \(X\)-evolve's efficacy across three distinct hard optimization
problems. For the cap set problem, we discover a larger partial admissible set,
establishing a new tighter asymptotic lower bound for the cap set constant (\(C
\ge 2.2203\)). In information theory, we uncover a larger independent set for
the 15-vertex cycle graph (\(\mathcal{C}_{15}^{\boxtimes 5}\), size 19,946),
thereby raising the known lower bound on its Shannon capacity. Furthermore, for
the NP-hard online bin packing problem, we generate heuristics that
consistently outperform standard strategies across established benchmarks. By
evolving solution spaces, our method considerably improves search
effectiveness, making it possible to tackle high-dimensional problems that were
previously computationally prohibitive.

</details>


### [222] [Deep Reinforcement Learning with anticipatory reward in LSTM for Collision Avoidance of Mobile Robots](https://arxiv.org/abs/2508.07941)
*Olivier Poulet,Frédéric Guinand,François Guérin*

Main category: cs.AI

TL;DR: 提出一种基于LSTM短期预测的碰撞风险预判方法，通过动态调整DQN奖励减少碰撞。


<details>
  <summary>Details</summary>
Motivation: 在无通信或标识的受限环境中，机器人运动易碰撞，需低成本解决方案。

Method: 使用LSTM预测机器人位置，动态调整DQN奖励以预判碰撞风险。

Result: 在1Hz采样频率下，碰撞次数显著减少，稳定性提升。

Conclusion: 该方法计算成本低，适合嵌入式系统实现。

Abstract: This article proposes a collision risk anticipation method based on
short-term prediction of the agents position. A Long Short-Term Memory (LSTM)
model, trained on past trajectories, is used to estimate the next position of
each robot. This prediction allows us to define an anticipated collision risk
by dynamically modulating the reward of a Deep Q-Learning Network (DQN) agent.
The approach is tested in a constrained environment, where two robots move
without communication or identifiers. Despite a limited sampling frequency (1
Hz), the results show a significant decrease of the collisions number and a
stability improvement. The proposed method, which is computationally
inexpensive, appears particularly attractive for implementation on embedded
systems.

</details>


### [223] [FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis](https://arxiv.org/abs/2508.07950)
*Chen Shen,Wanqing Zhang,Kehan Li,Erwen Huang,Haitao Bi,Aiying Fan,Yiwen Shen,Hongmei Dong,Ji Zhang,Yuming Shao,Zengjia Liu,Xinshe Liu,Tao Li,Chunxia Yan,Shuanliang Fan,Di Wu,Jianhua Ma,Bin Cong,Zhenyuan Wang,Chunfeng Lian*

Main category: cs.AI

TL;DR: FEAT是一个基于多智能体AI的框架，用于自动化和标准化法医死因调查，通过领域适应的大语言模型提高诊断一致性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决法医死因鉴定中的人力短缺和诊断差异问题，特别是在中国等高需求法医体系中。

Method: FEAT采用多智能体架构，包括任务分解的中央规划器、证据分析的专业本地求解器、迭代优化的记忆与反思模块，以及结论合成的全局求解器。结合工具增强推理、分层检索增强生成和法医调优的LLM。

Result: FEAT在多样化的中国案例中表现优于现有AI系统，实现了跨地区的稳健泛化和高专家一致性。

Conclusion: FEAT是首个基于LLM的法医AI系统，结合AI效率和人类监督，有望提升法医服务的公平性和可靠性。

Abstract: Forensic cause-of-death determination faces systemic challenges, including
workforce shortages and diagnostic variability, particularly in high-volume
systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic
AgenT), a multi-agent AI framework that automates and standardizes death
investigations through a domain-adapted large language model. FEAT's
application-oriented architecture integrates: (i) a central Planner for task
decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a
Memory & Reflection module for iterative refinement, and (iv) a Global Solver
for conclusion synthesis. The system employs tool-augmented reasoning,
hierarchical retrieval-augmented generation, forensic-tuned LLMs, and
human-in-the-loop feedback to ensure legal and medical validity. In evaluations
across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI
systems in both long-form autopsy analyses and concise cause-of-death
conclusions. It demonstrated robust generalization across six geographic
regions and achieved high expert concordance in blinded validations. Senior
pathologists validated FEAT's outputs as comparable to those of human experts,
with improved detection of subtle evidentiary nuances. To our knowledge, FEAT
is the first LLM-based AI agent system dedicated to forensic medicine, offering
scalable, consistent death certification while maintaining expert-level rigor.
By integrating AI efficiency with human oversight, this work could advance
equitable access to reliable medicolegal services while addressing critical
capacity constraints in forensic systems.

</details>


### [224] [Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths](https://arxiv.org/abs/2508.08001)
*Rui Yao,Qi Chai,Jinhai Yao,Siyuan Li,Junhao Chen,Qi Zhang,Hao Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLM的、具有不确定性感知的框架，用于解析Fedspeak并分类其隐含的货币政策立场。


<details>
  <summary>Details</summary>
Motivation: Fedspeak作为美联储的战略沟通工具，对市场预期和经济条件有重要影响，自动解析其隐含信号具有高价值。

Method: 结合货币政策传导机制的领域知识，提出动态不确定性解码模块，增强语义和上下文表示。

Result: 实验表明该框架在政策立场分析任务上达到最优性能，不确定性感知显著提升分类准确性和可靠性。

Conclusion: 不确定性感知模块有效诊断模型误差，验证了其在提升模型性能中的实用性。

Abstract: "Fedspeak", the stylized and often nuanced language used by the U.S. Federal
Reserve, encodes implicit policy signals and strategic stances. The Federal
Open Market Committee strategically employs Fedspeak as a communication tool to
shape market expectations and influence both domestic and global economic
conditions. As such, automatically parsing and interpreting Fedspeak presents a
high-impact challenge, with significant implications for financial forecasting,
algorithmic trading, and data-driven policy analysis. In this paper, we propose
an LLM-based, uncertainty-aware framework for deciphering Fedspeak and
classifying its underlying monetary policy stance. Technically, to enrich the
semantic and contextual representation of Fedspeak texts, we incorporate
domain-specific reasoning grounded in the monetary policy transmission
mechanism. We further introduce a dynamic uncertainty decoding module to assess
the confidence of model predictions, thereby enhancing both classification
accuracy and model reliability. Experimental results demonstrate that our
framework achieves state-of-the-art performance on the policy stance analysis
task. Moreover, statistical analysis reveals a significant positive correlation
between perceptual uncertainty and model error rates, validating the
effectiveness of perceptual uncertainty as a diagnostic signal.

</details>


### [225] [Fitting Description Logic Ontologies to ABox and Query Examples](https://arxiv.org/abs/2508.08007)
*Maurice Funk,Marvin Grosser,Carsten Lutz*

Main category: cs.AI

TL;DR: 论文研究了基于本体介导查询的拟合问题，通过正负例拟合本体，分析了不同查询语言的复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决如何通过正负例拟合出满足条件的本体，以支持本体介导查询的需求。

Method: 使用描述逻辑ALC和ALCI作为本体语言，结合原子查询、联合查询等语言，分析拟合问题的计算复杂度。

Result: 发现拟合问题对AQs和完整CQs是CONP复杂度，对CQs和UCQs是2EXPTIME完全问题。

Conclusion: 研究为不同查询语言下的本体拟合问题提供了理论支持，揭示了其计算复杂度。

Abstract: We study a fitting problem inspired by ontology-mediated querying: given a
collection
  of positive and negative examples of
  the form $(\mathcal{A},q)$ with
  $\mathcal{A}$ an ABox and $q$ a Boolean query, we seek
  an ontology $\mathcal{O}$ that satisfies $\mathcal{A} \cup \mathcal{O} \vDash
q$ for all positive examples and $\mathcal{A} \cup \mathcal{O}\not\vDash q$ for
all negative examples.
  We consider the description logics $\mathcal{ALC}$ and $\mathcal{ALCI}$ as
ontology languages and
  a range of query languages that
  includes atomic queries (AQs), conjunctive queries (CQs), and unions thereof
(UCQs).
  For all of the resulting fitting problems,
  we provide
  effective characterizations and determine the computational complexity
  of deciding whether a fitting ontology exists. This problem turns out to be
${\small CO}NP$ for AQs and full CQs
  and $2E{\small XP}T{\small IME}$-complete for CQs and UCQs.
  These results hold for both $\mathcal{ALC}$ and $\mathcal{ALCI}$.

</details>


### [226] [AdaptFlow: Adaptive Workflow Optimization via Meta-Learning](https://arxiv.org/abs/2508.08053)
*Runchuan Zhu,Bowen Jiang,Lingrui Mei,Fangkai Yang,Lu Wang,Haoxiang Gao,Fengshuo Bai,Pu Zhao,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: AdaptFlow是一个基于自然语言的元学习框架，通过学习通用工作流初始化，快速适应子任务，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态模板或手动设计工作流，缺乏适应性和可扩展性。

Method: 采用双层优化方案：内循环通过LLM反馈优化子任务工作流，外循环更新共享初始化以跨任务泛化。

Result: 在问答、代码生成和数学推理任务中表现优异，优于手动和自动基线。

Conclusion: AdaptFlow通过语言引导的修改实现强泛化能力，达到最先进水平。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in agentic workflows, which are structured sequences of LLM invocations
intended to solve complex tasks. However, existing approaches often rely on
static templates or manually designed workflows, which limit adaptability to
diverse tasks and hinder scalability. We propose AdaptFlow, a natural
language-based meta-learning framework inspired by model-agnostic meta-learning
(MAML). AdaptFlow learns a generalizable workflow initialization that enables
rapid subtask-level adaptation. It employs a bi-level optimization scheme: the
inner loop refines the workflow for a specific subtask using LLM-generated
feedback, while the outer loop updates the shared initialization to perform
well across tasks. This setup allows AdaptFlow to generalize effectively to
unseen tasks by adapting the initialized workflow through language-guided
modifications. Evaluated across question answering, code generation, and
mathematical reasoning benchmarks, AdaptFlow consistently outperforms both
manually crafted and automatically searched baselines, achieving
state-of-the-art results with strong generalization across tasks and models.
The source code and data are available at
https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.

</details>


### [227] [FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence](https://arxiv.org/abs/2508.08075)
*Meishen He,Wenjun Ma,Jiao Wang,Huijun Yue,Xiaoma Fan*

Main category: cs.AI

TL;DR: 本文提出了一种基于Dempster-Shafer理论的开放世界信息融合方法（FNBT），用于解决异构框架下的证据融合问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，数据或模型常来自不同区域或组织，导致异构框架，传统融合方法效果不佳。

Method: 提出FNBT方法，通过扩展框架和全否定机制，使现有组合规则适用于异构框架。

Result: 理论证明FNBT满足三个理想性质，实验显示其在分类任务中表现优异，并解决了Zadeh反例。

Conclusion: FNBT方法在异构框架下具有理论和实践优势。

Abstract: The Dempster-Shafer theory of evidence has been widely applied in the field
of information fusion under uncertainty. Most existing research focuses on
combining evidence within the same frame of discernment. However, in real-world
scenarios, trained algorithms or data often originate from different regions or
organizations, where data silos are prevalent. As a result, using different
data sources or models to generate basic probability assignments may lead to
heterogeneous frames, for which traditional fusion methods often yield
unsatisfactory results. To address this challenge, this study proposes an
open-world information fusion method, termed Full Negation Belief
Transformation (FNBT), based on the Dempster-Shafer theory. More specially, a
criterion is introduced to determine whether a given fusion task belongs to the
open-world setting. Then, by extending the frames, the method can accommodate
elements from heterogeneous frames. Finally, a full negation mechanism is
employed to transform the mass functions, so that existing combination rules
can be applied to the transformed mass functions for such information fusion.
Theoretically, the proposed method satisfies three desirable properties, which
are formally proven: mass function invariance, heritability, and essential
conflict elimination. Empirically, FNBT demonstrates superior performance in
pattern classification tasks on real-world datasets and successfully resolves
Zadeh's counterexample, thereby validating its practical effectiveness.

</details>


### [228] [TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork](https://arxiv.org/abs/2508.08115)
*Pranav Pushkar Mishra,Mohammad Arvan,Mohan Zalake*

Main category: cs.AI

TL;DR: TeamMedAgents是一种多智能体方法，将人类团队合作的心理学模型应用于医疗决策，通过六大核心团队合作组件提升LLM性能，在多个医疗基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 将人类团队合作的理论模型（如Salas的“Big Five”模型）转化为计算多智能体系统，以提升医疗决策的准确性和协作效率。

Method: 通过模块化机制实现六大团队合作组件（如团队领导、共享心智模型等），并在自适应协作架构中评估不同任务和领域下的多智能体配置。

Result: 在8个医疗基准测试中，7个表现显著提升，并通过消融实验揭示了不同任务和领域的最优团队配置。

Conclusion: TeamMedAgents为关键决策领域的多智能体系统设计提供了理论基础和实践方法，推动了协作AI的发展。

Abstract: We present TeamMedAgents, a novel multi-agent approach that systematically
integrates evidence-based teamwork components from human-human collaboration
into medical decision-making with large language models (LLMs). Our approach
validates an organizational psychology teamwork model from human collaboration
to computational multi-agent medical systems by operationalizing six core
teamwork components derived from Salas et al.'s "Big Five" model: team
leadership, mutual performance monitoring, team orientation, shared mental
models, closed-loop communication, and mutual trust. We implement and evaluate
these components as modular, configurable mechanisms within an adaptive
collaboration architecture while assessing the effect of the number of agents
involved based on the task's requirements and domain. Systematic evaluation of
computational implementations of teamwork behaviors across eight medical
benchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,
Path-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8
evaluated datasets. Controlled ablation studies conducted on 50 questions per
configuration across 3 independent runs provide mechanistic insights into
individual component contributions, revealing optimal teamwork configurations
that vary by reasoning task complexity and domain-specific requirements. Our
ablation analyses reveal dataset-specific optimal teamwork configurations,
indicating that different medical reasoning modalities benefit from distinct
collaborative patterns. TeamMedAgents represents an advancement in
collaborative AI by providing a systematic translation of established teamwork
theories from human collaboration into agentic collaboration, establishing a
foundation for evidence-based multi-agent system design in critical
decision-making domains.

</details>


### [229] [BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks](https://arxiv.org/abs/2508.08127)
*Rui Miao,Yixin Liu,Yili Wang,Xu Shen,Yue Tan,Yiwei Dai,Shirui Pan,Xin Wang*

Main category: cs.AI

TL;DR: BlindGuard是一种无监督防御方法，用于检测多智能体系统中的恶意代理，无需攻击标签或先验知识。


<details>
  <summary>Details</summary>
Motivation: 现有监督防御方法依赖标记数据，不适用于实际场景，因此需要一种更通用的无监督防御方法。

Method: BlindGuard通过分层代理编码器捕捉代理的个体、邻域和全局交互模式，并结合噪声注入和对比学习训练检测模型。

Result: 实验表明，BlindGuard能有效检测多种攻击类型，且泛化能力优于监督基线方法。

Conclusion: BlindGuard为多智能体系统提供了一种实用且通用的无监督防御解决方案。

Abstract: The security of LLM-based multi-agent systems (MAS) is critically threatened
by propagation vulnerability, where malicious agents can distort collective
decision-making through inter-agent message interactions. While existing
supervised defense methods demonstrate promising performance, they may be
impractical in real-world scenarios due to their heavy reliance on labeled
malicious agents to train a supervised malicious detection model. To enable
practical and generalizable MAS defenses, in this paper, we propose BlindGuard,
an unsupervised defense method that learns without requiring any
attack-specific labels or prior knowledge of malicious behaviors. To this end,
we establish a hierarchical agent encoder to capture individual, neighborhood,
and global interaction patterns of each agent, providing a comprehensive
understanding for malicious agent detection. Meanwhile, we design a
corruption-guided detector that consists of directional noise injection and
contrastive learning, allowing effective detection model training solely on
normal agent behaviors. Extensive experiments show that BlindGuard effectively
detects diverse attack types (i.e., prompt injection, memory poisoning, and
tool attack) across MAS with various communication patterns while maintaining
superior generalizability compared to supervised baselines. The code is
available at: https://github.com/MR9812/BlindGuard.

</details>


### [230] [From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework](https://arxiv.org/abs/2508.08147)
*Yunkai Hu,Tianqiao Zhao,Meng Yue*

Main category: cs.AI

TL;DR: 论文提出了一种基于LLM的代理，将电力系统优化的自然语言描述转换为可求解的数学模型，并通过验证和修复确保可行性，生成最优解。


<details>
  <summary>Details</summary>
Motivation: 直接使用LLM生成解决方案常导致不可行或次优结果，因其缺乏数值精度和约束处理能力。本文旨在结合LLM与优化求解器，提升解决方案的可靠性。

Method: 通过领域感知提示和模式整合LLM，系统验证和迭代修复确保可行性，生成可求解的数学模型和用户友好的结果。

Result: 以机组组合问题为例，代理生成了最优或接近最优的调度方案及目标成本，验证了方法的有效性。

Conclusion: 结合AI与传统优化框架，能够高效连接高层问题描述与可执行数学模型，提升能源系统决策效率。

Abstract: This paper introduces a novel Large Language Models (LLMs)-assisted agent
that automatically converts natural-language descriptions of power system
optimization scenarios into compact, solver-ready formulations and generates
corresponding solutions. In contrast to approaches that rely solely on LLM to
produce solutions directly, the proposed method focuses on discovering a
mathematically compatible formulation that can be efficiently solved by
off-the-shelf optimization solvers. Directly using LLMs to produce solutions
often leads to infeasible or suboptimal results, as these models lack the
numerical precision and constraint-handling capabilities of established
optimization solvers. The pipeline integrates a domain-aware prompt and schema
with an LLM, enforces feasibility through systematic validation and iterative
repair, and returns both solver-ready models and user-facing results. Using the
unit commitment problem as a representative case study, the agent produces
optimal or near-optimal schedules along with the associated objective costs.
Results demonstrate that coupling the solver with task-specific validation
significantly enhances solution reliability. This work shows that combining AI
with established optimization frameworks bridges high-level problem
descriptions and executable mathematical models, enabling more efficient
decision-making in energy systems

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [231] [AMP-based Joint Activity Detection and Channel Estimation for Massive Grant-Free Access in OFDM-based Wideband Systems](https://arxiv.org/abs/2508.06540)
*Zhiyan Li,Ying Cui,Danny H. K. Tsang*

Main category: cs.IT

TL;DR: 论文提出两种基于近似消息传递（AMP）的算法（AMP-A-EC和AMP-A-AC），用于解决OFDM系统中设备活动检测和信道估计的精度与计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有OFDM系统中的设备活动检测和信道估计方法在频率选择性衰落环境下存在精度不足或计算时间过长的问题，需要改进。

Method: 提出精确的时域信号模型，构建新的因子图，并设计两种AMP算法（AMP-A-EC和AMP-A-AC）来解决MAP和MMSE问题。

Result: 两种算法在活动检测和信道估计中表现优异，且AMP-A-AC计算复杂度更低。数值结果验证了其性能优势。

Conclusion: 所提算法为OFDM系统的免授权接入提供了高效解决方案，具有显著应用价值。

Abstract: To realize orthogonal frequency division multiplexing (OFDM)-based grant-free
access for wideband systems under frequency-selective fading, existing device
activity detection and channel estimation methods need substantial accuracy
improvement or computation time reduction. In this paper, we aim to resolve
this issue. First, we present an exact time-domain signal model for OFDM-based
grant-free access under frequency-selective fading. Then, we present a maximum
a posteriori (MAP)-based device activity detection problem and two minimum mean
square error (MMSE)-based channel estimation problems. The MAP-based device
activity detection problem and one of the MMSE-based channel estimation
problems are formulated for the first time. Next, we build a new factor graph
that captures the exact statistics of time-domain channels and device
activities. Based on it, we propose two approximate message passing (AMP)-based
algorithms, AMP-A-EC and AMP-A-AC, to approximately solve the MAP-based device
activity detection problem and two MMSE-based channel estimation problems. Both
proposed algorithms alleviate the AMP's inherent convergence problem when the
pilot length is smaller or comparable to the number of active devices. Then, we
analyze AMP-A-EC's error probability of activity detection and mean square
error (MSE) of channel estimation via state evolution and show that AMP-A-AC
has the lower computational complexity (in dominant term). Finally, numerical
results show the two proposed AMP-based algorithms' superior performance and
respective preferable regions, revealing their significant values for
OFDM-based grant-free access.

</details>


### [232] [Communication-Learning Co-Design for Differentially Private Over-the-Air Federated Distillation](https://arxiv.org/abs/2508.06557)
*Zihao Hu,Jia Yan,Ying-Jun Angela Zhang*

Main category: cs.IT

TL;DR: 论文提出了一种新颖的差分隐私无线联邦蒸馏框架，通过多接入信道的叠加特性实现设备间模型输出的噪声扰动共享，优化通信与学习的协同设计，提升收敛速度并满足隐私要求。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在大模型时代面临通信效率和隐私保护的挑战，需要一种更高效且隐私安全的解决方案。

Method: 提出差分隐私无线联邦蒸馏框架，利用多接入信道的叠加特性共享噪声扰动模型输出，优化通信与学习的协同设计。

Result: 理论分析和数值实验表明，该方法在减少通信开销的同时，实现了更好的学习-隐私权衡。

Conclusion: 差分隐私无线联邦蒸馏框架在通信效率和隐私保护方面优于传统联邦学习方法。

Abstract: The ever-growing learning model size nowadays challenges the communication
efficiency and privacy preservation of the traditional federated learning (FL).
In this paper, we propose a novel differentially private (DP) over-the-air
federated distillation (FD) framework, where wireless devices (WDs)
periodically share noise-perturbed model outputs with the parameter server by
harnessing the superposition property of multi-access channels. Accordingly,
over-the-air FD enables the shared responsibility of the DP preservation on the
low-dimensional disclosed signals among WDs. We study the
communication-learning co-design problem in differentially private over-the-air
FD, aiming to maximize the learning convergence rate while meeting the transmit
power and DP requirements of WDs. The main challenge is rooted in the
intractable learning and privacy analysis in over-the-air FD, together with the
strong coupling among the decision variables spanning two timescales. To tackle
this problem, we first derive the analytical learning convergence rate and
privacy losses of WDs, based on which the optimal transceiver design per FD
round and long-term training rounds decision are obtained in the closed forms.
Numerical results demonstrate that the proposed differentially private
over-the-air FD approach achieves a better learning-privacy trade-off with
largely-reduced communication overhead than the conventional FL benchmarks.

</details>


### [233] [When isometry and equivalence for skew constacyclic codes coincide](https://arxiv.org/abs/2508.06695)
*Monica Nevins,Susanne Pumpluen*

Main category: cs.IT

TL;DR: 论文证明了$(n,\\sigma)$-等距和$(n,\\sigma)$-等价在大多数斜$(\sigma,a)$-常循环码中重合，并提出了新的等价和等距定义以更精确分类。


<details>
  <summary>Details</summary>
Motivation: 研究斜常循环码的等价性和等距性，以更精确地分类这些码。

Method: 在非结合Petit代数中研究Hamming权重保持同态，并确定其性质。

Result: 证明了在非结合代数中，Hamming权重保持同态的度数必须为1，并提出了新的等价和等距定义。

Conclusion: 新定义能更精确地捕捉Hamming保持同构，从而改进斜常循环码的分类。

Abstract: We show that the notions of $(n,\sigma)$-isometry and
$(n,\sigma)$-equivalence introduced by Ou-azzou et al coincide for most skew
$(\sigma,a)$-constacyclic codes of length $n$. To prove this, we show that all
Hamming-weight-preserving homomorphisms between their ambient algebras must
have degree one when those algebras are nonassociative. We work in the general
setting of commutative base rings $S$. As a consequence, we propose new
definitions of equivalence and isometry of skew constacyclic codes that exactly
capture all Hamming-preserving isomorphisms, and lead to tighter
classifications. In the process we determine homomorphisms between
nonassociative Petit algebras, prioritizing the algebras
$S[t;\sigma]/S[t;\sigma](t^n-a)$, which give rise to skew constacyclic codes.

</details>


### [234] [Generalized Samorodnitsky noisy function inequalities, with applications to error-correcting codes](https://arxiv.org/abs/2508.06940)
*Olakunle S. Abawonse,Jan Hazla,Ryan O'Donnell*

Main category: cs.IT

TL;DR: 论文将Samorodnitsky不等式推广到任意乘积概率分布，并确定了最优参数λ，应用于编码理论。


<details>
  <summary>Details</summary>
Motivation: 扩展Samorodnitsky不等式的适用范围，使其适用于更一般的概率分布和参数，并验证其在编码理论中的应用。

Method: 将不等式推广到任意乘积概率分布，并计算最优λ值。

Result: 确定了λ的最优值，并证明了其在有限字母表线性码中的应用。

Conclusion: 推广后的不等式具有更广泛的适用性，并在编码理论中提供了新的工具。

Abstract: An inequality by Samorodnitsky states that if $f : \mathbb{F}_2^n \to
\mathbb{R}$ is a nonnegative boolean function, and $S \subseteq [n]$ is chosen
by randomly including each coordinate with probability a certain $\lambda =
\lambda(q,\rho) < 1$, then \begin{equation}
  \log \|T_\rho f\|_q \leq \mathbb{E}_{S} \log \|\mathbb{E}(f|S)\|_q\;.
\end{equation} Samorodnitsky's inequality has several applications to the
theory of error-correcting codes. Perhaps most notably, it can be used to show
that \emph{any} binary linear code (with minimum distance $\omega(\log n)$)
that has vanishing decoding error probability on the BEC$(\lambda)$ (binary
erasure channel) also has vanishing decoding error on \emph{all} memoryless
symmetric channels with capacity above some $C = C(\lambda)$.
  Samorodnitsky determined the optimal $\lambda = \lambda(q,\rho)$ for his
inequality in the case that $q \geq 2$ is an integer. In this work, we
generalize the inequality to $f : \Omega^n \to \mathbb{R}$ under any product
probability distribution $\mu^{\otimes n}$ on $\Omega^n$; moreover, we
determine the optimal value of $\lambda = \lambda(q,\mu,\rho)$ for any real $q
\in [2,\infty]$, $\rho \in [0,1]$, and distribution~$\mu$. As one consequence,
we obtain the aforementioned coding theory result for linear codes over
\emph{any} finite alphabet.

</details>


### [235] [Neural Beam Field for Spatial Beam RSRP Prediction](https://arxiv.org/abs/2508.06956)
*Keqiang Guo,Yuheng Zhong,Xin Tong,Jiangbin Lyu,Rui Zhang*

Main category: cs.IT

TL;DR: 论文提出了一种名为Neural Beam Field（NBF）的混合神经物理框架，用于高效且可解释的空间波束RSRP预测。通过结合多路径条件功率剖面（MCPP）和Transformer网络，NBF在预测精度和训练效率上显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在密集多用户无线网络中，准确预测波束级RSRP对波束管理至关重要，但由于高测量开销和快速信道变化，这一任务具有挑战性。

Method: NBF采用了一种解耦的“黑盒-白盒”设计：Transformer网络从稀疏用户测量数据中学习MCPP，而物理模块解析推断波束RSRP统计。此外，还引入了预训练和校准（PaC）策略以提高收敛性和适应性。

Result: 实验结果表明，NBF在预测精度、训练效率和泛化能力上显著优于传统方法，同时保持较小的模型规模。

Conclusion: NBF为下一代密集无线网络中的智能波束管理提供了一种可扩展且基于物理的解决方案。

Abstract: Accurately predicting beam-level reference signal received power (RSRP) is
essential for beam management in dense multi-user wireless networks, yet
challenging due to high measurement overhead and fast channel variations. This
paper proposes Neural Beam Field (NBF), a hybrid neural-physical framework for
efficient and interpretable spatial beam RSRP prediction. Central to our
approach is the introduction of the Multi-path Conditional Power Profile
(MCPP), which bridges site-specific multipath propagation with antenna/beam
configurations via closed-form analytical modeling. We adopt a decoupled
``blackbox-whitebox" design: a Transformer-based deep neural network (DNN)
learns the MCPP from sparse user measurements and positions, while a
physics-inspired module analytically infers beam RSRP statistics. To improve
convergence and adaptivity, we further introduce a Pretrain-and-Calibrate (PaC)
strategy that leverages ray-tracing priors and on-site calibration using RSRP
data. Extensive simulations results demonstrate that NBF significantly
outperforms conventional table-based channel knowledge maps (CKMs) and pure
blackbox DNNs in prediction accuracy, training efficiency, and generalization,
while maintaining a compact model size. The proposed framework offers a
scalable and physically grounded solution for intelligent beam management in
next-generation dense wireless networks.

</details>


### [236] [Neural Channel Knowledge Map Assisted Scheduling Optimization of Active IRSs in Multi-User Systems](https://arxiv.org/abs/2508.07009)
*Xintong Chen,Zhenyu Jiang,Jiangbin Lyu,Liqun Fu*

Main category: cs.IT

TL;DR: 论文提出了一种基于神经通道知识图（CKM）的调度框架，通过Transformer网络预测频谱效率，并设计了低复杂度调度算法SM-IB，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 智能反射面（IRS）在下一代无线网络中潜力巨大，但面临双路径损耗和多用户调度复杂性的挑战。

Method: 设计了两个级联网络LPS-Net和SE-Net预测链路功率统计和频谱效率，并提出了SM-IB调度算法。

Result: 神经CKM显著提高了预测精度和计算效率，SM-IB算法以低复杂度实现了接近最优的最大-最小吞吐量。

Conclusion: 提出的方法有效解决了IRS系统中的关键挑战，为未来无线网络提供了高效解决方案。

Abstract: Intelligent Reflecting Surfaces (IRSs) have potential for significant
performance gains in next-generation wireless networks but face key challenges,
notably severe double-pathloss and complex multi-user scheduling due to
hardware constraints. Active IRSs partially address pathloss but still require
efficient scheduling in cell-level multi-IRS multi-user systems, whereby the
overhead/delay of channel state acquisition and the scheduling complexity both
rise dramatically as the user density and channel dimensions increase.
Motivated by these challenges, this paper proposes a novel scheduling framework
based on neural Channel Knowledge Map (CKM), designing Transformer-based deep
neural networks (DNNs) to predict ergodic spectral efficiency (SE) from
historical channel/throughput measurements tagged with user positions.
Specifically, two cascaded networks, LPS-Net and SE-Net, are designed to
predict link power statistics (LPS) and ergodic SE accurately. We further
propose a low-complexity Stable Matching-Iterative Balancing (SM-IB) scheduling
algorithm. Numerical evaluations verify that the proposed neural CKM
significantly enhances prediction accuracy and computational efficiency, while
the SM-IB algorithm effectively achieves near-optimal max-min throughput with
greatly reduced complexity.

</details>


### [237] [Generalized Quasi-Cyclic LDPC Codes: Design and Efficient Encoding](https://arxiv.org/abs/2508.07030)
*Roxana Smarandache,David G. M. Mitchell,Anthony Gómez-Fonseca*

Main category: cs.IT

TL;DR: 本文研究了准循环广义低密度奇偶校验（QC-GLDPC）码的构造方法，提出了一种基于多项式矩阵小子的编码矩阵构造方法，并分析了其性能与实现效率。


<details>
  <summary>Details</summary>
Motivation: GLDPC码在低延迟通信中表现优异，但缺乏高效的编码构造方法，本文旨在解决这一问题。

Method: 通过分析多项式矩阵的小子，构造多种形式的生成矩阵，并应用双图提升改进码参数。

Result: 提出的方法能高效实现编码，并提供码的最小距离和维度的上下界。

Conclusion: 该方法为QC-GLDPC码的构造和性能优化提供了实用工具。

Abstract: Generalized low-density parity-check (GLDPC) codes, where single parity-check
constraints on the code bits are replaced with generalized constraints (an
arbitrary linear code), are a promising class of codes for low-latency
communication. The block error rate performance of the GLDPC codes, combined
with a complementary outer code, has been shown to outperform a variety of
state-of-the-art code and decoder designs with suitable lengths and rates for
the 5G ultra-reliable low-latency communication (URLLC) regime. A major
drawback of these codes is that it is not known how to construct appropriate
polynomial matrices to encode them efficiently. In this paper, we analyze
practical constructions of quasi-cyclic GLDPC (QC-GLDPC) codes and show how to
construct polynomial generator matrices in various forms using minors of the
polynomial matrix. The approach can be applied to fully generalized matrices or
partially generalized (with mixed constraint node types) to find better
performance/rate trade-offs. The resulting encoding matrices are presented in
useful forms that facilitate efficient implementation. The rich substructure
displayed also provides us with new methods of determining low weight
codewords, providing lower and upper bounds on the minimum distance and often
giving those of weight equal to the minimum distance. Based on the minors of
the polynomial parity-check matrix, we also give a formula for the rank of any
parity-check matrix representing a QC-LDPC or QC-GLDPC code, and hence, the
dimension of the code. Finally, we show that by applying double graph-liftings,
the code parameters can be improved without affecting the ability to obtain a
polynomial generator matrix.

</details>


### [238] [Realistic Evaluation of Impedance-Based RIS Modeling: Practical Insights and Applications](https://arxiv.org/abs/2508.07098)
*Ayane Lebeta Goshu,Placido Mursia,Vincenzo Sciancalepore,Marco Di Renzo,Xavier Costa-Pérez*

Main category: cs.IT

TL;DR: 本文分析了可重构智能表面（RIS）中结构散射的影响，比较了传统模型与电磁一致性模型的性能，揭示了当前建模方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统RIS模型忽略了复杂的电磁现象（如互耦合），而电磁一致性模型更准确，但结构散射问题尚未充分研究。

Method: 通过全波仿真分析结构散射的影响，并比较传统与电磁一致性模型。

Result: 研究发现当前建模方法在缓解结构散射问题上存在局限性。

Conclusion: 需要新的优化策略来解决结构散射问题，提升RIS的实际性能。

Abstract: Reconfigurable Intelligent Surfaces (RISs) have emerged as a promising
technology for next-generation wireless communications, offering
energy-efficient control of electromagnetic (EM) waves. While conventional RIS
models based on phase shifts and amplitude adjustments have been widely
studied, they overlook complex EM phenomena such as mutual coupling, which are
crucial for advanced wave manipulations. Recent efforts in EM-consistent
modelling have provided more accurate representations of RIS behavior,
highlighting challenges like structural scattering-an unwanted signal
reflection that can lead to interference. In this paper, we analyze the impact
of structural scattering in RIS architectures and compare traditional and
EM-consistent models through full-wave simulations, thus providing practical
insights on the realistic performance of current RIS designs. Our findings
reveal the limitations of current modelling approaches in mitigating this
issue, underscoring the need for new optimization strategies.

</details>


### [239] [Duality on group algebras over finite chain rings: applications to additive group codes](https://arxiv.org/abs/2508.07461)
*Maryam Bajalan,Javier de la Cruz,Alexandre Fotue Tabue,Edgar Martínez-Moro*

Main category: cs.IT

TL;DR: 论文研究了有限群环上的加法群码及其互补对，通过环扩展和模同构分解群环，并构造了对称非退化的迹欧几里得内积，揭示了编码理论与模正交分解、表示论的联系。


<details>
  <summary>Details</summary>
Motivation: 研究有限链环扩展下的群环结构及其在编码理论中的应用，特别是加法群码的性质和互补对的关系。

Method: 首先分析环扩展$S|R$，建立模同构分解群环；其次构造迹欧几里得内积，研究加法互补对的正交性质。

Result: 证明了加法互补对的正交补与群环上的反自同构映射相关，揭示了编码理论与模分解、表示论的联系。

Conclusion: 通过群环结构和内积性质，为加法群码及其互补对提供了理论框架，拓展了编码理论与代数结构的交叉研究。

Abstract: Given a finite group $G$ and an extension of finite chain rings $S|R$, one
can consider the group rings $\mathscr{S} = S[G]$ and $\mathscr{R} = R[G]$. The
group ring $\mathscr{S}$ can be viewed as an $R$-bimodule, and any of its
$R$-submodules naturally inherits an $R$-bimodule structure; in the framework
of coding theory, these are called \emph{additive group codes}, more precisely
a (left) additive group code of is a linear code which is the image of a (left)
ideal of a group algebra via an isomorphism which maps $G$ to the standard
basis of $S^n$, where $n=|G|$. In the first part of the paper, the ring
extension $S|R$ is studied, and several $R$-module isomorphisms are established
for decomposing group rings, thereby providing a characterization of the
structure of additive group codes. In the second part, we construct a
symmetric, nondegenerate trace-Euclidean inner product on $\mathscr{S}$. Two
additive group codes $\mathcal{C}$ and $\mathcal{D}$ form an \emph{additive
complementary pair} (ACP) if $\mathcal{C} + \mathcal{D} = \mathscr{S}$ and
$\mathcal{C} \cap \mathcal{D} = \{0\}$. For two-sided ACPs, we prove that the
orthogonal complement of one code under the trace-Euclidean duality is
precisely the image of the other under an involutive anti-automorphism of
$\mathscr{S}$, linking coding-theoretical ACPs with module orthogonal
direct-sum decompositions, representation theory, and the structure of group
algebras over finite chain rings.

</details>


### [240] [Structured Superposition of Autoencoders for UEP Codes at Intermediate Blocklengths](https://arxiv.org/abs/2508.07487)
*Vukan Ninkovic,Dejan Vukobratovic*

Main category: cs.IT

TL;DR: 论文提出了一种基于自编码器的结构化UEP编码方法，通过分块设计扩展了编码长度，提升了性能和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现代通信系统需要不同可靠性级别的UEP编码，但现有基于自编码器的UEP方法在中等块长度下复杂度高，应用受限。

Method: 采用叠加编码和干扰消除解码的结构化自编码器架构，将编码和解码分解为更小的子块，实现灵活调整可靠性级别。

Result: 数值结果表明，该方法优于传统的随机叠加编码UEP方案，提升了性能和可扩展性。

Conclusion: 结构化自编码器UEP编码为下一代网络提供了高效且可扩展的解决方案。

Abstract: Unequal error protection (UEP) coding that enables differentiated reliability
levels within a transmitted message is essential for modern communication
systems. Autoencoder (AE)-based code designs have shown promise in the context
of learned equal error protection (EEP) coding schemes. However, their
application to UEP remains largely unexplored, particularly at intermediate
blocklengths, due to the increasing complexity of AE-based models. Inspired by
the proven effectiveness of superposition coding and successive interference
cancellation (SIC) decoding in conventional UEP schemes, we propose a
structured AE-based architecture that extends AE-based UEP codes to
substantially larger blocklengths while maintaining efficient training. By
structuring encoding and decoding into smaller AE subblocks, our method
provides a flexible framework for fine-tuning UEP reliability levels while
adapting to diverse system parameters. Numerical results show that the proposed
approach improves over established achievability bounds of randomized
superposition coding-based UEP schemes with SIC decoding, making the proposed
structured AE-based UEP codes a scalable and efficient solution for
next-generation networks.

</details>


### [241] [Extended AB Algorithms for Bistatic Integrated Sensing and Communications Systems](https://arxiv.org/abs/2508.07567)
*Tian Jiao,Yanlin Geng,Zhiqiang Wei,Zai Yang*

Main category: cs.IT

TL;DR: 提出扩展的Arimoto-Blahut算法，用于计算双基地ISAC系统中的率失真权衡，解决了现有算法处理非凸约束的局限性。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络中集成感知与通信（ISAC）至关重要，计算其率失真权衡是关键问题。

Method: 引入辅助变量将非凸失真约束转化为线性约束，并基于AB算法框架开发扩展算法，适用于平方误差和对数损失失真度量。

Result: 数值结果验证了所提算法的有效性。

Conclusion: 扩展的AB算法成功解决了非凸约束问题，为ISAC系统的率失真权衡提供了有效工具。

Abstract: Integrated sensing and communication (ISAC) is pivotal for next-generation
wireless networks, rendering the computation of rate-distortion trade-off in
ISAC systems critically important. In this paper, we propose the extended
Arimoto-Blahut (AB) algorithms to calculate the rate-distortion trade-off in
bistatic ISAC systems, which overcome the limitation of existing AB algorithms
in handling non-convex constraints. Specifically, we introduce auxiliary
variables to transform non-convex distortion constraints into linear
constraints, prove that the reformulated linearly-constrained optimization
problem maintains the same optimal solution as the original problem, and
develop extended AB algorithms for both squared error and logarithmic loss
distortion metrics based on the framework of AB algorithm. Numerical results
validate the effectiveness of the proposed algorithm.

</details>


### [242] [QoS-Aware Integrated Sensing, Communication, and Control with Movable Antenna](https://arxiv.org/abs/2508.07799)
*Yike Wang,Zhike Wu,Jiang Chen,Chunjie Wang,Xuhui Zhang,Yanyan Shen*

Main category: cs.IT

TL;DR: 提出了一种基于可移动天线的ISCC系统，通过优化天线位置和波束成形策略，解决了动态干扰和信道衰减问题，显著提升了数据速率和控制QoS。


<details>
  <summary>Details</summary>
Motivation: 动态干扰和信道衰减限制了ISCC系统的潜力，需要一种新方法来提升性能。

Method: 提出了一种基于交替优化（AO）的算法，优化天线位置和波束成形策略。

Result: 数值结果表明，所提算法在数据速率和控制QoS方面优于基准方案。

Conclusion: 可移动天线和AO算法有效提升了ISCC系统的性能。

Abstract: Integrated sensing, communication, and control (ISCC) has emerged as a key
enabler for low-altitude wireless networks with enhanced adaptability through
resource allocation co-design and intelligent environment awareness. However,
dynamic interference and channel attenuation constrain the potential of the
ISCC system. To address this challenge, we propose a novel movable
antenna-empowered ISCC system. An achievable data rate maximization problem is
formulated while guaranteeing the sensing and control quality-of-service (QoS)
by optimizing the positions of the antennas and the beamforming strategy for
communication, sensing, and control co-design. An efficient alternating
optimization (AO)-based algorithm is proposed to solve the highly coupled
non-convex problem. Numerical results demonstrate that the proposed AO-based
algorithm achieves substantial gains in the achievable data rate and the
control QoS compared with benchmark schemes.

</details>


### [243] [Age of Information Minimization in Goal-Oriented Communication with Processing and Cost of Actuation Error Constraints](https://arxiv.org/abs/2508.07865)
*Rishabh S. Pomaje,Jayanth S.,Rajshekhar V. Bhat,Nikolaos Pappas*

Main category: cs.IT

TL;DR: 研究目标导向通信系统，通过优化采样和传输策略，最小化信息年龄（AoI），同时考虑传输成本和语义误差成本。


<details>
  <summary>Details</summary>
Motivation: 传统AoI优化忽略底层过程动态和语义误差成本，需综合考虑以提升系统性能。

Method: 提出静态随机策略，在可行策略中实现AoI的有限倍优化，并通过数值实验验证。

Result: 策略在AoI优化上接近最优，同时揭示了AoI、CAE与成本之间的权衡关系。

Conclusion: 综合优化AoI和语义误差成本更有效，静态随机策略为可行解决方案。

Abstract: We study a goal-oriented communication system in which a source monitors an
environment that evolves as a discrete-time, two-state Markov chain. At each
time slot, a controller decides whether to sample the environment and if so
whether to transmit a raw or processed sample, to the controller. Processing
improves transmission reliability over an unreliable wireless channel, but
incurs an additional cost. The objective is to minimize the long-term average
age of information (AoI), subject to constraints on the costs incurred at the
source and the cost of actuation error (CAE), a semantic metric that assigns
different penalties to different actuation errors. Although reducing AoI can
potentially help reduce CAE, optimizing AoI alone is insufficient, as it
overlooks the evolution of the underlying process. For instance, faster source
dynamics lead to higher CAE for the same average AoI, and different AoI
trajectories can result in markedly different CAE under identical average AoI.
To address this, we propose a stationary randomized policy that achieves an
average AoI within a bounded multiplicative factor of the optimal among all
feasible policies. Extensive numerical experiments are conducted to
characterize system behavior under a range of parameters. These results offer
insights into the feasibility of the optimization problem, the structure of
near-optimal actions, and the fundamental trade-offs between AoI, CAE, and the
costs involved.

</details>


### [244] [Adaptive Source-Channel Coding for Semantic Communications](https://arxiv.org/abs/2508.07958)
*Dongxu Li,Kai Yuan,Jianhao Huang,Chuan Huang,Xiaoqi Qin,Shuguang Cui,Ping Zhang*

Main category: cs.IT

TL;DR: 提出了一种自适应源信道编码（ASCC）方案，用于语义通信（SemComs），解决了现有联合源信道编码（JSCC）与现有系统不兼容且无法适应源或信道变化的问题。


<details>
  <summary>Details</summary>
Motivation: 当前SemComs中的JSCC与现有通信系统不兼容且无法适应源或信道变化，而分离源信道编码（SSCC）在有限块长下表现不佳。

Method: 提出ASCC方案，结合DNN语义源编码和传统数字信道编码，通过逻辑回归建模端到端失真，并利用连续凸近似优化联合源信道编码率和功率分配。

Result: 仿真结果表明，ASCC方案在单信道和平行信道场景下均优于典型的深度JSCC和SSCC方案，且完全兼容实际数字系统。

Conclusion: ASCC方案在性能和兼容性上优于现有方法，为语义通信提供了更优的解决方案。

Abstract: Semantic communications (SemComs) have emerged as a promising paradigm for
joint data and task-oriented transmissions, combining the demands for both the
bit-accurate delivery and end-to-end (E2E) distortion minimization. However,
current joint source-channel coding (JSCC) in SemComs is not compatible with
the existing communication systems and cannot adapt to the variations of the
sources or the channels, while separate source-channel coding (SSCC) is
suboptimal in the finite blocklength regime. To address these issues, we
propose an adaptive source-channel coding (ASCC) scheme for SemComs over
parallel Gaussian channels, where the deep neural network (DNN)-based semantic
source coding and conventional digital channel coding are separately deployed
and adaptively designed. To enable efficient adaptation between the source and
channel coding, we first approximate the E2E data and semantic distortions as
functions of source coding rate and bit error ratio (BER) via logistic
regression, where BER is further modeled as functions of signal-to-noise ratio
(SNR) and channel coding rate. Then, we formulate the weighted sum E2E
distortion minimization problem for joint source-channel coding rate and power
allocation over parallel channels, which is solved by the successive convex
approximation. Finally, simulation results demonstrate that the proposed ASCC
scheme outperforms typical deep JSCC and SSCC schemes for both the single- and
parallel-channel scenarios while maintaining full compatibility with practical
digital systems.

</details>


### [245] [Random Modulation: Achieving Asymptotic Replica Optimality over Arbitrary Norm-Bounded and Spectrally Convergent Channel Matrices](https://arxiv.org/abs/2508.08099)
*Lei Liu,Yuhao Chi,Shunqi Huang*

Main category: cs.IT

TL;DR: 本文提出了一种与信道矩阵解耦的随机调制技术，适用于任意范数有界和频谱收敛的信道矩阵，并通过构建等效密集随机信道矩阵确保信号经历足够的统计信道衰落。


<details>
  <summary>Details</summary>
Motivation: 解决现有调制技术对信道矩阵的依赖问题，提升线性系统中近似消息传递（AMP）检测器的性能。

Method: 提出随机调制技术，构建等效密集随机信道矩阵，并设计低复杂度跨域记忆近似消息传递（CD-MAMP）检测器。

Result: 数值结果显示，随机调制在BER和BLER性能上比现有技术提升2-3 dB。

Conclusion: 随机调制技术在性能和复杂度上均优于现有技术，适用于多种信道条件。

Abstract: This paper introduces a random modulation technique that is decoupled from
the channel matrix, allowing it to be applied to arbitrary norm-bounded and
spectrally convergent channel matrices. The proposed random modulation
constructs an equivalent dense and random channel matrix, ensuring that the
signals undergo sufficient statistical channel fading. It also guarantees the
asymptotic replica maximum a posteriori (MAP) bit-error rate (BER) optimality
of approximate message passing (AMP)-type detectors for linear systems with
arbitrary norm-bounded and spectrally convergent channel matrices when their
state evolution has a unique fixed point. Then, a low-complexity cross-domain
memory approximate message passing (CD-MAMP) detector is proposed for random
modulation, leveraging the sparsity of the time-domain channel and the
randomness of the random transform-domain channel. Furthermore, the optimal
power allocation schemes are derived to minimize the replica MAP BER and
maximize the replica constrained capacity of random-modulated linear systems,
assuming the availability of channel state information (CSI) at the
transceiver. Numerical results show that the proposed random modulation can
achieve BER and block-error rate (BLER) performance gains of up to 2 - 3 dB
compared to existing OFDM/OTFS/AFDM with 5G-NR LDPC codes, under both average
and optimized power allocation.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [246] [A Tight Lower Bound for the Approximation Guarantee of Higher-Order Singular Value Decomposition](https://arxiv.org/abs/2508.06693)
*Matthew Fahrbach,Mehrdad Ghadiri*

Main category: cs.DS

TL;DR: 证明了高阶奇异值分解（HOSVD）的经典近似保证是紧的，并构造了一个张量实例，表明HOSVD的近似比无法改进。进一步证明了ST-HOSVD和HOOI算法的近似保证也是紧的。


<details>
  <summary>Details</summary>
Motivation: 验证HOSVD及其变体算法（ST-HOSVD和HOOI）的近似比是否已达到理论极限。

Method: 通过构造特定张量实例，展示HOSVD、ST-HOSVD和HOOI算法在特定情况下达到其理论近似比。

Result: HOSVD、ST-HOSVD和HOOI的近似比均为$N/(1+\varepsilon)$，无法进一步改进。

Conclusion: HOSVD及其变体算法的近似保证已达到理论极限，无法优化。

Abstract: We prove that the classic approximation guarantee for the higher-order
singular value decomposition (HOSVD) is tight by constructing a tensor for
which HOSVD achieves an approximation ratio of $N/(1+\varepsilon)$, for any
$\varepsilon > 0$. This matches the upper bound of De Lathauwer et al. (2000a)
and shows that the approximation ratio of HOSVD cannot be improved. Using a
more advanced construction, we also prove that the approximation guarantees for
the ST-HOSVD algorithm of Vannieuwenhoven et al. (2012) and higher-order
orthogonal iteration (HOOI) of De Lathauwer et al. (2000b) are tight by showing
that they can achieve their worst-case approximation ratio of $N / (1 +
\varepsilon)$, for any $\varepsilon > 0$.

</details>


### [247] [Approximating High-Dimensional Earth Mover's Distance as Fast as Closest Pair](https://arxiv.org/abs/2508.06774)
*Lorenzo Beretta,Vincent Cohen-Addad,Rajesh Jayaram,Erik Waingarten*

Main category: cs.DS

TL;DR: 论文提出了一种从$(1+\varepsilon)$-近似地球移动距离（EMD）到$(1+\varepsilon)$-近似最近对（CP）的归约方法，改进了高维EMD的最快近似算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于改进高维EMD的近似算法效率，利用CP问题的快速算法来优化EMD的计算时间。

Method: 方法是通过归约将EMD问题转化为CP问题，并利用Multiplicative Weights Update框架的次线性实现来隐式更新权重。

Result: 结果表明，通过归约和隐式权重更新，实现了EMD的$(1+\varepsilon)$-近似算法，运行时间为$n^{2-\tilde{\Omega}(\varepsilon^{1/3})}$，优于之前的最快算法。

Conclusion: 结论是该方法显著提升了高维EMD的近似计算效率，为相关领域提供了新的技术工具。

Abstract: We give a reduction from $(1+\varepsilon)$-approximate Earth Mover's Distance
(EMD) to $(1+\varepsilon)$-approximate Closest Pair (CP). As a consequence, we
improve the fastest known approximation algorithm for high-dimensional EMD.
Here, given $p\in [1, 2]$ and two sets of $n$ points $X,Y \subseteq (\mathbb
R^d,\ell_p)$, their EMD is the minimum cost of a perfect matching between $X$
and $Y$, where the cost of matching two vectors is their $\ell_p$ distance.
Further, CP is the basic problem of finding a pair of points realizing $\min_{x
\in X, y\in Y} ||x-y||_p$. Our contribution is twofold: we show that if a
$(1+\varepsilon)$-approximate CP can be computed in time $n^{2-\phi}$, then a
$1+O(\varepsilon)$ approximation to EMD can be computed in time
$n^{2-\Omega(\phi)}$; plugging in the fastest known algorithm for CP [Alman,
Chan, Williams FOCS'16], we obtain a $(1+\varepsilon)$-approximation algorithm
for EMD running in time $n^{2-\tilde{\Omega}(\varepsilon^{1/3})}$ for
high-dimensional point sets, which improves over the prior fastest running time
of $n^{2-\Omega(\varepsilon^2)}$ [Andoni, Zhang FOCS'23]. Our main technical
contribution is a sublinear implementation of the Multiplicative Weights Update
framework for EMD. Specifically, we demonstrate that the updates can be
executed without ever explicitly computing or storing the weights; instead, we
exploit the underlying geometric structure to perform the updates implicitly.

</details>


### [248] [Controlling tail risk in two-slope ski rental](https://arxiv.org/abs/2508.06809)
*Qiming Cui,Michael Dinitz*

Main category: cs.DS

TL;DR: 论文研究了带有尾部风险的一般两斜率滑雪租赁问题的最优解，扩展了现有研究，并揭示了新的解决方案结构。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决现实世界中“租或买”场景中尾部风险对决策的影响，扩展了经典滑雪租赁问题的分析。

Method: 通过结构定理分析最优解的特征，并设计两种算法：一种基于贪心算法和二分搜索的快速近似解，另一种基于线性规划的精确解。

Result: 研究发现尾部风险导致最优解结构复杂化，可能出现无限期不购买或非平凡购买概率的情况，且解不唯一。

Conclusion: 论文通过结构定理和算法设计，为带有尾部风险的两斜率滑雪租赁问题提供了新的解决方案和分析框架。

Abstract: We study the optimal solution to a general two-slope ski rental problem with
a tail risk, i.e., the chance of the competitive ratio exceeding a value
$\gamma$ is bounded by $\delta$. This extends the recent study of tail bounds
for ski rental by [Dinitz et al. SODA 2024] to the two-slope version defined by
[Lotker et al. IPL 2008]. In this version, even after "buying," we must still
pay a rental cost at each time step, though it is lower after buying. This
models many real-world "rent-or-buy" scenarios where a one-time investment
decreases (but does not eliminate) the per-time cost.
  Despite this being a simple extension of the classical problem, we find that
adding tail risk bounds creates a fundamentally different solution structure.
For example, in our setting there is a possibility that we never buy in an
optimal solution (which can also occur without tail bounds), but more strangely
(and unlike the case without tail bounds or the classical case with tail
bounds) we also show that the optimal solution might need to have nontrivial
probabilities of buying even at finite points beyond the time corresponding to
the buying cost. Moreover, in many regimes there does not exist a unique
optimal solution. As our first contribution, we develop a series of structure
theorems to characterize some features of optimal solutions.
  The complex structure of optimal solutions makes it more difficult to develop
an algorithm to compute such a solution. As our second contribution, we utilize
our structure theorems to design two algorithms: one based on a greedy
algorithm combined with binary search that is fast but yields arbitrarily close
to optimal solutions, and a slower algorithm based on linear programming which
computes exact optimal solutions.

</details>


### [249] [A near-linear time approximation scheme for $(k,\ell)$-median clustering under discrete Fréchet distance](https://arxiv.org/abs/2508.07008)
*Anne Driemel,Jan Höckendorff,Ioannis Psarros,Christian Sohler*

Main category: cs.DS

TL;DR: 本文提出了一种针对离散Fréchet距离下$(k,\ell)$-中值问题的近似算法，首次实现了在$\ell$和$\varepsilon$为常数时的近线性时间$(1+\varepsilon)$-近似解。


<details>
  <summary>Details</summary>
Motivation: 研究如何在离散Fréchet距离下高效解决$(k,\ell)$-中值问题，特别是在$k$较大时。

Method: 引入新的离散Fréchet距离降维技术，并改进Cohen-Addad等人的算法以处理降维后的输入。

Result: 实现了近线性时间的$(1+\varepsilon)$-近似算法，并改进了核心集构造，使其大小与输入时间序列的数量和复杂度无关。

Conclusion: 该算法为离散Fréchet距离下的$(k,\ell)$-中值问题提供了高效解决方案，同时改进了核心集构造方法。

Abstract: A time series of complexity $m$ is a sequence of $m$ real valued
measurements. The discrete Fr\'echet distance $d_{dF}(x,y)$ is a distance
measure between two time series $x$ and $y$ of possibly different complexity.
Given a set of $n$ time series represented as $m$-dimensional vectors over the
reals, the $(k,\ell)$-median problem under discrete Fr\'echet distance aims to
find a set $C$ of $k$ time series of complexity $\ell$ such that $$\sum_{x\in
P} \min_{c\in C} d_{dF}(x,c)$$ is minimized. In this paper, we give the first
near-linear time $(1+\varepsilon)$-approximation algorithm for this problem
when $\ell$ and $\varepsilon$ are constants but $k$ can be as large as
$\Omega(n)$. We obtain our result by introducing a new dimension reduction
technique for discrete Fr\'echet distance and then adapt an algorithm of
Cohen-Addad et al. (J. ACM 2021) to work on the dimension-reduced input. As a
byproduct we also improve the best coreset construction for $(k,\ell)$-median
under discrete Fr\'echet distance (Cohen-Addad et al., SODA 2025) and show that
its size can be independent of the number of input time series \emph{ and }
their complexity.

</details>


### [250] [Unbiased Insights: Optimal Streaming Algorithms for $\ell_p$ Sampling, the Forget Model, and Beyond](https://arxiv.org/abs/2508.07067)
*Honghao Lin,Hoai-An Nguyen,William Swartworth,David P. Woodruff*

Main category: cs.DS

TL;DR: 研究在单次插入数据流中的ℓp采样和频率矩估计，提出空间复杂度优化的采样器，并扩展到连续采样和更复杂的模型。


<details>
  <summary>Details</summary>
Motivation: 解决在数据流中高效采样和估计频率矩的问题，特别是在包含遗忘操作和复杂函数的情况下。

Method: 提出空间复杂度优化的ℓp采样器，并设计无偏估计器处理遗忘操作和非线性挑战。

Result: 实现了近最优的Fp估计算法，解决了PODS'24中的三个开放问题，并扩展到更复杂的模型和函数。

Conclusion: 提出的方法在复杂数据流模型中实现了高效的采样和估计，具有广泛的应用潜力。

Abstract: We study $\ell_p$ sampling and frequency moment estimation in a single-pass
insertion-only data stream. For $p \in (0,2)$, we present a nearly
space-optimal approximate $\ell_p$ sampler that uses $\widetilde{O}(\log n
\log(1/\delta))$ bits of space and for $p = 2$, we present a sampler with space
complexity $\widetilde{O}(\log^2 n \log(1/\delta))$. This space complexity is
optimal for $p \in (0, 2)$ and improves upon prior work by a $\log n$ factor.
We further extend our construction to a continuous $\ell_p$ sampler, which
outputs a valid sample index at every point during the stream.
  Leveraging these samplers, we design nearly unbiased estimators for $F_p$ in
data streams that include forget operations, which reset individual element
frequencies and introduce significant non-linear challenges. As a result, we
obtain near-optimal algorithms for estimating $F_p$ for all $p$ in this model,
originally proposed by Pavan, Chakraborty, Vinodchandran, and Meel [PODS'24],
resolving all three open problems they posed.
  Furthermore, we generalize this model to what we call the suffix-prefix
deletion model, and extend our techniques to estimate entropy as a corollary of
our moment estimation algorithms. Finally, we show how to handle arbitrary
coordinate-wise functions during the stream, for any $g \in \mathbb{G}$, where
$\mathbb{G}$ includes all (linear or non-linear) contraction functions.

</details>


### [251] [Optimizing Districting Plans to Maximize Majority-Minority Districts via IPs and Local Search](https://arxiv.org/abs/2508.07446)
*Daniel Brous,David Shmoys*

Main category: cs.DS

TL;DR: 本文提出了一种基于整数规划的方法，用于生成更多多数-少数族裔选区的选区划分计划，优于现有的启发式算法。


<details>
  <summary>Details</summary>
Motivation: 在选区划分诉讼中，如何有效执行《投票权法案》是关键，而现有方法（如短爆发算法）在生成多数-少数族裔选区方面存在不足。

Method: 基于整数规划和列生成算法，结合随机分层分区算法，设计了一种新的方法，并通过局部重新优化和紧凑性提升算法改进结果。

Result: 新方法在多个数据集上表现优于短爆发算法，能生成更多多数-少数族裔选区的全州计划。

Conclusion: 整数规划方法在生成公平且紧凑的选区划分计划方面具有显著优势。

Abstract: In redistricting litigation, effective enforcement of the Voting Rights Act
has often involved providing the court with districting plans that display a
larger number of majority-minority districts than the current proposal (as was
true, for example, in what followed Allen v. Milligan concerning the
congressional districting plan for Alabama in 2023). Recent work by Cannon et
al. proposed a heuristic algorithm for generating plans to optimize
majority-minority districts, which they called short bursts; that algorithm
relies on a sophisticated random walk over the space of all plans,
transitioning in bursts, where the initial plan for each burst is the most
successful plan from the previous burst. We propose a method based on integer
programming, where we build upon another previous work, the stochastic
hierarchical partitioning algorithm, which heuristically generates a robust set
of potential districts (viewed as columns in a standard set partitioning
formulation); that approach was designed to optimize a different notion of
fairness across a statewide plan. We design a new column generation algorithm
to find plans via integer programming that outperforms short bursts on multiple
data sets in generating statewide plans with significantly more
majority-minority districts. These results also rely on a new local
re-optimization algorithm to iteratively improve on any baseline solution, as
well as an algorithm to increase the compactness of districts in plans
generated (without impacting the number of majority-minority districts).

</details>


### [252] [Simple Algorithms for Fully Dynamic Edge Connectivity](https://arxiv.org/abs/2508.07783)
*Yotam Kenneth-Mordoch,Robert Krauthgamer*

Main category: cs.DS

TL;DR: 论文提出了两种随机算法，用于动态维护图的边连通性，分别实现了不同的时间复杂度和查询效率。


<details>
  <summary>Details</summary>
Motivation: 解决动态图中边插入和删除时的边连通性维护问题，目标是提供更简单或更高效的算法。

Method: 第一种算法采用随机化方法，实现最坏情况下每边更新的时间复杂度为$\tilde{O}(n)$；第二种算法进一步优化，实现最坏情况下更新时间为$\tilde{O}(n/\lambda_G)$，查询时间为$\tilde{O}(n^2/\lambda_G^2)$。

Result: 第一种算法简化了分析，匹配了已知的时间复杂度；第二种算法首次在边连通性较大时（$\lambda_G = \omega(\sqrt{n})$）实现了更新和查询时间$o(n)$。

Conclusion: 论文展示了两种高效的随机算法，为动态边连通性问题提供了新的解决方案，特别是在高连通性情况下表现优异。

Abstract: In the fully dynamic edge connectivity problem, the input is a simple graph
$G$ undergoing edge insertions and deletions, and the goal is to maintain its
edge connectivity, denoted $\lambda_G$. We present two simple randomized
algorithms solving this problem. The first algorithm maintains the edge
connectivity in worst-case update time $\tilde{O}(n)$ per edge update, matching
the known bound but with simpler analysis. Our second algorithm achieves
worst-case update time $\tilde{O}(n/\lambda_G)$ and worst-case query time
$\tilde{O}(n^2/\lambda_G^2)$, which is the first algorithm with worst-case
update and query time $o(n)$ for large edge connectivity, namely, $\lambda_G =
\omega(\sqrt{n})$.

</details>


### [253] [Nearly Optimal Bounds for Stochastic Online Sorting](https://arxiv.org/abs/2508.07823)
*Yang Hu*

Main category: cs.DS

TL;DR: 本文研究了随机在线排序问题，提出了一个近乎最优的算法，其预期成本为对数级别，并证明了成本下界。


<details>
  <summary>Details</summary>
Motivation: 尽管随机输入直觉上应带来更好的成本界限，但之前的最佳算法仅达到次优结果，因此需要更高效的算法。

Method: 提出了一种新算法，利用随机输入的特性，实现了预期成本为对数级别的性能。

Result: 算法实现了预期成本$\log n\cdot 2^{O(\log^* n)}$，并证明了成本下界为$\Omega(\log n)$。

Conclusion: 研究表明随机在线排序可以实现近乎最优的成本，算法性能显著优于之前的结果。

Abstract: In the online sorting problem, we have an array $A$ of $n$ cells, and receive
a stream of $n$ items $x_1,\dots,x_n\in [0,1]$. When an item arrives, we need
to immediately and irrevocably place it into an empty cell. The goal is to
minimize the sum of absolute differences between adjacent items, which is
called the \emph{cost} of the algorithm. It has been shown by Aamand,
Abrahamsen, Beretta, and Kleist (SODA 2023) that when the stream
$x_1,\dots,x_n$ is generated adversarially, the optimal cost bound for any
deterministic algorithm is $\Theta(\sqrt{n})$.
  In this paper, we study the stochastic version of online sorting, where the
input items $x_1,\dots,x_n$ are sampled uniformly at random. Despite the
intuition that the stochastic version should yield much better cost bounds, the
previous best algorithm for stochastic online sorting by Abrahamsen, Bercea,
Beretta, Klausen and Kozma (ESA 2024) only achieves $\tilde{O}(n^{1/4})$ cost,
which seems far from optimal. We show that stochastic online sorting indeed
allows for much more efficient algorithms, by presenting an algorithm that
achieves expected cost $\log n\cdot 2^{O(\log^* n)}$. We also prove a cost
lower bound of $\Omega(\log n)$, thus show that our algorithm is nearly
optimal.

</details>


### [254] [Sparsifying Cayley Graphs on Every Group](https://arxiv.org/abs/2508.08078)
*Jun-Ting Hsieh,Daniel Z. Lee,Sidhanth Mohanty,Aaron Putterman,Rachel Yun Zhang*

Main category: cs.DS

TL;DR: 论文证明了Cayley图存在谱稀疏化器，并提供了高效算法，同时研究了非阿贝尔群上线性方程的稀疏化问题。


<details>
  <summary>Details</summary>
Motivation: 研究Cayley图是否能够保留少量生成器的稀疏化器，以及非阿贝尔群上线性方程的稀疏化限制。

Method: 提出了一种证明Cayley图谱稀疏化器存在性的方法，并设计了高效算法，同时分析了非阿贝尔群线性方程的稀疏化需求。

Result: 证明了Cayley图存在谱稀疏化器，并展示了算法的高效性；发现非阿贝尔群线性方程需要超多项式数量的方程才能近似保持解的数量。

Conclusion: Cayley图的稀疏化与线性方程的稀疏化在非阿贝尔群上存在本质区别。

Abstract: A classic result in graph theory, due to Batson, Spielman, and Srivastava
(STOC 2009) shows that every graph admits a $(1 \pm \varepsilon)$ cut (or
spectral) sparsifier which preserves only $O(n / \varepsilon^2)$ reweighted
edges. However, when applying this result to \emph{Cayley graphs}, the
resulting sparsifier is no longer necessarily a Cayley graph -- it can be an
arbitrary subset of edges.
  Thus, a recent line of inquiry, and one which has only seen minor progress,
asks: for any group $G$, do all Cayley graphs over the group $G$ admit
sparsifiers which preserve only $\mathrm{polylog}(|G|)/\varepsilon^2$ many
re-weighted generators?
  As our primary contribution, we answer this question in the affirmative,
presenting a proof of the existence of such Cayley graph spectral sparsifiers,
along with an efficient algorithm for finding them. Our algorithm even extends
to \emph{directed} Cayley graphs, if we instead ask only for cut sparsification
instead of spectral sparsification.
  We additionally study the sparsification of linear equations over non-abelian
groups. In contrast to the abelian case, we show that for non-abelian valued
equations, super-polynomially many linear equations must be preserved in order
to approximately preserve the number of satisfied equations for any input.
Together with our Cayley graph sparsification result, this provides a formal
separation between Cayley graph sparsification and sparsifying linear
equations.

</details>


### [255] [Sparsifying Sums of Positive Semidefinite Matrices](https://arxiv.org/abs/2508.08169)
*Arpon Basu,Pravesh K. Kothari,Yang P. Liu,Raghu Meka*

Main category: cs.DS

TL;DR: 本文重新研究了任意正半定（PSD）矩阵和的谱稀疏化问题，提出了一种基于新参数$N^*(\mathcal{A})$的实例特定稀疏化方法，改进了现有结果，并应用于Cayley图的稀疏化。


<details>
  <summary>Details</summary>
Motivation: 现有PSD矩阵稀疏化方法的结果过于粗糙，无法有效应用于Cayley图的稀疏化，因此需要一种更精细的理论。

Method: 提出了一种基于新参数$N^*(\mathcal{A})$（连通性阈值）的实例特定稀疏化方法，构建稀疏权重$\mu$，确保近似误差在$(1\pm\epsilon)$范围内。

Result: 稀疏化器仅需$O(\epsilon^{-2}N^*(\mathcal{A}) (\log n)(\log r))$个矩阵，且可在随机多项式时间内构造。此外，证明了$N^*(\mathcal{A})$是稀疏化的下限。

Conclusion: 该方法显著改进了Cayley图的稀疏化结果，首次为非$\mathbb{F}_2^n$群提供了非平凡稀疏化界限。

Abstract: In this paper, we revisit spectral sparsification for sums of arbitrary
positive semidefinite (PSD) matrices. Concretely, for any collection of PSD
matrices $\mathcal{A} = \{A_1, A_2, \ldots, A_r\} \subset \mathbb{R}^{n \times
n}$, given any subset $T \subseteq [r]$, our goal is to find sparse weights
$\mu \in \mathbb{R}_{\geq 0}^r$ such that $(1 - \epsilon) \sum_{i \in T} A_i
\preceq \sum_{i \in T} \mu_i A_i \preceq (1 + \epsilon) \sum_{i \in T} A_i.$
This generalizes spectral sparsification of graphs which corresponds to
$\mathcal{A}$ being the set of Laplacians of edges. It also captures
sparsifying Cayley graphs by choosing a subset of generators. The former has
been extensively studied with optimal sparsifiers known. The latter has
received attention recently and was solved for a few special groups (e.g.,
$\mathbb{F}_2^n$).
  Prior work shows any sum of PSD matrices can be sparsified down to $O(n)$
elements. This bound however turns out to be too coarse and in particular
yields no non-trivial bound for building Cayley sparsifiers for Cayley graphs.
  In this work, we develop a new, instance-specific (i.e., specific to a given
collection $\mathcal{A}$) theory of PSD matrix sparsification based on a new
parameter $N^*(\mathcal{A})$ which we call connectivity threshold that
generalizes the threshold of the number of edges required to make a graph
connected.
  Our main result gives a sparsifier that uses at most
$O(\epsilon^{-2}N^*(\mathcal{A}) (\log n)(\log r))$ matrices and is
constructible in randomized polynomial time. We also show that we need
$N^*(\mathcal{A})$ elements to sparsify for any $\epsilon < 0.99$.
  As the main application of our framework, we prove that any Cayley graph can
be sparsified to $O(\epsilon^{-2}\log^4 N)$ generators. Previously, a
non-trivial bound on Cayley sparsifiers was known only in the case when the
group is $\mathbb{F}_2^n$.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [256] [BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation](https://arxiv.org/abs/2508.06781)
*Christos Tsirigotis,Vaibhav Adlakha,Joao Monteiro,Aaron Courville,Perouz Taslakian*

Main category: cs.IR

TL;DR: BiXSE是一种基于LLM生成的分级相关性标签的点式训练方法，通过优化二元交叉熵（BCE）实现细粒度监督，显著降低标注和计算成本。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的相关性通常是连续的，而传统方法依赖二元标签，无法充分利用分级相关性信息。LLM的发展使得生成细粒度标签成为可能。

Method: BiXSE将LLM生成的分级相关性分数解释为概率目标，利用单标签查询-文档对实现细粒度监督，并通过批量负样本降低计算成本。

Result: 在多个基准测试（MMTEB、BEIR、TREC-DL）中，BiXSE表现优于基于softmax的对比学习（InfoNCE），并与强排序基线相当或更优。

Conclusion: BiXSE为密集检索模型提供了一种高效、可扩展的训练方法，尤其适用于分级相关性监督日益普及的场景。

Abstract: Neural sentence embedding models for dense retrieval typically rely on binary
relevance labels, treating query-document pairs as either relevant or
irrelevant. However, real-world relevance often exists on a continuum, and
recent advances in large language models (LLMs) have made it feasible to scale
the generation of fine-grained graded relevance labels. In this work, we
propose BiXSE, a simple and effective pointwise training method that optimizes
binary cross-entropy (BCE) over LLM-generated graded relevance scores. BiXSE
interprets these scores as probabilistic targets, enabling granular supervision
from a single labeled query-document pair per query. Unlike pairwise or
listwise losses that require multiple annotated comparisons per query, BiXSE
achieves strong performance with reduced annotation and compute costs by
leveraging in-batch negatives. Extensive experiments across sentence embedding
(MMTEB) and retrieval benchmarks (BEIR, TREC-DL) show that BiXSE consistently
outperforms softmax-based contrastive learning (InfoNCE), and matches or
exceeds strong pairwise ranking baselines when trained on LLM-supervised data.
BiXSE offers a robust, scalable alternative for training dense retrieval models
as graded relevance supervision becomes increasingly accessible.

</details>


### [257] [CLAP: Coreference-Linked Augmentation for Passage Retrieval](https://arxiv.org/abs/2508.06941)
*Huanwei Xu,Lin Xu,Liang Yuan*

Main category: cs.IR

TL;DR: CLAP是一种轻量级的LLM扩展框架，通过分段、解决共指链和生成伪查询，提升密集检索器的性能，尤其在跨域场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统LLM扩展方法在密集检索器中表现不佳，因语义漂移和与预训练语义空间不对齐，且段落中仅部分内容与查询相关，其余引入噪声。

Method: CLAP将段落分段为连贯块，解决共指链，生成与密集检索器表示对齐的伪查询，结合全局和细粒度信号。

Result: CLAP显著提升性能，nDCG@10绝对提升达20.68%，在跨域场景中尤其突出。

Conclusion: CLAP通过逻辑中心流程实现鲁棒、领域无关的泛化，优于传统方法。

Abstract: Large Language Model (LLM)-based passage expansion has shown promise for
enhancing first-stage retrieval, but often underperforms with dense retrievers
due to semantic drift and misalignment with their pretrained semantic space.
Beyond this, only a portion of a passage is typically relevant to a query,
while the rest introduces noise--an issue compounded by chunking techniques
that break coreference continuity. We propose Coreference-Linked Augmentation
for Passage Retrieval (CLAP), a lightweight LLM-based expansion framework that
segments passages into coherent chunks, resolves coreference chains, and
generates localized pseudo-queries aligned with dense retriever
representations. A simple fusion of global topical signals and fine-grained
subtopic signals achieves robust performance across domains. CLAP yields
consistent gains even as retriever strength increases, enabling dense
retrievers to match or surpass second-stage rankers such as BM25 + MonoT5-3B,
with up to 20.68% absolute nDCG@10 improvement. These improvements are
especially notable in out-of-domain settings, where conventional LLM-based
expansion methods relying on domain knowledge often falter. CLAP instead adopts
a logic-centric pipeline that enables robust, domain-agnostic generalization.

</details>


### [258] [Blending Sequential Embeddings, Graphs, and Engineered Features: 4th Place Solution in RecSys Challenge 2025](https://arxiv.org/abs/2508.06970)
*Sergei Makeev,Alexandr Andreev,Vladimir Baikalov,Vladislav Tytskiy,Aleksei Krasilnikov,Kirill Khrylchenko*

Main category: cs.IR

TL;DR: 本文介绍了团队ambitious在RecSys Challenge 2025中提出的第四名解决方案，专注于通用行为建模。


<details>
  <summary>Details</summary>
Motivation: 挑战的目标是为六个不同的下游任务生成有效的用户嵌入。

Method: 解决方案结合了：(1) 序列编码器捕捉用户兴趣的时间演变，(2) 图神经网络增强泛化能力，(3) 深度交叉网络建模高阶特征交互，(4) 关键特征工程。

Result: 该方法在挑战中取得了第四名的成绩。

Conclusion: 通过多技术融合和特征工程，团队成功实现了通用行为建模的目标。

Abstract: This paper describes the 4th-place solution by team ambitious for the RecSys
Challenge 2025, organized by Synerise and ACM RecSys, which focused on
universal behavioral modeling. The challenge objective was to generate user
embeddings effective across six diverse downstream tasks. Our solution
integrates (1) a sequential encoder to capture the temporal evolution of user
interests, (2) a graph neural network to enhance generalization, (3) a deep
cross network to model high-order feature interactions, and (4)
performance-critical feature engineering.

</details>


### [259] [ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability](https://arxiv.org/abs/2508.07050)
*Wenhan Liu,Xinyu Ma,Weiwei Sun,Yutao Zhu,Yuchen Li,Dawei Yin,Zhicheng Dou*

Main category: cs.IR

TL;DR: 本文提出了一种自动化生成推理密集型训练数据的框架，并设计了一种两阶段训练方法（SFT和RL），显著提升了列表排序性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的列表排序在复杂场景下表现不佳，主要因缺乏推理密集型训练数据。

Method: 1. 自动化生成高质量训练数据；2. 两阶段训练：SFT学习推理模式，RL增强排序能力。

Result: ReasonRank在BRIGHT榜单上达到SOTA性能（40.6），且延迟低于Rank1。

Conclusion: ReasonRank通过数据合成和两阶段训练，显著提升了推理密集型排序性能。

Abstract: Large Language Model (LLM) based listwise ranking has shown superior
performance in many passage ranking tasks. With the development of Large
Reasoning Models, many studies have demonstrated that step-by-step reasoning
during test-time helps improve listwise ranking performance. However, due to
the scarcity of reasoning-intensive training data, existing rerankers perform
poorly in many complex ranking scenarios and the ranking ability of
reasoning-intensive rerankers remains largely underdeveloped. In this paper, we
first propose an automated reasoning-intensive training data synthesis
framework, which sources training queries and passages from diverse domains and
applies DeepSeek-R1 to generate high-quality training labels. A
self-consistency data filtering mechanism is designed to ensure the data
quality. To empower the listwise reranker with strong reasoning ability, we
further propose a two-stage post-training approach, which includes a cold-start
supervised fine-tuning (SFT) stage for reasoning pattern learning and a
reinforcement learning (RL) stage for further ranking ability enhancement.
During the RL stage, based on the nature of listwise ranking, we design a
multi-view ranking reward, which is more effective than a ranking metric-based
reward. Extensive experiments demonstrate that our trained reasoning-intensive
reranker \textbf{ReasonRank} outperforms existing baselines significantly and
also achieves much lower latency than pointwise reranker Rank1. \textbf{Through
further experiments, our ReasonRank has achieved state-of-the-art (SOTA)
performance 40.6 on the BRIGHT
leaderboard\footnote{https://brightbenchmark.github.io/}.} Our codes are
available at https://github.com/8421BCD/ReasonRank.

</details>


### [260] [Uncertainty-Aware Semantic Decoding for LLM-Based Sequential Recommendation](https://arxiv.org/abs/2508.07210)
*Chenke Yin,Li Fan,Jia Wang,Dongxiao Hu,Haichao Zhang,Chong Zhang,Yang Xiang*

Main category: cs.IR

TL;DR: 提出了一种不确定性感知语义解码（USD）框架，通过结合基于logit的聚类和自适应评分，改进了序列推荐任务中的下一项预测。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在序列推荐任务中仍依赖自然语言处理的解码策略，导致文本生成目标与推荐目标不匹配。

Method: 采用logit向量聚类形成语义等价组，重新分配概率质量，并通过熵控制评分和采样温度。

Result: 在Amazon Product数据集上，HR@3、NDCG@3和MRR@3分别提升18.5%、11.9%和10.8%。

Conclusion: 语义聚类和不确定性评估的结合能提供更可靠和准确的推荐。

Abstract: Large language models have been widely applied to sequential recommendation
tasks, yet during inference, they continue to rely on decoding strategies
developed for natural language processing. This creates a mismatch between
text-generation objectives and recommendation next item selection objectives.
This paper addresses this limitation by proposing an Uncertainty-aware Semantic
Decoding (USD) framework that combines logit-based clustering with adaptive
scoring to improve next-item predictions. Our approach clusters items with
similar logit vectors into semantic equivalence groups, then redistributes
probability mass within these clusters and computes entropy across them to
control item scoring and sampling temperature during recommendation inference.
Experiments on Amazon Product datasets (six domains) gains of 18.5\% in HR@3,
11.9\% in NDCG@3, and 10.8\% in MRR@3 compared to state-of-the-art baselines.
Hyperparameter analysis confirms the optimal parameters among various settings,
and experiments on H\&M, and Netflix datasets indicate that the framework can
adapt to differing recommendation domains. The experimental results confirm
that integrating semantic clustering and uncertainty assessment yields more
reliable and accurate recommendations.

</details>


### [261] [Selection and Exploitation of High-Quality Knowledge from Large Language Models for Recommendation](https://arxiv.org/abs/2508.07223)
*Guanchen Wang,Mingming Ha,Tianbao Ma,Linxun Chen,Zhaojie Liu,Guorui Zhou,Kun Gai*

Main category: cs.IR

TL;DR: 论文提出KSER框架，通过知识过滤和嵌入对齐模块解决LLMs在推荐系统中知识冗余和同质化问题，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 利用LLMs的泛化能力和推理能力提升推荐系统性能，但LLMs生成的知识存在幻觉、冗余和同质化问题，直接使用会导致性能下降。

Method: 提出KSER框架，包含知识过滤模块（ESFNet）和嵌入对齐模块，并设计两种训练策略（全参数训练和仅提取器训练）。

Result: 实验验证了知识过滤和对齐模块的必要性，以及仅提取器训练策略的高效性。

Conclusion: KSER框架有效解决了LLMs知识在推荐系统中的问题，提升了推荐性能。

Abstract: In recent years, there has been growing interest in leveraging the impressive
generalization capabilities and reasoning ability of large language models
(LLMs) to improve the performance of recommenders. With this operation,
recommenders can access and learn the additional world knowledge and reasoning
information via LLMs. However, in general, for different users and items, the
world knowledge derived from LLMs suffers from issues of hallucination, content
redundant, and information homogenization. Directly feeding the generated
response embeddings into the recommendation model can lead to unavoidable
performance deterioration. To address these challenges, we propose a Knowledge
Selection \& Exploitation Recommendation (KSER) framework, which effectively
select and extracts the high-quality knowledge from LLMs. The framework
consists of two key components: a knowledge filtering module and a embedding
spaces alignment module. In the knowledge filtering module, a Embedding
Selection Filter Network (ESFNet) is designed to assign adaptive weights to
different knowledge chunks in different knowledge fields. In the space
alignment module, an attention-based architecture is proposed to align the
semantic embeddings from LLMs with the feature space used to train the
recommendation models. In addition, two training
strategies--\textbf{all-parameters training} and \textbf{extractor-only
training}--are proposed to flexibly adapt to different downstream tasks and
application scenarios, where the extractor-only training strategy offers a
novel perspective on knowledge-augmented recommendation. Experimental results
validate the necessity and effectiveness of both the knowledge filtering and
alignment modules, and further demonstrate the efficiency and effectiveness of
the extractor-only training strategy.

</details>


### [262] [SocRipple: A Two-Stage Framework for Cold-Start Video Recommendations](https://arxiv.org/abs/2508.07241)
*Amit Jaspal,Kapil Dalwani,Ajantha Ramineni*

Main category: cs.IR

TL;DR: SocRipple是一个两阶段检索框架，用于解决社交图谱平台中冷启动物品分发问题，通过社交连接和KNN搜索提升分发效果。


<details>
  <summary>Details</summary>
Motivation: 冷启动物品因缺乏交互历史而难以个性化分发，传统方法效果不佳。

Method: 第一阶段利用创作者社交关系进行初始曝光，第二阶段基于早期交互信号和用户嵌入进行KNN搜索扩展分发。

Result: 实验显示SocRipple将冷启动物品分发提升36%，同时保持用户参与率。

Conclusion: SocRipple有效平衡新物品曝光与个性化推荐。

Abstract: Most industry scale recommender systems face critical cold start challenges
new items lack interaction history, making it difficult to distribute them in a
personalized manner. Standard collaborative filtering models underperform due
to sparse engagement signals, while content only approaches lack user specific
relevance. We propose SocRipple, a novel two stage retrieval framework tailored
for coldstart item distribution in social graph based platforms. Stage 1
leverages the creators social connections for targeted initial exposure. Stage
2 builds on early engagement signals and stable user embeddings learned from
historical interactions to "ripple" outwards via K Nearest Neighbor (KNN)
search. Large scale experiments on a major video platform show that SocRipple
boosts cold start item distribution by +36% while maintaining user engagement
rate on cold start items, effectively balancing new item exposure with
personalized recommendations.

</details>


### [263] [PrLM: Learning Explicit Reasoning for Personalized RAG via Contrastive Reward Optimization](https://arxiv.org/abs/2508.07342)
*Kepu Zhang,Teng Shi,Weijie Yu,Jun Xu*

Main category: cs.IR

TL;DR: PrLM是一个强化学习框架，通过显式推理用户档案提升个性化检索增强生成（RAG）的性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大语言模型隐式整合检索内容，易受检索质量影响且可能偏离用户偏好。

Method: 提出PrLM，利用强化学习框架训练大语言模型显式推理用户档案，并通过对比训练的个人化奖励模型指导学习。

Result: 在三个个性化文本生成数据集上，PrLM表现优于现有方法，且对不同检索器和档案数量具有鲁棒性。

Conclusion: PrLM通过显式推理用户档案，显著提升了个性化检索增强生成的性能和鲁棒性。

Abstract: Personalized retrieval-augmented generation (RAG) aims to produce
user-tailored responses by incorporating retrieved user profiles alongside the
input query. Existing methods primarily focus on improving retrieval and rely
on large language models (LLMs) to implicitly integrate the retrieved context
with the query. However, such models are often sensitive to retrieval quality
and may generate responses that are misaligned with user preferences. To
address this limitation, we propose PrLM, a reinforcement learning framework
that trains LLMs to explicitly reason over retrieved user profiles. Guided by a
contrastively trained personalization reward model, PrLM effectively learns
from user responses without requiring annotated reasoning paths. Experiments on
three personalized text generation datasets show that PrLM outperforms existing
methods and remains robust across varying numbers of retrieved profiles and
different retrievers.

</details>


### [264] [Are Multimodal Embeddings Truly Beneficial for Recommendation? A Deep Dive into Whole vs. Individual Modalities](https://arxiv.org/abs/2508.07399)
*Yu Ye,Junchen Fu,Yu Song,Kaiwen Zheng,Joemon M. Jose*

Main category: cs.IR

TL;DR: 本文通过大规模实证研究验证了多模态嵌入在推荐系统中的实际效果，发现文本模态单独使用时性能接近完整多模态设置，而图像模态单独使用效果较差。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态推荐（MMRec）已成为主流范式，但其核心假设（多模态嵌入能提升推荐性能）缺乏全面实证验证，本文旨在填补这一研究空白。

Method: 采用模态剔除策略（将文本或视觉嵌入设为常量或随机噪声），评估14种先进MMRec模型，分析多模态嵌入的整体及单独效果。

Result: 多模态嵌入整体提升推荐性能，尤其是通过图融合模型；文本模态单独效果接近完整多模态，图像模态单独效果较差。

Conclusion: 研究为MMRec领域提供了基础性见解和实践指导，并公开代码和数据集以促进未来研究。

Abstract: Multimodal recommendation (MMRec) has emerged as a mainstream paradigm,
typically leveraging text and visual embeddings extracted from pre-trained
models such as Sentence-BERT, Vision Transformers, and ResNet. This approach is
founded on the intuitive assumption that incorporating multimodal embeddings
can enhance recommendation performance. However, despite its popularity, this
assumption lacks comprehensive empirical verification. This presents a critical
research gap. To address it, we pose the central research question of this
paper: Are multimodal embeddings truly beneficial for recommendation? To answer
this question, we conduct a large-scale empirical study examining the role of
text and visual embeddings in modern MMRec models, both as a whole and
individually. Specifically, we pose two key research questions: (1) Do
multimodal embeddings as a whole improve recommendation performance? (2) Is
each individual modality - text and image - useful when used alone? To isolate
the effect of individual modalities - text or visual - we employ a modality
knockout strategy by setting the corresponding embeddings to either constant
values or random noise. To ensure the scale and comprehensiveness of our study,
we evaluate 14 widely used state-of-the-art MMRec models. Our findings reveal
that: (1) multimodal embeddings generally enhance recommendation performance -
particularly when integrated through more sophisticated graph-based fusion
models. Surprisingly, commonly adopted baseline models with simple fusion
schemes, such as VBPR and BM3, show only limited gains. (2) The text modality
alone achieves performance comparable to the full multimodal setting in most
cases, whereas the image modality alone does not. These results offer
foundational insights and practical guidance for the MMRec community. We will
release our code and datasets to facilitate future research.

</details>


### [265] [Orthogonal Low Rank Embedding Stabilization](https://arxiv.org/abs/2508.07574)
*Kevin Zielnicki,Ko-Jen Hsiao*

Main category: cs.IR

TL;DR: 提出一种正交低秩变换方法，稳定推荐系统中的用户/物品嵌入空间，确保重训练时嵌入维度一致。


<details>
  <summary>Details</summary>
Motivation: 解决模型重训练时嵌入空间不稳定的问题，避免对下游应用的影响。

Method: 结合低秩奇异值分解和正交Procrustes变换，将嵌入映射到标准化空间。

Result: 计算高效、无损且轻量级，保持点积和推理质量，减少操作负担。

Conclusion: 该方法不改变训练目标或嵌入结构，可与其他稳定技术无缝集成。

Abstract: The instability of embedding spaces across model retraining cycles presents
significant challenges to downstream applications using user or item embeddings
derived from recommendation systems as input features. This paper introduces a
novel orthogonal low-rank transformation methodology designed to stabilize the
user/item embedding space, ensuring consistent embedding dimensions across
retraining sessions. Our approach leverages a combination of efficient low-rank
singular value decomposition and orthogonal Procrustes transformation to map
embeddings into a standardized space. This transformation is computationally
efficient, lossless, and lightweight, preserving the dot product and inference
quality while reducing operational burdens. Unlike existing methods that modify
training objectives or embedding structures, our approach maintains the
integrity of the primary model application and can be seamlessly integrated
with other stabilization techniques.

</details>


### [266] [Towards Comprehensible Recommendation with Large Language Model Fine-tuning](https://arxiv.org/abs/2508.07595)
*Yunze Luo,Yinjie Jiang,Gaode Chen,Xinghua Zhang,Jun Zhang,Jian Liang,Kaigui Bian*

Main category: cs.IR

TL;DR: CURec框架通过预训练和强化学习优化LLM，生成与推荐目标对齐的内容特征，提升推荐系统的语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 传统推荐方法难以捕捉用户偏好的语义信息，LLM方法则缺乏推荐对齐的推理能力，导致语义-协作鸿沟。

Method: CURec通过预训练对齐LLM与推荐目标，设计奖励模型评估生成原因，并用RL微调LLM，最后整合到下游推荐模型中。

Result: 在公开基准测试中，CURec表现优于现有方法。

Conclusion: CURec有效解决了语义-协作鸿沟问题，提升了推荐系统的性能和可解释性。

Abstract: Recommender systems have become increasingly ubiquitous in daily life. While
traditional recommendation approaches primarily rely on ID-based
representations or item-side content features, they often fall short in
capturing the underlying semantics aligned with user preferences (e.g.,
recommendation reasons for items), leading to a semantic-collaborative gap.
Recently emerged LLM-based feature extraction approaches also face a key
challenge: how to ensure that LLMs possess recommendation-aligned reasoning
capabilities and can generate accurate, personalized reasons to mitigate the
semantic-collaborative gap. To address these issues, we propose a novel Content
Understanding from a Collaborative Perspective framework (CURec), which
generates collaborative-aligned content features for more comprehensive
recommendations. \method first aligns the LLM with recommendation objectives
through pretraining, equipping it with instruction-following and
chain-of-thought reasoning capabilities. Next, we design a reward model
inspired by traditional recommendation architectures to evaluate the quality of
the recommendation reasons generated by the LLM. Finally, using the reward
signals, CURec fine-tunes the LLM through RL and corrects the generated reasons
to ensure their accuracy. The corrected reasons are then integrated into a
downstream recommender model to enhance comprehensibility and recommendation
performance. Extensive experiments on public benchmarks demonstrate the
superiority of CURec over existing methods.

</details>


### [267] [UMRE: A Unified Monotonic Transformation for Ranking Ensemble in Recommender Systems](https://arxiv.org/abs/2508.07613)
*Zhengrui Xu,Zhe Yang,Zhengxiao Guo,Shukai Liu,Luocheng Lin,Xiaoyan Liu,Yongqi Liu,Han Li*

Main category: cs.IR

TL;DR: 提出了一种名为UMRE的新框架，用于改进工业推荐系统中的集成排序问题，通过自动学习单调函数和个性化权重，取代传统的手工调优方法。


<details>
  <summary>Details</summary>
Motivation: 传统集成排序方法依赖手工设计的非线性变换和调优权重，效率低下且难以达到帕累托最优。

Method: UMRE使用无约束单调神经网络（UMNN）学习单调函数，并结合轻量级排序模型和帕累托最优策略动态调整权重。

Result: 在公开数据集和在线A/B测试中，UMRE表现出优异的性能和泛化能力。

Conclusion: UMRE通过自动化方法显著提升了推荐系统的效率和个性化能力。

Abstract: Industrial recommender systems commonly rely on ensemble sorting (ES) to
combine predictions from multiple behavioral objectives. Traditionally, this
process depends on manually designed nonlinear transformations (e.g.,
polynomial or exponential functions) and hand-tuned fusion weights to balance
competing goals -- an approach that is labor-intensive and frequently
suboptimal in achieving Pareto efficiency. In this paper, we propose a novel
Unified Monotonic Ranking Ensemble (UMRE) framework to address the limitations
of traditional methods in ensemble sorting. UMRE replaces handcrafted
transformations with Unconstrained Monotonic Neural Networks (UMNN), which
learn expressive, strictly monotonic functions through the integration of
positive neural integrals. Subsequently, a lightweight ranking model is
employed to fuse the prediction scores, assigning personalized weights to each
prediction objective. To balance competing goals, we further introduce a Pareto
optimality strategy that adaptively coordinates task weights during training.
UMRE eliminates manual tuning, maintains ranking consistency, and achieves
fine-grained personalization. Experimental results on two public recommendation
datasets (Kuairand and Tenrec) and online A/B tests demonstrate impressive
performance and generalization capabilities.

</details>


### [268] [Encode Me If You Can: Learning Universal User Representations via Event Sequence Autoencoding](https://arxiv.org/abs/2508.07748)
*Anton Klenitskiy,Artem Fatkulin,Daria Denisova,Anton Pembek,Alexey Vasilev*

Main category: cs.IR

TL;DR: 论文提出了一种基于GRU自编码器的通用用户行为表示方法，通过将用户历史行为序列编码为固定大小的向量，并在RecSys Challenge 2025中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 构建通用的用户行为表示可以减少任务特定的特征工程和模型重训练需求，提高机器学习管线的可扩展性和效率。

Method: 将用户历史行为转换为单一时间序列，使用GRU自编码器重建序列并提取固定大小的潜在向量，同时探索了多种嵌入生成方法并通过拼接输出向量形成统一表示。

Result: 该方法在RecSys Challenge 2025中表现优异，团队获得第二名。

Conclusion: 通过GRU自编码器和多方法组合的嵌入策略，成功构建了通用的用户行为表示，适用于多种下游任务。

Abstract: Building universal user representations that capture the essential aspects of
user behavior is a crucial task for modern machine learning systems. In
real-world applications, a user's historical interactions often serve as the
foundation for solving a wide range of predictive tasks, such as churn
prediction, recommendations, or lifetime value estimation. Using a
task-independent user representation that is effective across all such tasks
can reduce the need for task-specific feature engineering and model retraining,
leading to more scalable and efficient machine learning pipelines. The goal of
the RecSys Challenge 2025 by Synerise was to develop such Universal Behavioral
Profiles from logs of past user behavior, which included various types of
events such as product purchases, page views, and search queries. We propose a
method that transforms the entire user interaction history into a single
chronological sequence and trains a GRU-based autoencoder to reconstruct this
sequence from a fixed-size vector. If the model can accurately reconstruct the
sequence, the latent vector is expected to capture the key behavioral patterns.
In addition to this core model, we explored several alternative methods for
generating user embeddings and combined them by concatenating their output
vectors into a unified representation. This ensemble strategy further improved
generalization across diverse downstream tasks and helped our team,
ai_lab_recsys, achieve second place in the RecSys Challenge 2025.

</details>


### [269] [Recommendation Is a Dish Better Served Warm](https://arxiv.org/abs/2508.07856)
*Danil Gusak,Nikita Sukhorukov,Evgeny Frolov*

Main category: cs.IR

TL;DR: 论文探讨了推荐系统中冷启动阈值的设定问题，指出其不一致性会影响评估结果的可比性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 冷用户和物品的过滤阈值通常随意设定且差异大，导致评估结果不一致。

Method: 通过实验逐步调整用户和物品的交互次数，研究不同数据集和基线推荐系统中的阈值选择。

Result: 发现不一致的阈值选择会导致数据浪费或冷实例误分类为暖实例，增加系统噪声。

Conclusion: 需要更系统的方法设定冷启动阈值，以提高推荐系统的评估可靠性。

Abstract: In modern recommender systems, experimental settings typically include
filtering out cold users and items based on a minimum interaction threshold.
However, these thresholds are often chosen arbitrarily and vary widely across
studies, leading to inconsistencies that can significantly affect the
comparability and reliability of evaluation results. In this paper, we
systematically explore the cold-start boundary by examining the criteria used
to determine whether a user or an item should be considered cold. Our
experiments incrementally vary the number of interactions for different items
during training, and gradually update the length of user interaction histories
during inference. We investigate the thresholds across several widely used
datasets, commonly represented in recent papers from top-tier conferences, and
on multiple established recommender baselines. Our findings show that
inconsistent selection of cold-start thresholds can either result in the
unnecessary removal of valuable data or lead to the misclassification of cold
instances as warm, introducing more noise into the system.

</details>


### [270] [Careful Queries, Credible Results: Teaching RAG Models Advanced Web Search Tools with Reinforcement Learning](https://arxiv.org/abs/2508.07956)
*Yuqin Dai,Shuo Yang,Guoqing Wang,Yong Deng,Zhanwei Zhang,Jun Yin,Pengyu Zeng,Zhenzhe Ying,Changhua Meng,Can Yi,Yuchen Zhou,Weiqiang Wang,Shuai Lu*

Main category: cs.IR

TL;DR: WebFilter是一个新的RAG框架，通过生成受限查询和过滤不可靠内容，提升检索精度和答案质量。


<details>
  <summary>Details</summary>
Motivation: 解决网络环境中普遍存在的错误信息和未充分利用网络工具的问题，以提高RAG系统的检索准确性。

Method: 结合检索过滤机制和行为驱动的奖励策略，优化查询生成和检索结果。

Result: 在领域内和领域外基准测试中，WebFilter的表现优于现有RAG方法。

Conclusion: WebFilter显著提升了RAG系统的检索精度和答案质量，有效应对了网络环境中的挑战。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating up-to-date external knowledge, yet real-world web environments
present unique challenges. These limitations manifest as two key challenges:
pervasive misinformation in the web environment, which introduces unreliable or
misleading content that can degrade retrieval accuracy, and the
underutilization of web tools, which, if effectively employed, could enhance
query precision and help mitigate this noise, ultimately improving the
retrieval results in RAG systems. To address these issues, we propose
WebFilter, a novel RAG framework that generates source-restricted queries and
filters out unreliable content. This approach combines a retrieval filtering
mechanism with a behavior- and outcome-driven reward strategy, optimizing both
query formulation and retrieval outcomes. Extensive experiments demonstrate
that WebFilter improves answer quality and retrieval precision, outperforming
existing RAG methods on both in-domain and out-of-domain benchmarks.

</details>


### [271] [Improving Document Retrieval Coherence for Semantically Equivalent Queries](https://arxiv.org/abs/2508.07975)
*Stefano Campese,Alessandro Moschitti,Ivano Lauriola*

Main category: cs.IR

TL;DR: 本文提出了一种改进的多负排名损失方法，用于训练密集检索模型，以提高模型在语义相似查询下检索文档的一致性。实验表明，该方法降低了模型的敏感性并提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的密集检索模型对查询和文档词汇敏感，微小变化可能导致检索结果显著不同。本文旨在通过改进损失函数提升模型在语义相似查询下的一致性。

Method: 提出了一种改进的多负排名损失，惩罚语义等效查询在检索前k文档时的差异。

Result: 在多个数据集（MS-MARCO、Natural Questions、BEIR、TREC DL 19/20）上的实验显示，优化后的模型敏感性降低，准确性提高。

Conclusion: 改进的损失函数有效提升了密集检索模型的一致性和性能。

Abstract: Dense Retrieval (DR) models have proven to be effective for Document
Retrieval and Information Grounding tasks. Usually, these models are trained
and optimized for improving the relevance of top-ranked documents for a given
query. Previous work has shown that popular DR models are sensitive to the
query and document lexicon: small variations of it may lead to a significant
difference in the set of retrieved documents. In this paper, we propose a
variation of the Multi-Negative Ranking loss for training DR that improves the
coherence of models in retrieving the same documents with respect to
semantically similar queries. The loss penalizes discrepancies between the
top-k ranked documents retrieved for diverse but semantic equivalent queries.
We conducted extensive experiments on various datasets, MS-MARCO, Natural
Questions, BEIR, and TREC DL 19/20. The results show that (i) models optimizes
by our loss are subject to lower sensitivity, and, (ii) interestingly, higher
accuracy.

</details>


### [272] [DIVER: A Multi-Stage Approach for Reasoning-intensive Information Retrieval](https://arxiv.org/abs/2508.07995)
*Meixiu Long,Duolin Sun,Dan Yang,Junjie Wang,Yue Shen,Jian Wang,Peng Wei,Jinjie Gu,Jiahai Wang*

Main category: cs.IR

TL;DR: DIVER是一个针对推理密集型信息检索的检索流程，通过文档处理、查询扩展、推理增强检索器和重排序器，显著提升了复杂查询的性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索方法难以处理涉及抽象推理、类比思维或多步推理的查询，DIVER旨在解决这一问题。

Method: DIVER包含四个组件：文档处理、LLM驱动的查询扩展、推理增强检索器和结合LLM评分的重排序器。

Result: 在BRIGHT基准测试中，DIVER的nDCG@10得分达到41.6和28.9，优于现有推理感知模型。

Conclusion: DIVER证明了推理感知检索策略在复杂任务中的有效性，代码和模型即将发布。

Abstract: Retrieval-augmented generation has achieved strong performance on
knowledge-intensive tasks where query-document relevance can be identified
through direct lexical or semantic matches. However, many real-world queries
involve abstract reasoning, analogical thinking, or multi-step inference, which
existing retrievers often struggle to capture. To address this challenge, we
present \textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive
information retrieval. DIVER consists of four components: document processing
to improve input quality, LLM-driven query expansion via iterative document
interaction, a reasoning-enhanced retriever fine-tuned on synthetic
multi-domain data with hard negatives, and a pointwise reranker that combines
LLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark,
DIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original
queries, consistently outperforming competitive reasoning-aware models. These
results demonstrate the effectiveness of reasoning-aware retrieval strategies
in complex real-world tasks. Our code and retrieval model will be released
soon.

</details>


### [273] [Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation](https://arxiv.org/abs/2508.08042)
*Van-Khang Nguyen,Duc-Hoang Pham,Huy-Son Nguyen,Cam-Van Thi Nguyen,Hoang-Quynh Le,Duc-Trong Le*

Main category: cs.IR

TL;DR: 论文提出了一种名为MAMEX的新型混合专家框架，用于多模态冷启动推荐，通过动态利用不同模态的潜在表示，显著提升了推荐效果。


<details>
  <summary>Details</summary>
Motivation: 冷启动场景下，新物品因交互历史有限难以有效推荐，而现有方法对多模态数据的整合过于简单，无法捕捉模态间复杂关系。

Method: MAMEX采用模态特定专家网络和可学习门控机制，动态加权各模态贡献，适应不同模态的内容特性。

Result: 在基准数据集上，MAMEX在冷启动场景中表现优于现有方法，具有更高的准确性和适应性。

Conclusion: MAMEX通过动态模态整合，有效解决了冷启动推荐问题，代码已开源。

Abstract: Recommendation systems have faced significant challenges in cold-start
scenarios, where new items with a limited history of interaction need to be
effectively recommended to users. Though multimodal data (e.g., images, text,
audio, etc.) offer rich information to address this issue, existing approaches
often employ simplistic integration methods such as concatenation, average
pooling, or fixed weighting schemes, which fail to capture the complex
relationships between modalities. Our study proposes a novel Mixture of Experts
(MoE) framework for multimodal cold-start recommendation, named MAMEX, which
dynamically leverages latent representation from different modalities. MAMEX
utilizes modality-specific expert networks and introduces a learnable gating
mechanism that adaptively weights the contribution of each modality based on
its content characteristics. This approach enables MAMEX to emphasize the most
informative modalities for each item while maintaining robustness when certain
modalities are less relevant or missing. Extensive experiments on benchmark
datasets show that MAMEX outperforms state-of-the-art methods in cold-start
scenarios, with superior accuracy and adaptability. For reproducibility, the
code has been made available on Github https://github.com/L2R-UET/MAMEX.

</details>


### [274] [HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches](https://arxiv.org/abs/2508.08088)
*Jiejun Tan,Zhicheng Dou,Yan Yu,Jiehan Cheng,Qiang Ju,Jian Xie,Ji-Rong Wen*

Main category: cs.IR

TL;DR: 论文提出了一种分层代理深度搜索框架HierSearch，通过分层强化学习解决多源检索中的训练效率低和工具掌握不足问题。


<details>
  <summary>Details</summary>
Motivation: 企业需要能够同时利用本地和网络语料库的私有深度搜索系统，但现有方法局限于单一知识源，且传统强化学习存在训练效率低和工具掌握不足的问题。

Method: 提出HierSearch框架，包括低层的本地和网络深度搜索代理，以及高层的规划代理，并设计知识精炼器过滤错误信息。

Result: 实验表明，HierSearch在六个基准测试中优于传统强化学习和多源检索增强生成基线。

Conclusion: HierSearch通过分层设计和知识精炼，显著提升了多源深度搜索的性能。

Abstract: Recently, large reasoning models have demonstrated strong mathematical and
coding abilities, and deep search leverages their reasoning capabilities in
challenging information retrieval tasks. Existing deep search works are
generally limited to a single knowledge source, either local or the Web.
However, enterprises often require private deep search systems that can
leverage search tools over both local and the Web corpus. Simply training an
agent equipped with multiple search tools using flat reinforcement learning
(RL) is a straightforward idea, but it has problems such as low training data
efficiency and poor mastery of complex tools. To address the above issue, we
propose a hierarchical agentic deep search framework, HierSearch, trained with
hierarchical RL. At the low level, a local deep search agent and a Web deep
search agent are trained to retrieve evidence from their corresponding domains.
At the high level, a planner agent coordinates low-level agents and provides
the final answer. Moreover, to prevent direct answer copying and error
propagation, we design a knowledge refiner that filters out hallucinations and
irrelevant evidence returned by low-level agents. Experiments show that
HierSearch achieves better performance compared to flat RL, and outperforms
various deep search and multi-source retrieval-augmented generation baselines
in six benchmarks across general, finance, and medical domains.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [275] [Narrative Memory in Machines: Multi-Agent Arc Extraction in Serialized TV](https://arxiv.org/abs/2508.07010)
*Roberto Balestri,Guglielmo Pescatore*

Main category: cs.MM

TL;DR: 论文提出了一种多智能体系统（MAS），用于通过计算记忆架构提取和分析电视连续剧的叙事弧，结合人类记忆模拟和LLM技术，测试结果显示系统能有效识别特定叙事类型，但存在局限性。


<details>
  <summary>Details</summary>
Motivation: 电视连续剧的复杂叙事结构需要高效的信息管理方法，传统分析方法难以应对其时间分布性，因此需要一种结合计算记忆和人类理解的系统。

Method: 采用多智能体系统（MAS），结合LLM（语义记忆）、向量数据库（情景记忆）和工作记忆模拟，从剧集摘要中提取叙事弧，并通过图形界面实现人机协作。

Result: 系统成功识别了三种叙事弧类型（Anthology、Soap、Genre-Specific），但在处理重叠弧和不透明动态时表现有限。

Conclusion: 该记忆导向方法展示了AI与人类协作的潜力，未来将扩展多模态输入和测试范围。

Abstract: Serialized television narratives present significant analytical challenges
due to their complex, temporally distributed storylines that necessitate
sophisticated information management. This paper introduces a multi-agent
system (MAS) designed to extract and analyze narrative arcs by implementing
principles of computational memory architectures. The system conceptualizes
narrative understanding through analogues of human memory: Large Language
Models (LLMs) provide a form of semantic memory for general narrative patterns,
while a vector database stores specific arc progressions as episodic memories.
A multi-agent workflow simulates working memory processes to integrate these
information types. Tested on the first season of Grey's Anatomy (ABC 2005-),
the MAS identifies three arc types: Anthology (self-contained), Soap
(relationship-focused), and Genre-Specific. These arcs and their episodic
developments are stored in a vector database, facilitating structured analysis
and semantic comparison. To bridge automation with critical interpretation, a
graphical interface enables human oversight and refinement of the system's
narrative memory. While demonstrating strong performance in identifying
Anthology Arcs and character entities, the system's reliance on textual
paratexts (episode summaries) revealed limitations in discerning overlapping
arcs and opaque dynamics, underscoring the challenges in computational memory
consolidation versus human holistic understanding. This memory-centric approach
highlights the potential of combining AI-driven memory processing with human
expertise. Beyond television, it offers promise for serialized written formats
where narrative is entirely text-based. Future work will focus on integrating
multimodal inputs to enrich episodic memory, refining memory integration
mechanisms within the MAS, and expanding testing across diverse genres.

</details>


### [276] [Reversible Video Steganography Using Quick Response Codes and Modified ElGamal Cryptosystem](https://arxiv.org/abs/2508.07289)
*Ramadhan J. Mstafa*

Main category: cs.MM

TL;DR: 本文提出了一种基于DWT和QR码的可逆视频隐写方法，结合改进的ElGamal加密系统，提高了安全性和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 随着互联网技术的快速发展，隐私和数据安全问题日益突出。视频隐写术在保护敏感信息方面具有重要作用，但现有方法在视觉不可感知性、鲁棒性和嵌入容量方面存在不足。

Method: 提出了一种结合DWT和QR码的可逆视频隐写方法，使用改进的ElGamal算法加密QR码，并通过LSB技术将加密后的QR码嵌入视频帧的不同子带和分量中。

Result: 实验表明，该方法具有高安全性、高隐蔽性，并能抵抗多种噪声攻击，平均SSIM超过0.91，PSNR平均为52.143 dB，嵌入容量为1 bpp。

Conclusion: 该方法在视觉不可感知性、鲁棒性和嵌入容量方面优于现有方法，为视频隐写提供了一种有效的解决方案。

Abstract: The rapid transmission of multimedia information has been achieved mainly by
recent advancements in the Internet's speed and information technology. In
spite of this, advancements in technology have resulted in breaches of privacy
and data security. When it comes to protecting private information in today's
Internet era, digital steganography is vital. Many academics are interested in
digital video because it has a great capability for concealing important data.
There have been a vast number of video steganography solutions developed lately
to guard against the theft of confidential data. The visual imperceptibility,
robustness, and embedding capacity of these approaches are all challenges that
must be addressed. In this paper, a novel solution to reversible video
steganography based on DWT and QR codes is proposed to address these concerns.
In order to increase the security level of the suggested method, an enhanced
ElGamal cryptosystem has also been proposed. Prior to the embedding stage, the
suggested method uses the modified ElGamal algorithm to encrypt secret QR
codes. Concurrently, it applies two-dimensional DWT on the Y-component of each
video frame resulting in LL, LH, HL, and HH sub-bands. Then, the encrypted Low
(L), Medium (M), Quantile (Q), and High (H) QR codes are embedded into the HL
sub-band, HH sub-band, U-component, and V-component of video frames,
respectively, using the LSB technique. As a consequence of extensive testing of
the approach, it was shown to be very secure and highly invisible, as well as
highly resistant to attacks from Salt & Pepper, Gaussian, Poisson, and Speckle
noises, which has an average SSIM of more than 0.91. Aside from visual
imperceptibility, the suggested method exceeds current methods in terms of PSNR
average of 52.143 dB, and embedding capacity 1 bpp.

</details>


### [277] [FineBadminton: A Multi-Level Dataset for Fine-Grained Badminton Video Understanding](https://arxiv.org/abs/2508.07554)
*Xusheng He,Wei Liu,Shanshan Ma,Qian Liu,Chenghao Ma,Jianlong Wu*

Main category: cs.MM

TL;DR: 论文介绍了FineBadminton数据集和FBBench基准，用于提升多模态大语言模型（MLLMs）在羽毛球运动中的细粒度视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在复杂高速运动（如羽毛球）的细粒度分析中表现不佳，主要因缺乏丰富且领域特定的标注数据集。

Method: 提出FineBadminton数据集，采用多级语义标注层次和创新标注流程（MLLM生成建议+人工优化），并开发FBBench基准。提出基于关键帧选择和视觉信息浓缩的基线方法。

Result: 当前MLLMs在深度运动视频分析中仍面临挑战，但提出的策略显著提升了性能。

Conclusion: FineBadminton和FBBench为细粒度视频理解和运动智能研究提供了关键生态系统。

Abstract: Fine-grained analysis of complex and high-speed sports like badminton
presents a significant challenge for Multimodal Large Language Models (MLLMs),
despite their notable advancements in general video understanding. This
difficulty arises primarily from the scarcity of datasets with sufficiently
rich and domain-specific annotations. To bridge this gap, we introduce
FineBadminton, a novel and large-scale dataset featuring a unique multi-level
semantic annotation hierarchy (Foundational Actions, Tactical Semantics, and
Decision Evaluation) for comprehensive badminton understanding. The
construction of FineBadminton is powered by an innovative annotation pipeline
that synergistically combines MLLM-generated proposals with human refinement.
We also present FBBench, a challenging benchmark derived from FineBadminton, to
rigorously evaluate MLLMs on nuanced spatio-temporal reasoning and tactical
comprehension. Together, FineBadminton and FBBench provide a crucial ecosystem
to catalyze research in fine-grained video understanding and advance the
development of MLLMs in sports intelligence. Furthermore, we propose an
optimized baseline approach incorporating Hit-Centric Keyframe Selection to
focus on pivotal moments and Coordinate-Guided Condensation to distill salient
visual information. The results on FBBench reveal that while current MLLMs
still face significant challenges in deep sports video analysis, our proposed
strategies nonetheless achieve substantial performance gains. The project
homepage is available at https://finebadminton.github.io/FineBadminton/.

</details>


### [278] [MSPT: A Lightweight Face Image Quality Assessment Method with Multi-stage Progressive Training](https://arxiv.org/abs/2508.07590)
*Xiongwei Xiao,Baoying Chen,Jishen Zeng,Jianquan Yang*

Main category: cs.MM

TL;DR: 提出了一种轻量级人脸质量评估网络MSPT，通过多阶段渐进训练策略，在保持高效推理的同时达到高性能。


<details>
  <summary>Details</summary>
Motivation: 传统人脸质量评估方法泛化性差，而学习型方法虽性能优越但计算和存储成本高，难以实际部署。

Method: 采用三阶段渐进训练策略，逐步引入多样化数据样本并提高输入图像分辨率。

Result: 在VQualA 2025基准测试中排名第二，性能与现有最优方法相当或更好。

Conclusion: MSPT在轻量级网络中有效学习复杂质量特征，同时显著减少灾难性遗忘，适合实际部署。

Abstract: Accurately assessing the perceptual quality of face images is crucial,
especially with the rapid progress in face restoration and generation.
Traditional quality assessment methods often struggle with the unique
characteristics of face images, limiting their generalizability. While
learning-based approaches demonstrate superior performance due to their strong
fitting capabilities, their high complexity typically incurs significant
computational and storage costs, hindering practical deployment. To address
this, we propose a lightweight face quality assessment network with Multi-Stage
Progressive Training (MSPT). Our network employs a three-stage progressive
training strategy that gradually introduces more diverse data samples and
increases input image resolution. This novel approach enables lightweight
networks to achieve high performance by effectively learning complex quality
features while significantly mitigating catastrophic forgetting. Our MSPT
achieved the second highest score on the VQualA 2025 face image quality
assessment benchmark dataset, demonstrating that MSPT achieves comparable or
better performance than state-of-the-art methods while maintaining efficient
inference.

</details>


### [279] [AD-AVSR: Asymmetric Dual-stream Enhancement for Robust Audio-Visual Speech Recognition](https://arxiv.org/abs/2508.07608)
*Junxiao Xue,Xiaozhen Liu,Xuecheng Wu,Xinyi Yin,Danlei Huang,Fei Yu*

Main category: cs.MM

TL;DR: AD-AVSR框架通过双向模态增强改进音频-视觉语音识别，解决了现有方法在不对称信息条件下的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视觉语音识别方法在噪声环境中表现受限，无法有效捕捉异构和互补的音频-视觉数据关联。

Method: 提出AD-AVSR框架，采用音频双流编码策略和双向模态增强模块（音频感知视觉优化和跨模态噪声抑制掩码），并引入阈值选择机制。

Result: 在LRS2和LRS3数据集上，AD-AVSR性能优于现有方法，尤其在噪声鲁棒性方面表现突出。

Conclusion: AD-AVSR通过双向信息流和噪声抑制设计，显著提升了音频-视觉语音识别的性能。

Abstract: Audio-visual speech recognition (AVSR) combines audio-visual modalities to
improve speech recognition, especially in noisy environments. However, most
existing methods deploy the unidirectional enhancement or symmetric fusion
manner, which limits their capability to capture heterogeneous and
complementary correlations of audio-visual data-especially under asymmetric
information conditions. To tackle these gaps, we introduce a new AVSR framework
termed AD-AVSR based on bidirectional modality enhancement. Specifically, we
first introduce the audio dual-stream encoding strategy to enrich audio
representations from multiple perspectives and intentionally establish
asymmetry to support subsequent cross-modal interactions. The enhancement
process involves two key components, Audio-aware Visual Refinement Module for
enhanced visual representations under audio guidance, and Cross-modal Noise
Suppression Masking Module which refines audio representations using visual
cues, collaboratively leading to the closed-loop and bidirectional information
flow. To further enhance correlation robustness, we adopt a threshold-based
selection mechanism to filter out irrelevant or weakly correlated audio-visual
pairs. Extensive experimental results on the LRS2 and LRS3 datasets indicate
that our AD-AVSR consistently surpasses SOTA methods in both performance and
noise robustness, highlighting the effectiveness of our model design.

</details>


### [280] [Towards Multimodal Sentiment Analysis via Contrastive Cross-modal Retrieval Augmentation and Hierachical Prompts](https://arxiv.org/abs/2508.07666)
*Xianbing Zhao,Shengzun Yang,Buzhou Tang,Ronghuan Jiang*

Main category: cs.MM

TL;DR: 提出了一种多模态检索增强框架，结合样本间模态级和跨样本样本级参考上下文，以增强多模态特征。


<details>
  <summary>Details</summary>
Motivation: 当前跨模态方法主要关注单个样本内的模态级参考上下文，忽略了跨样本关系作为样本级参考上下文的潜力。

Method: 设计了对比跨模态检索模块和两种提示（模态级和样本级），并构建了跨模态检索增强编码器。

Result: 在两个公开数据集上验证了模型的有效性和优越性。

Conclusion: 该框架成功整合了多层次的参考上下文，提升了多模态情感分析的性能。

Abstract: Multimodal sentiment analysis is a fundamental problem in the field of
affective computing. Although significant progress has been made in cross-modal
interaction, it remains a challenge due to the insufficient reference context
in cross-modal interactions. Current cross-modal approaches primarily focus on
leveraging modality-level reference context within a individual sample for
cross-modal feature enhancement, neglecting the potential cross-sample
relationships that can serve as sample-level reference context to enhance the
cross-modal features. To address this issue, we propose a novel multimodal
retrieval-augmented framework to simultaneously incorporate inter-sample
modality-level reference context and cross-sample sample-level reference
context to enhance the multimodal features. In particular, we first design a
contrastive cross-modal retrieval module to retrieve semantic similar samples
and enhance target modality. To endow the model to capture both inter-sample
and intra-sample information, we integrate two different types of prompts,
modality-level prompts and sample-level prompts, to generate modality-level and
sample-level reference contexts, respectively. Finally, we design a cross-modal
retrieval-augmented encoder that simultaneously leverages modality-level and
sample-level reference contexts to enhance the target modality. Extensive
experiments demonstrate the effectiveness and superiority of our model on two
publicly available datasets.

</details>


### [281] [Mining the Social Fabric: Unveiling Communities for Fake News Detection in Short Videos](https://arxiv.org/abs/2508.07992)
*Haisong Gong,Bolan Su,Xinrong Zhang,Jing Li,Qiang Liu,Shu Wu,Liang Wang*

Main category: cs.MM

TL;DR: DugFND提出了一种基于双社区图（上传者社区和事件驱动社区）的假新闻检测方法，通过异构图注意力网络提升现有视频分类器的性能。


<details>
  <summary>Details</summary>
Motivation: 短视频平台假新闻传播迅速且难以检测，现有方法忽视了视频、上传者和事件之间的隐含关系。

Method: 构建异构图连接上传者、视频和事件节点，设计时间感知的异构图注意力网络，并通过重建预训练优化节点表示。

Result: 在公开数据集上，DugFND显著提升了假新闻检测的性能。

Conclusion: 双社区建模对短视频假新闻检测具有重要价值。

Abstract: Short video platforms have become a major medium for information sharing, but
their rapid content generation and algorithmic amplification also enable the
widespread dissemination of fake news. Detecting misinformation in short videos
is challenging due to their multi-modal nature and the limited context of
individual videos. While recent methods focus on analyzing content
signals-visual, textual, and audio-they often overlook implicit relationships
among videos, uploaders, and events. To address this gap, we propose DugFND
(Dual-community graph for fake news detection), a novel method that enhances
existing video classifiers by modeling two key community patterns: (1) uploader
communities, where uploaders with shared interests or similar content creation
patterns group together, and (2) event-driven communities, where videos related
to the same or semantically similar public events form localized clusters. We
construct a heterogeneous graph connecting uploader, video, and event nodes,
and design a time-aware heterogeneous graph attention network to enable
effective message passing. A reconstruction-based pretraining phase further
improves node representation learning. DugFND can be applied to any pre-trained
classifier. Experiments on public datasets show that our method achieves
significant performance gains, demonstrating the value of dual-community
modeling for fake news detection in short videos.

</details>


### [282] [VGGSounder: Audio-Visual Evaluations for Foundation Models](https://arxiv.org/abs/2508.08237)
*Daniil Zverev,Thaddäus Wiedemer,Ameya Prabhu,Matthias Bethge,Wieland Brendel,A. Sophia Koepke*

Main category: cs.MM

TL;DR: VGGSounder数据集存在标注不完整、类别重叠和模态不对齐问题，作者提出改进版VGGSounder，用于更准确评估音频-视觉基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有VGGSounder数据集存在缺陷，导致对多模态模型评估不准确，需改进以更可靠评估模型性能。

Method: 重新标注VGGSound数据集，创建多标签测试集VGGSounder，引入模态混淆指标分析模型性能下降。

Result: VGGSounder提供了更精确的模态性能分析，并揭示模型在多模态输入时的局限性。

Conclusion: 改进后的VGGSounder能更可靠评估音频-视觉基础模型，为未来研究提供更准确基准。

Abstract: The emergence of audio-visual foundation models underscores the importance of
reliably assessing their multi-modal understanding. The VGGSounder dataset is
commonly used as a benchmark for evaluation audio-visual classification.
However, our analysis identifies several limitations of VGGSounder, including
incomplete labelling, partially overlapping classes, and misaligned modalities.
These lead to distorted evaluations of auditory and visual capabilities. To
address these limitations, we introduce VGGSounder, a comprehensively
re-annotated, multi-label test set that extends VGGSound and is specifically
designed to evaluate audio-visual foundation models. VGGSounder features
detailed modality annotations, enabling precise analyses of modality-specific
performance. Furthermore, we reveal model limitations by analysing performance
degradation when adding another input modality with our new modality confusion
metric.

</details>
