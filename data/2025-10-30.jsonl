{"id": "2510.25220", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25220", "abs": "https://arxiv.org/abs/2510.25220", "authors": ["Zhijie Lin", "Zhuofeng Li", "Chenglei Dai", "Wentian Bao", "Shuai Lin", "Enyun Yu", "Haoxiang Zhang", "Liang Zhao"], "title": "GReF: A Unified Generative Framework for Efficient Reranking via Ordered Multi-token Prediction", "comment": "Accepted by CIKM 2025", "summary": "In a multi-stage recommendation system, reranking plays a crucial role in\nmodeling intra-list correlations among items. A key challenge lies in exploring\noptimal sequences within the combinatorial space of permutations. Recent\nresearch follows a two-stage (generator-evaluator) paradigm, where a generator\nproduces multiple feasible sequences, and an evaluator selects the best one. In\npractice, the generator is typically implemented as an autoregressive model.\nHowever, these two-stage methods face two main challenges. First, the\nseparation of the generator and evaluator hinders end-to-end training. Second,\nautoregressive generators suffer from inference efficiency. In this work, we\npropose a Unified Generative Efficient Reranking Framework (GReF) to address\nthe two primary challenges. Specifically, we introduce Gen-Reranker, an\nautoregressive generator featuring a bidirectional encoder and a dynamic\nautoregressive decoder to generate causal reranking sequences. Subsequently, we\npre-train Gen-Reranker on the item exposure order for high-quality parameter\ninitialization. To eliminate the need for the evaluator while integrating\nsequence-level evaluation during training for end-to-end optimization, we\npropose post-training the model through Rerank-DPO. Moreover, for efficient\nautoregressive inference, we introduce ordered multi-token prediction (OMTP),\nwhich trains Gen-Reranker to simultaneously generate multiple future items\nwhile preserving their order, ensuring practical deployment in real-time\nrecommender systems. Extensive offline experiments demonstrate that GReF\noutperforms state-of-the-art reranking methods while achieving latency that is\nnearly comparable to non-autoregressive models. Additionally, GReF has also\nbeen deployed in a real-world video app Kuaishou with over 300 million daily\nactive users, significantly improving online recommendation quality."}
{"id": "2510.25259", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25259", "abs": "https://arxiv.org/abs/2510.25259", "authors": ["Yehjin Shin", "Jeongwhan Choi", "Seojin Kim", "Noseong Park"], "title": "TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation", "comment": "The 39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "Recently, convolutional filters have been increasingly adopted in sequential\nrecommendation for their ability to capture local sequential patterns. However,\nmost of these models complement convolutional filters with self-attention. This\nis because convolutional filters alone, generally fixed filters, struggle to\ncapture global interactions necessary for accurate recommendation. We propose\nTime-Variant Convolutional Filters for Sequential Recommendation (TV-Rec), a\nmodel inspired by graph signal processing, where time-variant graph filters\ncapture position-dependent temporal variations in user sequences. By replacing\nboth fixed kernels and self-attention with time-variant filters, TV-Rec\nachieves higher expressive power and better captures complex interaction\npatterns in user behavior. This design not only eliminates the need for\nself-attention but also reduces computation while accelerating inference.\nExtensive experiments on six public benchmarks show that TV-Rec outperforms\nstate-of-the-art baselines by an average of 7.49%."}
{"id": "2510.25285", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.25285", "abs": "https://arxiv.org/abs/2510.25285", "authors": ["Qiushi Pan", "Hao Wang", "Guoyuan An", "Luankang Zhang", "Wei Guo", "Yong Liu"], "title": "Revisiting scalable sequential recommendation with Multi-Embedding Approach and Mixture-of-Experts", "comment": null, "summary": "In recommendation systems, how to effectively scale up recommendation models\nhas been an essential research topic. While significant progress has been made\nin developing advanced and scalable architectures for sequential\nrecommendation(SR) models, there are still challenges due to items'\nmulti-faceted characteristics and dynamic item relevance in the user context.\nTo address these issues, we propose Fuxi-MME, a framework that integrates a\nmulti-embedding strategy with a Mixture-of-Experts (MoE) architecture.\nSpecifically, to efficiently capture diverse item characteristics in a\ndecoupled manner, we decompose the conventional single embedding matrix into\nseveral lower-dimensional embedding matrices. Additionally, by substituting\nrelevant parameters in the Fuxi Block with an MoE layer, our model achieves\nadaptive and specialized transformation of the enriched representations.\nEmpirical results on public datasets show that our proposed framework\noutperforms several competitive baselines."}
{"id": "2510.25402", "categories": ["cs.IR", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.25402", "abs": "https://arxiv.org/abs/2510.25402", "authors": ["Yuqian Chai", "Chaochao Wang", "Weilei Wang"], "title": "Towards Automated Quality Assurance of Patent Specifications: A Multi-Dimensional LLM Framework", "comment": null, "summary": "Despite the surge in patent applications and emergence of AI drafting tools,\nsystematic evaluation of patent content quality has received limited research\nattention. To address this gap, We propose to evaluate patents using regulatory\ncompliance, technical coherence, and figure-reference consistency detection\nmodules, and then generate improvement suggestions via an integration module.\nThe framework is validated on a comprehensive dataset comprising 80\nhuman-authored and 80 AI-generated patents from two patent drafting tools.\nExperimental results show balanced accuracies of 99.74\\%, 82.12\\%, and 91.2\\%\nrespectively across the three detection modules when validated against expert\nannotations. Additional analysis was conducted to examine defect distributions\nacross patent sections, technical domains, and authoring sources. Section-based\nanalysis indicates that figure-text consistency and technical detail precision\nrequire particular attention. Mechanical Engineering and Construction show more\nclaim-specification inconsistencies due to complex technical documentation\nrequirements. AI-generated patents show a significant gap compared to\nhuman-authored ones. While human-authored patents primarily contain\nsurface-level errors like typos, AI-generated patents exhibit more structural\ndefects in figure-text alignment and cross-references."}
{"id": "2510.24769", "categories": ["cs.MM", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.24769", "abs": "https://arxiv.org/abs/2510.24769", "authors": ["Mojtaba Mozhganfar", "Pooya Jamshidi", "Seyyed Ali Aghamiri", "Mohsen Ghasemi", "Mahdi Dolati", "Farzad Tashtarian", "Ahmad Khonsari", "Christian Timmerer"], "title": "YTLive: A Dataset of Real-World YouTube Live Streaming Sessions", "comment": null, "summary": "Live streaming plays a major role in today's digital platforms, supporting\nentertainment, education, social media, etc. However, research in this field is\nlimited by the lack of large, publicly available datasets that capture\nreal-time viewer behavior at scale. To address this gap, we introduce YTLive, a\npublic dataset focused on YouTube Live. Collected through the YouTube\nResearcher Program over May and June 2024, YTLive includes more than 507000\nrecords from 12156 live streams, tracking concurrent viewer counts at\nfive-minute intervals along with precise broadcast durations. We describe the\ndataset design and collection process and present an initial analysis of\ntemporal viewing patterns. Results show that viewer counts are higher and more\nstable on weekends, especially during afternoon hours. Shorter streams attract\nlarger and more consistent audiences, while longer streams tend to grow slowly\nand exhibit greater variability. These insights have direct implications for\nadaptive streaming, resource allocation, and Quality of Experience (QoE)\nmodeling. YTLive offers a timely, open resource to support reproducible\nresearch and system-level innovation in live streaming. The dataset is publicly\navailable at github."}
{"id": "2510.24954", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.24954", "abs": "https://arxiv.org/abs/2510.24954", "authors": ["Aaron Bernstein", "Henry Fleischmann", "George Z. Li", "Bernhard Haeupler", "Maximilian Probst Gutenberg", "Gary Hoppenworth", "Seth Pettie", "Thatchaphol Saranurak", "Yonggang Jiang", "Leon Schiller"], "title": "Reviving Thorup's Shortcut Conjecture", "comment": null, "summary": "We aim to revive Thorup's conjecture [Thorup, WG'92] on the existence of\nreachability shortcuts with ideal size-diameter tradeoffs. Thorup originally\nasked whether, given any graph $G=(V,E)$ with $m$ edges, we can add\n$m^{1+o(1)}$ ``shortcut'' edges $E_+$ from the transitive closure $E^*$ of $G$\nso that $\\text{dist}_{G_+}(u,v) \\leq m^{o(1)}$ for all $(u,v)\\in E^*$, where\n$G_+=(V,E\\cup E_+)$. The conjecture was refuted by Hesse [Hesse, SODA'03],\nfollowed by significant efforts in the last few years to optimize the lower\nbounds.\n  In this paper we observe that although Hesse refuted the letter of Thorup's\nconjecture, his work~[Hesse, SODA'03] -- and all followup work -- does not\nrefute the spirit of the conjecture, which should allow $G_+$ to contain both\nnew (shortcut) edges and new Steiner vertices. Our results are as follows.\n  (1) On the positive side, we present explicit attacks that break all known\nshortcut lower bounds when Steiner vertices are allowed.\n  (2) On the negative side, we rule out ideal $m^{1+o(1)}$-size,\n$m^{o(1)}$-diameter shortcuts whose ``thickness'' is $t=o(\\log n/\\log \\log n)$,\nmeaning no path can contain $t$ consecutive Steiner vertices.\n  (3) We propose a candidate hard instance as the next step toward resolving\nthe revised version of Thorup's conjecture.\n  Finally, we show promising implications. Almost-optimal parallel algorithms\nfor computing a generalization of the shortcut that approximately preserves\ndistances or flows imply almost-optimal parallel algorithms with $m^{o(1)}$\ndepth for exact shortcut paths and exact maximum flow. The state-of-the-art\nalgorithms have much worse depth of $n^{1/2+o(1)}$ [Rozho\\v{n}, Haeupler,\nMartinsson, STOC'23] and $m^{1+o(1)}$ [Chen, Kyng, Liu, FOCS'22], respectively."}
{"id": "2510.24761", "categories": ["cs.DB", "cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24761", "abs": "https://arxiv.org/abs/2510.24761", "authors": ["Anirudh Ganesh", "Nitin Sood"], "title": "ODataX: A Progressive Evolution of the Open Data Protocol", "comment": null, "summary": "The Open Data Protocol (OData) provides a standardized approach for building\nand consuming RESTful APIs with rich query capabilities. Despite its power and\nmaturity, OData adoption remains confined primarily to enterprise environments,\nparticularly within Microsoft and SAP ecosystems. This paper analyzes the key\nbarriers preventing wider OData adoption and introduces ODataX, an evolved\nversion of the protocol designed to address these limitations. ODataX maintains\nbackward compatibility with OData v4 while introducing progressive complexity\ndisclosure through simplified query syntax, built-in performance guardrails via\nquery cost estimation, and enhanced caching mechanisms. This work aims to\nbridge the gap between enterprise-grade query standardization and the\nsimplicity demanded by modern web development practices."}
{"id": "2510.24763", "categories": ["cs.IT", "cs.AI", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.24763", "abs": "https://arxiv.org/abs/2510.24763", "authors": ["Tingting Huang", "Jundong Chen", "Huanqiang Zeng", "Guofa Cai", "Georges Kaddoum"], "title": "Dual-Domain Deep Learning-Assisted NOMA-CSK Systems for Secure and Efficient Vehicular Communications", "comment": null, "summary": "Ensuring secure and efficient multi-user (MU) transmission is critical for\nvehicular communication systems. Chaos-based modulation schemes have garnered\nconsiderable interest due to their benefits in physical layer security.\nHowever, most existing MU chaotic communication systems, particularly those\nbased on non-coherent detection, suffer from low spectral efficiency due to\nreference signal transmission, and limited user connectivity under orthogonal\nmultiple access (OMA). While non-orthogonal schemes, such as sparse code\nmultiple access (SCMA)-based DCSK, have been explored, they face high\ncomputational complexity and inflexible scalability due to their fixed codebook\ndesigns. This paper proposes a deep learning-assisted power domain\nnon-orthogonal multiple access chaos shift keying (DL-NOMA-CSK) system for\nvehicular communications. A deep neural network (DNN)-based demodulator is\ndesigned to learn intrinsic chaotic signal characteristics during offline\ntraining, thereby eliminating the need for chaotic synchronization or reference\nsignal transmission. The demodulator employs a dual-domain feature extraction\narchitecture that jointly processes the time-domain and frequency-domain\ninformation of chaotic signals, enhancing feature learning under dynamic\nchannels. The DNN is integrated into the successive interference cancellation\n(SIC) framework to mitigate error propagation issues. Theoretical analysis and\nextensive simulations demonstrate that the proposed system achieves superior\nperformance in terms of spectral efficiency (SE), energy efficiency (EE), bit\nerror rate (BER), security, and robustness, while maintaining lower\ncomputational complexity compared to traditional MU-DCSK and existing DL-aided\nschemes. These advantages validate its practical viability for secure vehicular\ncommunications."}
{"id": "2510.24872", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.24872", "abs": "https://arxiv.org/abs/2510.24872", "authors": ["Ayelet Amster", "Lioz Akirav", "Rica Gonen", "Erel Segal-Halevi"], "title": "What Are People's Actual Utility Functions in Budget Aggregation?", "comment": null, "summary": "While participatory budgeting and budget-aggregation mechanisms require\nassumptions about how voters evaluate non-ideal budget allocations, little\nempirical evidence exists to validate which utility models accurately capture\nhuman preferences. We conducted structured polls with human participants to\ntest whether real people's preferences conform to commonly assumed utility\nfunctions such as $\\ell_1$, $\\ell_2$ and Leontief. Our results suggest that\nthese models may have limited explanatory power for actual behavior: most\nparticipants showed inconsistent patterns across different metric comparisons,\nand standard assumptions of project symmetry and sign symmetry -- core features\nof common distance-based metrics -- received little empirical support. However,\nwe find encouraging evidence for more fundamental preference structures: a\nlarge majority of participants showed consistency with star-shaped preferences,\nas well as with peak-linear utility functions, where utility changes\nproportionally with distance from the ideal budget. These findings have\nimportant implications for designers of budget aggregation mechanisms. While\ntheoretical results demonstrate impossibility results for standard distance\nmetrics regarding truthfulness, Pareto-efficiency, and proportionality, our\nevidence suggests alternative modeling approaches may be warranted. More\nbroadly, this work introduces a systematic methodology to empirically test the\nutility function assumptions that underpin budget aggregation theories, paving\nthe way for more robust and realistic mechanism design."}
{"id": "2510.25428", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25428", "abs": "https://arxiv.org/abs/2510.25428", "authors": ["Thang-Long Nguyen-Ho", "Minh-Khoi Pham", "Hoang-Bao Le"], "title": "Alibaba International E-commerce Product Search Competition DcuRAGONs Team Technical Report", "comment": "Alibaba International E-commerce Product Search Competition @ CIKM\n  2025", "summary": "This report details our methodology and results developed for the\nMultilingual E-commerce Search Competition. The problem aims to recognize\nrelevance between user queries versus product items in a multilingual context\nand improve recommendation performance on e-commerce platforms. Utilizing Large\nLanguage Models (LLMs) and their capabilities in other tasks, our data-centric\nmethod achieved the highest score compared to other solutions during the\ncompetition. Final leaderboard is publised at\nhttps://alibaba-international-cikm2025.github.io. The source code for our\nproject is published at https://github.com/nhtlongcs/e-commerce-product-search."}
{"id": "2510.25225", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2510.25225", "abs": "https://arxiv.org/abs/2510.25225", "authors": ["Shota Nakada", "Kazuhiro Saito", "Yuchi Ishikawa", "Hokuto Munakata", "Tatsuya Komatsu", "Masayoshi Kondo"], "title": "Hallucination Localization in Video Captioning", "comment": "under review", "summary": "We propose a novel task, hallucination localization in video captioning,\nwhich aims to identify hallucinations in video captions at the span level (i.e.\nindividual words or phrases). This allows for a more detailed analysis of\nhallucinations compared to existing sentence-level hallucination detection\ntask. To establish a benchmark for hallucination localization, we construct\nHLVC-Dataset, a carefully curated dataset created by manually annotating 1,167\nvideo-caption pairs from VideoLLM-generated captions. We further implement a\nVideoLLM-based baseline method and conduct quantitative and qualitative\nevaluations to benchmark current performance on hallucination localization."}
{"id": "2510.25043", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.25043", "abs": "https://arxiv.org/abs/2510.25043", "authors": ["Karthekeyan Chandrasekaran", "Chandra Chekuri", "Weihang Wang", "Weihao Zhu"], "title": "Hedgegraph Polymatroids", "comment": null, "summary": "Graphs and hypergraphs combine expressive modeling power with algorithmic\nefficiency for a wide range of applications. Hedgegraphs generalize hypergraphs\nfurther by grouping hyperedges under a color/hedge. This allows hedgegraphs to\nmodel dependencies between hyperedges and leads to several applications.\nHowever, it poses algorithmic challenges. In particular, the cut function is\nnot submodular, which has been a barrier to algorithms for connectivity. In\nthis work, we introduce two alternative partition-based measures of\nconnectivity in hedgegraphs and study their structural and algorithmic aspects.\nInstead of the cut function, we investigate a polymatroid associated with\nhedgegraphs. The polymatroidal lens leads to new tractability results as well\nas insightful generalizations of classical results on graphs and hypergraphs."}
{"id": "2510.25017", "categories": ["cs.DB", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25017", "abs": "https://arxiv.org/abs/2510.25017", "authors": ["Qi Lin", "Zhenyu Zhang", "Viraj Thakkar", "Zhenjie Sun", "Mai Zheng", "Zhichao Cao"], "title": "StorageXTuner: An LLM Agent-Driven Automatic Tuning Framework for Heterogeneous Storage Systems", "comment": "ArXiv version; Affiliations: Arizona State University (Lin, Zhang,\n  Thakkar, Sun, Cao) and Iowa State University (Zheng)", "summary": "Automatically configuring storage systems is hard: parameter spaces are large\nand conditions vary across workloads, deployments, and versions. Heuristic and\nML tuners are often system specific, require manual glue, and degrade under\nchanges. Recent LLM-based approaches help but usually treat tuning as a\nsingle-shot, system-specific task, which limits cross-system reuse, constrains\nexploration, and weakens validation. We present StorageXTuner, an LLM\nagent-driven auto-tuning framework for heterogeneous storage engines.\nStorageXTuner separates concerns across four agents - Executor (sandboxed\nbenchmarking), Extractor (performance digest), Searcher (insight-guided\nconfiguration exploration), and Reflector (insight generation and management).\nThe design couples an insight-driven tree search with layered memory that\npromotes empirically validated insights and employs lightweight checkers to\nguard against unsafe actions. We implement a prototype and evaluate it on\nRocksDB, LevelDB, CacheLib, and MySQL InnoDB with YCSB, MixGraph, and TPC-H/C.\nRelative to out-of-the-box settings and to ELMo-Tune, StorageXTuner reaches up\nto 575% and 111% higher throughput, reduces p99 latency by as much as 88% and\n56%, and converges with fewer trials."}
{"id": "2510.25002", "categories": ["cs.IT", "cs.CV", "cs.MM", "eess.IV", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.25002", "abs": "https://arxiv.org/abs/2510.25002", "authors": ["Zhenyu Liu", "Yi Ma", "Rahim Tafazolli", "Zhi Ding"], "title": "Resi-VidTok: An Efficient and Decomposed Progressive Tokenization Framework for Ultra-Low-Rate and Lightweight Video Transmission", "comment": null, "summary": "Real-time transmission of video over wireless networks remains highly\nchallenging, even with advanced deep models, particularly under severe channel\nconditions such as limited bandwidth and weak connectivity. In this paper, we\npropose Resi-VidTok, a Resilient Tokenization-Enabled framework designed for\nultra-low-rate and lightweight video transmission that delivers strong\nrobustness while preserving perceptual and semantic fidelity on commodity\ndigital hardware. By reorganizing spatio--temporal content into a discrete,\nimportance-ordered token stream composed of key tokens and refinement tokens,\nResi-VidTok enables progressive encoding, prefix-decodable reconstruction, and\ngraceful quality degradation under constrained channels. A key contribution is\na resilient 1D tokenization pipeline for video that integrates differential\ntemporal token coding, explicitly supporting reliable recovery from incomplete\ntoken sets using a single shared framewise decoder--without auxiliary temporal\nextractors or heavy generative models. Furthermore, stride-controlled frame\nsparsification combined with a lightweight decoder-side interpolator reduces\ntransmission load while maintaining motion continuity. Finally, a\nchannel-adaptive source--channel coding and modulation scheme dynamically\nallocates rate and protection according to token importance and channel\ncondition, yielding stable quality across adverse SNRs. Evaluation results\nindicate robust visual and semantic consistency at channel bandwidth ratios\n(CBR) as low as 0.0004 and real-time reconstruction at over 30 fps,\ndemonstrating the practicality of Resi-VidTok for energy-efficient,\nlatency-sensitive, and reliability-critical wireless applications."}
{"id": "2510.24906", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24906", "abs": "https://arxiv.org/abs/2510.24906", "authors": ["Mikołaj Czarnecki", "Michał Korniak", "Oskar Skibski", "Piotr Skowron"], "title": "Fair Indivisible Payoffs through Shapley Value", "comment": null, "summary": "We consider the problem of payoff division in indivisible coalitional games,\nwhere the value of the grand coalition is a natural number. This number\nrepresents a certain quantity of indivisible objects, such as parliamentary\nseats, kidney exchanges, or top features contributing to the outcome of a\nmachine learning model. The goal of this paper is to propose a fair method for\ndividing these objects among players. To achieve this, we define the\nindivisible Shapley value and study its properties. We demonstrate our proposed\ntechnique using three case studies, in particular, we use it to identify key\nregions of an image in the context of an image classification task."}
{"id": "2510.25488", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.25488", "abs": "https://arxiv.org/abs/2510.25488", "authors": ["Yiteng Tu", "Weihang Su", "Yujia Zhou", "Yiqun Liu", "Fen Lin", "Qin Liu", "Qingyao Ai"], "title": "Generalized Pseudo-Relevance Feedback", "comment": null, "summary": "Query rewriting is a fundamental technique in information retrieval (IR). It\ntypically employs the retrieval result as relevance feedback to refine the\nquery and thereby addresses the vocabulary mismatch between user queries and\nrelevant documents. Traditional pseudo-relevance feedback (PRF) and its\nvector-based extension (VPRF) improve retrieval performance by leveraging\ntop-retrieved documents as relevance feedback. However, they are constructed\nbased on two major hypotheses: the relevance assumption (top documents are\nrelevant) and the model assumption (rewriting methods need to be designed\nspecifically for particular model architectures). While recent large language\nmodels (LLMs)-based generative relevance feedback (GRF) enables model-free\nquery reformulation, it either suffers from severe LLM hallucination or, again,\nrelies on the relevance assumption to guarantee the effectiveness of rewriting\nquality. To overcome these limitations, we introduce an assumption-relaxed\nframework: \\textit{Generalized Pseudo Relevance Feedback} (GPRF), which\nperforms model-free, natural language rewriting based on retrieved documents,\nnot only eliminating the model assumption but also reducing dependence on the\nrelevance assumption. Specifically, we design a utility-oriented training\npipeline with reinforcement learning to ensure robustness against noisy\nfeedback. Extensive experiments across multiple benchmarks and retrievers\ndemonstrate that GPRF consistently outperforms strong baselines, establishing\nit as an effective and generalizable framework for query rewriting."}
{"id": "2510.25600", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2510.25600", "abs": "https://arxiv.org/abs/2510.25600", "authors": ["Zhonghua Jiang", "Kunxi Li", "Yiyun Zhou", "Sihao Liu", "Zhaode Wang", "Chengfei lv", "Shengyu Zhang"], "title": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse Attention for Vision-Language Large Models", "comment": null, "summary": "Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation."}
{"id": "2510.25664", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.25664", "abs": "https://arxiv.org/abs/2510.25664", "authors": ["Kristóf Bérczi", "Karthekeyan Chandrasekaran", "Tamás Király", "Daniel P. Szabo"], "title": "$\\{s,t\\}$-Separating Principal Partition Sequence of Submodular Functions", "comment": null, "summary": "Narayanan and Fujishige showed the existence of the principal partition\nsequence of a submodular function, a structure with numerous applications in\nareas such as clustering, fast algorithms, and approximation algorithms. In\nthis work, motivated by two applications, we develop a theory of\n$\\{s,t\\}$-separating principal partition sequence of a submodular function. We\ndefine this sequence, show its existence, and design a polynomial-time\nalgorithm to construct it. We show two applications: (1) approximation\nalgorithm for the $\\{s,t\\}$-separating submodular $k$-partitioning problem for\nmonotone and posimodular functions and (2) polynomial-time algorithm for the\nhypergraph orientation problem of finding an orientation that simultaneously\nhas strong connectivity at least $k$ and $(s,t)$-connectivity at least $\\ell$."}
{"id": "2510.25143", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.25143", "abs": "https://arxiv.org/abs/2510.25143", "authors": ["Mingze Xia", "Yuxiao Li", "Pu Jiao", "Bei Wang", "Xin Liang", "Hanqi Guo"], "title": "Time-varying Vector Field Compression with Preserved Critical Point Trajectories", "comment": null, "summary": "Scientific simulations and observations are producing vast amounts of\ntime-varying vector field data, making it hard to store them for archival\npurposes and transmit them for analysis. Lossy compression is considered a\npromising approach to reducing these data because lossless compression yields\nlow compression ratios that barely mitigate the problem. However, directly\napplying existing lossy compression methods to timevarying vector fields may\nintroduce undesired distortions in critical-point trajectories, a crucial\nfeature that encodes key properties of the vector field. In this work, we\npropose an efficient lossy compression framework that exactly preserves all\ncritical-point trajectories in time-varying vector fields. Our contributions\nare threefold. First, we extend the theory for preserving critical points in\nspace to preserving critical-point trajectories in space-time, and develop a\ncompression framework to realize the functionality. Second, we propose a\nsemi-Lagrange predictor to exploit the spatiotemporal correlations in\nadvectiondominated regions, and combine it with the traditional Lorenzo\npredictor for improved compression efficiency. Third, we evaluate our method\nagainst state-of-the-art lossy and lossless compressors using four real-world\nscientific datasets. Experimental results demonstrate that the proposed method\ndelivers up to 124.48X compression ratios while effectively preserving all\ncritical-point trajectories. This compression ratio is up to 56.07X higher than\nthat of the best lossless compressors, and none of the existing lossy\ncompressors can preserve all critical-point trajectories at similar compression\nratios."}
{"id": "2510.25181", "categories": ["cs.IT", "cs.AI", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.25181", "abs": "https://arxiv.org/abs/2510.25181", "authors": ["Yixiang Zhou", "Tong Wu", "Meixia Tao", "Jianhua Mo"], "title": "Fed-PELAD: Communication-Efficient Federated Learning for Massive MIMO CSI Feedback with Personalized Encoders and a LoRA-Adapted Shared Decoder", "comment": null, "summary": "This paper addresses the critical challenges of communication overhead, data\nheterogeneity, and privacy in deep learning for channel state information (CSI)\nfeedback in massive MIMO systems. To this end, we propose Fed-PELAD, a novel\nfederated learning framework that incorporates personalized encoders and a\nLoRA-adapted shared decoder. Specifically, personalized encoders are trained\nlocally on each user equipment (UE) to capture device-specific channel\ncharacteristics, while a shared decoder is updated globally via the\ncoordination of the base station (BS) by using Low-Rank Adaptation (LoRA). This\ndesign ensures that only compact LoRA adapter parameters instead of full model\nupdates are transmitted for aggregation. To further enhance convergence\nstability, we introduce an alternating freezing strategy with calibrated\nlearning-rate ratio during LoRA aggregation. Extensive simulations on\n3GPP-standard channel models demonstrate that Fed-PELAD requires only 42.97\\%\nof the uplink communication cost compared to conventional methods while\nachieving a performance gain of 1.2 dB in CSI feedback accuracy under\nheterogeneous conditions."}
{"id": "2510.25080", "categories": ["cs.GT", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25080", "abs": "https://arxiv.org/abs/2510.25080", "authors": ["Will Wolf"], "title": "Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response Games", "comment": "24 pages, 7 figures", "summary": "Card games are widely used to study sequential decision-making under\nuncertainty, with real-world analogues in negotiation, finance, and\ncybersecurity. Typically, these games fall into three categories based on the\nflow of control: strictly-sequential (where players alternate single actions),\ndeterministic-response (where some actions trigger a fixed outcome), and\nunbounded reciprocal-response (where alternating counterplays are permitted). A\nless-explored but strategically rich structure exists: the bounded one-sided\nresponse. This dynamic occurs when a player's action briefly transfers control\nto the opponent, who must satisfy a fixed condition through one or more\nsequential moves before the turn resolves. We term games featuring this\nmechanism Bounded One-Sided Response Games (BORGs).\n  We introduce a modified version of Monopoly Deal as a benchmark environment\nthat specifically isolates the BORG dynamic, where a Rent action forces the\nopponent to sequentially choose payment assets. We demonstrate that the\ngold-standard algorithm, Counterfactual Regret Minimization (CFR), successfully\nconverges on effective strategies for this domain without requiring novel\nalgorithmic extensions. To support efficient, reproducible experimentation, we\npresent a lightweight, full-stack research platform that unifies the\nenvironment, a parallelized CFR runtime, and a human-playable web interface,\nall runnable on a single workstation. This system provides a practical\nfoundation for exploring state representation and policy learning in bounded\none-sided response settings.\n  The trained CFR agent and source code are available at\nhttps://monopolydeal.ai."}
{"id": "2510.25622", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.25622", "abs": "https://arxiv.org/abs/2510.25622", "authors": ["Yi Xu", "Moyu Zhang", "Chaofan Fan", "Jinxin Hu", "Xiaochen Li", "Yu Zhang", "Xiaoyi Zeng", "Jing Zhang"], "title": "MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for Semantic IDs Learning in Recommendation", "comment": null, "summary": "Industrial recommender systems rely on unique Item Identifiers (ItemIDs).\nHowever, this method struggles with scalability and generalization in large,\ndynamic datasets that have sparse long-tail data.Content-based Semantic IDs\n(SIDs) address this by sharing knowledge through content quantization. However,\nby ignoring dynamic behavioral properties, purely content-based SIDs have\nlimited expressive power. Existing methods attempt to incorporate behavioral\ninformation but overlook a critical distinction: unlike relatively uniform\ncontent features, user-item interactions are highly skewed and diverse,\ncreating a vast information gap in quality and quantity between popular and\nlong-tail items. This oversight leads to two critical limitations: (1) Noise\nCorruption: Indiscriminate behavior-content alignment allows collaborative\nnoise from long-tail items to corrupt their content representations, leading to\nthe loss of critical multimodal information. (2)Signal Obscurity: The\nequal-weighting scheme for SIDs fails to reflect the varying importance of\ndifferent behavioral signals, making it difficult for downstream tasks to\ndistinguish important SIDs from uninformative ones. To tackle these issues, we\npropose a mixture-of-quantization framework, MMQ-v2, to adaptively Align,\nDenoise, and Amplify multimodal information from content and behavior\nmodalities for semantic IDs learning. The semantic IDs generated by this\nframework named ADA-SID. It introduces two innovations: an adaptive\nbehavior-content alignment that is aware of information richness to shield\nrepresentations from noise, and a dynamic behavioral router to amplify critical\nsignals by applying different weights to SIDs. Extensive experiments on public\nand large-scale industrial datasets demonstrate ADA-SID's significant\nsuperiority in both generative and discriminative recommendation tasks."}
{"id": "2510.25002", "categories": ["cs.IT", "cs.CV", "cs.MM", "eess.IV", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.25002", "abs": "https://arxiv.org/abs/2510.25002", "authors": ["Zhenyu Liu", "Yi Ma", "Rahim Tafazolli", "Zhi Ding"], "title": "Resi-VidTok: An Efficient and Decomposed Progressive Tokenization Framework for Ultra-Low-Rate and Lightweight Video Transmission", "comment": null, "summary": "Real-time transmission of video over wireless networks remains highly\nchallenging, even with advanced deep models, particularly under severe channel\nconditions such as limited bandwidth and weak connectivity. In this paper, we\npropose Resi-VidTok, a Resilient Tokenization-Enabled framework designed for\nultra-low-rate and lightweight video transmission that delivers strong\nrobustness while preserving perceptual and semantic fidelity on commodity\ndigital hardware. By reorganizing spatio--temporal content into a discrete,\nimportance-ordered token stream composed of key tokens and refinement tokens,\nResi-VidTok enables progressive encoding, prefix-decodable reconstruction, and\ngraceful quality degradation under constrained channels. A key contribution is\na resilient 1D tokenization pipeline for video that integrates differential\ntemporal token coding, explicitly supporting reliable recovery from incomplete\ntoken sets using a single shared framewise decoder--without auxiliary temporal\nextractors or heavy generative models. Furthermore, stride-controlled frame\nsparsification combined with a lightweight decoder-side interpolator reduces\ntransmission load while maintaining motion continuity. Finally, a\nchannel-adaptive source--channel coding and modulation scheme dynamically\nallocates rate and protection according to token importance and channel\ncondition, yielding stable quality across adverse SNRs. Evaluation results\nindicate robust visual and semantic consistency at channel bandwidth ratios\n(CBR) as low as 0.0004 and real-time reconstruction at over 30 fps,\ndemonstrating the practicality of Resi-VidTok for energy-efficient,\nlatency-sensitive, and reliability-critical wireless applications."}
{"id": "2510.25401", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.25401", "abs": "https://arxiv.org/abs/2510.25401", "authors": ["Jiahao Lou", "Quan Yu", "Shufeng Gong", "Song Yu", "Yanfeng Zhang", "Ge Yu"], "title": "DGAI: Decoupled On-Disk Graph-Based ANN Index for Efficient Updates and Queries", "comment": "12 pages", "summary": "On-disk graph-based indexes are widely used in approximate nearest neighbor\n(ANN) search systems for large-scale, high-dimensional vectors. However,\ntraditional coupled storage methods, which store vectors within the index, are\ninefficient for index updates. Coupled storage incurs excessive redundant\nvector reads and writes when updating the graph topology, leading to\nsignificant invalid I/O. To address this issue, we propose a decoupled storage\narchitecture. While a decoupled architecture reduces query performance. To\novercome this limitation, we design two tailored strategies: (i) a three-stage\nquery mechanism that leverages multiple PQ compressed vectors to filter invalid\nI/O and computations, and (ii) an incremental page-level topological reordering\nstrategy that incrementally inserts new nodes into pages containing their most\nsimilar neighbors to mitigate read amplification. Together, these techniques\nsubstantially reduce both I/O and computational overhead during ANN search.\nExperimental results show that the decoupled architecture improves update speed\nby 10.05x for insertions and 6.89x for deletions, while the three-stage query\nand incremental reordering enhance query efficiency by 2.66x compared to the\ntraditional coupled architecture."}
{"id": "2510.25266", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.25266", "abs": "https://arxiv.org/abs/2510.25266", "authors": ["Ziwei Liu", "Wen Chen", "Zhendong Li", "Qiong Wu"], "title": "Joint Spatial Registration and Resource Allocation for Transmissive RIS Enabled Cooperative ISCC Networks", "comment": null, "summary": "In this paper, we propose a novel transmissive reconfigurable intelligent\nsurface (TRIS) transceiver-driven cooperative integrated sensing, computing,\nand communication (ISCC) network to meet the requirement for a diverse network\nwith low energy consumption. The cooperative base stations (BSs) are equipped\nwith TRIS transceivers to accomplish sensing data acquisition, communication\noffloading, and computation in a time slot. In order to obtain higher\ncooperation gain, we utilize a signal-level spatial registration algorithm,\nwhich is realized by adjusting the beamwidth. Meanwhile, for more efficient\noffloading of the computational task, multistream communication is considered,\nand rank-$N$ constraints are introduced, which are handled using an iterative\nrank minimization (IRM) scheme. We construct an optimization problem with the\nobjective function of minimizing the total energy consumption of the network to\njointly optimize the beamforming matrix, time slot allocation, sensing data\nallocation and sensing beam scheduling variables. Due to the coupling of the\nvariables, the proposed problem is a non-convex optimization problem, which we\ndecouple and solve using a block coordinate descent (BCD) scheme. Finally,\nnumerical simulation results confirm the superiority of the proposed scheme in\nimproving the overall network performance and reducing the total energy\nconsumption of the network."}
{"id": "2510.25144", "categories": ["cs.GT", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.25144", "abs": "https://arxiv.org/abs/2510.25144", "authors": ["Kaya Alpturer", "Kushal Babel", "Aditya Saraf"], "title": "Timing Games in Responsive Consensus Protocols", "comment": "36 pages, 6 figures", "summary": "Optimistic responsiveness -- the ability of a consensus protocol to operate\nat the speed of the network -- is widely used in consensus protocol design to\noptimize latency and throughput. However, blockchain applications incentivize\nvalidators to play timing games by strategically delaying their proposals,\nsince increased block time correlates with greater rewards. Consequently, it\nmay appear that responsiveness (even under optimistic conditions) is impossible\nin blockchain protocols. In this work, we develop a model of timing games in\nresponsive consensus protocols and find a prisoner's dilemma structure, where\ncooperation (proposing promptly) is in the validators' best interest, but\nindividual incentives encourage validators to delay proposals selfishly. To\nattain desirable equilibria, we introduce dynamic block rewards that decrease\nwith round time to explicitly incentivize faster proposals. Delays are measured\nthrough a voting mechanism, where other validators vote on the current leader's\nround time. By carefully setting the protocol parameters, the voting mechanism\nallows validators to coordinate and reach the cooperative equilibrium,\nbenefiting all through a higher rate-of-reward. Thus, instead of responsiveness\nbeing an unattainable property due to timing games, we show that responsiveness\nitself can promote faster block proposals. One consequence of moving from a\nstatic to dynamic block reward is that validator utilities become more\nsensitive to latency, worsening the gap between the best- and worst-connected\nvalidators. Our analysis shows, however, that this effect is minor in both\ntheoretical latency models and simulations based on real-world networks."}
{"id": "2510.25718", "categories": ["cs.IR", "cs.DL"], "pdf": "https://arxiv.org/pdf/2510.25718", "abs": "https://arxiv.org/abs/2510.25718", "authors": ["Jamie Mahowald", "Benjamin Charles Germain Lee"], "title": "Retrieval-Augmented Search for Large-Scale Map Collections with ColPali", "comment": "5 pages, 5 figures", "summary": "Multimodal approaches have shown great promise for searching and navigating\ndigital collections held by libraries, archives, and museums. In this paper, we\nintroduce map-RAS: a retrieval-augmented search system for historic maps. In\naddition to introducing our framework, we detail our publicly-hosted demo for\nsearching 101,233 map images held by the Library of Congress. With our system,\nusers can multimodally query the map collection via ColPali, summarize search\nresults using Llama 3.2, and upload their own collections to perform\ninter-collection search. We articulate potential use cases for archivists,\ncurators, and end-users, as well as future work with our system in both machine\nlearning and the digital humanities. Our demo can be viewed at:\nhttp://www.mapras.com."}
{"id": "2510.25684", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.25684", "abs": "https://arxiv.org/abs/2510.25684", "authors": ["Yujun He", "Hangdong Zhao", "Simon Frisk", "Yifei Yang", "Kevin Kristensen", "Paraschos Koutris", "Xiangyao Yu"], "title": "One Join Order Does Not Fit All: Reducing Intermediate Results with Per-Split Query Plans", "comment": null, "summary": "Minimizing intermediate results is critical for efficient multi-join query\nprocessing. Although the seminal Yannakakis algorithm offers strong guarantees\nfor acyclic queries, cyclic queries remain an open challenge. In this paper, we\npropose SplitJoin, a framework that introduces split as a first-class query\noperator. By partitioning input tables into heavy and light parts, SplitJoin\nallows different data partitions to use distinct query plans, with the goal of\nreducing intermediate sizes using existing binary join engines. We\nsystematically explore the design space for split-based optimizations,\nincluding threshold selection, split strategies, and join ordering after\nsplits. Implemented as a front-end to DuckDB and Umbra, SplitJoin achieves\nsubstantial improvements: on DuckDB, SplitJoin completes 43 social network\nqueries (vs. 29 natively), achieving 2.1x faster runtime and 7.9x smaller\nintermediates on average (up to 13.6x and 74x, respectively); on Umbra, it\ncompletes 45 queries (vs. 35), achieving 1.3x speedups and 1.2x smaller\nintermediates on average (up to 6.1x and 2.1x, respectively)."}
{"id": "2510.25305", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.25305", "abs": "https://arxiv.org/abs/2510.25305", "authors": ["Yitzchak Grunbaum", "Eitan Yaakobi"], "title": "General Coverage Models: Structure, Monotonicity, and Shotgun Sequencing", "comment": "13 pages", "summary": "We study coverage processes in which each draw reveals a subset of $[n]$, and\nthe goal is to determine the expected number of draws until all items are seen\nat least once. A classical example is the Coupon Collector's Problem, where\neach draw reveals exactly one item. Motivated by shotgun DNA sequencing, we\nintroduce a model where each draw is a contiguous window of fixed length, in\nboth cyclic and non-cyclic variants. We develop a unifying combinatorial tool\nthat shifts the task of finding coverage time from probability, to a counting\nproblem over families of subsets of $[n]$ that together contain all items,\nenabling exact calculation. Using this result, we obtain exact expressions for\nthe window models. We then leverage past results on a continuous analogue of\nthe cyclic window model to analyze the asymptotic behavior of both models. We\nfurther study what we call uniform $\\ell$-regular models, where every draw has\nsize $\\ell$ and every item appears in the same number of admissible draws. We\ncompare these to the batch sampling model, in which all $\\ell$-subsets are\ndrawn uniformly at random and present upper and lower bounds, which were also\nobtained independently by Berend and Sher. We conjecture, and prove for special\ncases, that this model maximizes the coverage time among all uniform\n$\\ell$-regular models. Finally, we prove a universal upper bound on the entire\nclass of uniform $\\ell$-regular models, which illuminates the fact that many\nsampling models share the same leading asymptotic order, while potentially\ndiffering significantly in lower-order terms."}
{"id": "2510.25209", "categories": ["cs.GT", "cs.MA", "F.2.2"], "pdf": "https://arxiv.org/pdf/2510.25209", "abs": "https://arxiv.org/abs/2510.25209", "authors": ["Koustav De"], "title": "On Robust Popular Matchings with Tie-Bounded Preferences and Stable Matchings with Two-Sided Ties", "comment": null, "summary": "We are given a bipartite graph $G = \\left( A \\cup B, E \\right)$. In the\none-sided model, every $a \\in A$ (often called agents) ranks its neighbours $z\n\\in N_{a}$ strictly, and no $b \\in B$ has any preference order over its\nneighbours $y \\in N_{b}$, and vertices in $B$ abstain from casting their votes\nto matchings. In the two-sided model with one-sided ties, every $a \\in A$ ranks\nits neighbours $z \\in N_{a}$ strictly, and every $b \\in B$ puts all of its\nneighbours into a single large tie, i.e., $b \\in B$ prefers every $y \\in N_{b}$\nequally. In this two-sided model with one-sided ties, when two matchings\ncompete in a majority election, $b \\in B$ abstains from casting its vote for a\nmatching when both the matchings saturate $b$ or both leave $b$ unsaturated;\nelse $b$ prefers the matching where it is saturated. A popular matching $M$ is\n\\emph{robust} if it remains popular among multiple instances.\n  We have analysed the cases when a robust popular matching exists in the\none-sided model where only one agent alters her preference order among the\ninstances, and we have proposed a polynomial-time algorithm to decide if there\nexists a robust popular matching when instances differ only with respect to the\npreference orders of a single agent.\n  We give a simple characterisation of popular matchings in the two-sided model\nwith one-sided ties. We show that in the two-sided model with one-sided ties,\nif the input instances differ only with respect to the preference orders of a\nsingle agent, there is a polynomial-time algorithm to decide whether there\nexists a robust popular matching. We have been able to decide the stable\nmatching problem in bipartite graphs $G = (A \\cup B, E)$ where \\textit{both}\nsides have weak preferences (ties allowed), with the restriction that every tie\nhas length at most $k$."}
{"id": "2510.24761", "categories": ["cs.DB", "cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24761", "abs": "https://arxiv.org/abs/2510.24761", "authors": ["Anirudh Ganesh", "Nitin Sood"], "title": "ODataX: A Progressive Evolution of the Open Data Protocol", "comment": null, "summary": "The Open Data Protocol (OData) provides a standardized approach for building\nand consuming RESTful APIs with rich query capabilities. Despite its power and\nmaturity, OData adoption remains confined primarily to enterprise environments,\nparticularly within Microsoft and SAP ecosystems. This paper analyzes the key\nbarriers preventing wider OData adoption and introduces ODataX, an evolved\nversion of the protocol designed to address these limitations. ODataX maintains\nbackward compatibility with OData v4 while introducing progressive complexity\ndisclosure through simplified query syntax, built-in performance guardrails via\nquery cost estimation, and enhanced caching mechanisms. This work aims to\nbridge the gap between enterprise-grade query standardization and the\nsimplicity demanded by modern web development practices."}
{"id": "2510.25343", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.25343", "abs": "https://arxiv.org/abs/2510.25343", "authors": ["Hadi Aghaee", "Christian Deppe", "Holger Boche"], "title": "Network Oblivious Transfer via Noisy Broadcast Channels", "comment": null, "summary": "This paper investigates information-theoretic oblivious transfer via a\ndiscrete memoryless broadcast channel with one sender and two receivers. We\nanalyze both non-colluding and colluding honest-but-curious user models and\nestablish general upper bounds on the achievable oblivious transfer capacity\nregion for each case. Two explicit oblivious transfer protocols are proposed.\nThe first ensures correctness and privacy for independent, non-colluding\nreceivers by leveraging the structure of binary erasure broadcast channels. The\nsecond protocol, secure even under receiver collusion, introduces additional\nentropy-sharing and privacy amplification mechanisms to preserve secrecy\ndespite information leakage between users. Our results show that for the\nnon-colluding case, the upper and lower bounds on oblivious transfer capacity\ncoincide, providing a complete characterization of the achievable region. The\nwork provides a unified theoretical framework bridging network information\ntheory and cryptographic security, highlighting the potential of noisy\nbroadcast channels as powerful primitives for multi-user privacy-preserving\ncommunication."}
{"id": "2510.25582", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25582", "abs": "https://arxiv.org/abs/2510.25582", "authors": ["Spyros Angelopoulos", "Bertrand Simon"], "title": "Learning-Augmented Online Bidding in Stochastic Settings", "comment": null, "summary": "Online bidding is a classic optimization problem, with several applications\nin online decision-making, the design of interruptible systems, and the\nanalysis of approximation algorithms. In this work, we study online bidding\nunder learning-augmented settings that incorporate stochasticity, in either the\nprediction oracle or the algorithm itself. In the first part, we study bidding\nunder distributional predictions, and find Pareto-optimal algorithms that offer\nthe best-possible tradeoff between the consistency and the robustness of the\nalgorithm. In the second part, we study the power and limitations of randomized\nbidding algorithms, by presenting upper and lower bounds on the\nconsistency/robustness tradeoffs. Previous works focused predominantly on\noracles that do not leverage stochastic information on the quality of the\nprediction, and deterministic algorithms."}
{"id": "2510.25346", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.25346", "abs": "https://arxiv.org/abs/2510.25346", "authors": ["Chi Qiu", "Wen Chen", "Qingqing Wu", "Fen Hou", "Wanming Hao", "Ruiqi Liu", "Derrick Wing Kwan Ng"], "title": "Joint Beamforming Design and Resource Allocation for IRS-Assisted Full-Duplex Terahertz Systems", "comment": null, "summary": "Intelligent reflecting surface (IRS)-assisted full-duplex (FD) terahertz\n(THz) communication systems have emerged as a promising paradigm to satisfy the\nescalating demand for ultra-high data rates and spectral efficiency in future\nwireless networks. However, the practical deployment of such systems presents\nunique technical challenges, stemming from severe propagation loss,\nfrequency-dependent molecular absorption in the THz band, and the presence of\nstrong residual self-interference (SI) inherent to FD communications. To tackle\nthese issues, this paper proposes a joint resource allocation framework that\naims to maximize the weighted minimum rate among all users, thereby ensuring\nfairness in quality of service. Specifically, the proposed design jointly\noptimizes IRS reflecting phase shifts, uplink/downlink transmit power control,\nsub-band bandwidth allocation, and sub-band assignment, explicitly capturing\nthe unique propagation characteristics of THz channels and the impact of\nresidual SI. To strike an balance between system performance and computational\ncomplexity, two computationally efficient algorithms are developed under\ndistinct spectrum partitioning schemes: one assumes equal sub-band bandwidth\nallocation to facilliate tractable optimization, while the other introduces\nadaptive bandwidth allocation to further enhance spectral utilization and\nsystem flexibility. Simulation results validate the effectiveness of the\nproposed designs and demonstrate that the adopted scheme achieves significant\nspectral efficiency improvements over benchmark schemes."}
{"id": "2510.25389", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.25389", "abs": "https://arxiv.org/abs/2510.25389", "authors": ["Meng Hua", "Haotian Wu", "Deniz Gündüz"], "title": "AirCNN via Reconfigurable Intelligent Surfaces: Architecture Design and Implementation", "comment": "Using wireless hardware to implement neural networks; This work is\n  submitted to IEEE journal for possible publication", "summary": "This paper introduces AirCNN, a novel paradigm for implementing convolutional\nneural networks (CNNs) via over-the-air (OTA) analog computation. By leveraging\nmultiple reconfigurable intelligent surfaces (RISs) and transceiver designs, we\nengineer the ambient wireless propagation environment to emulate the operations\nof a CNN layer. To comprehensively evaluate AirCNN, we consider two types of\nCNNs, namely classic two-dimensional (2D) convolution (Conv2d) and light-weight\nconvolution, i.e., depthwise separable convolution (ConvSD). For Conv2d\nrealization via OTA computation, we propose and analyze two RIS-aided\ntransmission architectures: multiple-input multiple-output (MIMO) and\nmultiple-input single-output (MISO), balancing transmission overhead and\nemulation performance. We jointly optimize all parameters, including the\ntransmitter precoder, receiver combiner, and RIS phase shifts, under practical\nconstraints such as transmit power budget and unit-modulus phase shift\nrequirements. We further extend the framework to ConvSD, which requires\ndistinct transmission strategies for depthwise and pointwise convolutions.\nSimulation results demonstrate that the proposed AirCNN architectures can\nachieve satisfactory classification performance. Notably, Conv2d MISO\nconsistently outperforms Conv2d MIMO across various settings, while for ConvSD,\nMISO is superior only under poor channel conditions. Moreover, employing\nmultiple RISs significantly enhances performance compared to a single RIS,\nespecially in line-of-sight (LoS)-dominated wireless environments."}
{"id": "2510.25578", "categories": ["cs.IT", "math.IT", "94B05, 11T71, 11T23"], "pdf": "https://arxiv.org/pdf/2510.25578", "abs": "https://arxiv.org/abs/2510.25578", "authors": ["Mrinal Kanti Bose", "Abhay Kumar Singh"], "title": "Several classes of $p$-ary linear codes with few-weights derived from Weil sums", "comment": null, "summary": "Linear codes with few weights have been a significant area of research in\ncoding theory for many years, due to their applications in secret sharing\nschemes, authentication codes, association schemes, and strongly regular\ngraphs. Inspired by the works of Cheng and Gao \\cite{P8} and Wu, Li and Zeng\n\\cite{P12}, in this paper, we propose several new classes of few-weight linear\ncodes over the finite field $\\mathbb{F}_{p}$ through the selection of two\nspecific defining sets. Consequently, we obtain five classes of $4$-weight\nlinear codes and one class of $2$-weight linear codes from our first defining\nset. Furthermore, by employing weakly regular bent functions in our second\ndefining set, we derive two classes of $6$-weight codes, two classes of\n$8$-weight codes, and one class of $9$-weight codes. The parameters and weight\ndistributions of all these constructed codes are wholly determined by detailed\ncalculations on certain Weil sums over finite fields. In addition, we identify\nan optimal class of $2$-weight codes that meet the Griesmer bound."}
{"id": "2510.25592", "categories": ["cs.IT", "math.CO", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.25592", "abs": "https://arxiv.org/abs/2510.25592", "authors": ["Hagai Berend", "Ohad Elishco", "Moshe Schwartz"], "title": "On Multidimensional 2-Weight-Limited Burst-Correcting Codes", "comment": null, "summary": "We consider multidimensional codes capable of correcting a burst error of\nweight at most $2$. When two positions are in error, the burst limits their\nrelative position. We study three such limitations: the $L_\\infty$ distance\nbetween the positions is bounded, the $L_1$ distance between the positions is\nbounded, or the two positions are on an axis-parallel line with bounded\ndistance between them. In all cases we provide explicit code constructions, and\ncompare their excess redundancy to a lower bound we prove."}
{"id": "2510.25736", "categories": ["cs.IT", "cs.CR", "cs.DC", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.25736", "abs": "https://arxiv.org/abs/2510.25736", "authors": ["Shreya Meel", "Sennur Ulukus"], "title": "Effect of Full Common Randomness Replication in Symmetric PIR on Graph-Based Replicated Systems", "comment": null, "summary": "We revisit the problem of symmetric private information retrieval (SPIR) in\nsettings where the database replication is modeled by a simple graph. Here,\neach vertex corresponds to a server, and a message is replicated on two servers\nif and only if there is an edge between them. To satisfy the requirement of\ndatabase privacy, we let all the servers share some common randomness,\nindependent of the messages. We aim to quantify the improvement in SPIR\ncapacity, i.e., the maximum ratio of the number of desired and downloaded\nsymbols, compared to the setting with graph-replicated common randomness.\nTowards this, we develop an algorithm to convert a class of PIR schemes into\nthe corresponding SPIR schemes, thereby establishing a capacity lower bound on\ngraphs for which such schemes exist. This includes the class of path and cyclic\ngraphs for which we derive capacity upper bounds that are tighter than the\ntrivial bounds given by the respective PIR capacities. For the special case of\npath graph with three vertices, we identify the SPIR capacity to be\n$\\frac{1}{2}$."}
{"id": "2510.25740", "categories": ["cs.IT", "math.IT", "math.PR", "q-fin.MF", "q-fin.PM", "94A15, 94A17, 91G10, 91G80, 60F10, 62B10"], "pdf": "https://arxiv.org/pdf/2510.25740", "abs": "https://arxiv.org/abs/2510.25740", "authors": ["Steven Campbell", "Ting-Kam Leonard Wong"], "title": "A mathematical study of the excess growth rate", "comment": "54 pages, 2 figures", "summary": "We study the excess growth rate -- a fundamental logarithmic functional\narising in portfolio theory -- from the perspective of information theory. We\nshow that the excess growth rate can be connected to the R\\'{e}nyi and cross\nentropies, the Helmholtz free energy, L. Campbell's measure of average code\nlength and large deviations. Our main results consist of three axiomatic\ncharacterization theorems of the excess growth rate, in terms of (i) the\nrelative entropy, (ii) the gap in Jensen's inequality, and (iii) the\nlogarithmic divergence that generalizes the Bregman divergence. Furthermore, we\nstudy maximization of the excess growth rate and compare it with the growth\noptimal portfolio. Our results not only provide theoretical justifications of\nthe significance of the excess growth rate, but also establish new connections\nbetween information theory and quantitative finance."}
