<div id=toc></div>

# Table of Contents

- [cs.DS](#cs.DS) [Total: 3]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.DB](#cs.DB) [Total: 3]


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [1] [Exploiting Low Scanwidth to Resolve Soft Polytomies](https://arxiv.org/abs/2511.20771)
*Sebastian Bruchhold,Mathias Weller*

Main category: cs.DS

TL;DR: 本文提出了一种解决软树包含问题的新算法，该算法在扫描宽度较低的实际系统发育网络中具有更好的计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统的树包含问题对生物数据中支持度较差的支系不够鲁棒，可能导致假阴性结果。软树包含问题考虑了不确定性，但现有算法效率有限。

Method: 利用系统发育网络在实践中通常具有低扫描宽度的特性，开发了基于树扩展扫描宽度的参数化算法。

Result: 算法的时间复杂度为$2^{O(Δ_T \cdot k \cdot \log(k))} \cdot n^{O(1)}$，其中$k = \operatorname{sw}(Γ) + Δ_N$，在扫描宽度较低的实际网络中表现良好。

Conclusion: 该算法为软树包含问题提供了实用的解决方案，特别适用于扫描宽度较低的实际系统发育网络。

Abstract: Phylogenetic networks allow modeling reticulate evolution, capturing events such as hybridization and horizontal gene transfer. A fundamental computational problem in this context is the Tree Containment problem, which asks whether a given phylogenetic network is compatible with a given phylogenetic tree. However, the classical statement of the problem is not robust to poorly supported branches in biological data, possibly leading to false negatives. In an effort to address this, a relaxed version that accounts for uncertainty, called Soft Tree Containment, has been introduced by Bentert, Malík, and Weller [SWAT'18]. We present an algorithm that solves Soft Tree Containment in $2^{O(Δ_T \cdot k \cdot \log(k))} \cdot n^{O(1)}$ time, where $k = \operatorname{sw}(Γ) + Δ_N$, with $Δ_T$ and $Δ_N$ denoting the maximum out-degrees in the tree and the network, respectively, and $\operatorname{sw}(Γ)$ denoting the ``scanwidth'' [Berry, Scornavacca, and Weller, SOFSEM'20] of a given tree extension of the network, while $n$ is the input size. Our approach leverages the fact that phylogenetic networks encountered in practice often exhibit low scanwidth, making the problem more tractable.

</details>


### [2] [Quadratic-Time Algorithm for the Maximum-Weight $(k, \ell)$-Sparse Subgraph Problem](https://arxiv.org/abs/2511.20882)
*Bence Deák,Péter Madarasi*

Main category: cs.DS

TL;DR: 提出了首个O(n² + m)时间算法来计算最大权重的(k, ℓ)-稀疏子图，解决了之前算法分析错误的问题，并在刚性理论中有重要应用


<details>
  <summary>Details</summary>
Motivation: (k, ℓ)-稀疏图在组合优化和刚性理论中有广泛应用，但之前提出的O(n² + m)时间算法基于错误分析，需要确定这个时间界限是否可达

Method: 结合高效数据结构和精化分析，开发了新的算法框架

Result: 成功实现了O(n² + m)时间复杂度的算法，比之前的O(nm)方法更快

Conclusion: 该算法为刚性理论中的关键问题提供了更快的解决方案，包括计算最小权重冗余刚性和全局刚性子图等应用

Abstract: The family of $(k, \ell)$-sparse graphs, introduced by Lorea, plays a central role in combinatorial optimization and has a wide range of applications, particularly in rigidity theory. A key algorithmic challenge is to compute a maximum-weight $(k, \ell)$-sparse subgraph of a given edge-weighted graph. Although prior approaches have long provided an $O(nm)$-time solution, a previously proposed $O(n^2 + m)$ method was based on an incorrect analysis, leaving open whether this bound is achievable.
  We answer this question affirmatively by presenting the first $O(n^2 + m)$-time algorithm for computing a maximum-weight $(k, \ell)$-sparse subgraph, which combines an efficient data structure with a refined analysis. This quadratic-time algorithm enables faster solutions to key problems in rigidity theory, including computing minimum-weight redundantly rigid and globally rigid subgraphs. Further applications include enumerating non-crossing minimally rigid frameworks and recognizing kinematic joints. Our implementation of the proposed algorithm is publicly available online.

</details>


### [3] [Sublinear Time Low-Rank Approximation of Hankel Matrices](https://arxiv.org/abs/2511.21418)
*Michael Kapralov,Cameron Musco,Kshiteej Sheth*

Main category: cs.DS

TL;DR: 提出了第一个在亚线性时间内计算半正定Hankel矩阵低秩近似因式分解的算法，该算法在O(log n log(1/ε))时间内找到秩为O(log n log(1/ε))的Hankel近似矩阵，误差界与Beckermann-Townsend理论结果匹配。


<details>
  <summary>Details</summary>
Motivation: Hankel矩阵在计算数学、工程和理论计算机科学中广泛出现，已知半正定Hankel矩阵总是近似低秩的，但现有算法无法在亚线性时间内实现这一理论保证。

Method: 首先证明了结构保持的存在性结果，即存在满足Beckermann-Townsend误差界的低秩Hankel近似；然后利用Hankel矩阵的Vandermonde结构和通用岭杠杆得分界限，设计基于采样的亚线性时间算法。

Result: 算法在polylog(n, 1/ε)时间内计算秩为O(log n log(1/ε))的Hankel矩阵因式分解，误差界为O(||E||_F) + ε||H||_F，其中E是任意非Hankel噪声矩阵。

Conclusion: 这是第一个在亚线性时间内实现半正定Hankel矩阵低秩近似的算法，具有鲁棒性，可视为有限维AAK定理的算法实现。

Abstract: Hankel matrices are an important class of highly-structured matrices, arising across computational mathematics, engineering, and theoretical computer science. It is well-known that positive semidefinite (PSD) Hankel matrices are always approximately low-rank. In particular, a celebrated result of Beckermann and Townsend shows that, for any PSD Hankel matrix $H \in \mathbb{R}^{n \times n}$ and any $ε> 0$, letting $H_k$ be the best rank-$k$ approximation of $H$, $\|H-H_k\|_F \leq ε\|H\|_F$ for $k = O(\log n \log(1/ε))$. As such, PSD Hankel matrices are natural targets for low-rank approximation algorithms. We give the first such algorithm that runs in \emph{sublinear time}. In particular, we show how to compute, in $\polylog(n, 1/ε)$ time, a factored representation of a rank-$O(\log n \log(1/ε))$ Hankel matrix $\widehat{H}$ matching the error guarantee of Beckermann and Townsend up to constant factors. We further show that our algorithm is \emph{robust} -- given input $H+E$ where $E \in \mathbb{R}^{n \times n}$ is an arbitrary non-Hankel noise matrix, we obtain error $\|H - \widehat{H}\|_F \leq O(\|E\|_F) + ε\|H\|_F$. Towards this algorithmic result, our first contribution is a \emph{structure-preserving} existence result - we show that there exists a rank-$k$ \emph{Hankel} approximation to $H$ matching the error bound of Beckermann and Townsend. Our result can be interpreted as a finite-dimensional analog of the widely applicable AAK theorem, which shows that the optimal low-rank approximation of an infinite Hankel operator is itself Hankel. Armed with our existence result, and leveraging the well-known Vandermonde structure of Hankel matrices, we achieve our sublinear time algorithm using a sampling-based approach that relies on universal ridge leverage score bounds for Vandermonde matrices.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [4] [Computing Evolutionarily Stable Strategies in Multiplayer Games](https://arxiv.org/abs/2511.20859)
*Sam Ganzfried*

Main category: cs.GT

TL;DR: 提出了一种计算非退化正规形式游戏中所有进化稳定策略的算法


<details>
  <summary>Details</summary>
Motivation: 在多玩家（三个或更多玩家）的非退化正规形式游戏中，计算所有进化稳定策略是一个具有挑战性的问题，需要有效的算法来解决

Method: 开发了一种专门针对三玩家及以上非退化正规形式游戏的算法，用于计算所有进化稳定策略

Result: 成功实现了在多玩家非退化正规形式游戏中计算所有进化稳定策略的目标

Conclusion: 该算法为解决多玩家非退化正规形式游戏中的进化稳定策略计算问题提供了有效工具

Abstract: We present an algorithm for computing all evolutionarily stable strategies in nondegenerate normal-form games with three or more players.

</details>


### [5] [Utilitarian Guarantees for the Method of Equal Shares](https://arxiv.org/abs/2511.20929)
*Anton Baychkov,Markus Brill,Jannik Peters*

Main category: cs.GT

TL;DR: 本文分析了参与式预算中平等份额方法(MES)的功利主义福利保证，发现在满足扩展合理性表示的条件下，MES在功利福利方面达到了最优保证。


<details>
  <summary>Details</summary>
Motivation: 参与式预算研究越来越关注公平性和比例性规则，但比例性可能以功利主义福利为代价。本文旨在形式化这种关系，为MES方法提供功利福利的最小保证。

Method: 使用DNS函数子类分析MES的功利福利保证，参数化考虑项目的最小和最大成本，并与多赢家选举的现有保证进行比较。

Result: 证明了MES在满足扩展合理性表示的条件下，其功利福利保证是渐近紧的，表明没有其他比例性规则能比MES获得更好的功利保证。

Conclusion: MES在满足比例性要求的同时，在功利福利方面达到了最优性能，为参与式预算中的规则选择提供了重要理论依据。

Abstract: In recent years, research in Participatory Budgeting (PB) has put a greater emphasis on rules satisfying notions of fairness and proportionality, with the Method of Equal Shares (MES) being a prominent example. However, proportionality can come at a cost to the total utilitarian welfare. Our work formalizes this relationship, by deriving minimum utilitarian welfare guarantees for MES for a subclass of satisfaction functions called DNS functions, which includes two of the most popular ways of measuring a voter's utility in the PB setting: considering (1) the total cost of approved projects or (2) the total number of those projects. Our results are parameterized in terms of minimum and maximum project costs, which allows us to improve on the mostly negative results found in prior studies, and reduce to the existing multiwinner guarantee when project costs are equal. We show that our guarantees are asymptotically tight for rules satisfying Extended Justified Representation up to one project, showing that no proportional rule can achieve a better utilitarian guarantee than MES.

</details>


### [6] [Cycle Cancellation for Submodular Fractional Allocations and Applications](https://arxiv.org/abs/2511.21099)
*Chandra Chekuri,Pooja Kulkarni,Ruta Mehta,Jan Vondrak*

Main category: cs.GT

TL;DR: 本文证明了子模估值下的循环消除引理，并应用于三个分配问题：最大最小公平（Santa Claus问题）、纳什社会福利（NSW）和最大最小份额（MMS），获得了新的近似算法。


<details>
  <summary>Details</summary>
Motivation: 当代理人的估值是加法时，Lenstra等人的循环消除引理在舍入算法的设计和分析中起着关键作用。本文旨在为子模估值情况证明类似的引理。

Method: 使用循环消除算法，在分数分配的支持图中消除循环，同时保证每个代理人的价值（使用多线性扩展衡量）不会减少。结合其他思想应用于三个分配目标。

Result: 对于子模NSW问题获得1/5近似；对于MMS问题通过新简单算法获得1/2(1-1/e)近似；对于物品价值"小"或代理人数量恒定的特殊情况获得紧/最佳已知近似算法。

Conclusion: 所有结果都在价值查询模型中实现，为子模估值下的离散分配问题提供了新的算法和理论保证。

Abstract: We consider discrete allocation problem where $m$ indivisible goods are to be divided among $n$ agents. When agents' valuations are additive, the well-known cycle cancelling lemma by Lenstra, Shmoys, and Tardos plays a key role in design and analysis of rounding algorithms.
  In this paper, we prove an analogous lemma for the case of submodular valuations. Our algorithm removes cycles in the support graph of a fractional allocation while guaranteeing that each agent's value, measured using the multilinear extension, does not decrease.
  We demonstrate applications of the cycle-canceling algorithm, along with other ideas, to obtain new algorithms and results for three well-studied allocation objectives: max-min (Santa Claus problem), Nash social welfare (NSW), and maximin-share (MMS). For the submodular NSW problem, we obtain a $\frac{1}{5}$-approximation; for the MMS problem, we obtain a $\frac{1}{2}(1-1/e)$-approximation through new simple algorithms. For various special cases where the goods are "small" valued or the number of agents is constant, we obtain tight/best-known approximation algorithms. All our results are in the value-oracle model.

</details>


### [7] [Tâtonnement Dynamics for Fisher Markets with Chores](https://arxiv.org/abs/2511.21162)
*Bhaskar Ray Chaudhury,Christian Kroer,Ruta Mehta,Tianlong Nan*

Main category: cs.GT

TL;DR: 本文首次研究家务市场的tâtonnement动态过程，提出了相对tâtonnement方法并证明了其在连续、凸且1-齐次(CCH)负效用函数下的收敛性，特别对凸CES负效用函数获得了O(1/ε²)的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统tâtonnement动态在商品市场中的收敛性已有深入研究，但在家务市场中缺乏类似结果。家务市场面临新的挑战，如均衡价格集的非凸性和超额需求的非单调性，导致朴素tâtonnement发散。

Method: 提出相对tâtonnement过程，根据归一化超额需求更新价格。利用[CKMN24]引入的Eisenberg-Gale型对偶程序的广义目标函数进行证明。对凸CES负效用函数，利用负效用函数的极规度(或规度对偶)分析收敛速度。

Result: 证明了相对tâtonnement在适当步长选择下对CCH负效用函数收敛到竞争均衡。对凸CES负效用函数，获得了O(1/ε²)的二次收敛速度。基于[AH58]框架，完整刻画了竞争均衡的局部稳定性。

Conclusion: 相对tâtonnement过程能够克服家务市场中的收敛障碍，为家务市场的价格动态过程提供了有效的收敛保证，扩展了tâtonnement理论的应用范围。

Abstract: In this paper, we initiate the study of tâtonnement dynamics in markets with chores. Tâtonnement is a fundamental market dynamics, capturing how prices evolve when they are adjusted in proportion of their excess demand. While its convergence to a competitive equilibrium (CE) is well understood in goods markets for broad classes of utilities, no analogous results are known for chore markets.
  Analyzing tâtonnement in the chores market presents new challenges. Several elegant structural properties that facilitate convergence in goods markets-such as convexity of the equilibrium price set and monotonicity of excess demand under the tâtonnement price updates-fail to hold in the chore setting. Consistent with these difficulties, we first show that naïve tâtonnement diverges. To overcome this, we propose a modified process called relative tâtonnement, where prices are updated according to normalized excess demand. We prove its convergence to a CE under suitable step-size choices for a broad class of disutility functions, namely continuous, convex, and 1-homogeneous (CCH) disutilities. This class includes many standard forms such as linear and convex CES disutilities. Our proof proceeds by introducing a nonsmooth, nonconvex yet regular objective function-a generalization of the objective in the Eisenberg-Gale-type dual program introduced by [CKMN24].
  For convex CES disutilities, where disutility is the weighted $p$-norm of the individual chore disutilities for $p \in (1, \infty)$, we show that relative tâtonnement converges to an $\varepsilon$-CE in $O(1/\varepsilon^2)$ iterations. This quadratic convergence rate is established by leveraging the polar gauge (or gauge dual) of the disutility function. Finally, following the framework of [AH58], we analyze the stability of CE and provide a complete characterization of local stability.

</details>


### [8] [Arctic Auctions, Linear Fisher Markets, and Rational Convex Programs](https://arxiv.org/abs/2511.21637)
*Vijay V. Vazirani*

Main category: cs.GT

TL;DR: 将北极拍卖与线性费舍尔市场统一，为差异化商品的高效分配提供理论基础，并首次提出计算北极拍卖均衡的组合多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 解决复杂市场中差异化商品的高效分配问题，统一经济学和算法博弈论中的两个基础构造。

Method: 通过理性凸程序捕捉北极拍卖的均衡，并开发组合多项式时间算法来计算均衡。

Result: 证明了北极拍卖均衡可由理性凸程序表示，并首次实现了该均衡的组合多项式时间计算。

Conclusion: 成功统一了两个重要市场模型，为复杂市场中的商品分配提供了高效的计算解决方案。

Abstract: This paper unifies two foundational constructs from economics and algorithmic game theory, the Arctic Auction and the linear Fisher market, to address the efficient allocation of differentiated goods in complex markets. Our main contributions are showing that an equilibrium for the Arctic Auction is captured by a Rational Convex Program, and deriving the first combinatorial polynomial-time algorithm for computing Arctic Auction equilibria.

</details>


### [9] [Choosing What Game to Play without Selecting Equilibria: Inferring Safe (Pareto) Improvements in Binary Constraint Structures](https://arxiv.org/abs/2511.21262)
*Caspar Oesterheld,Vincent Conitzer*

Main category: cs.GT

TL;DR: 本文研究如何在给定游戏集合和结果对应假设下推导安全改进关系，证明了该问题的计算复杂性为co-NP完全，并分析了自然推理规则的完备性。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，当存在多个均衡时，如何判断哪个游戏对特定玩家、社会偏好或主控者更有利是一个概念上不明确的问题。需要基于结果对应假设来建立游戏间的比较关系。

Method: 首先分析在给定游戏集合和任意结果对应假设下推导安全改进关系的计算复杂性；然后研究自然推理规则在结果对应推导中的完备性，包括一般情况下的不完备性和在自然假设下的完备性。

Result: 证明了推导安全改进关系是co-NP完全问题；发现自然推理规则在一般情况下不完备，但在自然且普遍适用的结果对应假设下是完备的。

Conclusion: 为游戏比较提供了理论基础，明确了安全改进关系推导的计算复杂性和推理规则的适用范围，对多智能体系统设计具有指导意义。

Abstract: We consider a setting in which a principal gets to choose which game from some given set is played by a group of agents. The principal would like to choose a game that favors one of the players, the social preferences of the players, or the principal's own preferences. Unfortunately, given the potential multiplicity of equilibria, it is conceptually unclear how to tell which of even any two games is better. Oesterheld et al. (2022) propose that we use assumptions about outcome correspondence -- i.e., about how the outcomes of different games relate -- to allow comparisons in some cases. For example, it seems reasonable to assume that isomorphic games are played isomorphically. From such assumptions we can sometimes deduce that the outcome of one game G' is guaranteed to be better than the outcome of another game G, even if we do not have beliefs about how each of G and G' will be played individually. Following Oesterheld et al., we then call G' a safe improvement on G.
  In this paper, we study how to derive safe improvement relations. We first show that if we are given a set of games and arbitrary assumptions about outcome correspondence between these games, deriving safe improvement relations is co-NP-complete. We then study the (in)completeness of a natural set of inference rules for outcome correspondence. We show that in general the inference rules are incomplete. However, we also show that under natural, generally applicable assumptions about outcome correspondence the rules are complete.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [10] [E-GEO: A Testbed for Generative Engine Optimization in E-Commerce](https://arxiv.org/abs/2511.20867)
*Puneet S. Bagga,Vivek F. Farias,Tamar Korkotashvili,Tianyi Peng,Yuhang Wu*

Main category: cs.IR

TL;DR: 提出了E-GEO基准，这是首个专门为电商生成引擎优化设计的基准，包含7000多个真实消费者产品查询和相关商品列表，并评估了15种重写启发式方法，开发了轻量级迭代提示优化算法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的兴起，生成引擎正在重塑检索任务，电商中的对话购物代理需要生成引擎优化来提升内容可见性和相关性，但目前缺乏系统研究和基准。

Method: 构建E-GEO基准数据集，评估15种常见重写启发式方法，并开发了轻量级迭代提示优化算法来优化生成引擎优化问题。

Result: 优化后的提示揭示了稳定、领域无关的模式，表明存在"普遍有效"的生成引擎优化策略，优化算法显著优于基线方法。

Conclusion: E-GEO基准填补了电商生成引擎优化的研究空白，提出的优化方法能有效提升内容在生成引擎中的可见性，发现了普遍有效的优化模式。

Abstract: With the rise of large language models (LLMs), generative engines are becoming powerful alternatives to traditional search, reshaping retrieval tasks. In e-commerce, for instance, conversational shopping agents now guide consumers to relevant products. This shift has created the need for generative engine optimization (GEO)--improving content visibility and relevance for generative engines. Yet despite its growing importance, current GEO practices are ad hoc, and their impacts remain poorly understood, especially in e-commerce. We address this gap by introducing E-GEO, the first benchmark built specifically for e-commerce GEO. E-GEO contains over 7,000 realistic, multi-sentence consumer product queries paired with relevant listings, capturing rich intent, constraints, preferences, and shopping contexts that existing datasets largely miss. Using this benchmark, we conduct the first large-scale empirical study of e-commerce GEO, evaluating 15 common rewriting heuristics and comparing their empirical performance. To move beyond heuristics, we further formulate GEO as a tractable optimization problem and develop a lightweight iterative prompt-optimization algorithm that can significantly outperform these baselines. Surprisingly, the optimized prompts reveal a stable, domain-agnostic pattern--suggesting the existence of a "universally effective" GEO strategy. Our data and code are publicly available at https://github.com/psbagga17/E-GEO.

</details>


### [11] [Generating Querying Code from Text for Multi-Modal Electronic Health Record](https://arxiv.org/abs/2511.20904)
*Mengliang ZHang*

Main category: cs.IR

TL;DR: 提出了TQGen数据集和TQGen-EHRQuery框架，用于电子健康记录中的自然语言到查询生成，通过医疗知识模块和问题模板匹配模块解决复杂医学术语和多样化查询的挑战。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录包含大量结构化和非结构化数据，查询相关信息需要复杂的数据库操作，增加了临床医生的工作负担。复杂的表关系和专业术语限制了查询准确性。

Method: 构建TQGen数据集，提出TQGen-EHRQuery框架，包含医疗知识模块和问题模板匹配模块。引入工具集概念，将文本处理模块封装为可调用工具，提高处理效率和灵活性。

Result: 通过广泛实验评估数据集和工作流程的有效性，证明了其在增强EHR系统信息查询方面的潜力。

Conclusion: 所提出的数据集和框架能够有效提升电子健康记录系统中的信息查询能力，解决了复杂医疗术语和多样化查询类型的挑战。

Abstract: Electronic health records (EHR) contain extensive structured and unstructured data, including tabular information and free-text clinical notes. Querying relevant patient information often requires complex database operations, increasing the workload for clinicians. However, complex table relationships and professional terminology in EHRs limit the query accuracy. In this work, we construct a publicly available dataset, TQGen, that integrates both \textbf{T}ables and clinical \textbf{T}ext for natural language-to-query \textbf{Gen}eration. To address the challenges posed by complex medical terminology and diverse types of questions in EHRs, we propose TQGen-EHRQuery, a framework comprising a medical knowledge module and a questions template matching module. For processing medical text, we introduced the concept of a toolset, which encapsulates the text processing module as a callable tool, thereby improving processing efficiency and flexibility. We conducted extensive experiments to assess the effectiveness of our dataset and workflow, demonstrating their potential to enhance information querying in EHR systems.

</details>


### [12] [Beyond Patch Aggregation: 3-Pass Pyramid Indexing for Vision-Enhanced Document Retrieval](https://arxiv.org/abs/2511.21121)
*Anup Roy,Rishabh Gyanendra Upadhyay,Animesh Rameshbhai Panara,Robin Mills*

Main category: cs.IR

TL;DR: VisionRAG是一个免OCR、模型无关的多模态检索系统，通过金字塔索引框架直接对文档图像进行索引，保留布局信息，仅需存储17-27个向量/页，在金融文档基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统基于OCR的文档检索流程维护成本高、对布局变化敏感且容易丢失空间线索。现有的视觉优先检索方法虽然能保留结构，但存在内存开销大、与特定视觉骨干绑定等问题。

Method: 采用三阶段金字塔索引框架，直接对文档图像进行索引，通过全局页面摘要、章节标题、视觉热点和事实级线索构建语义向量，使用互惠排名融合进行检索排名。

Result: 在FinanceBench上达到0.8051的准确率（@10），在TAT DQA上达到0.9629的召回率（@100），存储效率与基于补丁的方法相当。

Conclusion: 免OCR、基于摘要指导的多模态检索是传统文本提取流程的实用且可扩展的替代方案。

Abstract: Document centric RAG pipelines usually begin with OCR, followed by brittle heuristics for chunking, table parsing, and layout reconstruction. These text first workflows are costly to maintain, sensitive to small layout shifts, and often lose the spatial cues that contain the answer. Vision first retrieval has emerged as a strong alternative. By operating directly on page images, systems like ColPali and ColQwen preserve structure and reduce pipeline complexity while achieving strong benchmark performance. However, these late interaction models tie retrieval to a specific vision backbone and require storing hundreds of patch embeddings per page, creating high memory overhead and complicating large scale deployment.
  We introduce VisionRAG, a multimodal retrieval system that is OCR free and model agnostic. VisionRAG indexes documents directly as images, preserving layout, tables, and spatial cues, and builds semantic vectors without committing to a specific extraction. Our three pass pyramid indexing framework creates vectors using global page summaries, section headers, visual hotspots, and fact level cues. These summaries act as lightweight retrieval surrogates. At query time, VisionRAG retrieves the most relevant pages using the pyramid index, then forwards the raw page image encoded as base64 to a multimodal LLM for final question answering. During retrieval, reciprocal rank fusion integrates signals across the pyramid to produce robust ranking.
  VisionRAG stores only 17 to 27 vectors per page, matching the efficiency of patch based methods while staying flexible across multimodal encoders. On financial document benchmarks, it achieves 0.8051 accuracy at 10 on FinanceBench and 0.9629 recall at 100 on TAT DQA. These results show that OCR free, summary guided multimodal retrieval is a practical and scalable alternative to traditional text extraction pipelines.

</details>


### [13] [FITRep: Attention-Guided Item Representation via MLLMs](https://arxiv.org/abs/2511.21389)
*Guoxiao Zhang,Ao Li,Tan Qu,Qianlong Xie,Xingxing Wang*

Main category: cs.IR

TL;DR: FITRep是一个基于注意力引导的白盒项目表示框架，通过概念层次信息提取、结构保持降维和FAISS聚类来解决多模态项目去重中的局部结构坍塌问题。


<details>
  <summary>Details</summary>
Motivation: 在线平台存在大量视觉和文本相似的近重复项目，导致用户体验下降。现有的多模态大语言模型方法将表示视为黑盒，忽略了结构关系，造成局部结构坍塌问题。

Method: FITRep包含三个组件：(1) CHIE：使用MLLMs提取层次化语义概念；(2) SPDR：基于UMAP的自适应降维方法进行高效信息压缩；(3) FBC：基于FAISS的聚类，为每个项目分配唯一聚类ID。

Result: 在美团广告系统部署后，在线A/B测试显示FITRep实现了+3.60%的点击率和+4.25%的千次展示收益提升。

Conclusion: FITRep框架有效解决了多模态项目去重中的局部结构坍塌问题，在实际应用中表现出显著效果和商业价值。

Abstract: Online platforms usually suffer from user experience degradation due to near-duplicate items with similar visuals and text. While Multimodal Large Language Models (MLLMs) enable multimodal embedding, existing methods treat representations as black boxes, ignoring structural relationships (e.g., primary vs. auxiliary elements), leading to local structural collapse problem. To address this, inspired by Feature Integration Theory (FIT), we propose FITRep, the first attention-guided, white-box item representation framework for fine-grained item deduplication. FITRep consists of: (1) Concept Hierarchical Information Extraction (CHIE), using MLLMs to extract hierarchical semantic concepts; (2) Structure-Preserving Dimensionality Reduction (SPDR), an adaptive UMAP-based method for efficient information compression; and (3) FAISS-Based Clustering (FBC), a FAISS-based clustering that assigns each item a unique cluster id using FAISS. Deployed on Meituan's advertising system, FITRep achieves +3.60% CTR and +4.25% CPM gains in online A/B tests, demonstrating both effectiveness and real-world impact.

</details>


### [14] [RIA: A Ranking-Infused Approach for Optimized listwise CTR Prediction](https://arxiv.org/abs/2511.21394)
*Guoxiao Zhang,Tan Qu,Ao Li,DongLin Ni,Qianlong Xie,Xingxing Wang*

Main category: cs.IR

TL;DR: RIA是一个统一的端到端重排序框架，通过共享表示将点排序和列表排序无缝集成，解决了现有方法解耦排序与重排序导致的组合稀疏性和表示能力受限问题。


<details>
  <summary>Details</summary>
Motivation: 现有重排序方法通常将排序和重排序解耦，导致列表评估模型在严格延迟约束下存在组合稀疏性和有限表示能力的问题。

Method: 提出RIA框架，包含四个关键组件：用户候选双Transformer进行细粒度建模、上下文感知用户历史目标模块学习位置敏感偏好、列表多HSTU模块捕获层次化物品依赖、嵌入缓存模块在推理时平衡效率与效果。

Result: 在公开和工业数据集上显著超越最先进模型，AUC和LogLoss指标均有显著提升。在美团广告系统在线A/B测试中，点击率提升1.69%，千次展示成本提升4.54%。

Conclusion: RIA通过统一架构实现了排序与重排序的有效集成，在保持低延迟的同时实现了显著的性能提升，证明了端到端框架在推荐系统中的价值。

Abstract: Reranking improves recommendation quality by modeling item interactions. However, existing methods often decouple ranking and reranking, leading to weak listwise evaluation models that suffer from combinatorial sparsity and limited representational power under strict latency constraints. In this paper, we propose RIA (Ranking-Infused Architecture), a unified, end-to-end framework that seamlessly integrates pointwise and listwise evaluation. RIA introduces four key components: (1) the User and Candidate DualTransformer (UCDT) for fine-grained user-item-context modeling; (2) the Context-aware User History and Target (CUHT) module for position-sensitive preference learning; (3) the Listwise Multi-HSTU (LMH) module to capture hierarchical item dependencies; and (4) the Embedding Cache (EC) module to bridge efficiency and effectiveness during inference. By sharing representations across ranking and reranking, RIA enables rich contextual knowledge transfer while maintaining low latency. Extensive experiments show that RIA outperforms state-of-the-art models on both public and industrial datasets, achieving significant gains in AUC and LogLoss. Deployed in Meituan advertising system, RIA yields a +1.69% improvement in Click-Through Rate (CTR) and a +4.54% increase in Cost Per Mille (CPM) in online A/B tests.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [15] [Prompt-Aware Adaptive Elastic Weight Consolidation for Continual Learning in Medical Vision-Language Models](https://arxiv.org/abs/2511.20732)
*Ziyuan Gao,Philippe Morel*

Main category: cs.MM

TL;DR: 提出PA-EWC方法解决医学AI系统在临床部署中的灾难性遗忘问题，通过提示引导的参数专业化保护关键知识，在多种医学影像数据集上验证有效性


<details>
  <summary>Details</summary>
Motivation: 医学AI系统在临床环境中面临灾难性遗忘问题，特别是在医学视觉语言模型中需要保持复杂跨模态对齐，同时学习新的成像协议

Method: PA-EWC方法基于参数功能角色分类（视觉描述、空间引导、医学语义），结合自适应Fisher信息计算和梯度稳定性分析，开发基于医学术语密度的加权复杂度指标

Result: 在五个医学影像数据集上评估，PA-EWC相比基线方法减少灾难性遗忘达17.58%，在胸部X光病理定位和息肉分割任务上分别提升4.30%和6.06%

Conclusion: PA-EWC通过提示感知的参数专业化有效解决了医学AI系统的持续学习挑战，为临床部署提供了可靠解决方案

Abstract: Medical AI systems face catastrophic forgetting when deployed in clinical settings, where models must learn new imaging protocols while retaining prior diagnostic capabilities. This challenge is particularly acute for medical vision-language models that must preserve complex cross-modal alignments between medical images and clinical terminology across diverse imaging modalities. We introduce Prompt- Aware Adaptive Elastic Weight Consolidation (PA-EWC), a novel continual learning approach that addresses catastrophic forgetting through prompt-guided parameter specialization. Our method systematically categorizes model parameters based on their functional roles in processing visual-descriptive, spatial-guided, and medical-semantic information, enabling targeted protection of critical knowledge while allowing adaptation to new clinical requirements. PA-EWC incorporates adaptive Fisher Information computation with gradient stability analysis and develops weighted complexity metrics based on medical terminology density. We evaluate our approach across five medical imaging datasets (Kvasir-SEG, ISIC 2018, CheXlocalize, BUSI, CAMUS) representing diverse modalities including endoscopy, dermoscopy, radiography, and ultrasound. Experimental results demonstrate that PA-EWC reduces catastrophic forgetting by up to 17.58% compared to baseline methods, with performance improvements of 4.30% on chest X-ray pathology localization and 6.06% on polyp segmentation.

</details>


### [16] [AV-Edit: Multimodal Generative Sound Effect Editing via Audio-Visual Semantic Joint Control](https://arxiv.org/abs/2511.21146)
*Xinyue Guo,Xiaoran Yang,Lipan Zhang,Jianxuan Yang,Zhao Wang,Jian Luan*

Main category: cs.MM

TL;DR: AV-Edit是一个生成式音效编辑框架，通过联合利用视觉、音频和文本语义，实现对视频中现有音频轨道的细粒度编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的音效编辑方法仅依赖低级信号处理或粗粒度文本提示，导致灵活性有限且音频质量不佳。

Method: 使用对比音频-视觉掩码自编码器进行多模态预训练，学习对齐的跨模态表示，然后训练多模态扩散变换器，通过基于相关性的特征门控训练策略去除视觉无关声音并生成与视频内容一致的缺失音频元素。

Result: 实验表明AV-Edit能够基于视觉内容生成高质量音频并进行精确修改，在音效编辑领域达到最先进性能，在音频生成领域表现出强大竞争力。

Conclusion: AV-Edit通过多模态方法显著提升了音效编辑的灵活性和音频质量，为视频音频编辑提供了有效的解决方案。

Abstract: Sound effect editing-modifying audio by adding, removing, or replacing elements-remains constrained by existing approaches that rely solely on low-level signal processing or coarse text prompts, often resulting in limited flexibility and suboptimal audio quality. To address this, we propose AV-Edit, a generative sound effect editing framework that enables fine-grained editing of existing audio tracks in videos by jointly leveraging visual, audio, and text semantics. Specifically, the proposed method employs a specially designed contrastive audio-visual masking autoencoder (CAV-MAE-Edit) for multimodal pre-training, learning aligned cross-modal representations. These representations are then used to train an editorial Multimodal Diffusion Transformer (MM-DiT) capable of removing visually irrelevant sounds and generating missing audio elements consistent with video content through a correlation-based feature gating training strategy. Furthermore, we construct a dedicated video-based sound editing dataset as an evaluation benchmark. Experiments demonstrate that the proposed AV-Edit generates high-quality audio with precise modifications based on visual content, achieving state-of-the-art performance in the field of sound effect editing and exhibiting strong competitiveness in the domain of audio generation.

</details>


### [17] [PixelatedScatter: Arbitrary-level Visual Abstraction for Large-scale Multiclass Scatterplots](https://arxiv.org/abs/2511.21244)
*Ziheng Guo,Tianxiang Wei,Zeyu Li,Lianghao Zhang,Sisi Li,Jiawan Zhang*

Main category: cs.MM

TL;DR: 提出一种针对大规模散点图的视觉抽象方法，特别关注中低密度区域的特征保留，通过等密度分区、像素分配和数据重建三个步骤实现更好的特征保持。


<details>
  <summary>Details</summary>
Motivation: 大规模散点图中过度绘制不可避免，现有抽象方法在中低密度区域会丢失特征，需要一种能在任意抽象级别下更好地保留特征的方法。

Method: 三个紧密互连的步骤：1) 将散点图划分为等密度区域并均衡视觉密度；2) 在每个区域内为不同类别分配像素；3) 基于像素重建数据分布。

Result: 用户研究、定量和定性评估表明，相比先前方法，该方法能更好地保留特征，在处理超高动态范围数据分布时具有特殊优势。

Conclusion: 该方法为大规模散点图提供了一种有效的视觉抽象解决方案，特别在中低密度区域的特征保留方面表现优异。

Abstract: Overdraw is inevitable in large-scale scatterplots. Current scatterplot abstraction methods lose features in medium-to-low density regions. We propose a visual abstraction method designed to provide better feature preservation across arbitrary abstraction levels for large-scale scatterplots, particularly in medium-to-low density regions. The method consists of three closely interconnected steps: first, we partition the scatterplot into iso-density regions and equalize visual density; then, we allocate pixels for different classes within each region; finally, we reconstruct the data distribution based on pixels. User studies, quantitative and qualitative evaluations demonstrate that, compared to previous methods, our approach better preserves features and exhibits a special advantage when handling ultra-high dynamic range data distributions.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [18] [MorphingDB: A Task-Centric AI-Native DBMS for Model Management and Inference](https://arxiv.org/abs/2511.21160)
*Wu Sai,Xia Ruichen,Yang Dingyu,Wang Rui,Lai Huihang,Guan Jiarui,Bai Jiameng,Zhang Dongxiang,Tang Xiu,Xie Zhongle,Lu Peng,Chen Gang*

Main category: cs.DB

TL;DR: MorphingDB是一个任务中心的AI原生数据库系统，在PostgreSQL中自动化模型存储、选择和推理，通过专门的模式和多维张量数据类型支持灵活存储，采用两阶段迁移学习框架进行模型选择，并通过预嵌入和DAG批处理优化推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有AI原生数据库解决方案存在模型中心设计需要手动选择配置模型导致开发成本高，或任务中心AutoML方法计算成本高且与DBMS集成差的问题。

Method: 1) 引入专门模式和多维张量数据类型支持BLOB-based模型存储；2) 设计两阶段迁移学习框架，通过历史任务离线嵌入构建可迁移子空间，在线投影进行实时任务处理；3) 采用预嵌入和向量共享消除冗余计算，DAG批处理管道与成本感知调度优化推理。

Result: 在9个公共数据集（序列、NLP、图像任务）上优于AI原生DBMS（EvaDB、Madlib、GaussML）和AutoML平台（AutoGluon、AutoKeras、AutoSklearn），在模型选择的准确性、资源消耗和时间成本之间取得良好平衡，推理吞吐量和资源效率显著提升。

Conclusion: MorphingDB作为PostgreSQL扩展，实现了模型存储、选择和推理的自动化，在准确性、资源效率和时间成本方面表现出色，为AI原生数据库提供了有效的解决方案。

Abstract: The increasing demand for deep neural inference within database environments has driven the emergence of AI-native DBMSs. However, existing solutions either rely on model-centric designs requiring developers to manually select, configure, and maintain models, resulting in high development overhead, or adopt task-centric AutoML approaches with high computational costs and poor DBMS integration. We present MorphingDB, a task-centric AI-native DBMS that automates model storage, selection, and inference within PostgreSQL. To enable flexible, I/O-efficient storage of deep learning models, we first introduce specialized schemas and multi-dimensional tensor data types to support BLOB-based all-in-one and decoupled model storage. Then we design a transfer learning framework for model selection in two phases, which builds a transferability subspace via offline embedding of historical tasks and employs online projection through feature-aware mapping for real-time tasks. To further optimize inference throughput, we propose pre-embedding with vectoring sharing to eliminate redundant computations and DAG-based batch pipelines with cost-aware scheduling to minimize the inference time. Implemented as a PostgreSQL extension with LibTorch, MorphingDB outperforms AI-native DBMSs (EvaDB, Madlib, GaussML) and AutoML platforms (AutoGluon, AutoKeras, AutoSklearn) across nine public datasets, encompassing series, NLP, and image tasks. Our evaluation demonstrates a robust balance among accuracy, resource consumption, and time cost in model selection and significant gains in throughput and resource efficiency.

</details>


### [19] [HIRE: A Hybrid Learned Index for Robust and Efficient Performance under Mixed Workloads](https://arxiv.org/abs/2511.21307)
*Xinyi Zhang,Liang Liang,Anastasia Ailamaki,Jianliang Xu*

Main category: cs.DB

TL;DR: HIRE是一种混合内存索引结构，结合传统索引的稳健性和学习索引的预测能力，在范围查询吞吐量、尾延迟和整体稳定性方面优于最先进的学习索引和传统结构。


<details>
  <summary>Details</summary>
Motivation: 学习索引在点查询上表现优异，但存在高尾延迟、范围查询性能不佳以及在不同工作负载下效果不一致的问题，需要一种能持续提供高效性能的混合索引结构。

Method: 采用混合叶子节点适应不同数据分布和工作负载，模型加速的内部节点配合基于日志的更新，非阻塞的成本驱动重新校准机制，以及考虑叶子节点和内部节点错误的跨层优化批量加载算法。

Result: 在多个真实数据集上的实验表明，HIRE在混合工作负载下实现高达41.7倍的吞吐量提升，在各种场景下尾延迟降低高达98%。

Conclusion: HIRE成功结合了传统索引和学习索引的优势，在保持最坏情况稳定性的同时显著提升了查询性能和系统稳定性。

Abstract: Indexes are critical for efficient data retrieval and updates in modern databases. Recent advances in machine learning have led to the development of learned indexes, which model the cumulative distribution function of data to predict search positions and accelerate query processing. While learned indexes substantially outperform traditional structures for point lookups, they often suffer from high tail latency, suboptimal range query performance, and inconsistent effectiveness across diverse workloads. To address these challenges, this paper proposes HIRE, a hybrid in-memory index structure designed to deliver efficient performance consistently. HIRE combines the structural and performance robustness of traditional indexes with the predictive power of model-based prediction to reduce search overhead while maintaining worst-case stability. Specifically, it employs (1) hybrid leaf nodes adaptive to varying data distributions and workloads, (2) model-accelerated internal nodes augmented by log-based updates for efficient updates, (3) a nonblocking, cost-driven recalibration mechanism for dynamic data, and (4) an inter-level optimized bulk-loading algorithm accounting for leaf and internal-node errors. Experimental results on multiple real-world datasets demonstrate that HIRE outperforms both state-of-the-art learned indexes and traditional structures in range-query throughput, tail latency, and overall stability. Compared to state-of-the-art learned indexes and traditional indexes, HIRE achieves up to 41.7$\times$ higher throughput under mixed workloads, reduces tail latency by up to 98% across varying scenarios.

</details>


### [20] [Beyond Accuracy: An Empirical Study of Uncertainty Estimation in Imputation](https://arxiv.org/abs/2511.21607)
*Zarin Tahia Hossain,Mostafa Milani*

Main category: cs.DB

TL;DR: 系统评估了不同插补方法的不确定性校准性能，发现准确性和校准性往往不一致，为数据清洗和下游机器学习提供了不确定性感知的插补方法选择指南


<details>
  <summary>Details</summary>
Motivation: 现代插补方法不仅追求准确重建，还通过不同方式表示和量化不确定性，但这些不确定性估计的可靠性和校准性仍缺乏系统研究

Method: 对三大类代表性插补方法进行系统实证研究：统计方法（MICE、SoftImpute）、分布对齐（OT-Impute）和深度生成方法（GAIN、MIWAE、TabCSDI），通过多轮变异性、条件采样和预测分布建模三种互补途径估计不确定性，使用校准曲线和期望校准误差进行评估

Result: 准确性和校准性经常不一致：重建精度高的模型不一定能提供可靠的不确定性。分析了方法在准确性、校准性和运行时间之间的权衡，确定了稳定配置

Conclusion: 为数据清洗和下游机器学习管道中的不确定性感知插补方法选择提供了实用指南

Abstract: Handling missing data is a central challenge in data-driven analysis. Modern imputation methods not only aim for accurate reconstruction but also differ in how they represent and quantify uncertainty. Yet, the reliability and calibration of these uncertainty estimates remain poorly understood. This paper presents a systematic empirical study of uncertainty in imputation, comparing representative methods from three major families: statistical (MICE, SoftImpute), distribution alignment (OT-Impute), and deep generative (GAIN, MIWAE, TabCSDI). Experiments span multiple datasets, missingness mechanisms (MCAR, MAR, MNAR), and missingness rates. Uncertainty is estimated through three complementary routes: multi-run variability, conditional sampling, and predictive-distribution modeling, and evaluated using calibration curves and the Expected Calibration Error (ECE). Results show that accuracy and calibration are often misaligned: models with high reconstruction accuracy do not necessarily yield reliable uncertainty. We analyze method-specific trade-offs among accuracy, calibration, and runtime, identify stable configurations, and offer guidelines for selecting uncertainty-aware imputers in data cleaning and downstream machine learning pipelines.

</details>
