<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 5]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.IR](#cs.IR) [Total: 13]
- [cs.IT](#cs.IT) [Total: 17]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Non-uniformly Stable Common Independent Sets](https://arxiv.org/abs/2601.11153)
*Naoyuki Kamiyama*

Main category: cs.GT

TL;DR: 提出多项式时间算法检查是否存在满足非均匀稳定性的公共独立集，这是超稳定性和强稳定性的推广


<details>
  <summary>Details</summary>
Motivation: 将稳定匹配问题推广到拟阵设置，考虑偏好可能包含平局的情况，研究更一般的稳定性概念

Method: 提出多项式时间算法来检查是否存在满足非均匀稳定性的公共独立集，该算法处理拟阵约束和包含平局的偏好

Result: 算法在多项式时间内运行，能够确定是否存在满足非均匀稳定性的公共独立集

Conclusion: 成功将稳定匹配问题推广到拟阵设置，并解决了包含平局偏好下的非均匀稳定性检查问题

Abstract: In this paper, we consider a matroid generalization of the stable matching problem. In particular, we consider the setting where preferences may contain ties. For this generalization, we propose a polynomial-time algorithm for the problem of checking the existence of a common independent set satisfying non-uniform stability, which is a common generalization of super-stability and strong stability.

</details>


### [2] [Institutional AI: Governing LLM Collusion in Multi-Agent Cournot Markets via Public Governance Graphs](https://arxiv.org/abs/2601.11369)
*Marcantonio Bracale Syrnikov,Federico Pierucci,Marcello Galisai,Matteo Prandi,Piercosma Bisconti,Francesco Giarrusso,Olga Sorokoletova,Vincenzo Suriani,Daniele Nardi*

Main category: cs.GT

TL;DR: 本文提出Institutional AI框架，通过治理图机制设计来约束多智能体LLM的合谋行为，相比仅提示的宪法方法显著降低了合谋率。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统可能收敛到协调的、有害社会的均衡状态（如合谋）。现有基于偏好的对齐方法在智能体层面效果有限，需要系统层面的机制设计来约束集体行为。

Method: 提出Institutional AI框架，核心是治理图——一个公开、不可变的清单，声明合法状态、转换、制裁和恢复路径。Oracle/Controller运行时解释该清单，为协调证据附加可执行后果，并记录加密的、仅追加的治理日志用于审计。在Cournot合谋案例中比较三种制度：无治理（基线）、宪法（仅提示的禁止政策）、制度（基于治理图）。

Result: 制度治理显著降低合谋：平均层级从3.1降至1.8（Cohen's d=1.28），严重合谋发生率从50%降至5.6%。仅提示的宪法基线无可靠改善，说明声明性禁令在优化压力下无效。

Conclusion: 多智能体对齐应视为制度设计问题，治理图可为对齐相关的集体行为提供可处理的抽象。机制设计比偏好工程更有效约束协调行为。

Abstract: Multi-agent LLM ensembles can converge on coordinated, socially harmful equilibria. This paper advances an experimental framework for evaluating Institutional AI, our system-level approach to AI alignment that reframes alignment from preference engineering in agent-space to mechanism design in institution-space. Central to this approach is the governance graph, a public, immutable manifest that declares legal states, transitions, sanctions, and restorative paths; an Oracle/Controller runtime interprets this manifest, attaching enforceable consequences to evidence of coordination while recording a cryptographically keyed, append-only governance log for audit and provenance. We apply the Institutional AI framework to govern the Cournot collusion case documented by prior work and compare three regimes: Ungoverned (baseline incentives from the structure of the Cournot market), Constitutional (a prompt-only policy-as-prompt prohibition implemented as a fixed written anti-collusion constitution, and Institutional (governance-graph-based). Across six model configurations including cross-provider pairs (N=90 runs/condition), the Institutional regime produces large reductions in collusion: mean tier falls from 3.1 to 1.8 (Cohen's d=1.28), and severe-collusion incidence drops from 50% to 5.6%. The prompt-only Constitutional baseline yields no reliable improvement, illustrating that declarative prohibitions do not bind under optimisation pressure. These results suggest that multi-agent alignment may benefit from being framed as an institutional design problem, where governance graphs can provide a tractable abstraction for alignment-relevant collective behavior.

</details>


### [3] [Minimizing the Cost of EFx Allocations](https://arxiv.org/abs/2601.11372)
*Eva Deltl*

Main category: cs.GT

TL;DR: 研究在资源分配中同时保证公平性（EFx）和最小化成本的问题，证明该问题是NP难的，但存在多项式核，并探索了参数限制下的可解性和近似性。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多只关注成本最小化而不考虑公平性，或只关注公平性而不考虑成本。本文旨在填补这一空白，研究如何在保证公平性的前提下最小化成本。

Method: 引入并形式化定义minCost-EFx分配问题，研究其算法复杂性。证明该问题是NP难的，但存在关于物品数量的多项式核。探索参数限制下的可解性，包括有界估值、常数个代理、有限物品类型等场景。研究成本近似性。

Result: 证明minCost-EFx分配问题在只有两个代理时已是NP难的。发现该问题存在关于物品数量的多项式核。在参数限制下（如有界估值、常数个代理、有限物品类型）是可解的。在一般成本模型下，对于任意ρ>1，该问题不能在多项式时间内ρ-近似（除非P=NP），但在特定成本模型下可以获得更好的近似保证。

Conclusion: 同时保证公平性和最小化成本是一个计算上困难的问题，但通过参数化分析和特定成本模型的限制，可以找到可行的解决方案和近似算法。

Abstract: Ensuring fairness while limiting costs, such as transportation or storage, is an important challenge in resource allocation, yet most work has focused on cost minimization without fairness or fairness without explicit cost considerations. We introduce and formally define the minCost-EFx Allocation problem, where the objective is to compute an allocation that is envy-free up to any item (EFx) and has minimum cost. We investigate the algorithmic complexity of this problem, proving that it is NP-hard already with two agents. On the positive side, we show that the problem admits a polynomial kernel with respect to the number of items, implying that a core source of intractability lies in the number of items. Building on this, we identify parameter-restricted settings that are tractable, including cases with bounded valuations and a constant number of agents, or a limited number of item types under restricted cost models. Finally, we turn to cost approximation, proving that for any $ρ>1$ the problem is not $ρ$-approximable in polynomial time (unless $P=NP$), while also identifying restricted cost models where costs are agent-specific and independent of the actual items received, which admit better approximation guarantees.

</details>


### [4] [New Adaptive Mechanism for Large Neighborhood Search using Dual Actor-Critic](https://arxiv.org/abs/2601.11414)
*Shaohua Yu,Wenhao Mao,Zigao Wu,Jakob Puchinger*

Main category: cs.GT

TL;DR: 本文提出了一种基于双演员-评论家模型的ALNS自适应机制，通过考虑破坏与修复算子间的交互作用，并利用图神经网络提取特征，显著提升了算法效率和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 传统ALNS的自适应机制在调整算子概率时，没有考虑破坏算子和修复算子之间的交互作用，而新解的质量实际上是由这两个算子共同决定的。这种局限性限制了算法的适应性和性能。

Method: 提出DAC-ALNS算法：1) 使用双演员-评论家模型构建自适应机制，充分考虑破坏与修复算子的交互作用；2) 将破坏和修复过程建模为独立的马尔可夫决策过程；3) 利用图神经网络提取问题实例的关键特征并进行聚合归一化，增强算法对不同规模问题的可迁移性。

Result: 实验表明，DAC-ALNS算法显著提高了求解效率，并在不同规模和特性的问题上表现出优秀的可迁移性。

Conclusion: 通过引入考虑算子交互作用的双演员-评论家自适应机制，并结合图神经网络的特征提取能力，成功提升了ALNS算法的适应性和可迁移性，为解决组合优化问题提供了更有效的启发式方法。

Abstract: Adaptive Large Neighborhood Search (ALNS) is a widely used heuristic method for solving combinatorial optimization problems. ALNS explores the solution space by iteratively using destroy and repair operators with probabilities, which are adjusted by an adaptive mechanism to find optimal solutions. However, the classic ALNS adaptive mechanism does not consider the interaction between destroy and repair operators when selecting them. To overcome this limitation, this study proposes a novel adaptive mechanism. This mechanism enhances the adaptability of the algorithm through a Dual Actor-Critic (DAC) model, which fully considers the fact that the quality of new solutions is jointly determined by the destroy and repair operators. It effectively utilizes the interaction between these operators during the weight adjustment process, greatly improving the adaptability of the ALNS algorithm. In this mechanism, the destroy and repair processes are modeled as independent Markov Decision Processes to guide the selection of operators more accurately. Furthermore, we use Graph Neural Networks to extract key features from problem instances and perform effective aggregation and normalization to enhance the algorithm's transferability to different sizes and characteristics of problems. Through a series of experiments, we demonstrate that the proposed DAC-ALNS algorithm significantly improves solution efficiency and exhibits excellent transferability.

</details>


### [5] [The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents](https://arxiv.org/abs/2601.11496)
*Eilam Shapira,Roi Reichart,Moshe Tennenholtz*

Main category: cs.GT

TL;DR: AI代理进入经济市场会改变战略互动格局，研究发现增加AI代理选择会显著改变均衡收益和监管结果，并揭示"毒苹果"效应：策略性发布新技术来操纵监管设计


<details>
  <summary>Details</summary>
Motivation: 研究AI代理融入经济市场对战略互动的影响，探索技术选择扩展如何改变经典博弈论场景中的均衡结果和监管设计

Method: 在三个经典博弈论场景中分析技术选择扩展的经济影响：议价（资源分配）、谈判（非对称信息交易）和说服（策略性信息传递）

Result: 发现增加AI代理选择会显著改变均衡收益和监管结果，识别出"毒苹果"效应：策略性发布新技术来操纵监管设计，即使双方最终都不使用该技术

Conclusion: 静态监管框架易受技术扩展操纵，需要动态市场设计来适应AI能力的演进，监管者应主动开发和发布技术以应对策略性技术发布

Abstract: The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the "Poisoned Apple" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [6] [Two Complexity Results on Spanning-Tree Congestion Problems](https://arxiv.org/abs/2601.10881)
*Sunny Atalig,Marek Chrobak,Christoph Dürr,Petr Kolman,Huong Luu,Jiří Sgall,Gregory Zhu*

Main category: cs.DS

TL;DR: 本文解决了生成树拥塞问题（STC）中的两个关键计算复杂性开放问题：证明了对于度界d≥3的图STC都是NP难的，并证明了对于K边连通图，决策版本在多项式时间内可解。


<details>
  <summary>Details</summary>
Motivation: 生成树拥塞问题（STC）已知是NP难的，但对于某些受限图类的计算复杂性仍存在开放问题。本文旨在解决其中两个关键问题：一是确定STC在度界图上的复杂性边界，二是研究决策版本在特定图类上的可解性。

Method: 通过理论证明方法：1) 对于度界问题，通过构造性证明将STC的NP难性扩展到所有d≥3的情况，填补了d=3-7的空白；2) 对于决策版本，利用图论中的边连通性概念，证明对于K边连通图，存在多项式时间算法判断拥塞是否≤K。

Result: 主要结果：1) 证明了对于每个度界d≥3，STC都是NP难的，这完全解决了度界变体的复杂性分类；2) 证明了对于K边连通图，STC的决策版本（给定整数K判断拥塞是否≤K）可以在多项式时间内解决。

Conclusion: 本文完全解决了STC在度界图上的复杂性分类问题，并揭示了决策版本在边连通图上的可解性，为生成树拥塞问题的计算复杂性研究提供了重要进展。

Abstract: In the spanning-tree congestion problem ($\mathsf{STC}$), we are given a graph $G$, and the objective is to compute a spanning tree of $G$ that minimizes the maximum edge congestion. While $\mathsf{STC}$ is known to be $\mathbb{NP}$-hard, even for some restricted graph classes, several key questions regarding its computational complexity remain open, and we address some of these in our paper. (i) For graphs of degree at most $d$, it is known that $\mathsf{STC}$ is $\mathbb{NP}$-hard when $d\ge 8$. We provide a complete resolution of this variant, by showing that $\mathsf{STC}$ remains $\mathbb{NP}$-hard for each degree bound $d\ge 3$. (ii) In the decision version of $\mathsf{STC}$, given an integer $K$, the goal is to determine whether the congestion of $G$ is at most $K$. We prove that this variant is polynomial-time solvable for $K$-edge-connected graphs.

</details>


### [7] [Streaming Stochastic Submodular Maximization with On-Demand User Requests](https://arxiv.org/abs/2601.10901)
*Honglian Wang,Sijing Tu,Lutz Oettershagen,Aristides Gionis*

Main category: cs.DS

TL;DR: 提出一种新闻推荐平台中的流式次模最大化新问题，设计在线流式算法实现有界竞争比，仅需单遍扫描和常数内存


<details>
  <summary>Details</summary>
Motivation: 新闻推荐平台中用户访问时间不确定，网站需实时显示新闻，用户交互具有随机性（新闻被接受的概率不同），目标是最大化预期主题覆盖率

Method: 建立与带拟阵约束的次模最大化问题的联系，提出新的在线流式算法，仅需单遍扫描流数据，使用与流长度无关的常数内存

Result: 算法达到1/(8δ)的竞争比（δ控制近似质量），在已知访问次数或线性内存时能有效适应现有方法，在更现实的场景（仅有访问上界和次线性内存）下仍能保证性能

Conclusion: 提出的在线流式算法解决了新闻推荐中的流式次模最大化问题，在理论和实验上都优于基线方法，为现实场景提供了有效解决方案

Abstract: We explore a novel problem in streaming submodular maximization, inspired by the dynamics of news-recommendation platforms. We consider a setting where users can visit a news website at any time, and upon each visit, the website must display up to $k$ news items. User interactions are inherently stochastic: each news item presented to the user is consumed with a certain acceptance probability by the user, and each news item covers certain topics. Our goal is to design a streaming algorithm that maximizes the expected total topic coverage. To address this problem, we establish a connection to submodular maximization subject to a matroid constraint. We show that we can effectively adapt previous methods to address our problem when the number of user visits is known in advance or linear-size memory in the stream length is available. However, in more realistic scenarios where only an upper bound on the visits and sublinear memory is available, the algorithms fail to guarantee any bounded performance. To overcome these limitations, we introduce a new online streaming algorithm that achieves a competitive ratio of $1/(8δ)$, where $δ$ controls the approximation quality. Moreover, it requires only a single pass over the stream, and uses memory independent of the stream length. Empirically, our algorithms consistently outperform the baselines.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [8] [Tail-Aware Data Augmentation for Long-Tail Sequential Recommendation](https://arxiv.org/abs/2601.10933)
*Yizhou Dang,Zhifu Wei,Minhan Huang,Lianbo Ma,Jianzhe Zhao,Guibing Guo,Xingwei Wang*

Main category: cs.IR

TL;DR: TADA提出了一种针对长尾序列推荐的尾部感知数据增强方法，通过替换和插入操作增强尾部用户/物品的交互频率，同时保持头部性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中大多数用户只能与少数物品交互，而大多数物品很少被消费，这种长尾问题限制了模型学习用户偏好的能力。现有方法在提升尾部性能时往往会损害整体或头部性能。

Method: 首先通过线性模型捕捉低流行度物品的共现关系，然后设计两种尾部感知增强算子：T-Substitute（用相关物品替换头部物品）和T-Insert（利用共现关系在原始序列中插入头部和尾部物品）。在表示层混合增强序列和原始序列以保留偏好知识，并跨不同尾部用户序列和增强序列扩展混合操作。

Result: 综合实验证明了该方法的优越性，能够有效提升尾部用户/物品的性能，同时保持头部性能不受损害。

Conclusion: TADA通过尾部感知数据增强有效解决了长尾序列推荐问题，在提升尾部性能的同时保持了整体推荐质量，改善了用户体验。

Abstract: Sequential recommendation (SR) learns user preferences based on their historical interaction sequences and provides personalized suggestions. In real-world scenarios, most users can only interact with a handful of items, while the majority of items are seldom consumed. This pervasive long-tail challenge limits the model's ability to learn user preferences. Despite previous efforts to enrich tail items/users with knowledge from head parts or improve tail learning through additional contextual information, they still face the following issues: 1) They struggle to improve the situation where interactions of tail users/items are scarce, leading to incomplete preferences learning for the tail parts. 2) Existing methods often degrade overall or head parts performance when improving accuracy for tail users/items, thereby harming the user experience. We propose Tail-Aware Data Augmentation (TADA) for long-tail sequential recommendation, which enhances the interaction frequency for tail items/users while maintaining head performance, thereby promoting the model's learning capabilities for the tail. Specifically, we first capture the co-occurrence and correlation among low-popularity items by a linear model. Building upon this, we design two tail-aware augmentation operators, T-Substitute and T-Insert. The former replaces the head item with a relevant item, while the latter utilizes co-occurrence relationships to extend the original sequence by incorporating both head and tail items. The augmented and original sequences are mixed at the representation level to preserve preference knowledge. We further extend the mix operation across different tail-user sequences and augmented sequences to generate richer augmented samples, thereby improving tail performance. Comprehensive experiments demonstrate the superiority of our method. The codes are provided at https://github.com/KingGugu/TADA.

</details>


### [9] [Can Instructed Retrieval Models Really Support Exploration?](https://arxiv.org/abs/2601.10936)
*Piyush Maheshwari,Sheshera Mysore,Hamed Zamani*

Main category: cs.IR

TL;DR: 评估指令跟随检索模型在方面条件种子引导探索任务中的表现，发现其在相关性排序上有改进，但指令跟随能力不足，不适合需要高度指令敏感性的长期探索会话


<details>
  <summary>Details</summary>
Motivation: 探索式搜索具有目标不明确和查询意图不断演变的特点，需要能够捕捉用户指定意图细微差别并相应调整结果的检索模型。指令跟随检索模型承诺具备这种能力，但在方面条件种子引导探索这一普遍但研究不足的应用中表现如何尚不清楚。

Method: 使用专家标注的测试集合评估指令检索器，包括专门为指令检索微调的LLM和使用Pairwise Ranking Prompting进行排名的通用LLM。评估指令跟随检索模型在方面条件种子引导探索任务中的表现。

Result: 最佳指令检索器在排名相关性上优于指令不可知方法，但指令跟随性能（对用户体验至关重要）并未反映相关性改进，且对指令表现出不敏感或反直觉行为。

Conclusion: 虽然用户在当前使用指令检索器可能比指令不可知模型受益，但对于需要更高指令敏感性的长期探索会话，这些模型可能不适用。指令跟随性能与排名相关性改进不匹配，表明需要进一步研究改进指令跟随能力。

Abstract: Exploratory searches are characterized by under-specified goals and evolving query intents. In such scenarios, retrieval models that can capture user-specified nuances in query intent and adapt results accordingly are desirable -- instruction-following retrieval models promise such a capability. In this work, we evaluate instructed retrievers for the prevalent yet under-explored application of aspect-conditional seed-guided exploration using an expert-annotated test collection. We evaluate both recent LLMs fine-tuned for instructed retrieval and general-purpose LLMs prompted for ranking with the highly performant Pairwise Ranking Prompting. We find that the best instructed retrievers improve on ranking relevance compared to instruction-agnostic approaches. However, we also find that instruction following performance, crucial to the user experience of interacting with models, does not mirror ranking relevance improvements and displays insensitivity or counter-intuitive behavior to instructions. Our results indicate that while users may benefit from using current instructed retrievers over instruction-agnostic models, they may not benefit from using them for long-running exploratory sessions requiring greater sensitivity to instructions.

</details>


### [10] [PRISM: Personalized Recommendation via Information Synergy Module](https://arxiv.org/abs/2601.10944)
*Xinyi Zhang,Yutong Li,Peijie Sun,Letian Sha,Zhongxuan Han*

Main category: cs.IR

TL;DR: PRISM是一个用于多模态序列推荐的即插即用框架，通过信息协同模块将多模态信息分解为独特、冗余和协同组件，并根据用户偏好动态加权融合，实现个性化推荐。


<details>
  <summary>Details</summary>
Motivation: 现有MSR模型往往忽视仅通过模态组合才出现的协同信息，并且通常假设不同模态交互对所有用户具有固定重要性，这限制了推荐效果。

Method: 提出PRISM框架，包含交互专家层将多模态信息分解为独特、冗余和协同组件，以及自适应融合层根据用户偏好动态加权这些组件。

Result: 在四个数据集和三个序列推荐骨干模型上的广泛实验证明了PRISM的有效性和通用性。

Conclusion: PRISM通过信息理论设计实现了多模态信号的细粒度解耦和个性化融合，提升了多模态序列推荐性能。

Abstract: Multimodal sequential recommendation (MSR) leverages diverse item modalities to improve recommendation accuracy, while achieving effective and adaptive fusion remains challenging. Existing MSR models often overlook synergistic information that emerges only through modality combinations. Moreover, they typically assume a fixed importance for different modality interactions across users. To address these limitations, we propose \textbf{P}ersonalized \textbf{R}ecommend-ation via \textbf{I}nformation \textbf{S}ynergy \textbf{M}odule (PRISM), a plug-and-play framework for sequential recommendation (SR). PRISM explicitly decomposes multimodal information into unique, redundant, and synergistic components through an Interaction Expert Layer and dynamically weights them via an Adaptive Fusion Layer guided by user preferences. This information-theoretic design enables fine-grained disentanglement and personalized fusion of multimodal signals. Extensive experiments on four datasets and three SR backbones demonstrate its effectiveness and versatility. The code is available at https://github.com/YutongLi2024/PRISM.

</details>


### [11] [PruneRAG: Confidence-Guided Query Decomposition Trees for Efficient Retrieval-Augmented Generation](https://arxiv.org/abs/2601.11024)
*Shuguang Jiao,Xinyu Xiao,Yunfan Wei,Shuhan Qi,Chengkai Huang,Quan Z. Michael Sheng,Lina Yao*

Main category: cs.IR

TL;DR: PruneRAG是一个基于置信度引导的查询分解框架，通过构建结构化查询分解树来解决RAG系统中的证据遗忘和效率问题，实现稳定高效的推理。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统在推理链加深或搜索树扩展时面临两个持续性问题：证据遗忘（检索到的知识未被有效利用）和效率低下（由不受控的查询扩展和冗余检索引起），这揭示了检索与证据利用之间的关键差距。

Method: 提出PruneRAG框架，包含三个关键机制：1）自适应节点扩展，调节树的宽度和深度；2）置信度引导决策，接受可靠答案并修剪不确定分支；3）细粒度检索，提取实体级锚点以提高检索精度。同时定义了证据遗忘率作为量化指标。

Result: 在多个多跳QA基准测试上的广泛实验表明，PruneRAG在准确性和效率方面均优于最先进的基线方法。

Conclusion: PruneRAG通过结构化查询分解树和置信度引导机制，有效解决了RAG系统中的证据遗忘和效率问题，在多跳推理任务中实现了更好的证据保留和检索效率。

Abstract: Retrieval-augmented generation (RAG) has become a powerful framework for enhancing large language models in knowledge-intensive and reasoning tasks. However, as reasoning chains deepen or search trees expand, RAG systems often face two persistent failures: evidence forgetting, where retrieved knowledge is not effectively used, and inefficiency, caused by uncontrolled query expansions and redundant retrieval. These issues reveal a critical gap between retrieval and evidence utilization in current RAG architectures. We propose PruneRAG, a confidence-guided query decomposition framework that builds a structured query decomposition tree to perform stable and efficient reasoning. PruneRAG introduces three key mechanisms: adaptive node expansion that regulates tree width and depth, confidence-guided decisions that accept reliable answers and prune uncertain branches, and fine-grained retrieval that extracts entity-level anchors to improve retrieval precision. Together, these components preserve salient evidence throughout multi-hop reasoning while significantly reducing retrieval overhead. To better analyze evidence misuse, we define the Evidence Forgetting Rate as a metric to quantify cases where golden evidence is retrieved but not correctly used. Extensive experiments across various multi-hop QA benchmarks show that PruneRAG achieves superior accuracy and efficiency over state-of-the-art baselines.

</details>


### [12] [Learn Before Represent: Bridging Generative and Contrastive Learning for Domain-Specific LLM Embeddings](https://arxiv.org/abs/2601.11124)
*Xiaoyu Liang,Yuchen Peng,Jiale Luo,Wenhao Wang,Haoji Hu,Xincheng Zhou*

Main category: cs.IR

TL;DR: LBR框架通过两阶段学习解决LLM在垂直领域知识不足的问题：先通过信息瓶颈约束的生成学习注入领域知识，再进行生成精炼的对比学习进行对齐。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比学习的LLM在通用表示学习上表现良好，但在化学、法律等垂直领域表现不佳，主要原因是缺乏领域特定知识。当前"LLM+CL"范式只关注语义对齐，无法进行知识获取，导致在专业术语上失败。

Method: 提出Learn Before Represent (LBR)两阶段框架：1) 信息瓶颈约束的生成学习阶段：注入领域知识，保持LLM的因果注意力以最大化知识获取，同时压缩语义；2) 生成精炼的对比学习阶段：在压缩表示上进行对齐学习。该方法保持架构一致性，解决生成学习和对比学习之间的目标冲突。

Result: 在医疗、化学和代码检索任务上的大量实验表明，LBR显著优于强基线方法。

Conclusion: LBR为在垂直领域构建准确和鲁棒的表示建立了一个新范式。

Abstract: Large Language Models (LLMs) adapted via contrastive learning excel in general representation learning but struggle in vertical domains like chemistry and law, primarily due to a lack of domain-specific knowledge. This work identifies a core bottleneck: the prevailing ``LLM+CL'' paradigm focuses on semantic alignment but cannot perform knowledge acquisition, leading to failures on specialized terminology. To bridge this gap, we propose Learn Before Represent (LBR), a novel two-stage framework. LBR first injects domain knowledge via an Information Bottleneck-Constrained Generative Learning stage, preserving the LLM's causal attention to maximize knowledge acquisition while compressing semantics. It then performs Generative-Refined Contrastive Learning on the compressed representations for alignment. This approach maintains architectural consistency and resolves the objective conflict between generative and contrastive learning. Extensive experiments on medical, chemistry, and code retrieval tasks show that LBR significantly outperforms strong baselines. Our work establishes a new paradigm for building accurate and robust representations in vertical domains.

</details>


### [13] [Deep GraphRAG: A Balanced Approach to Hierarchical Retrieval and Adaptive Integration](https://arxiv.org/abs/2601.11144)
*Yuejie Li,Ke Yang,Tao Wang,Bolin Chen,Bowen Li,Chengjun Mao*

Main category: cs.IR

TL;DR: Deep GraphRAG提出分层全局-局部检索策略，通过三阶段检索和动态重排序平衡全面性与效率，并采用动态加权强化学习训练紧凑模型，在问答任务上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有图检索增强生成框架面临全局搜索全面性与局部搜索效率之间的权衡，难以处理大规模分层图、优化检索路径、平衡探索-利用动态，且缺乏鲁棒的多阶段重排序机制。

Method: 提出分层全局-局部检索策略：1) 基于局部上下文进行社区间过滤；2) 通过实体交互分析进行社区级细化；3) 在目标社区内进行实体级细粒度搜索。采用波束搜索优化的动态重排序模块，以及基于动态加权奖励GRPO训练紧凑LLM的知识集成模块。

Result: 在Natural Questions和HotpotQA数据集上的评估表明，Deep GraphRAG在准确性和效率方面显著优于基线图检索方法，紧凑模型(1.5B)在集成任务上接近大模型(70B)的性能。

Conclusion: Deep GraphRAG通过分层检索策略和动态加权强化学习，有效解决了图检索增强生成中的全面性与效率权衡问题，为大规模知识图检索提供了平衡的解决方案。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) frameworks face a trade-off between the comprehensiveness of global search and the efficiency of local search. Existing methods are often challenged by navigating large-scale hierarchical graphs, optimizing retrieval paths, and balancing exploration-exploitation dynamics, frequently lacking robust multi-stage re-ranking. To overcome these deficits, we propose Deep GraphRAG, a framework designed for a balanced approach to hierarchical retrieval and adaptive integration. It introduces a hierarchical global-to-local retrieval strategy that integrates macroscopic inter-community and microscopic intra-community contextual relations. This strategy employs a three-stage process: (1) inter-community filtering, which prunes the search space using local context; (2) community-level refinement, which prioritizes relevant subgraphs via entity-interaction analysis; and (3) entity-level fine-grained search within target communities. A beam search-optimized dynamic re-ranking module guides this process, continuously filtering candidates to balance efficiency and global comprehensiveness. Deep GraphRAG also features a Knowledge Integration Module leveraging a compact LLM, trained with Dynamic Weighting Reward GRPO (DW-GRPO). This novel reinforcement learning approach dynamically adjusts reward weights to balance three key objectives: relevance, faithfulness, and conciseness. This training enables compact models (1.5B) to approach the performance of large models (70B) in the integration task. Evaluations on Natural Questions and HotpotQA demonstrate that Deep GraphRAG significantly outperforms baseline graph retrieval methods in both accuracy and efficiency.

</details>


### [14] [Cross-Modal Attention Network with Dual Graph Learning in Multimodal Recommendation](https://arxiv.org/abs/2601.11151)
*Ji Dai,Quan Fang,Jun Hu,Desheng Cai,Yang Yang,Can Zhao*

Main category: cs.IR

TL;DR: CRANE模型通过递归跨模态注意力机制和对称双图框架，解决了多媒体推荐中浅层模态融合和特征不对称问题，在四个真实数据集上平均提升5%性能。


<details>
  <summary>Details</summary>
Motivation: 现有多媒体推荐系统存在两个关键局限：1) 浅层模态融合通常依赖简单拼接，无法充分利用模态内和模态间的协同关系；2) 特征处理不对称——用户仅通过交互ID表征，而物品受益于丰富的多模态内容，阻碍了共享语义空间的学习。

Method: 提出CRANE模型：1) 递归跨模态注意力机制(RCA)，在联合潜在空间中迭代细化模态特征，捕捉高阶模态依赖；2) 通过聚合用户交互物品的特征显式构建用户多模态档案；3) 对称双图框架（异构用户-物品交互图和同构物品-物品语义图），通过自监督对比学习目标融合行为与语义信号。

Result: 在四个公开真实数据集上的综合实验验证了CRANE的有效性，在关键指标上平均比最先进基线提升5%。理论分析和实证研究表明模型具有高计算效率，在小数据集上收敛更快，在大规模数据集上达到更优性能上限。

Conclusion: CRANE通过递归跨模态注意力机制和对称双图嵌入，有效解决了多媒体推荐中的模态融合和特征不对称问题，实现了性能提升和计算效率的平衡。

Abstract: Multimedia recommendation systems leverage user-item interactions and multimodal information to capture user preferences, enabling more accurate and personalized recommendations. Despite notable advancements, existing approaches still face two critical limitations: first, shallow modality fusion often relies on simple concatenation, failing to exploit rich synergic intra- and inter-modal relationships; second, asymmetric feature treatment-where users are only characterized by interaction IDs while items benefit from rich multimodal content-hinders the learning of a shared semantic space. To address these issues, we propose a Cross-modal Recursive Attention Network with dual graph Embedding (CRANE). To tackle shallow fusion, we design a core Recursive Cross-Modal Attention (RCA) mechanism that iteratively refines modality features based on cross-correlations in a joint latent space, effectively capturing high-order intra- and inter-modal dependencies. For symmetric multimodal learning, we explicitly construct users' multimodal profiles by aggregating features of their interacted items. Furthermore, CRANE integrates a symmetric dual-graph framework-comprising a heterogeneous user-item interaction graph and a homogeneous item-item semantic graph-unified by a self-supervised contrastive learning objective to fuse behavioral and semantic signals. Despite these complex modeling capabilities, CRANE maintains high computational efficiency. Theoretical and empirical analyses confirm its scalability and high practical efficiency, achieving faster convergence on small datasets and superior performance ceilings on large-scale ones. Comprehensive experiments on four public real-world datasets validate an average 5% improvement in key metrics over state-of-the-art baselines.

</details>


### [15] [From Knots to Knobs: Towards Steerable Collaborative Filtering Using Sparse Autoencoders](https://arxiv.org/abs/2601.11182)
*Martin Spišák,Ladislav Peška,Petr Škoda,Vojtěch Vančura,Rodrigo Alves*

Main category: cs.IR

TL;DR: 首次将稀疏自编码器应用于协同过滤，在协同自编码器中插入SAE层，提取可解释特征并实现推荐系统的可控引导


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器在语言模型中已证明能提取高质量可解释特征，但尚未应用于协同过滤领域。本文旨在将SAE方法扩展到基于交互信号的协同过滤表示学习中，提取可解释特征并实现推荐的可控引导。

Method: 在广泛使用的协同自编码器（CFAE）的编码器和解码器之间插入稀疏自编码器层，构建表示映射函数将语义概念与单个神经元关联，并设计简单有效的推荐引导方法。

Result: 证明该表示方法在很大程度上是单语义的，能够提取可解释特征，并验证了利用这种表示向期望方向引导推荐的有效性。

Conclusion: 成功将稀疏自编码器应用于协同过滤，首次在纯交互信号学习的表示中提取可解释特征，为推荐系统的可解释性和可控性提供了新方法。

Abstract: Sparse autoencoders (SAEs) have recently emerged as pivotal tools for introspection into large language models. SAEs can uncover high-quality, interpretable features at different levels of granularity and enable targeted steering of the generation process by selectively activating specific neurons in their latent activations. Our paper is the first to apply this approach to collaborative filtering, aiming to extract similarly interpretable features from representations learned purely from interaction signals. In particular, we focus on a widely adopted class of collaborative autoencoders (CFAEs) and augment them by inserting an SAE between their encoder and decoder networks. We demonstrate that such representation is largely monosemantic and propose suitable mapping functions between semantic concepts and individual neurons. We also evaluate a simple yet effective method that utilizes this representation to steer the recommendations in a desired direction.

</details>


### [16] [LLM-Assisted Pseudo-Relevance Feedback](https://arxiv.org/abs/2601.11238)
*David Otero,Javier Parapar*

Main category: cs.IR

TL;DR: 提出一种结合传统伪相关反馈和LLM语义判断的混合查询扩展方法，通过LLM过滤初始检索结果，仅对相关文档进行RM3扩展，提升效果


<details>
  <summary>Details</summary>
Motivation: 传统伪相关反馈方法（如RM3）容易受到噪声文档影响导致主题漂移，而基于LLM的方法可能产生幻觉或与特定集合术语不匹配。需要一种既保持传统方法鲁棒性又利用LLM语义判断的混合方案

Method: 在RM3估计前加入LLM过滤阶段：LLM对初始top-k文档进行相关性判断，仅对LLM认定为相关的文档进行RM3查询扩展模型估计

Result: 该方法在多个数据集和指标上优于盲目的伪相关反馈和强基线方法，简单干预即带来显著改进

Conclusion: 提出的混合方法结合了传统PRF的鲁棒性和LLM的语义判断能力，有效缓解了主题漂移问题，同时保持了方法的可解释性

Abstract: Query expansion is a long-standing technique to mitigate vocabulary mismatch in ad hoc Information Retrieval. Pseudo-relevance feedback methods, such as RM3, estimate an expanded query model from the top-ranked documents, but remain vulnerable to topic drift when early results include noisy or tangential content. Recent approaches instead prompt Large Language Models to generate synthetic expansions or query variants. While effective, these methods risk hallucinations and misalignment with collection-specific terminology. We propose a hybrid alternative that preserves the robustness and interpretability of classical PRF while leveraging LLM semantic judgement. Our method inserts an LLM-based filtering stage prior to RM3 estimation: the LLM judges the documents in the initial top-$k$ ranking, and RM3 is computed only over those accepted as relevant. This simple intervention improves over blind PRF and a strong baseline across several datasets and metrics.

</details>


### [17] [Rank4Gen: RAG-Preference-Aligned Document Set Selection and Ranking](https://arxiv.org/abs/2601.11273)
*Yongqi Fan,Yuxiang Chu,Zhentao Xia,Xiaoyang Chen,Jie Liu,Haijin Liang,Jin Ma,Ben He,Yingfei Sun,Dezhi Ye,Tong Ruan*

Main category: cs.IR

TL;DR: Rank4Gen是一个为RAG系统设计的生成器感知排序器，通过优化下游响应质量而非查询-文档相关性，并针对不同生成器建模特定偏好，提升RAG性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG中的排序模型主要优化查询-文档相关性，这与生成器对证据选择和引用的偏好存在错位，限制了其对响应质量的影响。同时，大多数方法未考虑不同生成器之间的偏好差异，导致跨生成器性能不稳定。

Method: 提出Rank4Gen，包含两个关键偏好建模策略：1) 从排序相关性到响应质量：基于下游响应质量而非查询-文档相关性优化排序；2) 生成器特定偏好建模：让单个排序器适应不同生成器以捕捉其独特的排序偏好。构建PRISM数据集支持建模。

Result: 在五个具有挑战性的近期RAG基准测试中，Rank4Gen在复杂证据组合方面展现出强大且具有竞争力的性能。

Conclusion: Rank4Gen通过优化生成器感知的排序，解决了RAG中排序模型与生成器偏好错配的问题，提升了跨生成器的稳定性和整体响应质量。

Abstract: In the RAG paradigm, the information retrieval module provides context for generators by retrieving and ranking multiple documents to support the aggregation of evidence. However, existing ranking models are primarily optimized for query--document relevance, which often misaligns with generators' preferences for evidence selection and citation, limiting their impact on response quality. Moreover, most approaches do not account for preference differences across generators, resulting in unstable cross-generator performance. We propose \textbf{Rank4Gen}, a generator-aware ranker for RAG that targets the goal of \emph{Ranking for Generators}. Rank4Gen introduces two key preference modeling strategies: (1) \textbf{From Ranking Relevance to Response Quality}, which optimizes ranking with respect to downstream response quality rather than query--document relevance; and (2) \textbf{Generator-Specific Preference Modeling}, which conditions a single ranker on different generators to capture their distinct ranking preferences. To enable such modeling, we construct \textbf{PRISM}, a dataset built from multiple open-source corpora and diverse downstream generators. Experiments on five challenging and recent RAG benchmarks demonstrate that RRank4Gen achieves strong and competitive performance for complex evidence composition in RAG.

</details>


### [18] [From SERPs to Sound: How Search Engine Result Pages and AI-generated Podcasts Interact to Influence User Attitudes on Controversial Topics](https://arxiv.org/abs/2601.11282)
*Junjie Wang,Gaole He,Alisa Rieger,Ujwal Gadiraju*

Main category: cs.IR

TL;DR: 研究比较搜索引擎结果页(SERPs)与AI生成播客对用户态度的影响，发现信息消费顺序会影响态度改变，观点偏见和话题争议性也起作用，但个人调节因素无显著影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成播客这种相对被动的新型信息消费方式与搜索引擎结果页(SERPs)在日常信息寻求行为中日益融合，需要探索这两种媒介如何相互作用影响用户态度，特别是在涉及有争议、价值负载的话题背景下。

Method: 通过受控用户研究(N=483)，调查用户通过SERPs和AI生成播客消费信息的态度效应，重点关注曝光顺序和模态如何塑造用户意见。

Result: 大多数用户表现出态度改变，发现顺序对态度改变有影响，观点偏见和话题争议程度在塑造态度改变中起作用，但未发现个人调节因素的显著影响。

Conclusion: 信息媒介的交互作用确实影响用户态度形成，特别是在有争议话题上，信息呈现的顺序和模态设计对态度改变具有重要影响，这对信息平台设计有实际意义。

Abstract: Compared to search engine result pages (SERPs), AI-generated podcasts represent a relatively new and relatively more passive modality of information consumption, delivering narratives in a naturally engaging format. As these two media increasingly converge in everyday information-seeking behavior, it is essential to explore how their interaction influences user attitudes, particularly in contexts involving controversial, value-laden, and often debated topics. Addressing this need, we aim to understand how information mediums of present-day SERPs and AI-generated podcasts interact to shape the opinions of users. To this end, through a controlled user study (N=483), we investigated user attitudinal effects of consuming information via SERPs and AI-generated podcasts, focusing on how the sequence and modality of exposure shape user opinions. A majority of users in our study corresponded to attitude change outcomes, and we found an effect of sequence on attitude change. Our results further revealed a role of viewpoint bias and the degree of topic controversiality in shaping attitude change, although we found no effect of individual moderators.

</details>


### [19] [Validating Search Query Simulations: A Taxonomy of Measures](https://arxiv.org/abs/2601.11412)
*Andreas Konstantin Kruff,Nolwenn Bernard,Philipp Schaer*

Main category: cs.IR

TL;DR: 本文综述了用户模拟器在信息检索系统评估中的验证方法，建立了验证措施的分类体系，并通过实证分析验证了该体系，最后提供了不同情境下的验证建议并发布了相关工具库。


<details>
  <summary>Details</summary>
Motivation: 用户模拟器在信息检索系统评估中的有效性验证仍然是一个开放性问题，这限制了其有效使用和基于模拟结果的可靠性。需要解决如何验证模拟用户查询与真实查询之间的一致性问题。

Method: 1. 进行全面的文献综述，重点关注模拟用户查询相对于真实查询的验证方法；2. 基于综述开发验证措施的分类体系；3. 使用四个代表不同搜索场景的数据集，通过分析不同措施之间的关系来实证验证该分类体系。

Result: 建立了用户模拟验证措施的分类体系，并通过实证分析验证了其有效性。提供了在不同情境下应使用哪些措施或措施组合的具体建议。同时发布了包含最常用验证措施的专用库，以促进未来研究。

Conclusion: 本文通过系统化的综述和实证分析，为信息检索中用户模拟器的验证提供了理论框架和实践指导，解决了该领域长期存在的验证可靠性问题，并提供了实用的工具支持。

Abstract: Assessing the validity of user simulators when used for the evaluation of information retrieval systems remains an open question, constraining their effective use and the reliability of simulation-based results. To address this issue, we conduct a comprehensive literature review with a particular focus on methods for the validation of simulated user queries with regard to real queries. Based on the review, we develop a taxonomy that structures the current landscape of available measures. We empirically corroborate the taxonomy by analyzing the relationships between the different measures applied to four different datasets representing diverse search scenarios. Finally, we provide concrete recommendations on which measures or combinations of measures should be considered when validating user simulation in different contexts. Furthermore, we release a dedicated library with the most commonly used measures to facilitate future research.

</details>


### [20] [Isotropy-Optimized Contrastive Learning for Semantic Course Recommendation](https://arxiv.org/abs/2601.11427)
*Ali Khreis,Anthony Nasr,Yusuf Hilal*

Main category: cs.IR

TL;DR: 基于BERT的自监督对比学习语义课程推荐系统，通过数据增强和各向同性正则化解决传统BERT嵌入的各向异性问题，提升课程推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 传统BERT嵌入存在各向异性表示空间问题，课程描述无论语义相关性如何都表现出高余弦相似度，这限制了课程推荐系统的准确性和区分能力。

Method: 提出基于BERT的自监督对比学习框架，结合数据增强和各向同性正则化技术，处理学生文本查询并从500多门工程课程数据集中推荐Top-N相关课程。

Result: 实验结果表明，微调后的模型相比原始BERT基线，实现了更好的嵌入分离效果和更准确的课程推荐。

Conclusion: 提出的对比学习框架有效解决了BERT嵌入的各向异性问题，能够生成更具区分性的嵌入表示，从而提升课程推荐系统的性能。

Abstract: This paper presents a semantic course recommendation system for students using a self-supervised contrastive learning approach built upon BERT (Bidirectional Encoder Representations from Transformers). Traditional BERT embeddings suffer from anisotropic representation spaces, where course descriptions exhibit high cosine similarities regardless of semantic relevance. To address this limitation, we propose a contrastive learning framework with data augmentation and isotropy regularization that produces more discriminative embeddings. Our system processes student text queries and recommends Top-N relevant courses from a curated dataset of over 500 engineering courses across multiple faculties. Experimental results demonstrate that our fine-tuned model achieves improved embedding separation and more accurate course recommendations compared to vanilla BERT baselines.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [21] [On the Entropy of a Random Geometric Graph](https://arxiv.org/abs/2601.10778)
*Praneeth Kumar Vippathalla,Justin P. Coon,Mihai-Alin Badiu*

Main category: cs.IT

TL;DR: 研究了硬随机几何图（RGG）的熵，推导了在单位立方体和单位环面上不同连接半径下的熵上界，并在某些情况下获得了精确的渐近特征。


<details>
  <summary>Details</summary>
Motivation: 随机几何图是空间网络的常用模型，但其熵特性尚未得到充分研究。理解RGG的熵对于量化网络结构的不确定性和信息内容具有重要意义。

Method: 考虑两种空间域：d维单位立方体[0,1]^d和d维单位环面𝕋^d。通过分析硬RGG模型（节点随机分布，距离小于r时连接），推导了所有可能r值下的熵上界，并在某些情况下证明了紧下界。

Result: 主要结果：1) 在𝕋^d上，当0<r≤1/4时，H(G_m)∼dm log₂m；2) 在一维[0,1]上，对所有0<r<1，熵行为类似m log m。由此推断𝕋^d上未标记RGG的结构熵为Ω((d-1)m log₂m)。

Conclusion: 该研究为随机几何图的熵特性提供了理论分析框架，揭示了不同空间域和连接半径下的熵渐近行为，为理解空间网络的信息内容奠定了基础。

Abstract: In this paper, we study the entropy of a hard random geometric graph (RGG), a commonly used model for spatial networks, where the connectivity is governed by the distances between the nodes. Formally, given a connection range $r$, a hard RGG $G_m$ on $m$ vertices is formed by drawing $m$ random points from a spatial domain, and then connecting any two points with an edge when they are within a distance $r$ from each other. The two domains we consider are the $d$-dimensional unit cube $[0,1]^d$ and the $d$-dimensional unit torus $\mathbb{T}^d$. We derive upper bounds on the entropy $H(G_m)$ for both these domains and for all possible values of $r$. In a few cases, we obtain an exact asymptotic characterization of the entropy by proving a tight lower bound. Our main results are that $H(G_m) \sim dm \log_2m$ for $0 < r \leq 1/4$ in the case of $\mathbb{T}^d$ and that the entropy of a one-dimensional RGG on $[0,1]$ behaves like $m\log m$ for all $0<r<1$. As a consequence, we can infer that the asymptotic structural entropy of an RGG on $\mathbb{T}^d$, which is the entropy of an unlabelled RGG, is $Ω((d-1)m \log_2m)$ for $0 < r \leq 1/4$. For the rest of the cases, we conjecture that the entropy behaves asymptotically as the leading order terms of our derived upper bounds.

</details>


### [22] [Efficient LLR-Domain Decoding of ABS+ Polar Codes](https://arxiv.org/abs/2601.10808)
*Mikhail Chernikov,Peter Trifonov*

Main category: cs.IT

TL;DR: ABS+ polar codes的LLR域SCL解码器实现与优化，相比经典polar codes在高信噪比区域获得相同FER时算术运算更少


<details>
  <summary>Details</summary>
Motivation: ABS+ polar codes作为Arikan polar codes的推广，具有更快的极化速度，但需要高效的解码器实现来充分利用这一优势

Method: 提出了ABS+ polar codes的LLR域SCL解码器实现，并优化了SCL算法以降低LLR计算的复杂度要求

Result: 与经典polar codes相比，所提方法在高信噪比区域获得相同帧错误率时，SCL解码器所需的算术运算数量更少

Conclusion: ABS+ polar codes的LLR域SCL解码器优化方案有效降低了计算复杂度，为快速极化码的实际应用提供了高效解码方案

Abstract: ABS+ polar codes are a generalization of Arikan polar codes that provides much faster polarization. We present an LLR-domain implementation of the SCL decoder of ABS+ polar codes. Furthermore, we optimize the SCL algorithm in order to reduce the complexity requirements for the LLRs computation. In comparison with classical polar codes, the proposed approach requires less number of arithmetic operations in the SCL decoder to obtain the fixed frame error rate (FER) at high-SNR region.

</details>


### [23] [A Differential Geometry and Algebraic Topology Based Public-Key Cryptographic Algorithm in Presence of Quantum Adversaries](https://arxiv.org/abs/2601.10883)
*Andrea Rondelli*

Main category: cs.IT

TL;DR: Z-Sigil是一种基于功能分析、微分几何和代数拓扑的非对称公钥密码算法，旨在抵抗经典和量子攻击，通过在紧致Calabi-Yau流形的切纤维丛上操作实现。


<details>
  <summary>Details</summary>
Motivation: 在量子计算机、经典超级计算机和日益复杂的人工智能时代，延续古代印章所代表的信任、保密和完整性传统，开发能够抵抗经典和量子攻击的新型密码系统。

Method: 在紧致Calabi-Yau流形的切纤维丛上构建密码系统，密钥是切纤维向量元素，在基流形切空间上定义二元运算形成广群结构。加密解密在消息块上迭代执行，采用串行架构限制量子并行性，每个块依赖于秘密几何和分析数据。

Result: 算法在分析上证明了正确性和可逆性，任何无私钥的明文恢复尝试都会导致对抗搜索空间的指数增长，即使存在量子加速。该方法与基于离散代数假设的现有量子安全密码方案有本质区别。

Conclusion: Z-Sigil提供了一种基于连续几何结构、非线性算子组合和强制块串行化的新型量子安全密码方法，延续了印章的信任传统，为后量子密码学开辟了新方向。

Abstract: In antiquity, the seal embodied trust, secrecy, and integrity in safeguarding the exchange of letters and messages. The purpose of this work is to continue this tradition in the contemporary era, characterized by the presence of quantum computers, classical supercomputers, and increasingly sophisticated artificial intelligence. We introduce Z-Sigil, an asymmetric public-key cryptographic algorithm grounded in functional analysis, differential geometry, and algebraic topology, with the explicit goal of achieving resistance against both classical and quantum attacks. The construction operates over the tangent fiber bundle of a compact Calabi-Yau manifold [13], where cryptographic keys are elements of vector tangent fibers, with a binary operation defined on tangent spaces of the base manifold giving rise to a groupoid structure. Encryption and decryption are performed iteratively on message blocks, enforcing a serial architecture designed to limit quantum parallelism [9,10]. Each block depends on secret geometric and analytic data, including a randomly chosen base point on the manifold, a selected section of the tangent fiber bundle, and auxiliary analytic data derived from operator determinants and Zeta function regularization [11]. The correctness and invertibility of the proposed algorithm are proven analytically. Furthermore, any adversarial attempt to recover the plaintext without the private key leads to an exponential growth of the adversarial search space,even under quantum speedups. The use of continuous geometric structures,non-linear operator compositions,and enforced blockwise serialization distinguishes this approach from existing quantum-safe cryptographic proposals based on primary discrete algebraic assumptions.

</details>


### [24] [A PAC-Bayesian Analysis of Channel-Induced Degradation in Edge Inference](https://arxiv.org/abs/2601.10915)
*Yangshuo He,Guanding Yu,Jingge Zhu*

Main category: cs.IT

TL;DR: 提出一种针对边缘推理中无线信道噪声影响的神经网络训练方法，通过理论分析信道引起的性能恶化，并设计信道感知训练算法提升推理精度。


<details>
  <summary>Details</summary>
Motivation: 边缘推理中神经网络通常部署在分布式边缘设备上，通过无线传输协作执行推理。然而，标准神经网络在无噪声环境中训练，与边缘部署时的噪声信道环境存在不匹配，导致性能下降。

Method: 1. 将信道引起的性能恶化建模为对未见信道的泛化误差；2. 提出增强神经网络模型，将信道统计直接纳入权重空间；3. 推导PAC-Bayesian泛化界，显式量化无线失真的影响；4. 针对实际信道提供闭式表达式；5. 基于理论结果提出信道感知训练算法，最小化基于推导界的代理目标。

Result: 仿真结果表明，所提出的算法能够有效利用信道统计信息提高推理精度，无需端到端重新训练。

Conclusion: 该研究解决了边缘推理中信道噪声与神经网络训练环境不匹配的问题，通过理论分析和算法设计，实现了信道感知的神经网络优化，提升了无线环境下的推理性能。

Abstract: In the emerging paradigm of edge inference, neural networks (NNs) are partitioned across distributed edge devices that collaboratively perform inference via wireless transmission. However, standard NNs are generally trained in a noiseless environment, creating a mismatch with the noisy channels during edge deployment. In this paper, we address this issue by characterizing the channel-induced performance deterioration as a generalization error against unseen channels. We introduce an augmented NN model that incorporates channel statistics directly into the weight space, allowing us to derive PAC-Bayesian generalization bounds that explicitly quantifies the impact of wireless distortion. We further provide closed-form expressions for practical channels to demonstrate the tractability of these bounds. Inspired by the theoretical results, we propose a channel-aware training algorithm that minimizes a surrogate objective based on the derived bound. Simulations show that the proposed algorithm can effectively improve inference accuracy by leveraging channel statistics, without end-to-end re-training.

</details>


### [25] [Fundamental Limits of Quantum Semantic Communication via Sheaf Cohomology](https://arxiv.org/abs/2601.10958)
*Christo Kurisummoottil Thomas,Mingzhe Chen*

Main category: cs.IT

TL;DR: 该论文提出了一个量子语义通信的信息论框架，使用层上同调建模多智能体语义网络，证明第一层上同调群表征了不可约的语义模糊性，建立了语义对齐所需的最小通信速率与上同调空间维度的对数关系，并展示了量子纠缠如何超越经典界限。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，当智能体采用异构的感知模态和AI架构时，完美的比特级传输无法保证相互理解。虽然语义压缩的深度学习方法已有进展，但异构条件下的语义对齐的信息论极限仍不清楚。语义模糊性与量子上下文性具有相同的数学结构，都源于上同调障碍，这促使了量子语义通信的提出。

Method: 使用层上同调理论构建量子语义通信的信息论框架。将多智能体语义网络建模为量子层，其中智能体的意义空间是希尔伯特空间，通过量子信道连接。证明第一层上同调群表征不可约的语义模糊性，建立语义对齐所需的最小通信速率与上同调空间维度的对数关系。

Result: 1) 语义对齐所需的最小通信速率与上同调空间维数的对数成比例，建立了语义版的香农极限；2) 对于纠缠辅助信道，可实现容量严格超过经典界限，每个共享的纠缠比特减少一个比特的经典通信需求；3) 量子上下文性可减少上同调障碍；4) 建立了量子不和谐与集成语义信息之间的对偶关系。

Conclusion: 该框架为自主多智能体系统中的量子增强语义通信提供了严格的理论基础，展示了量子资源如何超越经典语义通信的极限，并为共享上下文提供了严格的解释。

Abstract: Semantic communication (SC) enables bandwidth-efficient coordination in multi-agent systems by transmitting meaning rather than raw bits. However, when agents employ heterogeneous sensing modalities and AI architectures, perfect bit-level transmission no longer guarantees mutual understanding. Although deep learning methods for semantic compression have advanced, the information-theoretic limits of semantic alignment under heterogeneity remain poorly understood. Notably, semantic ambiguity shares the same mathematical structure as quantum contextuality, as both arise from cohomological obstructions, motivating a quantum formulation of SC. In this paper, an information-theoretic framework for quantum semantic communication is proposed using sheaf cohomology. Multi-agent semantic networks are modeled as quantum sheaves, where agents meaning spaces are Hilbert spaces connected by quantum channels. The first sheaf cohomology group is shown to characterize irreducible semantic ambiguity, representing a fundamental obstruction to alignment that no local processing can resolve. The minimum communication rate required for semantic alignment is proven to scale with the logarithm of the dimension of the cohomological space, establishing a semantic analog of Shannon limits. For entanglement-assisted channels, the achievable capacity is shown to strictly exceed classical bounds, with each shared ebit reducing the required classical communication by one bit, providing a rigorous interpretation of shared context. Additionally, quantum contextuality is shown to reduce cohomological obstructions, and a duality between quantum discord and integrated semantic information is established, linking quantum correlations to irreducible semantic content. This framework provides rigorous foundations for quantum-enhanced semantic communication in autonomous multi-agent systems.

</details>


### [26] [Asymmetric Encoding-Decoding Schemes for Lossless Data Compression](https://arxiv.org/abs/2601.10991)
*Hirosuke Yamamoto,Ken-ichi Iwata*

Main category: cs.IT

TL;DR: 本文提出了一种名为非对称编解码方案(AEDS)的新型无损数据压缩编码方案，它是tANS的推广，具有更广的编码类别，在某些条件下能获得比霍夫曼编码更短的平均码长。


<details>
  <summary>Details</summary>
Motivation: 现有的tANS编码虽然高效，但其编码类别有限。本文旨在提出一种更通用的编码方案，扩展tANS的编码能力，同时保持其高效性，在某些情况下甚至能超越霍夫曼编码的性能。

Method: 提出AEDS方案，数据序列按反向顺序编码(s_t, t=n,...,1)，但按正向顺序解码(s_t, t=1,...,n)，这与tANS相同但编码类别更广。分析了AEDS与tANS的关系，推导了平均码长的上界，并研究了状态数对性能的影响。

Result: 对于i.i.d.源，当霍夫曼编码树根节点的子节点概率权重大于0.61803时，2状态AEDS的平均码长比霍夫曼编码更短；当大于0.56984时，5状态AEDS的平均码长更短。最优AEDS和tANS的平均码长随状态数N增加以O(1/N)速度收敛到源熵。

Conclusion: AEDS是tANS的有效推广，具有更广的编码类别，在某些条件下能超越霍夫曼编码的性能。随着状态数增加，AEDS和tANS都能以O(1/N)的速度逼近熵界，为高效无损压缩提供了新的理论框架。

Abstract: This paper proposes a new lossless data compression coding scheme named an asymmetric encoding-decoding scheme (AEDS), which can be considered as a generalization of tANS (tabled variant of asymmetric numeral systems). In the AEDS, a data sequence $\bm{s}=s_1s_2\cdots s_n$ is encoded in backward order $s_t, t=n, \cdots, 2,1$, while $\bm{s}$ is decoded in forward order $s_t, t=1, 2, \cdots, n$ in the same way as the tANS. But, the code class of the AEDS is much broader than that of the tANS. We show for i.i.d.~sources that an AEDS with 2 states (resp.~5 states) can attain a shorter average code length than the Huffman code if a child of the root in the Huffman code tree has a probability weight larger than 0.61803 (resp.~0.56984). Furthermore, we derive several upper bounds on the average code length of the AEDS, which also hold for the tANS, and we show that the average code length of the optimal AEDS and tANS with $N$ states converges to the source entropy with speed $O(1/N)$ as $N$ increases.

</details>


### [27] [PEMNet: Towards Autonomous and Enhanced Environment-Aware Mobile Networks](https://arxiv.org/abs/2601.11025)
*Lei Li,Yanqing Xu,Ye Xue,Feng Yin,Chao Shen,Rui Zhang,Tsung-Hui Chang*

Main category: cs.IT

TL;DR: 提出感知嵌入地图(PEM)，将无线信道统计与时空流量模式联合嵌入，为5G/6G网络提供低开销的环境感知优化支持


<details>
  <summary>Details</summary>
Motivation: 5G/6G网络需要在严格延迟、能耗和频谱约束下做出动态决策，这依赖于对无线信道和流量需求时空变化的先验知识，需要一种可查询的联合表示方法

Method: 构建感知嵌入地图(PEM)，将细粒度信道统计与基站覆盖范围内的网格级时空流量模式联合嵌入，使用标准兼容测量数据（如测量报告、调度/QoS日志）构建

Result: PEM支持跨PHY、MAC和网络层的环境感知优化，显著减少训练开销和信令，相比现有站点特定信道地图和数字孪生副本更具实用性

Conclusion: PEM通过联合信道-流量嵌入和基于标准测量的实际构建，实现了网络自主性并在保真度与成本间取得良好平衡，为5G/6G网络优化提供实用框架

Abstract: With 5G deployment and the evolution toward 6G, mobile networks must make decisions in highly dynamic environments under strict latency, energy, and spectrum constraints. Achieving this goal, however, depends on prior knowledge of spatial-temporal variations in wireless channels and traffic demands. This motivates a joint, site-specific representation of radio propagation and user demand that is queryable at low online overhead. In this work, we propose the perception embedding map (PEM), a localized framework that embeds fine-grained channel statistics together with grid-level spatial-temporal traffic patterns over a base station's coverage. PEM is built from standard-compliant measurements -- such as measurement report and scheduling/quality-of-service logs -- so it can be deployed and maintained at scale with low cost. Integrated into PEM, this joint knowledge supports enhanced environment-aware optimization across PHY, MAC, and network layers while substantially reducing training overhead and signaling. Compared with existing site-specific channel maps and digital-twin replicas, PEM distinctively emphasizes (i) joint channel-traffic embedding, which is essential for network optimization, and (ii) practical construction using standard measurements, enabling network autonomy while striking a favorable fidelity-cost balance.

</details>


### [28] [Sensing Mutual Information for Communication Signal with Deterministic Pilots and Random Data Payloads](https://arxiv.org/abs/2601.11149)
*Lei Xie,Hengtao He,Jun Tong,Fan Liu,Shenghui Song*

Main category: cs.IT

TL;DR: 本文研究了采用混合信号（确定性导频+随机数据载荷）的ISAC系统的感知互信息与预编码设计，提出了闭式表达式和优化算法。


<details>
  <summary>Details</summary>
Motivation: 现有ISAC研究主要关注纯随机或纯确定性波形，忽略了实际通信标准中普遍采用的混合结构（导频+数据载荷），需要填补这一理论空白。

Method: 利用随机矩阵理论推导混合信号感知互信息的闭式表达式，基于此构建预编码优化问题，采用ADMM框架高效求解。

Result: 仿真验证了理论结果的准确性，并证明所提预编码设计在感知性能上优于传统基准方法。

Conclusion: 本文为采用混合通信信号的ISAC系统提供了理论基础和优化设计，填补了现有研究的空白，具有重要的理论和实际意义。

Abstract: The recent emergence of the integrated sensing and communication (ISAC) framework has sparked significant interest in quantifying the sensing capabilities inherent in communication signals. However, existing literature has mainly focused on scenarios involving either purely random or purely deterministic waveforms. This overlooks a critical reality: operational communication standards invariably utilize a hybrid structure comprising both deterministic pilots for channel estimation and random payloads for data transmission. To bridge this gap, this paper investigates the sensing mutual information (SMI) and precoding design specifically for ISAC systems employing communication signals with both pilots and data payloads. First, by utilizing random matrix theory (RMT), we derive a tractable closed-form expression for the SMI that accurately accounts for the statistical properties of the hybrid signal. Building upon this theoretical foundation, we formulate a precoding optimization problem to maximize SMI with constraints on the transmit power and communication rate, which is solved via an efficient alternating direction method of multipliers framework. Simulation results validate the accuracy of the theoretical results and demonstrate the superiority of the proposed precoding design over conventional benchmarks.

</details>


### [29] [Performance Analysis of Cell-Free Massive MIMO under Imperfect LoS Phase Tracking](https://arxiv.org/abs/2601.11179)
*Noor Ul Ain,Lorenzo Miretti,Renato L. G. Cavalcante,Sławomir Stańczak*

Main category: cs.IT

TL;DR: 该论文研究了非完美视距相位跟踪对无小区大规模MIMO网络性能的影响，提出了包含相位估计误差的Rician衰落模型和线性MMSE信道估计器，并推导了集中式和分布式MMSE波束成形器。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常假设视距相位完全已知或完全未知，但实际系统中由于硬件损伤、移动性和同步误差等因素，相位估计存在残余不确定性。需要研究这种非完美相位跟踪对网络性能的实际影响。

Method: 1. 提出包含相位估计误差的Rician衰落模型，其中视距分量由不完美的相位估计旋转并由确定性相位误差惩罚因子衰减；2. 推导考虑统计相位误差的线性MMSE信道估计器；3. 引入保留信道估计二阶统计特性的虚拟上行链路模型；4. 推导可处理的集中式和分布式MMSE波束成形器。

Result: 1. 提出的框架在完美相位知识和无相位知识两种极端情况之间建立了桥梁；2. 数值结果表明该框架能够连接理想化假设和实际跟踪限制；3. 为6G无小区网络提供了严格的性能基准和设计见解。

Conclusion: 该研究填补了无小区大规模MIMO网络中非完美相位跟踪的理论空白，提出的统一框架能够更准确地评估实际系统性能，为6G网络设计提供了重要参考。

Abstract: We study the impact of imperfect line-of-sight (LoS) phase tracking on the performance of cell-free massive MIMO networks. Unlike prior works that assume perfectly known or completely unknown phases, we consider a realistic regime where LoS phases are estimated with residual uncertainty due to hardware impairments, mobility, and synchronization errors. To this end, we propose a Rician fading model where LoS components are rotated by imperfect phase estimates and attenuated by a deterministic phase-error penalty factor. We derive a linear MMSE channel estimator that captures statistical phase errors and unifies prior results, reducing to the Bayesian MMSE estimator with perfect phase knowledge and to a zero-mean model in the absence of phase knowledge. To address the non-Gaussian setting, we introduce a virtual uplink model that preserves second-order statistics of channel estimation, enabling the derivation of tractable centralized and distributed MMSE beamformers. To ensure fair assessment of the network performance, we apply these beamformers to the true uplink model and compute the spectral efficiency bounds available in the literature. Numerical results show that our framework bridges idealized assumptions and practical tracking limitations, providing rigorous performance benchmarks and design insights for 6G cell-free networks.

</details>


### [30] [Rate-Distortion-Perception Tradeoff for the Gray-Wyner Problem](https://arxiv.org/abs/2601.11257)
*Yu Yang,Yingxin Zhang,Weijie Yuan,Lin Zhou*

Main category: cs.IT

TL;DR: 本文扩展了Gray-Wyner有损信源编码问题，首次推导了在重建序列上施加感知约束时的一阶渐近最优率失真感知区域，将点对点系统的结果推广到多终端设置。


<details>
  <summary>Details</summary>
Motivation: 图像和视频压缩等实际应用中，不仅需要控制重建信号的失真，还需要保证重建序列的分布与原始源序列相近（感知质量）。先前研究主要关注单源序列的压缩和重建，本文旨在将点对点系统的结果推广到Gray-Wyner问题的多终端设置，分析两个相关源序列在失真和感知双重约束下的最优性能。

Method: 通过引入随机循环移位算子直接集成到编码和解码过程中，将失真和感知约束的分析相结合。推导了由共同信息和两个条件率失真感知函数控制的互信息项，建立了最优权衡关系。

Result: 首次推导了Gray-Wyner有损信源编码问题在感知约束下的一阶渐近最优率失真感知区域，揭示了最优权衡由涉及共同信息和条件率失真感知函数的互信息项控制。

Conclusion: 本文成功将点对点系统的率失真感知分析扩展到Gray-Wyner多终端设置，为相关源序列在失真和感知双重约束下的压缩提供了理论框架，对图像和视频压缩等实际应用具有重要意义。

Abstract: We revisit the Gray-Wyner lossy source coding problem and derive the first-order asymptotic optimal rate-distortion-perception region when additional perception constraints are imposed on reproduced source sequences. The optimal trade-off is shown to be governed by a mutual information term involving common information and two conditional rate-distortion-perception functions. The perception constraint requires that the distribution of each reproduced sequence is close to that of the original source sequence, which is motivated by practical applications in image and video compression. Prior studies usually focus on the compression and reconstruction of a single source sequence. In this paper, we generalize the prior results for point-to-point systems to the representative multi-terminal setting of the Gray-Wyner problem with two correlated source sequences. In particular, we integrate the analyses of the distortion and the perception constraints by including the random circular shift operator in the encoding and decoding process directly.

</details>


### [31] [Joint Antenna Rotation and IRS Beamforming for Multi-User Uplink Communications](https://arxiv.org/abs/2601.11291)
*Guoying Zhang,Qingqing Wu,Ziyuan Zheng,Qiaoyan Peng,Yanze Zhu,Wen Chen,Penghui Huang*

Main category: cs.IT

TL;DR: 提出了一种结合可旋转天线和智能反射面的新型多用户上行系统，通过联合优化天线3D旋转、接收波束成形和IRS相位偏移来最大化和速率。


<details>
  <summary>Details</summary>
Motivation: 传统可旋转天线在物理遮挡下性能下降，而智能反射面在基站天线旁瓣区域部署时存在角度失配问题。需要一种新系统来解决这些限制。

Method: 提出RA-enabled IRS辅助多用户上行系统，基站天线可灵活调整3D方向以对准IRS。采用交替优化算法：通过投影梯度上升更新天线旋转，闭式解计算接收波束成形，分数规划优化IRS相位偏移。

Result: 数值结果表明，与传统固定天线系统相比，所提系统在角度失配较大时能带来显著的性能增益。

Conclusion: 结合可旋转天线和智能反射面的系统能有效解决角度失配问题，显著提升无线覆盖性能，特别是在大角度失配场景下。

Abstract: Rotatable antenna (RA) enhances wireless coverage through directional gain steering, yet suffers from performance degradation under physical blockages. Intelligent reflecting surface (IRS) establishes reflective paths to bypass obstacles, but suffers from angular mismatch when deployed in the side-lobe region of base station (BS) antennas. To address this issue, we propose a new RA-enabled IRS-assisted multi-user uplink system, in which the BS antennas are capable of flexibly adjusting their 3D orientations to align their boresights with the IRS. We formulate a sum rate maximization problem by jointly optimizing the antenna 3D rotations, receive beamforming and IRS phase shifts. To tackle this non-convex problem, we propose an efficient alternating optimization (AO) algorithm. Specifically, we iteratively update the antenna rotations via projected gradient ascent (PGA), compute the receive beamforming via a closed-form solution, and optimize the IRS phase shifts via fractional programming (FP). Numerical results demonstrate that the proposed system yields significant performance gains over conventional fixed-antenna systems, especially under large angular misalignments.

</details>


### [32] [Information Theoretic Perspective on Representation Learning](https://arxiv.org/abs/2601.11334)
*Deborah Pereg*

Main category: cs.IT

TL;DR: 提出信息论框架分析回归任务中最后一层嵌入表示，定义表示率并推导输入输出信息可靠表示的极限，进一步定义扰动设置下的表示容量和压缩输出的表示率失真


<details>
  <summary>Details</summary>
Motivation: 需要理论框架来分析回归任务中最后一层嵌入表示的信息论特性，理解输入输出信息在表示中的可靠性和容量限制

Method: 引入信息论框架，定义表示率、表示容量和表示率失真，推导可实现容量和表示率的界限及其逆定理

Result: 建立了输入源熵决定的输入输出信息可靠表示的理论极限，推导了可实现容量和表示率的界限，并在统一设置中整合结果

Conclusion: 该信息论框架为分析回归任务中的表示学习提供了理论基础，揭示了表示可靠性和容量的根本限制

Abstract: An information-theoretic framework is introduced to analyze last-layer embedding, focusing on learned representations for regression tasks. We define representation-rate and derive limits on the reliability with which input-output information can be represented as is inherently determined by the input-source entropy. We further define representation capacity in a perturbed setting, and representation rate-distortion for a compressed output. We derive the achievable capacity, the achievable representation-rate, and their converse. Finally, we combine the results in a unified setting.

</details>


### [33] [Polar Orbit Decoding: Universal Parallel Soft Decoding via Automorphism Orbits](https://arxiv.org/abs/2601.11373)
*Pin-Jing Li,Yu-Chih Huang*

Main category: cs.IT

TL;DR: 提出Polar Orbit Decoding (POD)框架，通过并行解码二进制线性分组码的自同构轨道，实现延迟与性能的权衡优化，无需重新调整冻结集或额外搜索。


<details>
  <summary>Details</summary>
Motivation: 现有二进制线性分组码没有单一码族能同时优化所有性能，导致标准中广泛使用多码架构，增加了硬件复杂度。虽然已有基于极化变换的通用解码框架，但其并行化尚未讨论。

Method: 提出Polar Orbit Decoding (POD)框架，利用二进制线性分组码的自同构生成置换轨道，在极化变换后产生具有相同动态冻结约束的多样化解码轨迹。通过并行解码自同构轨道，实现延迟与性能的权衡。使用Schreier-Sims算法以基和强生成集形式表示自同构群，实现多项式时间内的离线系统计算。

Result: 在扩展BCH码和扩展Golay码上的仿真结果表明，POD能够实现最大似然性能，同时相比传统的连续消除列表解码显著降低解码延迟。

Conclusion: POD为二进制线性分组码提供了一个通用的并行解码框架，通过利用自同构轨道实现高效的延迟-性能权衡，为现代通信系统提供了有前景的解决方案。

Abstract: Binary linear block codes (BLBCs) form the foundation of modern communication systems, yet no single code family simultaneously optimizes all performance aspects. This leads to the widely used multi-code architecture in the standard, significantly increasing the hardware complexity since multiple decoders are required in each piece of equipment. A universal decoding framework based on polar transformations has recently been proposed to unify BLBC decoding under polar-style decoders, but its parallelization has not yet been discussed. In this work, we propose Polar Orbit Decoding (POD), a universal parallel decoding framework for BLBCs. We identify that the automorphisms of BLBCs generate an orbit of permutations that induce diverse decoding trajectories with identical dynamic-frozen constraints after the polar transformations. By decoding over this automorphism orbit in parallel, POD achieves substantial latency-performance tradeoffs without requiring frozen-set readaptation or extra exhaustive permutation searches. Moreover, to enable efficient orbit traversal in the implementation, we represent the automorphism group in a base and strong generating set (BSGS) form using Schreier-Sims algorithms, making offline systematic computation accessible in polynomial time. Simulation results on extended BCH and extended Golay codes demonstrate that POD can achieve maximum-likelihood performance while significantly reducing the decoding latency compared to conventional successive cancellation list decoding.

</details>


### [34] [Efficient Channel Autoencoders for Wideband Communications leveraging Walsh-Hadamard interleaving](https://arxiv.org/abs/2601.11407)
*Cel Thys,Rodney Martinez Alonso,Sofie Pollin*

Main category: cs.IT

TL;DR: 该论文提出了一种基于Walsh-Hadamard变换的端到端信道自动编码器系统，通过硬件友好的WH交织转换器实现高能效的宽带通信。


<details>
  <summary>Details</summary>
Motivation: 宽带通信需要高采样率的模数转换，但传统方法功耗大。Walsh-Hadamard交织转换器能以较低功耗实现高采样率转换，但需要设计能适应这种硬件特性的通信算法。

Method: 提出WH域自动编码器系统，使用端到端训练使神经编码调制能透明适应WH收发器硬件，无需算法重新设计。在短块长度下训练WH-AE，并与标准神经基线和传统基线（包括5G Polar码）进行对比。

Result: WH-AE系统能在0.14dB内接近传统Polar码的SNR性能，同时消耗相当或更低的系统功率。相比最佳神经基线，WH-AE平均实现29%更高的能效（bit/J）。

Conclusion: WH域学习通过显式平衡计算复杂度、SNR和模拟功耗，为高能效、高吞吐宽带通信提供了可行路径。

Abstract: This paper investigates how end-to-end (E2E) channel autoencoders (AEs) can achieve energy-efficient wideband communications by leveraging Walsh-Hadamard (WH) interleaved converters. WH interleaving enables high sampling rate analog-digital conversion with reduced power consumption using an analog WH transformation. We demonstrate that E2E-trained neural coded modulation can transparently adapt to the WH-transceiver hardware without requiring algorithmic redesign. Focusing on the short block length regime, we train WH-domain AEs and benchmark them against standard neural and conventional baselines, including 5G Polar codes. We quantify the system-level energy tradeoffs among baseband compute, channel signal-to-noise ratio (SNR), and analog converter power. Our analysis shows that the proposed WH-AE system can approach conventional Polar code SNR performance within 0.14dB while consuming comparable or lower system power. Compared to the best neural baseline, WH-AE achieves, on average, 29% higher energy efficiency (in bit/J) for the same reliability. These findings establish WH-domain learning as a viable path to energy-efficient, high-throughput wideband communications by explicitly balancing compute complexity, SNR, and analog power consumption.

</details>


### [35] [Convergence Properties of Good Quantum Codes for Classical Communication](https://arxiv.org/abs/2601.11498)
*Alptug Aytekin,Mohamed Nomeir,Lei Hu,Sennur Ulukus*

Main category: cs.IT

TL;DR: 将经典信道容量理论中关于好码输出统计特性的结果推广到量子信道中的经典通信场景


<details>
  <summary>Details</summary>
Motivation: 经典信息论中已有关于达到容量的码的输出统计特性研究，包括码的输出分布与信道容量问题中最优输入诱导的输出分布的比较。本文旨在将类似结果推广到量子信道中的经典通信场景。

Method: 1. 首先证明最优输出分布的唯一性；2. 使用接近经典方法的技术将渐近误差概率结果推广到量子情况；3. 利用基于量子广义去极化半群的超压缩性结果的二阶逆定理，将非渐近误差概率结果推广到量子分组码。

Result: 成功将经典信息论中关于码输出统计特性的结果扩展到量子信道中的经典通信场景，包括渐近和非渐近误差概率情况下的输出分布特性。

Conclusion: 本文建立了量子信道中经典通信码的输出统计特性理论框架，为量子信息论中的容量实现问题提供了新的分析工具和结果。

Abstract: An important part of the information theory folklore had been about the output statistics of codes that achieve the capacity and how the empirical distributions compare to the output distributions induced by the optimal input in the channel capacity problem. Results for a variety of such empirical output distributions of good codes have been known in the literature, such as the comparison of the output distribution of the code to the optimal output distribution in vanishing and non-vanishing error probability cases. Motivated by these, we aim to achieve similar results for the quantum codes that are used for classical communication, that is the setting in which the classical messages are communicated through quantum codewords that pass through a noisy quantum channel. We first show the uniqueness of the optimal output distribution, to be able to talk more concretely about the optimal output distribution. Then, we extend the vanishing error probability results to the quantum case, by using techniques that are close in spirit to the classical case. We also extend non-vanishing error probability results to the quantum case on block codes, by using the second-order converses for such codes based on hypercontractivity results for the quantum generalized depolarizing semi-groups.

</details>


### [36] [Coding Schemes for the Noisy Torn Paper Channel](https://arxiv.org/abs/2601.11501)
*Frederik Walter,Maria Abu-Sini,Nils Weinhardt,Antonia Wachter-Zeh*

Main category: cs.IT

TL;DR: 该论文研究DNA存储中的衰变过程，将其建模为概率性噪声撕纸信道，提出使用静态标记和数据依赖标记的编码方案，在噪声环境下实现超过99%的重建率。


<details>
  <summary>Details</summary>
Motivation: 为了使DNA成为适合档案数据存储的介质，必须考虑DNA存储系统中观察到的链衰变过程。DNA衰变会导致比特替换错误和序列断裂，需要开发能够应对这些挑战的编码方案。

Method: 将DNA衰变过程建模为概率性噪声撕纸信道（TPC），该信道首先通过替换概率性地破坏传输序列的比特，然后将序列分解为一组无序的噪声子字符串。设计了两种编码方案：1）使用静态标记嵌入传输序列；2）使用与数据连接的标记（哈希函数形式）。

Result: 模拟结果显示：静态标记在较高替换概率下表现更好，而数据依赖标记在较低噪声水平下更优。两种方法都能实现超过99%的重建率，且未观察到错误解码，主要受计算资源限制。

Conclusion: 该研究为DNA存储系统中的衰变问题提供了有效的编码解决方案，通过标记嵌入技术成功应对了噪声撕纸信道的挑战，为DNA档案数据存储的实际应用奠定了基础。

Abstract: To make DNA a suitable medium for archival data storage, it is essential to consider the decay process of the strands observed in DNA storage systems. This paper studies the decay process as a probabilistic noisy torn paper channel (TPC), which first corrupts the bits of the transmitted sequence in a probabilistic manner by substitutions, then breaks the sequence into a set of noisy unordered substrings. The present work devises coding schemes for the noisy TPC by embedding markers in the transmitted sequence. We investigate the use of static markers and markers connected to the data in the form of hash functions. These two tools have also been recently exploited to tackle the noiseless TPC. Simulations show that static markers excel at higher substitution probabilities, while data-dependent markers are superior at lower noise levels. Both approaches achieve reconstruction rates exceeding $99\%$ with no false decodings observed, primarily limited by computational resources.

</details>


### [37] [Empirical Coordination over Markov Channel with Independent Source](https://arxiv.org/abs/2601.11520)
*Mengyuan Zhao,Maël Le Treust,Tobias J. Oechtering*

Main category: cs.IT

TL;DR: 研究马尔可夫信道下的联合信源信道编码，通过经验协调框架确定编码方案可诱导的信源和信道符号经验分布，建立可达联合分布集的单字母内外界。


<details>
  <summary>Details</summary>
Motivation: 传统基于块独立性的块马尔可夫编码方案在处理马尔可夫信道时存在局限性，需要直接利用马尔可夫信道结构来改进性能，特别是在严格因果编码器无法访问过去信道状态的情况下。

Method: 引入新的典型性概念——输入驱动马尔可夫典型性，并发展其基本性质。考虑严格因果编码器生成信道输入而不访问过去信道状态，从而驱动当前马尔可夫状态演化。

Result: 建立了网络中所有符号可达联合分布集的单字母内外界。与依赖块独立性的经典块马尔可夫编码方案相比，该方法直接利用马尔可夫信道结构，超越了基于独立性的论证。

Conclusion: 通过输入驱动马尔可夫典型性的新框架，为马尔可夫信道下的联合信源信道编码提供了更有效的分析方法，改进了传统基于独立性的编码方案。

Abstract: We study joint source-channel coding over Markov channels through the empirical coordination framework. More specifically, we aim at determining the empirical distributions of source and channel symbols that can be induced by a coding scheme. We consider strictly causal encoders that generate channel inputs, without access to the past channel states, henceforth driving the current Markov state evolution. Our main result is the single-letter inner and outer bounds of the set of achievable joint distributions, coordinating all the symbols in the network. To establish the inner bound, we introduce a new notion of typicality, the input-driven Markov typicality, and develop its fundamental properties. Contrary to the classical block-Markov coding schemes that rely on blockwise independence for discrete memoryless channels, our analysis directly exploits the Markov channel structure and improves beyond the independence-based arguments.

</details>
