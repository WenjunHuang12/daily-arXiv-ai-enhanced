{"id": "2511.01921", "categories": ["cs.IT", "cs.AI", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.01921", "abs": "https://arxiv.org/abs/2511.01921", "authors": ["Roberta Fiandaca", "Manil Dev Gomony"], "title": "Fibbinary-Based Compression and Quantization for Efficient Neural Radio Receivers", "comment": null, "summary": "Neural receivers have shown outstanding performance compared to the\nconventional ones but this comes with a high network complexity leading to a\nheavy computational cost. This poses significant challenges in their deployment\non hardware-constrained devices. To address the issue, this paper explores two\noptimization strategies: quantization and compression. We introduce both\nuniform and non-uniform quantization such as the Fibonacci Code word\nQuantization (FCQ). A novel fine-grained approach to the Incremental Network\nQuantization (INQ) strategy is then proposed to compensate for the losses\nintroduced by the above mentioned quantization techniques. Additionally, we\nintroduce two novel lossless compression algorithms that effectively reduce the\nmemory size by compressing sequences of Fibonacci quantized parameters\ncharacterized by a huge redundancy. The quantization technique provides a\nsaving of 45\\% and 44\\% in the multiplier's power and area, respectively, and\nits combination with the compression determines a 63.4\\% reduction in memory\nfootprint, while still providing higher performances than a conventional\nreceiver."}
{"id": "2511.02189", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.02189", "abs": "https://arxiv.org/abs/2511.02189", "authors": ["Minje Kim", "Hongjae Nam", "Beomsoo Ko", "Hyeongjun Park", "Hwanjin Kim", "Dong-Hyun Jung", "Junil Choi"], "title": "Analysis of Beam Misalignment Effect in Inter-Satellite FSO Links", "comment": "12 pages, 11 figures, submitted to IEEE Transactions on Wireless\n  Communications (TWC)", "summary": "Free-space optical (FSO) communication has emerged as a promising technology\nfor inter-satellite links (ISLs) due to its high data rate, low power\nconsumption, and reduced interference. However, the performance of\ninter-satellite FSO systems is highly sensitive to beam misalignment. While\npointing-ahead angle (PAA) compensation is commonly employed, the effectiveness\nof PAA compensation depends on precise orbital knowledge and advanced alignment\nhardware, which are not always feasible in practice. To address this challenge,\nthis paper investigates the impact of beam misalignment on inter-satellite FSO\ncommunication. We derive a closed-form expression for the cumulative\ndistribution function (CDF) of the FSO channel under the joint jitter and\nmisalignment-induced pointing error, and introduce a truncated CDF formulation\nwith a bisection algorithm to efficiently compute outage probabilities with\nguaranteed convergence and minimal computational overhead. To make the analysis\nmore practical, we quantify displacement based on orbital dynamics. Numerical\nresults demonstrate that the proposed model closely matches Monte Carlo\nsimulations, making the proposed model highly useful to design inter-satellite\nFSO systems in practice."}
{"id": "2511.02216", "categories": ["cs.IT", "cs.AI", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.02216", "abs": "https://arxiv.org/abs/2511.02216", "authors": ["Hyemin Yu", "Hong-Chuan Yang"], "title": "Adaptive Cooperative Transmission Design for Ultra-Reliable Low-Latency Communications via Deep Reinforcement Learning", "comment": "Accepted at the AI4NextG Workshop, NeurIPS 2025", "summary": "Next-generation wireless communication systems must support ultra-reliable\nlow-latency communication (URLLC) service for mission-critical applications.\nMeeting stringent URLLC requirements is challenging, especially for two-hop\ncooperative communication. In this paper, we develop an adaptive transmission\ndesign for a two-hop relaying communication system. Each hop transmission\nadaptively configures its transmission parameters separately, including\nnumerology, mini-slot size, and modulation and coding scheme, for reliable\npacket transmission within a strict latency constraint. We formulate the\nhop-specific transceiver configuration as a Markov decision process (MDP) and\npropose a dual-agent reinforcement learning-based cooperative latency-aware\ntransmission (DRL-CoLA) algorithm to learn latency-aware transmission policies\nin a distributed manner. Simulation results verify that the proposed algorithm\nachieves the near-optimal reliability while satisfying strict latency\nrequirements."}
{"id": "2511.02284", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.02284", "abs": "https://arxiv.org/abs/2511.02284", "authors": ["Haohao Qin", "Bowen Gu", "Xianhua Yu", "Hao Xie", "Yongjun Xu", "Qihao Li", "Liejun Wang"], "title": "Revisiting Wireless-Powered MEC: A Cooperative Energy Recycling Framework for Task-Energy Co-Design", "comment": null, "summary": "Cooperative energy recycling (CER) offers a new way to boost energy\nutilization in wireless-powered multi-access edge computing (MEC) networks, yet\nits integration with computation-communication co-design remains underexplored.\nThis paper proposes a CER-enabled MEC framework that maximizes the minimum\ncomputable data among users under energy causality, latency, and power\nconstraints. The intractable problem is reformulated into a convex form through\nrelaxation, maximum ratio combining, and variable substitution, and closed-form\nsolutions are derived via Lagrangian duality and alternating optimization,\noffering analytical insights. Simulation results verify that the proposed CER\nmechanism markedly increases total computable data while maintaining equitable\nperformance across heterogeneous users."}
{"id": "2511.02157", "categories": ["cs.GT", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.02157", "abs": "https://arxiv.org/abs/2511.02157", "authors": ["Asrin Efe Yorulmaz", "Tamer Başar"], "title": "Near Optimal Convergence to Coarse Correlated Equilibrium in General-Sum Markov Games", "comment": null, "summary": "No-regret learning dynamics play a central role in game theory, enabling\ndecentralized convergence to equilibrium for concepts such as Coarse Correlated\nEquilibrium (CCE) or Correlated Equilibrium (CE). In this work, we improve the\nconvergence rate to CCE in general-sum Markov games, reducing it from the\npreviously best-known rate of $\\mathcal{O}(\\log^5 T / T)$ to a sharper\n$\\mathcal{O}(\\log T / T)$. This matches the best known convergence rate for CE\nin terms of $T$, number of iterations, while also improving the dependence on\nthe action set size from polynomial to polylogarithmic-yielding exponential\ngains in high-dimensional settings. Our approach builds on recent advances in\nadaptive step-size techniques for no-regret algorithms in normal-form games,\nand extends them to the Markovian setting via a stage-wise scheme that adjusts\nlearning rates based on real-time feedback. We frame policy updates as an\ninstance of Optimistic Follow-the-Regularized-Leader (OFTRL), customized for\nvalue-iteration-based learning. The resulting self-play algorithm achieves, to\nour knowledge, the fastest known convergence rate to CCE in Markov games."}
{"id": "2511.02234", "categories": ["cs.MM", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.02234", "abs": "https://arxiv.org/abs/2511.02234", "authors": ["Jiawei Liu", "Enis Berk Çoban", "Zarina Schevchenko", "Hao Tang", "Zhigang Zhu", "Michael I Mandel", "Johanna Devaney"], "title": "An Evaluation of Interleaved Instruction Tuning on Semantic Reasoning Performance in an Audio MLLM", "comment": null, "summary": "Standard training for Multi-modal Large Language Models (MLLMs) involves\nconcatenating non-textual information, like vision or audio, with a text\nprompt. This approach may not encourage deep integration of modalities,\nlimiting the model's ability to leverage the core language model's reasoning\ncapabilities. This work examined the impact of interleaved instruction tuning\nin an audio MLLM, where audio tokens are interleaved within the prompt. Using\nthe Listen, Think, and Understand (LTU) model as a testbed, we conduct an\nexperiment using the Synonym and Hypernym Audio Reasoning Dataset (SHARD), our\nnewly created reasoning benchmark for audio-based semantic reasoning focusing\non synonym and hypernym recognition. Our findings show that while even\nzero-shot interleaved prompting improves performance on our reasoning tasks, a\nsmall amount of fine-tuning using interleaved training prompts improves the\nresults further, however, at the expense of the MLLM's audio labeling ability."}
{"id": "2511.02214", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.02214", "abs": "https://arxiv.org/abs/2511.02214", "authors": ["Matija Bucić", "Zhongtian He", "Shang-En Huang", "Thatchaphol Saranurak"], "title": "Disjoint Paths in Expanders in Deterministic Almost-Linear Time via Hypergraph Perfect Matching", "comment": "SODA 2026", "summary": "We design efficient deterministic algorithms for finding short edge-disjoint\npaths in expanders. Specifically, given an $n$-vertex $m$-edge expander $G$ of\nconductance $\\phi$ and minimum degree $\\delta$, and a set of pairs\n$\\{(s_i,t_i)\\}_i$ such that each vertex appears in at most $k$ pairs, our\nalgorithm deterministically computes a set of edge-disjoint paths from $s_i$ to\n$t_i$, one for every $i$: (1) each of length at most $18 \\log (n)/\\phi$ and in\n$mn^{1+o(1)}\\min\\{k, \\phi^{-1}\\}$ total time, assuming $\\phi^3\\delta\\ge (35\\log\nn)^3 k$, or (2) each of length at most $n^{o(1)}/\\phi$ and in total\n$m^{1+o(1)}$ time, assuming $\\phi^3 \\delta \\ge n^{o(1)} k$. Before our work,\ndeterministic polynomial-time algorithms were known only for expanders with\nconstant conductance and were significantly slower. To obtain our result, we\ngive an almost-linear time algorithm for \\emph{hypergraph perfect matching}\nunder generalizations of Hall-type conditions (Haxell 1995), a powerful\nframework with applications in various settings, which until now has only\nadmitted large polynomial-time algorithms (Annamalai 2018)."}
{"id": "2511.01896", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.01896", "abs": "https://arxiv.org/abs/2511.01896", "authors": ["Alessandro Padella", "Francesco Vinci", "Massimiliano de Leoni"], "title": "An Experimental Comparison of Alternative Techniques for Event-Log Augmentation", "comment": null, "summary": "Process mining analyzes and improves processes by examining transactional\ndata stored in event logs, which record sequences of events with timestamps.\nHowever, the effectiveness of process mining, especially when combined with\nmachine or deep learning, depends on having large event logs. Event log\naugmentation addresses this limitation by generating additional traces that\nsimulate realistic process executions while considering various perspectives\nlike time, control-flow, workflow, resources, and domain-specific attributes.\nAlthough prior research has explored event-log augmentation techniques, there\nhas been no comprehensive comparison of their effectiveness. This paper reports\non an evaluation of seven state-of-the-art augmentation techniques across eight\nevent logs. The results are also compared with those obtained by a baseline\ntechnique based on a stochastic transition system. The comparison has been\ncarried on analyzing four different aspects: similarity, preservation of\npredictive information, information loss/enhancement, and computational times\nrequired. Results show that, considering the different criteria, a technique\nbased on a stochastic transition system combined with resource queue modeling\nwould provide higher quality synthetic event logs. Event-log augmentation\ntechniques are also compared with traditional data-augmentation techniques,\nshowing that the former provide significant benefits, whereas the latter fail\nto consider process constraints."}
{"id": "2511.02052", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02052", "abs": "https://arxiv.org/abs/2511.02052", "authors": ["Karol Radziszewski", "Michał Szpunar", "Piotr Ociepka", "Mateusz Buczyński"], "title": "Solving cold start in news recommendations: a RippleNet-based system for large scale media outlet", "comment": null, "summary": "We present a scalable recommender system implementation based on RippleNet,\ntailored for the media domain with a production deployment in Onet.pl, one of\nPoland's largest online media platforms. Our solution addresses the cold-start\nproblem for newly published content by integrating content-based item\nembeddings into the knowledge propagation mechanism of RippleNet, enabling\neffective scoring of previously unseen items. The system architecture leverages\nAmazon SageMaker for distributed training and inference, and Apache Airflow for\norchestrating data pipelines and model retraining workflows. To ensure\nhigh-quality training data, we constructed a comprehensive golden dataset\nconsisting of user and item features and a separate interaction table, all\nenabling flexible extensions and integration of new signals."}
{"id": "2511.02287", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.02287", "abs": "https://arxiv.org/abs/2511.02287", "authors": ["Haohao Qin", "Bowen Gu", "Dong Li", "Xianhua Yu", "Liejun Wang", "Yuanwei Liu", "Sumei Sun"], "title": "Fairness-Aware Computation Offloading in Wireless-Powered MEC Systems with Cooperative Energy Recycling", "comment": null, "summary": "In this paper, cooperative energy recycling (CER) is investigated in\nwireless-powered mobile edge computing systems. Unlike conventional\narchitectures that rely solely on a dedicated power source, wireless sensors\nare additionally enabled to recycle energy from peer transmissions. To evaluate\nsystem performance, a joint computation optimization problem is formulated that\nintegrates local computing and computation offloading, under an alpha-fairness\nobjective that balances total computable data and user fairness while\nsatisfying energy, latency, and task size constraints. Due to the inherent\nnon-convexity introduced by coupled resource variables and fairness\nregularization, a variable-substitution technique is employed to transform the\nproblem into a convex structure, which is then efficiently solved using\nLagrangian duality and alternating optimization. To characterize the\nfairness-efficiency tradeoff, closed-form solutions are derived for three\nrepresentative regimes: zero fairness, common fairness, and max-min fairness,\neach offering distinct system-level insights. Numerical results validate the\neffectiveness of the proposed CER-enabled framework, demonstrating significant\ngains in throughput and adaptability over benchmark schemes. The tunable alpha\nfairness mechanism provides flexible control over performance-fairness\ntrade-offs across diverse scenarios."}
{"id": "2511.02746", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.02746", "abs": "https://arxiv.org/abs/2511.02746", "authors": ["Jiaxin Song", "Parnian Shahkar", "Kate Donahue", "Bhaskar Ray Chaudhury"], "title": "Human-AI Collaboration with Misaligned Preferences", "comment": "37 pages, 8 figures, appeared at EAAMO'25", "summary": "In many real-life settings, algorithms play the role of assistants, while\nhumans ultimately make the final decision. Often, algorithms specifically act\nas curators, narrowing down a wide range of options into a smaller subset that\nthe human picks between: consider content recommendation or chatbot responses\nto questions with multiple valid answers. Crucially, humans may not know their\nown preferences perfectly either, but instead may only have access to a noisy\nsampling over preferences. Algorithms can assist humans by curating a smaller\nsubset of items, but must also face the challenge of misalignment: humans may\nhave different preferences from each other (and from the algorithm), and the\nalgorithm may not know the exact preferences of the human they are facing at\nany point in time. In this paper, we model and theoretically study such a\nsetting. Specifically, we show instances where humans benefit by collaborating\nwith a misaligned algorithm. Surprisingly, we show that humans gain more\nutility from a misaligned algorithm (which makes different mistakes) than from\nan aligned algorithm. Next, we build on this result by studying what properties\nof algorithms maximize human welfare when the goals could be either utilitarian\nwelfare or ensuring all humans benefit. We conclude by discussing implications\nfor designers of algorithmic tools and policymakers."}
{"id": "2511.02478", "categories": ["cs.MM", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02478", "abs": "https://arxiv.org/abs/2511.02478", "authors": ["Bingyan Xie", "Yongpeng Wu", "Yuxuan Shi", "Biqian Feng", "Wenjun Zhang", "Jihong Park", "Tony Quek"], "title": "Wireless Video Semantic Communication with Decoupled Diffusion Multi-frame Compensation", "comment": null, "summary": "Existing wireless video transmission schemes directly conduct video coding in\npixel level, while neglecting the inner semantics contained in videos. In this\npaper, we propose a wireless video semantic communication framework with\ndecoupled diffusion multi-frame compensation (DDMFC), abbreviated as WVSC-D,\nwhich integrates the idea of semantic communication into wireless video\ntransmission scenarios. WVSC-D first encodes original video frames as semantic\nframes and then conducts video coding based on such compact representations,\nenabling the video coding in semantic level rather than pixel level. Moreover,\nto further reduce the communication overhead, a reference semantic frame is\nintroduced to substitute motion vectors of each frame in common video coding\nmethods. At the receiver, DDMFC is proposed to generate compensated current\nsemantic frame by a two-stage conditional diffusion process. With both the\nreference frame transmission and DDMFC frame compensation, the bandwidth\nefficiency improves with satisfying video transmission performance.\nExperimental results verify the performance gain of WVSC-D over other DL-based\nmethods e.g. DVSC about 1.8 dB in terms of PSNR."}
{"id": "2511.02254", "categories": ["cs.DS", "cs.AI", "cs.CC"], "pdf": "https://arxiv.org/pdf/2511.02254", "abs": "https://arxiv.org/abs/2511.02254", "authors": ["Tan D. Tran", "Canh V. Pham"], "title": "Fast Approximation Algorithm for Non-Monotone DR-submodular Maximization under Size Constraint", "comment": null, "summary": "This work studies the non-monotone DR-submodular Maximization over a ground\nset of $n$ subject to a size constraint $k$. We propose two approximation\nalgorithms for solving this problem named FastDrSub and FastDrSub++. FastDrSub\noffers an approximation ratio of $0.044$ with query complexity of $O(n\n\\log(k))$. The second one, FastDrSub++, improves upon it with a ratio of\n$1/4-\\epsilon$ within query complexity of $(n \\log k)$ for an input parameter\n$\\epsilon >0$. Therefore, our proposed algorithms are the first constant-ratio\napproximation algorithms for the problem with the low complexity of $O(n\n\\log(k))$.\n  Additionally, both algorithms are experimentally evaluated and compared\nagainst existing state-of-the-art methods, demonstrating their effectiveness in\nsolving the Revenue Maximization problem with DR-submodular objective function.\nThe experimental results show that our proposed algorithms significantly\noutperform existing approaches in terms of both query complexity and solution\nquality."}
{"id": "2511.01942", "categories": ["cs.DB", "cond-mat.mtrl-sci", "cs.DL"], "pdf": "https://arxiv.org/pdf/2511.01942", "abs": "https://arxiv.org/abs/2511.01942", "authors": ["Khalil Rejiba", "Sang-Hyeok Lee", "Christina Gasper", "Martina Freund", "Sandra Korte-Kerzel", "Ulrich Kerzel"], "title": "Towards Defect Phase Diagrams: From Research Data Management to Automated Workflows", "comment": null, "summary": "Defect phase diagrams provide a unified description of crystal defect states\nfor materials design and are central to the scientific objectives of the\nCollaborative Research Centre (CRC) 1394. Their construction requires the\nsystematic integration of heterogeneous experimental and simulation data across\nresearch groups and locations. In this setting, research data management (RDM)\nis a key enabler of new scientific insight by linking distributed research\nactivities and making complex data reproducible and reusable.\n  To address the challenge of heterogeneous data sources and formats, a\ncomprehensive RDM infrastructure has been established that links experiment,\ndata, and analysis in a seamless workflow. The system combines: (1) a joint\nelectronic laboratory notebook and laboratory information management system,\n(2) easy-to-use large-object data storage, (3) automatic metadata extraction\nfrom heterogeneous and proprietary file formats, (4) interactive provenance\ngraphs for data exploration and reuse, and (5) automated reporting and analysis\nworkflows. The two key technological elements are the openBIS electronic\nlaboratory notebook and laboratory information management system, and a newly\ndeveloped companion application that extends openBIS with large-scale data\nhandling, automated metadata capture, and federated access to distributed\nresearch data.\n  This integrated approach reduces friction in data capture and curation,\nenabling traceable and reusable datasets that accelerate the construction of\ndefect phase diagrams across institutions."}
{"id": "2511.02113", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.02113", "abs": "https://arxiv.org/abs/2511.02113", "authors": ["Hai-Dang Kieu", "Min Xu", "Thanh Trung Huynh", "Dung D. Le"], "title": "Enhancing Multimodal Recommendations with Vision-Language Models and Information-Aware Fusion", "comment": null, "summary": "Recent advances in multimodal recommendation (MMR) have shown that\nincorporating rich content sources such as images and text can lead to\nsignificant gains representation quality. However, existing methods often rely\non coarse visual features and uncontrolled fusion, leading to redundant or\nmisaligned representations. As a result, visual encoders often fail to capture\nsalient, item-relevant semantics, limiting their contribution in multimodal\nfusion. From an information-theoretic perspective, effective fusion should\nbalance the unique, shared, and redundant information across modalities,\npreserving complementary cues while avoiding correlation bias. This paper\npresents VLIF, a vision-language and information-theoretic fusion framework\nthat enhances multimodal recommendation through two key components. (i) A\nVLM-based visual enrichment module generates fine-grained, title-guided\ndescriptions to transform product images into semantically aligned\nrepresentations. (ii) An information-aware fusion module, inspired by Partial\nInformation Decomposition (PID), disentangles redundant and synergistic signals\nacross modalities for controlled integration. Experiments on three Amazon\ndatasets demonstrate that VLIF consistently outperforms recent multimodal\nbaselines and substantially strengthens the contribution of visual features."}
{"id": "2511.02291", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.02291", "abs": "https://arxiv.org/abs/2511.02291", "authors": ["Kwonyeol Park", "Gyoseung Lee", "Hyeongtaek Lee", "Hwanjin Kim", "Junil Choi"], "title": "Downlink Channel Estimation for mmWave Systems with Impulsive Interference", "comment": "5 pages, 2 figures", "summary": "In this paper, we investigate a channel estimation problem in a downlink\nmillimeter-wave (mmWave) multiple-input multiple-output (MIMO) system, which\nsuffers from impulsive interference caused by hardware non-idealities or\nexternal disruptions. Specifically, impulsive interference presents a\nsignificant challenge to channel estimation due to its sporadic, unpredictable,\nand high-power nature. To tackle this issue, we develop a Bayesian channel\nestimation technique based on variational inference (VI) that leverages the\nsparsity of the mmWave channel in the angular domain and the intermittent\nnature of impulsive interference to minimize channel estimation errors. The\nproposed technique employs mean-field approximation to approximate posterior\ninference and integrates VI into the sparse Bayesian learning (SBL) framework.\nSimulation results demonstrate that the proposed technique outperforms\nbaselines in terms of channel estimation accuracy."}
{"id": "2511.02487", "categories": ["cs.DS", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.02487", "abs": "https://arxiv.org/abs/2511.02487", "authors": ["Weiming Feng", "Xiongxin Yang", "Yixiao Yu", "Yiyao Zhang"], "title": "Learning CNF formulas from uniform random solutions in the local lemma regime", "comment": null, "summary": "We study the problem of learning a $n$-variables $k$-CNF formula $\\Phi$ from\nits i.i.d. uniform random solutions, which is equivalent to learning a Boolean\nMarkov random field (MRF) with $k$-wise hard constraints. Revisiting Valiant's\nalgorithm (Commun. ACM'84), we show that it can exactly learn (1) $k$-CNFs with\nbounded clause intersection size under Lov\\'asz local lemma type conditions,\nfrom $O(\\log n)$ samples; and (2) random $k$-CNFs near the satisfiability\nthreshold, from $\\widetilde{O}(n^{\\exp(-\\sqrt{k})})$ samples. These results\nsignificantly improve the previous $O(n^k)$ sample complexity. We further\nestablish new information-theoretic lower bounds on sample complexity for both\nexact and approximate learning from i.i.d. uniform random solutions."}
{"id": "2511.02002", "categories": ["cs.DB", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.02002", "abs": "https://arxiv.org/abs/2511.02002", "authors": ["Xiangru Jian", "Zhengyuan Dong", "M. Tamer Özsu"], "title": "InteracSPARQL: An Interactive System for SPARQL Query Refinement Using Natural Language Explanations", "comment": "Working paper", "summary": "In recent years, querying semantic web data using SPARQL has remained\nchallenging, especially for non-expert users, due to the language's complex\nsyntax and the prerequisite of understanding intricate data structures. To\naddress these challenges, we propose InteracSPARQL, an interactive SPARQL query\ngeneration and refinement system that leverages natural language explanations\n(NLEs) to enhance user comprehension and facilitate iterative query refinement.\nInteracSPARQL integrates LLMs with a rule-based approach to first produce\nstructured explanations directly from SPARQL abstract syntax trees (ASTs),\nfollowed by LLM-based linguistic refinements. Users can interactively refine\nqueries through direct feedback or LLM-driven self-refinement, enabling the\ncorrection of ambiguous or incorrect query components in real time. We evaluate\nInteracSPARQL on standard benchmarks, demonstrating significant improvements in\nquery accuracy, explanation clarity, and overall user satisfaction compared to\nbaseline approaches. Our experiments further highlight the effectiveness of\ncombining rule-based methods with LLM-driven refinements to create more\naccessible and robust SPARQL interfaces."}
{"id": "2511.02181", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.02181", "abs": "https://arxiv.org/abs/2511.02181", "authors": ["Yuhan Wang", "Qing Xie", "Zhifeng Bao", "Mengzi Tang", "Lin Li", "Yongjian Liu"], "title": "KGBridge: Knowledge-Guided Prompt Learning for Non-overlapping Cross-Domain Recommendation", "comment": "13 pages, 4 figures", "summary": "Knowledge Graphs (KGs), as structured knowledge bases that organize\nrelational information across diverse domains, provide a unified semantic\nfoundation for cross-domain recommendation (CDR). By integrating symbolic\nknowledge with user-item interactions, KGs enrich semantic representations,\nsupport reasoning, and enhance model interpretability. Despite this potential,\nexisting KG-based methods still face major challenges in CDR, particularly\nunder non-overlapping user scenarios. These challenges arise from: (C1)\nsensitivity to KG sparsity and popularity bias, (C2) dependence on overlapping\nusers for domain alignment and (C3) lack of explicit disentanglement between\ntransferable and domain-specific knowledge, which limit effective and stable\nknowledge transfer. To this end, we propose KGBridge, a knowledge-guided prompt\nlearning framework for cross-domain sequential recommendation under\nnon-overlapping user scenarios. KGBridge comprises two core components: a\nKG-enhanced Prompt Encoder, which models relation-level semantics as soft\nprompts to provide structured and dynamic priors for user sequence modeling\n(addressing C1), and a Two-stage Training Paradigm, which combines cross-domain\npretraining and privacy-preserving fine-tuning to enable knowledge transfer\nwithout user overlap (addressing C2). By combining relation-aware semantic\ncontrol with correspondence-driven disentanglement, KGBridge explicitly\nseparates and balances domain-shared and domain-specific semantics, thereby\nmaintaining complementarity and stabilizing adaptation during fine-tuning\n(addressing C3). Extensive experiments on benchmark datasets demonstrate that\nKGBridge consistently outperforms state-of-the-art baselines and remains robust\nunder varying KG sparsity, highlighting its effectiveness in mitigating\nstructural imbalance and semantic entanglement in KG-enhanced cross-domain\nrecommendation."}
{"id": "2511.02297", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.02297", "abs": "https://arxiv.org/abs/2511.02297", "authors": ["Shi-Bing Li", "Ke Li", "Lei Yu"], "title": "Two-Parameter Rényi Information Quantities with Applications to Privacy Amplification and Soft Covering", "comment": null, "summary": "There are no universally accepted definitions of R\\'enyi conditional entropy\nand R\\'enyi mutual information, although motivated by different applications,\nseveral definitions have been proposed in the literature. In this paper, we\nconsider a family of two-parameter R\\'enyi conditional entropy and a family of\ntwo-parameter R\\'enyi mutual information. By performing a change of variables\nfor the parameters, the two-parameter R\\'enyi conditional entropy we study\ncoincides precisely with the definition introduced by Hayashi and Tan [IEEE\nTrans. Inf. Theory, 2016], and it also emerges naturally as the classical\nspecialization of the three-parameter quantum R\\'enyi conditional entropy\nrecently put forward by Rubboli, Goodarzi, and Tomamichel [arXiv:2410.21976\n(2024)]. We establish several fundamental properties of the two-parameter\nR\\'enyi conditional entropy, including monotonicity with respect to the\nparameters and variational expression. The associated two-parameter R\\'enyi\nmutual information considered in this paper is new and it unifies three\ncommonly used variants of R\\'enyi mutual information. For this quantity, we\nprove several important properties, including the non-negativity, additivity,\ndata processing inequality, monotonicity with respect to the parameters,\nvariational expression, as well as convexity and concavity. Finally, we\ndemonstrate that these two-parameter R\\'enyi information quantities can be used\nto characterize the strong converse exponents in privacy amplification and soft\ncovering problems under R\\'enyi divergence of order $\\alpha \\in (0, \\infty)$."}
{"id": "2511.02705", "categories": ["cs.DS", "cs.DM"], "pdf": "https://arxiv.org/pdf/2511.02705", "abs": "https://arxiv.org/abs/2511.02705", "authors": ["Nate Veldt"], "title": "A Simple and Fast $(3+\\varepsilon)$-approximation for Constrained Correlation Clustering", "comment": "Accepted for publication at the 2026 SIAM Symposium on Simplicity in\n  Algorithms (SOSA26)", "summary": "In Constrained Correlation Clustering, the goal is to cluster a complete\nsigned graph in a way that minimizes the number of negative edges inside\nclusters plus the number of positive edges between clusters, while respecting\nhard constraints on how to cluster certain friendly or hostile node pairs.\nFischer et al. [FKKT25a] recently developed a $\\tilde{O}(n^3)$-time\n16-approximation algorithm for this problem. We settle an open question posed\nby these authors by designing an algorithm that is equally fast but brings the\napproximation factor down to $(3+\\varepsilon)$ for arbitrary constant\n$\\varepsilon > 0$. Although several new algorithmic steps are needed to obtain\nour improved approximation, our approach maintains many advantages in terms of\nsimplicity. In particular, it relies mainly on rounding a (new) covering linear\nprogram, which can be approximated quickly and combinatorially. Furthermore,\nthe rounding step amounts to applying the very familiar Pivot algorithm to an\nauxiliary graph. Finally, we develop much simpler algorithms for instances that\ninvolve only friendly or only hostile constraints."}
{"id": "2511.02062", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02062", "abs": "https://arxiv.org/abs/2511.02062", "authors": ["Yuting Yang", "Tiancheng Yuan", "Jamal Hashim", "Thiago Garrett", "Jeffrey Qian", "Ann Zhang", "Yifan Wang", "Weijia Song", "Ken Birman"], "title": "Vortex: Hosting ML Inference and Knowledge Retrieval Services With Tight Latency and Throughput Requirements", "comment": null, "summary": "There is growing interest in deploying ML inference and knowledge retrieval\nas services that could support both interactive queries by end users and more\ndemanding request flows that arise from AIs integrated into a end-user\napplications and deployed as agents. Our central premise is that these latter\ncases will bring service level latency objectives (SLOs). Existing ML serving\nplatforms use batching to optimize for high throughput, exposing them to\nunpredictable tail latencies. Vortex enables an SLO-first approach. For\nidentical tasks, Vortex's pipelines achieve significantly lower and more stable\nlatencies than TorchServe and Ray Serve over a wide range of workloads, often\nenabling a given SLO target at more than twice the request rate. When RDMA is\navailable, the Vortex advantage is even more significant."}
{"id": "2511.02571", "categories": ["cs.IR", "math.PR", "Primary 60E05, 60C05, Secondary 62R07, 68T05"], "pdf": "https://arxiv.org/pdf/2511.02571", "abs": "https://arxiv.org/abs/2511.02571", "authors": ["Tetiana Manzhos", "Tetiana Ianevych", "Olga Melnyk"], "title": "Average Precision at Cutoff k under Random Rankings: Expectation and Variance", "comment": "17 pages, 2 tables, 2 figures", "summary": "Recommender systems and information retrieval platforms rely on ranking\nalgorithms to present the most relevant items to users, thereby improving\nengagement and satisfaction. Assessing the quality of these rankings requires\nreliable evaluation metrics. Among them, Mean Average Precision at cutoff k\n(MAP@k) is widely used, as it accounts for both the relevance of items and\ntheir positions in the list.\n  In this paper, the expectation and variance of Average Precision at k (AP@k)\nare derived since they can be used as biselines for MAP@k. Here, we covered two\nwidely used evaluation models: offline and online. The expectation establishes\nthe baseline, indicating the level of MAP@k that can be achieved by pure\nchance. The variance complements this baseline by quantifying the extent of\nrandom fluctuations, enabling a more reliable interpretation of observed\nscores."}
{"id": "2511.02320", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.02320", "abs": "https://arxiv.org/abs/2511.02320", "authors": ["Kwonyeol Park", "Hyuckjin Choi", "Beomsoo Ko", "Minje Kim", "Gyoseung Lee", "Daecheol Kwon", "Hyunjae Park", "Byungseung Kim", "Min-Ho Shin", "Junil Choi"], "title": "Anomaly Detection-Based UE-Centric Inter-Cell Interference Suppression", "comment": "14 pages, 14 figures", "summary": "The increasing spectral reuse can cause significant performance degradation\ndue to interference from neighboring cells. In such scenarios, developing\neffective interference suppression schemes is necessary to improve overall\nsystem performance. To tackle this issue, we propose a novel user\nequipment-centric interference suppression scheme, which effectively detects\ninter-cell interference (ICI) and subsequently applies interference whitening\nto mitigate ICI. The proposed scheme, named Z-refined deep support vector data\ndescription, exploits a one-class classification-based anomaly detection\ntechnique. Numerical results verify that the proposed scheme outperforms\nvarious baselines in terms of interference detection performance with limited\ntime or frequency resources for training and is comparable to the performance\nbased on an ideal genie-aided interference suppression scheme. Furthermore, we\ndemonstrate through test equipment experiments using a commercial\nfifth-generation modem chipset that the proposed scheme shows performance\nimprovements across various 3rd generation partnership project standard channel\nenvironments, including tapped delay line-A, -B, and -C models."}
{"id": "2511.02611", "categories": ["cs.DB", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.02611", "abs": "https://arxiv.org/abs/2511.02611", "authors": ["Andrea D'Ascenzo", "Julian Meffert", "Petra Mutzel", "Fabrizio Rossi"], "title": "Accelerating Graph Similarity Search through Integer Linear Programming", "comment": null, "summary": "The Graph Edit Distance (GED) is an important metric for measuring the\nsimilarity between two (labeled) graphs. It is defined as the minimum cost\nrequired to convert one graph into another through a series of (elementary)\nedit operations. Its effectiveness in assessing the similarity of large graphs\nis limited by the complexity of its exact calculation, which is NP-hard\ntheoretically and computationally challenging in practice. The latter can be\nmitigated by switching to the Graph Similarity Search under GED constraints,\nwhich determines whether the edit distance between two graphs is below a given\nthreshold. A popular framework for solving Graph Similarity Search under GED\nconstraints in a graph database for a query graph is the\nfilter-and-verification framework. Filtering discards unpromising graphs, while\nthe verification step certifies the similarity between the filtered graphs and\nthe query graph. To improve the filtering step, we define a lower bound based\non an integer linear programming formulation. We prove that this lower bound\ndominates the effective branch match-based lower bound and can also be computed\nefficiently. Consequently, we propose a graph similarity search algorithm that\nuses a hierarchy of lower bound algorithms and solves a novel integer\nprogramming formulation that exploits the threshold parameter. An extensive\ncomputational experience on a well-assessed test bed shows that our approach\nsignificantly outperforms the state-of-the-art algorithm on most of the\nexamined thresholds."}
{"id": "2511.02096", "categories": ["cs.DB", "cs.DM"], "pdf": "https://arxiv.org/pdf/2511.02096", "abs": "https://arxiv.org/abs/2511.02096", "authors": ["Savo Tomovic"], "title": "Numbering Combinations for Compact Representation of Many-to-Many Relationship Sets", "comment": null, "summary": "In this paper we propose an approach to implement specific relation-ship set\nbetween two entities called combinatorial relationship set. For the\ncombinatorial relationship set B between entity sets G and I the mapping\ncardinality is many-to-many. Additionally, entities from G can be uniquely\nencoded with a pair of values (h, k) generated with the procedure for numbering\ncombinations of entities from I. The encoding procedure is based on\ncombinatorial number system that provides a representation of all possible k\n-combinations of a set of n elements by a single number. In general\nmany-to-many relationship sets are represented by a relation or table, while\nthe combinatorial relationship is not physically stored as separate table.\nHowever, all information is encapsulated into a single column added to G. The\nnew column is a candidate key in G. Additional operation named Rank-Join to\nfundamental relational-algebra is presented to combine information from g and i\nassociated with a combinatorial relationship set. Motivation for combinatorial\nrelationship originates from challenges in designing and implementing\nmultivalued dimensions and bridge tables in data-warehouse models."}
{"id": "2511.02002", "categories": ["cs.DB", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.02002", "abs": "https://arxiv.org/abs/2511.02002", "authors": ["Xiangru Jian", "Zhengyuan Dong", "M. Tamer Özsu"], "title": "InteracSPARQL: An Interactive System for SPARQL Query Refinement Using Natural Language Explanations", "comment": "Working paper", "summary": "In recent years, querying semantic web data using SPARQL has remained\nchallenging, especially for non-expert users, due to the language's complex\nsyntax and the prerequisite of understanding intricate data structures. To\naddress these challenges, we propose InteracSPARQL, an interactive SPARQL query\ngeneration and refinement system that leverages natural language explanations\n(NLEs) to enhance user comprehension and facilitate iterative query refinement.\nInteracSPARQL integrates LLMs with a rule-based approach to first produce\nstructured explanations directly from SPARQL abstract syntax trees (ASTs),\nfollowed by LLM-based linguistic refinements. Users can interactively refine\nqueries through direct feedback or LLM-driven self-refinement, enabling the\ncorrection of ambiguous or incorrect query components in real time. We evaluate\nInteracSPARQL on standard benchmarks, demonstrating significant improvements in\nquery accuracy, explanation clarity, and overall user satisfaction compared to\nbaseline approaches. Our experiments further highlight the effectiveness of\ncombining rule-based methods with LLM-driven refinements to create more\naccessible and robust SPARQL interfaces."}
{"id": "2511.02325", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.02325", "abs": "https://arxiv.org/abs/2511.02325", "authors": ["Ankit Yadav", "Ritumoni Sarma"], "title": "$\\mathbb{F}_q\\mathbb{F}_{q^2}$-additive cyclic codes and their Gray images", "comment": null, "summary": "We investigate additive cyclic codes over the alphabet\n$\\mathbb{F}_{q}\\mathbb{F}_{q^2}$, where $q$ is a prime power. First, its\ngenerator polynomials and minimal spanning set are determined. Then, examples\nof $\\mathbb{F}_{q^2}$-additive cyclic codes that satisfy the well-known\nSingleton bound are constructed. Using a Gray map, we produce certain optimal\nlinear codes over $\\mathbb{F}_{3}$. Finally, we obtain a few optimal ternary\nlinear complementary dual (LCD) codes from\n$\\mathbb{F}_{3}\\mathbb{F}_{9}$-additive codes."}
{"id": "2511.02611", "categories": ["cs.DB", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.02611", "abs": "https://arxiv.org/abs/2511.02611", "authors": ["Andrea D'Ascenzo", "Julian Meffert", "Petra Mutzel", "Fabrizio Rossi"], "title": "Accelerating Graph Similarity Search through Integer Linear Programming", "comment": null, "summary": "The Graph Edit Distance (GED) is an important metric for measuring the\nsimilarity between two (labeled) graphs. It is defined as the minimum cost\nrequired to convert one graph into another through a series of (elementary)\nedit operations. Its effectiveness in assessing the similarity of large graphs\nis limited by the complexity of its exact calculation, which is NP-hard\ntheoretically and computationally challenging in practice. The latter can be\nmitigated by switching to the Graph Similarity Search under GED constraints,\nwhich determines whether the edit distance between two graphs is below a given\nthreshold. A popular framework for solving Graph Similarity Search under GED\nconstraints in a graph database for a query graph is the\nfilter-and-verification framework. Filtering discards unpromising graphs, while\nthe verification step certifies the similarity between the filtered graphs and\nthe query graph. To improve the filtering step, we define a lower bound based\non an integer linear programming formulation. We prove that this lower bound\ndominates the effective branch match-based lower bound and can also be computed\nefficiently. Consequently, we propose a graph similarity search algorithm that\nuses a hierarchy of lower bound algorithms and solves a novel integer\nprogramming formulation that exploits the threshold parameter. An extensive\ncomputational experience on a well-assessed test bed shows that our approach\nsignificantly outperforms the state-of-the-art algorithm on most of the\nexamined thresholds."}
{"id": "2511.02711", "categories": ["cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.02711", "abs": "https://arxiv.org/abs/2511.02711", "authors": ["Daren Chao", "Kaiwen Chen", "Naiqing Guan", "Nick Koudas"], "title": "Relational Deep Dive: Error-Aware Queries Over Unstructured Data", "comment": null, "summary": "Unstructured data is pervasive, but analytical queries demand structured\nrepresentations, creating a significant extraction challenge. Existing methods\nlike RAG lack schema awareness and struggle with cross-document alignment,\nleading to high error rates. We propose ReDD (Relational Deep Dive), a\nframework that dynamically discovers query-specific schemas, populates\nrelational tables, and ensures error-aware extraction with provable guarantees.\nReDD features a two-stage pipeline: (1) Iterative Schema Discovery (ISD)\nidentifies minimal, joinable schemas tailored to each query, and (2) Tabular\nData Population (TDP) extracts and corrects data using lightweight classifiers\ntrained on LLM hidden states. A main contribution of ReDD is SCAPE, a\nstatistically calibrated method for error detection with coverage guarantees,\nand SCAPE-HYB, a hybrid approach that optimizes the trade-off between accuracy\nand human correction costs. Experiments across diverse datasets demonstrate\nReDD's effectiveness, reducing data extraction errors from up to 30% to below\n1% while maintaining high schema completeness (100% recall) and precision.\nReDD's modular design enables fine-grained control over accuracy-cost\ntrade-offs, making it a robust solution for high-stakes analytical queries over\nunstructured corpora."}
{"id": "2511.02502", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.02502", "abs": "https://arxiv.org/abs/2511.02502", "authors": ["Razvan Gabriel Iagar", "David Puertas-Centeno"], "title": "Generalized informational functionals and new monotone measures of statistical complexity", "comment": null, "summary": "In this paper we introduce a biparametric family of transformations which can\nbe seen as an extension of the so-called up and down transformations. This new\nclass of transformations allows to us to introduce new informational\nfunctionals, which we have called \\textit{down-moments} and \\textit{cumulative\nupper-moments}. A remarkable fact is that the down-moments provide, in some\ncases, an interpolation between the $p$-th moments and the power R\\'enyi\nentropies of a probability density. We establish new and sharp inequalities\nrelating these new functionals to the classical informational measures such as\nmoments, R\\'enyi and Shannon entropies and Fisher information measures. We also\ngive the optimal bounds as well as the minimizing densities, which are in some\ncases expressed in terms of the generalized trigonometric functions. We\nfurthermore define new classes of measures of statistical complexity obtained\nas quotients of the new functionals, and establish monotonicity properties for\nthem through an algebraic conjugation of up and down transformations. All of\nthese properties highlight an intricate structure of functional inequalities."}
{"id": "2511.02674", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.02674", "abs": "https://arxiv.org/abs/2511.02674", "authors": ["Tim Otto"], "title": "EasyTUS: A Comprehensive Framework for Fast and Accurate Table Union Search across Data Lakes", "comment": "Copyright 2025 IEEE. This is the author's version of the work that\n  has been accepted for publication in Proceedings of the IEEE International\n  Conference on Big Data (IEEE BigData 2025). The final version of record is\n  available at: tba", "summary": "Data lakes enable easy maintenance of heterogeneous data in its native form.\nWhile this flexibility can accelerate data ingestion, it shifts the complexity\nof data preparation and query processing to data discovery tasks. One such task\nis Table Union Search (TUS), which identifies tables that can be unioned with a\ngiven input table. In this work, we present EasyTUS, a comprehensive framework\nthat leverages Large Language Models (LLMs) to perform efficient and scalable\nTable Union Search across data lakes. EasyTUS implements the search pipeline as\nthree modular steps: Table Serialization for consistent formatting and\nsampling, Table Representation that utilizes LLMs to generate embeddings, and\nVector Search that leverages approximate nearest neighbor indexing for semantic\nmatching. To enable reproducible and systematic evaluation, in this paper, we\nalso introduce TUSBench, a novel standardized benchmarking environment within\nthe EasyTUS framework. TUSBench supports unified comparisons across approaches\nand data lakes, promoting transparency and progress in the field. Our\nexperiments using TUSBench show that EasyTUS consistently outperforms most of\nthe state-of the-art approaches, achieving improvements in average of up to\n34.3% in Mean Average Precision (MAP), up to 79.2x speedup in data preparation,\nand up to 7.7x faster query processing performance. Furthermore, EasyTUS\nmaintains strong performance even in metadata-absent settings, highlighting its\nrobustness and adaptability across data lakes."}
{"id": "2511.02519", "categories": ["cs.IT", "math.IT", "11T71, 14G50, 94B05, 94B65"], "pdf": "https://arxiv.org/pdf/2511.02519", "abs": "https://arxiv.org/abs/2511.02519", "authors": ["Guanghui Zhang", "Bocong Chen", "Liren Lin", "Hongwei Liu"], "title": "Improved AntiGriesmer Bounds for Linear Anticodes and Applications", "comment": null, "summary": "This paper improves the antiGriesmer bound for linear anticodes previously\nestablished by Chen and Xie (Journal of Algebra, 673 (2025) 304-320). While the\noriginal bound required the code length to satisfy $n < q^{k-1}$ and the dual\ncode to have minimum distance at least 3, our main result removes the length\nrestriction and relaxes the dual distance condition to at least 2.\nSpecifically, we prove that for any $[n,k]_q$ linear anticode $\\mathcal{C}$\nover $\\mathbb{F}_q$ with diameter $\\delta$ and $d(\\mathcal{C}^\\perp) \\geq 2$,\nthe inequality \\[ n \\leq \\sum_{i=0}^{k-1} \\left\\lfloor \\frac{\\delta}{q^i}\n\\right\\rfloor \\] holds. This generalization significantly broadens the\napplicability of the antiGriesmer bound. We derive several corollaries,\nincluding lower bounds on the diameter $\\delta$ in terms of $n$ and $k$, upper\nbounds on the code length $n$, and constraints on the dimension $k$.\nApplications to the construction and classification of linear codes with few\nweights are also discussed, along with examples demonstrating that our new\nbound can be sharper than previous ones. Our work unifies and extends earlier\nfindings, providing a more comprehensive framework for studying linear\nanticodes and their properties."}
{"id": "2511.02711", "categories": ["cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.02711", "abs": "https://arxiv.org/abs/2511.02711", "authors": ["Daren Chao", "Kaiwen Chen", "Naiqing Guan", "Nick Koudas"], "title": "Relational Deep Dive: Error-Aware Queries Over Unstructured Data", "comment": null, "summary": "Unstructured data is pervasive, but analytical queries demand structured\nrepresentations, creating a significant extraction challenge. Existing methods\nlike RAG lack schema awareness and struggle with cross-document alignment,\nleading to high error rates. We propose ReDD (Relational Deep Dive), a\nframework that dynamically discovers query-specific schemas, populates\nrelational tables, and ensures error-aware extraction with provable guarantees.\nReDD features a two-stage pipeline: (1) Iterative Schema Discovery (ISD)\nidentifies minimal, joinable schemas tailored to each query, and (2) Tabular\nData Population (TDP) extracts and corrects data using lightweight classifiers\ntrained on LLM hidden states. A main contribution of ReDD is SCAPE, a\nstatistically calibrated method for error detection with coverage guarantees,\nand SCAPE-HYB, a hybrid approach that optimizes the trade-off between accuracy\nand human correction costs. Experiments across diverse datasets demonstrate\nReDD's effectiveness, reducing data extraction errors from up to 30% to below\n1% while maintaining high schema completeness (100% recall) and precision.\nReDD's modular design enables fine-grained control over accuracy-cost\ntrade-offs, making it a robust solution for high-stakes analytical queries over\nunstructured corpora."}
{"id": "2511.02572", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.02572", "abs": "https://arxiv.org/abs/2511.02572", "authors": ["Rui Xu", "Yinghui Ye", "Xiaoli Chu", "Guangyue Lu", "Kai-Kit Wong", "Chan-Byoung Chae"], "title": "Performance Analysis of Single-Antenna Fluid Antenna Systems via Extreme Value Theory", "comment": null, "summary": "In single-antenna fluid antenna systems (FASs), the transceiver dynamically\nselects the antenna port with the strongest instantaneous channel to enhance\nlink reliability. However, deriving accurate yet tractable performance\nexpressions under fully correlated fading remains challenging, primarily due to\nthe absence of a closed-form distribution for the FAS channel. To address this\ngap, this paper develops a novel performance evaluation framework for FAS\noperating under fully correlated Rayleigh fading, by modeling the FAS channel\nthrough extreme value distributions (EVDs). We first justify the suitability of\nEVD modeling and approximate the FAS channel through the Gumbel distribution,\nwith parameters expressed as functions of the number of ports and the antenna\naperture size via the maximum likelihood (ML) criterion. Closed-form\nexpressions for the outage probability (OP) and ergodic capacity (EC) are then\nderived. While the Gumbel model provides an excellent fit, minor deviations\narise in the extreme-probability regions. To further improve accuracy, we\nextend the framework using the generalized extreme value (GEV) distribution and\nobtain closed-form OP and EC approximations based on ML-derived parameters.\nSimulation results confirm that the proposed GEV-based framework achieves\nsuperior accuracy over the Gumbel-based model, while both EVD-based approaches\noffer computationally efficient and analytically tractable tools for evaluating\nthe performance of FAS under realistic correlated fading conditions."}
{"id": "2511.02584", "categories": ["cs.IT", "cs.LG", "cs.NE", "math.IT", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2511.02584", "abs": "https://arxiv.org/abs/2511.02584", "authors": ["Mark Blümel", "Andreas C. Schneider", "Valentin Neuhaus", "David A. Ehrlich", "Marcel Graetz", "Michael Wibral", "Abdullah Makkeh", "Viola Priesemann"], "title": "Redundancy Maximization as a Principle of Associative Memory Learning", "comment": "21 pages, 8 figures", "summary": "Associative memory, traditionally modeled by Hopfield networks, enables the\nretrieval of previously stored patterns from partial or noisy cues. Yet, the\nlocal computational principles which are required to enable this function\nremain incompletely understood. To formally characterize the local information\nprocessing in such systems, we employ a recent extension of information theory\n- Partial Information Decomposition (PID). PID decomposes the contribution of\ndifferent inputs to an output into unique information from each input,\nredundant information across inputs, and synergistic information that emerges\nfrom combining different inputs. Applying this framework to individual neurons\nin classical Hopfield networks we find that below the memory capacity, the\ninformation in a neuron's activity is characterized by high redundancy between\nthe external pattern input and the internal recurrent input, while synergy and\nunique information are close to zero until the memory capacity is surpassed and\nperformance drops steeply. Inspired by this observation, we use redundancy as\nan information-theoretic learning goal, which is directly optimized for each\nneuron, dramatically increasing the network's memory capacity to 1.59, a more\nthan tenfold improvement over the 0.14 capacity of classical Hopfield networks\nand even outperforming recent state-of-the-art implementations of Hopfield\nnetworks. Ultimately, this work establishes redundancy maximization as a new\ndesign principle for associative memories and opens pathways for new\nassociative memory models based on information-theoretic goals."}
{"id": "2511.02803", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.02803", "abs": "https://arxiv.org/abs/2511.02803", "authors": ["Ismail Cosandal", "Sennur Ulukus"], "title": "Optimal Source Coding of Markov Chains for Real-Time Remote Estimation", "comment": null, "summary": "We revisit the source coding problem for a Markov chain under the assumption\nthat the transmission times and how fast the Markov chain transitions its state\nhappen at the same time-scale. Specifically, we assume that the transmission of\neach bit takes a single time slot, and the Markov chain updates its state in\nthe same time slot. Thus, the length of the codeword assigned to a symbol\ndetermines the number of non-transmitted symbols, as well as, the probability\nof the realization of the next symbol to be transmitted. We aim to minimize the\naverage transmission duration over an infinite horizon by proposing an optimal\nsource coding policy based on the last transmitted symbol and its transmission\nduration. To find the optimal policy, we formulate the problem with a Markov\ndecision process (MDP) by augmenting the symbols alongside the transmission\nduration of the symbols. Finally, we analyze two Huffman-based benchmark\npolicies and compare their performances with the proposed optimal policy. We\nobserve that, in randomly generated processes, our proposed optimal policy\ndecreases the average transmission duration compared to benchmark policies. The\nperformance gain varies based on the parameters of the Markov process."}
{"id": "2511.02813", "categories": ["cs.IT", "math.IT", "94B60"], "pdf": "https://arxiv.org/pdf/2511.02813", "abs": "https://arxiv.org/abs/2511.02813", "authors": ["Gustavo Terra Bastos", "Angelynn Álvarez", "Cameron Williams"], "title": "A Construction of Infinite Families of Self-Orthogonal Quasi-Cyclic Codes Using Constituent Codes.pdf", "comment": "20 pages", "summary": "Quasi-cyclic codes have been recently employed in the constructions of\nquantum error-correcting codes. In this paper, we propose a construction of\ninfinite families of quasi-cyclic codes which are self-orthogonal with respect\nto the Euclidean and Hermitian inner products. In particular, their dimension\nand a lower bound for their minimum distance are computed using their\nconstituent codes defined over field extensions of $\\mathbb{F}_q$. We also show\nthat the lower bound for the minimum distance satisfies the square-root-like\nlower bound and also show how self-dual quasi-cyclic codes can arise from our\nconstruction. Using the CSS construction, we show the existence of quantum\nerror-correcting codes with good parameters."}
