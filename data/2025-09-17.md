<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 43]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.IR](#cs.IR) [Total: 9]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 98]
- [cs.IT](#cs.IT) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [V-Math: An Agentic Approach to the Vietnamese National High School Graduation Mathematics Exams](https://arxiv.org/abs/2509.12251)
*Duong Q. Nguyen,Quy P. Nguyen,Nguyen Van Nhon,Quang-Thinh Bui,H. Nguyen-Xuan*

Main category: cs.AI

TL;DR: V-Math是一个面向越南高中生的自主智能框架，通过三个AI代理（题目生成器、解题器/解释器、个性化导师）帮助备考国家数学毕业考试，同时支持教师生成合规试题和建立题库。


<details>
  <summary>Details</summary>
Motivation: 帮助越南高中生更好地准备国家数学毕业考试，减轻教师手动出题的工作负担，提供个性化的学习支持和高质量的试题资源。

Method: 集成三个专门AI代理：基于规范矩阵的题目生成器、提供逐步推理的解题器/解释器、根据学生表现自适应的个性化导师系统。

Result: 初步评估显示V-Math能生成符合矩阵要求的考试题目，具有高解题准确率，提供连贯的解释，并丰富了练习材料的多样性。

Conclusion: V-Math有潜力支持规模化、公平的数学备考，符合国家标准，同时通过AI辅助的考试创建赋能教师。

Abstract: This paper develops an autonomous agentic framework called V-Math that aims
to assist Vietnamese high school students in preparing for the National High
School Graduation Mathematics Exams (NHSGMEs). The salient framework integrates
three specialized AI agents: a specification-matrix-conditioned question
generator, a solver/explainer for detailed step-by-step reasoning, and a
personalized tutor that adapts to student performance. Beyond enabling
self-paced student practice, V-Math supports teachers by generating innovative,
compliant exam questions and building diverse, high-quality question banks.
This reduces manual workload and enriches instructional resources. We describe
the system architecture, focusing on practice modes for learners and
teacher-oriented features for question generation. Preliminary evaluations
demonstrate that V-Math produces matrix-aligned exams with high solution
accuracy, delivers coherent explanations, and enhances the variety of practice
materials. These results highlight its potential to support scalable, equitable
mathematics preparation aligned with national standards while also empowering
teachers through AI-assisted exam creation.

</details>


### [2] [HLSMAC: A New StarCraft Multi-Agent Challenge for High-Level Strategic Decision-Making](https://arxiv.org/abs/2509.12927)
*Xingxing Hong,Yungong Wang,Dexin Jin,Ye Yuan,Ximing Huang,Zijian Wu,Wenxin Li*

Main category: cs.AI

TL;DR: HLSMAC是一个新的多智能体强化学习基准，基于三十六计设计了12个星际争霸II场景，用于评估高级战略决策能力，超越了传统微操测试。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体强化学习基准（如SMAC）主要关注微操层面，缺乏对高级战略智能的全面评估，需要新的基准来测试战略决策能力。

Method: 基于三十六计设计了12个星际争霸II场景，每个场景对应一个具体计策，包含战术机动、时机协调和欺骗等战略元素，并提出了超越胜率的多维度评估指标。

Result: 实验结果表明HLSMAC能够有效评估多智能体战略决策能力，为先进MARL算法和基于LLM的智能体提供了稳健的测试平台。

Conclusion: HLSMAC填补了多智能体强化学习在战略层面评估的空白，为研究高级战略决策能力提供了重要的基准测试环境。

Abstract: Benchmarks are crucial for assessing multi-agent reinforcement learning
(MARL) algorithms. While StarCraft II-related environments have driven
significant advances in MARL, existing benchmarks like SMAC focus primarily on
micromanagement, limiting comprehensive evaluation of high-level strategic
intelligence. To address this, we introduce HLSMAC, a new cooperative MARL
benchmark with 12 carefully designed StarCraft II scenarios based on classical
stratagems from the Thirty-Six Stratagems. Each scenario corresponds to a
specific stratagem and is designed to challenge agents with diverse strategic
elements, including tactical maneuvering, timing coordination, and deception,
thereby opening up avenues for evaluating high-level strategic decision-making
capabilities. We also propose novel metrics across multiple dimensions beyond
conventional win rate, such as ability utilization and advancement efficiency,
to assess agents' overall performance within the HLSMAC environment. We
integrate state-of-the-art MARL algorithms and LLM-based agents with our
benchmark and conduct comprehensive experiments. The results demonstrate that
HLSMAC serves as a robust testbed for advancing multi-agent strategic
decision-making.

</details>


### [3] [DISPLIB: a library of train dispatching problems](https://arxiv.org/abs/2509.12254)
*Oddvar Kloster,Bjørnar Luteberget,Carlo Mannino,Giorgio Sartor*

Main category: cs.AI

TL;DR: 提出了DISPLIB标准格式和数据集，用于统一列车调度优化问题的定义和评估，促进算法比较和可复现性研究


<details>
  <summary>Details</summary>
Motivation: 现有列车调度优化研究缺乏统一标准，代码和数据很少公开，导致难以比较不同算法的性能，阻碍了研究进展

Method: 定义通用的列车重路由和重调度问题格式DISPLIB，收集多个真实工业案例的问题实例，提供参考求解器实现

Result: 建立了公开可用的标准化数据集和问题定义，为研究人员提供了无需工业联系即可开展研究的平台

Conclusion: DISPLIB框架将促进列车调度优化领域的算法比较和协作研究，类似于MILP、SAT等其他优化领域的成功社区

Abstract: Optimization-based decision support systems have a significant potential to
reduce delays, and thus improve efficiency on the railways, by automatically
re-routing and re-scheduling trains after delays have occurred. The operations
research community has dedicated a lot of effort to developing optimization
algorithms for this problem, but each study is typically tightly connected with
a specific industrial use case. Code and data are seldom shared publicly. This
fact hinders reproducibility, and has led to a proliferation of papers
describing algorithms for more or less compatible problem definitions, without
any real opportunity for readers to assess their relative performance. Inspired
by the successful communities around MILP, SAT, TSP, VRP, etc., we introduce a
common problem definition and file format, DISPLIB, which captures all the main
features of train re-routing and re-scheduling. We have gathered problem
instances from multiple real-world use cases and made them openly available. In
this paper, we describe the problem definition, the industrial instances, and a
reference solver implementation. This allows any researcher or developer to
work on the train dispatching problem without an industrial connection, and
enables the research community to perform empirical comparisons between
solvers. All materials are available online at https://displib.github.io.

</details>


### [4] [InPhyRe Discovers: Large Multimodal Models Struggle in Inductive Physical Reasoning](https://arxiv.org/abs/2509.12263)
*Gautam Sreekumar,Vishnu Naresh Boddeti*

Main category: cs.AI

TL;DR: 提出了InPhyRe基准测试，用于评估大型多模态模型在违反物理规律的场景中的归纳物理推理能力，发现现有模型存在参数知识应用困难、语言偏见和视觉输入忽视等问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型仅编码训练时观察到的物理规律作为参数知识，无法处理违反这些规律的推理场景。人类具备从少量视觉示例中适应新物理环境的能力，这种归纳物理推理能力对安全关键应用至关重要，但现有基准测试未能评估这一能力。

Method: 提出InPhyRe基准测试，通过算法生成的合成碰撞视频来评估模型预测碰撞事件结果的能力。该基准测试13个大型多模态模型在违反物理规律场景下的表现。

Result: 研究发现：(1)模型难以将有限的参数物理知识应用于推理；(2)当演示样本违反物理规律时，模型的归纳物理推理能力较弱；(3)模型存在语言偏见，很大程度上忽视视觉输入，质疑其对视觉输入的可信度。

Conclusion: 大型多模态模型在归纳物理推理方面存在显著局限性，特别是在处理违反训练时物理规律的场景时表现不佳，这对其在安全关键应用中的可靠性提出了质疑。

Abstract: Large multimodal models (LMMs) encode universal physical laws observed during
training, such as momentum conservation, as parametric knowledge. It allows
LMMs to answer physical reasoning queries, such as the outcome of a potential
collision event from visual input. However, since parametric knowledge includes
only the physical laws seen during training, it is insufficient for reasoning
when the inference scenario violates these physical laws. In contrast, humans
possess the skill to adapt their physical reasoning to unseen physical
environments from a few visual examples. This ability, which we refer to as
inductive physical reasoning, is indispensable for LMMs if they are to replace
human agents in safety-critical applications. Despite its importance, existing
visual benchmarks evaluate only the parametric knowledge in LMMs, and not
inductive physical reasoning. To this end, we propose InPhyRe, the first visual
question answering benchmark to measure inductive physical reasoning in LMMs.
InPhyRe evaluates LMMs on their ability to predict the outcome of collision
events in algorithmically generated synthetic collision videos. By inspecting
13 LMMs, InPhyRe informs us that (1) LMMs struggle to apply their limited
parametric knowledge about universal physical laws to reasoning, (2) inductive
physical reasoning in LMMs is weak when demonstration samples violate universal
physical laws, and (3) inductive physical reasoning in LMMs suffers from
language bias and largely ignores the visual inputs, questioning the
trustworthiness of LMMs regarding visual inputs.

</details>


### [5] [LLMAP: LLM-Assisted Multi-Objective Route Planning with User Preferences](https://arxiv.org/abs/2509.12273)
*Liangqi Yuan,Dong-Jun Han,Christopher G. Brinton,Sabine Brunswicker*

Main category: cs.AI

TL;DR: 提出LLMAP系统，结合LLM解析自然语言偏好和多步图搜索算法，实现多约束条件下的最优路线规划


<details>
  <summary>Details</summary>
Motivation: 现有路线规划方法存在局限性：直接使用LLM处理地图数据困难，图搜索方法理解自然语言偏好能力有限，且用户时空分布高度异构不可预测

Method: 使用LLM-as-Parser解析自然语言、识别任务、提取用户偏好和任务依赖关系，结合多步图构造迭代搜索算法(MSGS)进行最优路线查找，采用多目标优化自适应调整权重

Result: 在14个国家27个城市的1000个路由提示上进行实验，结果显示该方法在多重约束下实现了优越性能

Conclusion: LLMAP系统能够有效处理自然语言驱动的路线规划问题，在保证多约束条件的同时实现最优路线规划

Abstract: The rise of large language models (LLMs) has made natural language-driven
route planning an emerging research area that encompasses rich user objectives.
Current research exhibits two distinct approaches: direct route planning using
LLM-as-Agent and graph-based searching strategies. However, LLMs in the former
approach struggle to handle extensive map data, while the latter shows limited
capability in understanding natural language preferences. Additionally, a more
critical challenge arises from the highly heterogeneous and unpredictable
spatio-temporal distribution of users across the globe. In this paper, we
introduce a novel LLM-Assisted route Planning (LLMAP) system that employs an
LLM-as-Parser to comprehend natural language, identify tasks, and extract user
preferences and recognize task dependencies, coupled with a Multi-Step Graph
construction with iterative Search (MSGS) algorithm as the underlying solver
for optimal route finding. Our multi-objective optimization approach adaptively
tunes objective weights to maximize points of interest (POI) quality and task
completion rate while minimizing route distance, subject to three key
constraints: user time limits, POI opening hours, and task dependencies. We
conduct extensive experiments using 1,000 routing prompts sampled with varying
complexity across 14 countries and 27 cities worldwide. The results demonstrate
that our approach achieves superior performance with guarantees across multiple
constraints.

</details>


### [6] [Developing an aeroponic smart experimental greenhouse for controlling irrigation and plant disease detection using deep learning and IoT](https://arxiv.org/abs/2509.12274)
*Mohammadreza Narimani,Ali Hajiahmad,Ali Moghimi,Reza Alimardani,Shahin Rafiee,Amir Hossein Mirzabe*

Main category: cs.AI

TL;DR: 开发智能气雾温室系统，结合物联网和人工智能技术监控环境条件和植物健康状态，VGG-19算法在病害检测中达到92%准确率


<details>
  <summary>Details</summary>
Motivation: 温室环境控制和植物状态监测对作物生产管理至关重要，需要开发智能化系统来实时监控和做出管理决策

Method: 开发物联网平台监控温湿度等环境参数，使用VGG-19、InceptionResNetV2和InceptionV3算法构建AI病害检测框架，通过定期图像采集分析植物健康状况

Result: 物联网系统成功在线发布温度、湿度、水流等数据并自动调整参数优化生长环境；VGG-19算法在干旱胁迫和锈病叶片识别中准确率达到92%，优于其他算法

Conclusion: 物联网与人工智能技术的结合能够有效实现温室环境的智能监控和植物病害的早期检测，为精准农业管理提供可靠的技术支持

Abstract: Controlling environmental conditions and monitoring plant status in
greenhouses is critical to promptly making appropriate management decisions
aimed at promoting crop production. The primary objective of this research
study was to develop and test a smart aeroponic greenhouse on an experimental
scale where the status of Geranium plant and environmental conditions are
continuously monitored through the integration of the internet of things (IoT)
and artificial intelligence (AI). An IoT-based platform was developed to
control the environmental conditions of plants more efficiently and provide
insights to users to make informed management decisions. In addition, we
developed an AI-based disease detection framework using VGG-19,
InceptionResNetV2, and InceptionV3 algorithms to analyze the images captured
periodically after an intentional inoculation. The performance of the AI
framework was compared with an expert's evaluation of disease status.
Preliminary results showed that the IoT system implemented in the greenhouse
environment is able to publish data such as temperature, humidity, water flow,
and volume of charge tanks online continuously to users and adjust the
controlled parameters to provide an optimal growth environment for the plants.
Furthermore, the results of the AI framework demonstrate that the VGG-19
algorithm was able to identify drought stress and rust leaves from healthy
leaves with the highest accuracy, 92% among the other algorithms.

</details>


### [7] [AIssistant: An Agentic Approach for Human--AI Collaborative Scientific Work on Reviews and Perspectives in Machine Learning](https://arxiv.org/abs/2509.12282)
*Sasi Kiran Gaddipati,Farhana Keya,Gollam Rabby,Sören Auer*

Main category: cs.AI

TL;DR: AIssistant是一个开源的人类-AI协作框架，旨在简化科学工作流的端到端创建，通过模块化工具和代理实现文献合成、实验、引用管理和LaTeX论文生成，同时保持人类监督以确保准确性。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助研究工具虽然强大但分散，缺乏以人为中心的工作流程，需要整合的协作框架来提升科研效率和质量。

Method: 开发AIssistant框架，集成模块化工具和代理进行文献合成、分段实验、引用管理和自动LaTeX生成，采用三层评估：独立人类评审、自动化LLM评审和程序主席监督。

Result: AIssistant提高了起草效率和主题一致性，但人类-AI协作对于保持事实正确性、方法合理性和伦理合规性仍然至关重要。

Conclusion: 尽管AIssistant在提升科研效率方面有效，但仍存在引用幻觉、难以适应动态论文结构以及多模态内容整合不完全等关键限制，需要进一步改进。

Abstract: Advances in AI-assisted research have introduced powerful tools for
literature retrieval, hypothesis generation, experimentation, and manuscript
preparation. However, systems remain fragmented and lack human-centred
workflows. To address these gaps, we introduce AIssistant, an agentic,
open-source Human-AI collaborative framework designed to simplify the
end-to-end creation of scientific workflows. Since our development is still in
an early stage, we present here the first experiments with AIssistant for
perspective and review research papers in machine learning. Our system
integrates modular tools and agents for literature synthesis, section-wise
experimentation, citation management, and automatic LaTeX paper text
generation, while maintaining human oversight at every stage to ensure
accuracy, coherence, and scholarly rigour. We conducted a comprehensive
evaluation across three layers: (1) Independent Human Review, following NeurIPS
double-blind standards; (2) Automated LLM Review, using GPT-5 as a scalable
human review proxy; and (3) Program Chair Oversight, where the chair monitors
the entire review process and makes final validation and acceptance decisions.
The results demonstrate that AIssistant improves drafting efficiency and
thematic consistency. Nonetheless, Human-AI collaboration remains essential for
maintaining factual correctness, methodological soundness, and ethical
compliance. Despite its effectiveness, we identify key limitations, including
hallucinated citations, difficulty adapting to dynamic paper structures, and
incomplete integration of multimodal content.

</details>


### [8] [Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition](https://arxiv.org/abs/2509.12423)
*Danielle Cohen,Yoni Halpern,Noam Kahlon,Joel Oren,Omri Berkovitch,Sapir Caduri,Ido Dagan,Anatoly Efros*

Main category: cs.AI

TL;DR: 提出了一种分解式方法，通过结构化交互摘要和意图提取，提升资源受限设备上用户意图理解的准确性，甚至超越大型多模态语言模型的基准性能。


<details>
  <summary>Details</summary>
Motivation: 解决在设备端运行的轻量级模型难以准确理解用户UI交互轨迹意图的问题，同时满足隐私保护、低成本和低延迟的需求。

Method: 采用两阶段方法：首先对每个用户动作进行结构化交互摘要，捕捉关键信息；然后使用微调模型在聚合摘要上进行意图提取。

Result: 该方法显著提升了资源受限模型的意图理解能力，性能甚至超过了大型多模态语言模型的基准表现。

Conclusion: 分解式方法为设备端智能代理提供了一种有效的意图理解解决方案，在保持隐私和效率的同时实现了优异的性能。

Abstract: Understanding user intents from UI interaction trajectories remains a
challenging, yet crucial, frontier in intelligent agent development. While
massive, datacenter-based, multi-modal large language models (MLLMs) possess
greater capacity to handle the complexities of such sequences, smaller models
which can run on-device to provide a privacy-preserving, low-cost, and
low-latency user experience, struggle with accurate intent inference. We
address these limitations by introducing a novel decomposed approach: first, we
perform structured interaction summarization, capturing key information from
each user action. Second, we perform intent extraction using a fine-tuned model
operating on the aggregated summaries. This method improves intent
understanding in resource-constrained models, even surpassing the base
performance of large MLLMs.

</details>


### [9] [Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization](https://arxiv.org/abs/2509.12434)
*Jiahao Yu,Zelei Cheng,Xian Wu,Xinyu Xing*

Main category: cs.AI

TL;DR: 本文提出了一个名为\sys的熵增强框架，用于改进大型语言模型在软件工程任务中的表现，通过保持策略熵和多轮交互优化，在SWE-bench基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在复杂软件工程任务中表现不佳，标准对齐方法如DPO和KTO虽然能对齐人类偏好，但会降低输出多样性，限制测试时扩展的效果。现有方法主要针对单轮任务，无法充分处理多轮推理和工具集成需求。

Method: 提出\sys框架，通过增强偏好目标来显式保持策略熵，并将学习推广到优化多轮交互而非单轮响应。还提出了结合学习验证器模型和模型无关方法的混合最佳轨迹选择方案。

Result: 在SWE-bench排行榜上，30B参数的模型在\lite上排名第1，在\verified上排名第4，仅被参数量超过10倍（>350B）的模型超越，在开源权重模型中建立了新的最先进结果。

Conclusion: \sys框架成功地将偏好优化算法适应到多轮工具辅助设置中，通过保持输出多样性和优化多轮交互，显著提升了LLM在复杂软件工程任务中的性能。

Abstract: Software engineering presents complex, multi-step challenges for Large
Language Models (LLMs), requiring reasoning over large codebases and
coordinated tool use. The difficulty of these tasks is exemplified by
benchmarks like SWE-bench, where current LLMs still struggle to resolve
real-world issues.
  A promising approach to enhance performance is test-time scaling (TTS), but
its gains are heavily dependent on the diversity of model outputs.
  While standard alignment methods such as Direct Preference Optimization (DPO)
and Kahneman-Tversky Optimization (KTO) are effective at aligning model outputs
with human preferences, this process can come at the cost of reduced diversity,
limiting the effectiveness of TTS.
  Additionally, existing preference optimization algorithms are typically
designed for single-turn tasks and do not fully address the complexities of
multi-turn reasoning and tool integration required for interactive coding
agents.
  To bridge this gap, we introduce \sys, an entropy-enhanced framework that
adapts existing preference optimization algorithms to the multi-turn,
tool-assisted setting.
  \sys augments the preference objective to explicitly preserve policy entropy
and generalizes learning to optimize over multi-turn interactions rather than
single-turn responses.
  We validate \sys by fine-tuning a diverse suite of models from different
families and sizes (up to 106B parameters).
  To maximize performance gains from TTS, we further propose a hybrid
best-trajectory selection scheme combining a learned verifier model with model
free approaches.
  On the \swebench leaderboard, our approach establishes new state-of-the-art
results among open-weight models. A 30B parameter model trained with \sys ranks
1st on \lite and 4th on \verified on the open-weight leaderboard, surpassed
only by models with over 10x more parameters(\eg$>$350B).

</details>


### [10] [Enhancing Physical Consistency in Lightweight World Models](https://arxiv.org/abs/2509.12437)
*Dingrui Wang,Zhexiao Sun,Zhouheng Li,Cheng Wang,Youlun Peng,Hongyuan Ye,Baha Zarrouki,Wei Li,Mattia Piccinini,Lei Xie,Johannes Betz*

Main category: cs.AI

TL;DR: PIWM是一个紧凑的鸟瞰图世界模型，通过Soft Mask训练和Warm Start推理技术，在保持小参数规模的同时显著提升物理动态建模性能


<details>
  <summary>Details</summary>
Motivation: 解决世界模型部署中大小与性能的权衡问题 - 大模型计算资源需求高难以在边缘设备部署，小模型物理建模精度不足

Method: 提出Physics-Informed BEV World Model (PIWM)：1) 使用Soft Mask训练改进动态物体建模和未来预测 2) 引入Warm Start推理技术零样本提升预测质量

Result: 在相同参数规模(400M)下，PIWM比基线模型提升60.6%的加权总分；最小PIWM模型(130M)比最大基线模型(400M)得分高7.4%，推理速度快28%

Conclusion: PIWM通过创新的训练和推理技术，实现了小参数规模下的高性能物理建模，为边缘设备部署高效世界模型提供了有效解决方案

Abstract: A major challenge in deploying world models is the trade-off between size and
performance. Large world models can capture rich physical dynamics but require
massive computing resources, making them impractical for edge devices. Small
world models are easier to deploy but often struggle to learn accurate physics,
leading to poor predictions. We propose the Physics-Informed BEV World Model
(PIWM), a compact model designed to efficiently capture physical interactions
in bird's-eye-view (BEV) representations. PIWM uses Soft Mask during training
to improve dynamic object modeling and future prediction. We also introduce a
simple yet effective technique, Warm Start, for inference to enhance prediction
quality with a zero-shot model. Experiments show that at the same parameter
scale (400M), PIWM surpasses the baseline by 60.6% in weighted overall score.
Moreover, even when compared with the largest baseline model (400M), the
smallest PIWM (130M Soft Mask) achieves a 7.4% higher weighted overall score
with a 28% faster inference speed.

</details>


### [11] [Reasoning Models Can be Accurately Pruned Via Chain-of-Thought Reconstruction](https://arxiv.org/abs/2509.12464)
*Ryan Lucas,Kayhan Behdin,Zhipeng Wang,Qingquan Song,Shao Tang,Rahul Mazumder*

Main category: cs.AI

TL;DR: 论文提出Reasoning-Aware Compression (RAC)方法，通过在剪枝过程中同时重构输入和思维链激活，显著提升推理语言模型的压缩性能。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络剪枝方法在推理语言模型上表现不佳，甚至会导致模型产生更多思维标记但性能下降，主要因为标准剪枝方法专注于输入重构，而推理是解码主导的任务。

Method: 在剪枝过程中联合重构输入和模型策略内思维链轨迹的激活，该方法可无缝集成到现有剪枝工作流（如SparseGPT）中。

Result: RAC方法显著提升了现有剪枝方法的性能，解决了推理模型压缩中的性能损失问题。

Conclusion: 通过考虑推理任务的特殊性，RAC为推理语言模型的高效压缩提供了简单有效的解决方案。

Abstract: Reasoning language models such as DeepSeek-R1 produce long chain-of-thought
traces during inference time which make them costly to deploy at scale. We show
that using compression techniques such as neural network pruning produces
greater performance loss than in typical language modeling tasks, and in some
cases can make the model slower since they cause the model to produce more
thinking tokens but with worse performance. We show that this is partly due to
the fact that standard LLM pruning methods often focus on input reconstruction,
whereas reasoning is a decode-dominated task. We introduce a simple, drop-in
fix: during pruning we jointly reconstruct activations from the input and the
model's on-policy chain-of-thought traces. This "Reasoning-Aware Compression"
(RAC) integrates seamlessly into existing pruning workflows such as SparseGPT,
and boosts their performance significantly. Code reproducing the results in the
paper can be found at: https://github.com/RyanLucas3/RAC

</details>


### [12] [Empowering Clinical Trial Design through AI: A Randomized Evaluation of PowerGPT](https://arxiv.org/abs/2509.12471)
*Yiwen Lu,Lu Li,Dazheng Zhang,Xinyao Jian,Tingyin Wang,Siqi Chen,Yuqing Lei,Jiayi Tong,Zhaohan Xi,Haitao Chu,Chongliang Luo,Alexis Ogdie,Brian Athey,Alparslan Turan,Michael Abramoff,Joseph C Cappelleri,Hua Xu,Yun Lu,Jesse Berlin,Daniel I. Sessler,David A. Asch,Xiaoqian Jiang,Yong Chen*

Main category: cs.AI

TL;DR: PowerGPT是一个AI驱动的系统，通过整合大语言模型和统计引擎，自动化临床试验设计中的检验选择和样本量计算，显著提高了任务完成率、准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 临床研究和试验设计中的样本量计算对功效分析至关重要，但其复杂性和对统计专业知识的依赖给许多研究人员造成了障碍。

Method: 开发了PowerGPT系统，将大语言模型与统计引擎集成，自动化测试选择和样本量估计过程。通过随机试验评估其有效性。

Result: PowerGPT显著提高了任务完成率（检验选择99.3% vs 88.9%，样本量计算99.3% vs 77.8%）和准确性（样本量估计94.1% vs 55.4%，p<0.001），同时减少了平均完成时间（4.0 vs 9.3分钟，p<0.001）。这些优势在各种统计检验中保持一致，惠及统计学家和非统计学家。

Conclusion: PowerGPT代表了一种可扩展的AI驱动方法，提高了临床研究中统计功效分析的可及性、效率和准确性，已在多个机构部署使用。

Abstract: Sample size calculations for power analysis are critical for clinical
research and trial design, yet their complexity and reliance on statistical
expertise create barriers for many researchers. We introduce PowerGPT, an
AI-powered system integrating large language models (LLMs) with statistical
engines to automate test selection and sample size estimation in trial design.
In a randomized trial to evaluate its effectiveness, PowerGPT significantly
improved task completion rates (99.3% vs. 88.9% for test selection, 99.3% vs.
77.8% for sample size calculation) and accuracy (94.1% vs. 55.4% in sample size
estimation, p < 0.001), while reducing average completion time (4.0 vs. 9.3
minutes, p < 0.001). These gains were consistent across various statistical
tests and benefited both statisticians and non-statisticians as well as
bridging expertise gaps. Already under deployment across multiple institutions,
PowerGPT represents a scalable AI-driven approach that enhances accessibility,
efficiency, and accuracy in statistical power analysis for clinical research.

</details>


### [13] [Physical Complexity of a Cognitive Artifact](https://arxiv.org/abs/2509.12495)
*Gülce Kardeş,David Krakauer,Joshua Grochow*

Main category: cs.AI

TL;DR: 本文通过分析Soma Cube物理拼图的计算复杂度，将计算机科学概念映射到认知问题解决策略，提出了"物质性原则"，展示了如何通过分层策略降低任务难度


<details>
  <summary>Details</summary>
Motivation: 认知科学和理论计算机科学都致力于分类和解释任务难度，本文旨在探索智能机制如何通过利用物理约束和认知策略来降低任务复杂性

Method: 通过分析Soma Cube拼图的分支因子（搜索树出度）来量化任务难度，逐步改进试错搜索方法：包括预处理（认知组块）、值排序（认知自由排序）、变量排序（认知支架）和剪枝（认知推理）

Result: 研究发现通过分层策略可以系统性地降低任务复杂度，有效利用物理制品能够通过利用物理约束来降低时间复杂性

Conclusion: 提出了一个将智能视为算法库的模型，该模型能够同时调用心智和物质的能力，展示了物质性原则在理解智能机制中的重要性

Abstract: Cognitive science and theoretical computer science both seek to classify and
explain the difficulty of tasks. Mechanisms of intelligence are those that
reduce task difficulty. Here we map concepts from the computational complexity
of a physical puzzle, the Soma Cube, onto cognitive problem-solving strategies
through a ``Principle of Materiality''. By analyzing the puzzle's branching
factor, measured through search tree outdegree, we quantitatively assess task
difficulty and systematically examine how different strategies modify
complexity. We incrementally refine a trial-and-error search by layering
preprocessing (cognitive chunking), value ordering (cognitive free-sorting),
variable ordering (cognitive scaffolding), and pruning (cognitive inference).
We discuss how the competent use of artifacts reduces effective time complexity
by exploiting physical constraints and propose a model of intelligence as a
library of algorithms that recruit the capabilities of both mind and matter.

</details>


### [14] [A Dimensionality-Reduced XAI Framework for Roundabout Crash Severity Insights](https://arxiv.org/abs/2509.12524)
*Rohit Chakraborty,Subasish Das*

Main category: cs.AI

TL;DR: 本研究通过可解释的两步工作流程分析俄亥俄州环岛事故，识别出四种事故模式，并使用SHAP解释树模型量化伤害驱动因素，发现黑暗、湿滑路面和高速条件下事故更严重，而清晰、低速环境下事故较轻。


<details>
  <summary>Details</summary>
Motivation: 环岛虽然减少了严重事故，但风险模式因条件而异。研究旨在开发一个可解释的工作流程，识别环岛事故中的共同发生因素和伤害驱动机制，为公共安全分析提供实用的可解释人工智能模板。

Method: 采用两步可解释工作流程：首先使用聚类对应分析（CCA）识别共同发生因素并得出四种事故模式；然后使用基于树的严重性模型，并通过SHAP进行解释，量化模式内和跨模式的伤害驱动因素。

Result: 研究结果显示，在黑暗、湿滑路面和较高限速条件下，与固定物体或角度事件同时发生时事故严重性更高；在清晰、低速环境下事故严重性较低。特定模式的解释突出了入口处（未能让行、间隙接受）、多车道循环内（不当操作）和减速期间（追尾）的机制。

Conclusion: 该工作流程将模式发现与案例级解释联系起来，支持现场筛查、对策选择和审计就绪的报告。对信息系统的贡献是为公共安全分析提供了一个实用的可解释人工智能模板。

Abstract: Roundabouts reduce severe crashes, yet risk patterns vary by conditions. This
study analyzes 2017-2021 Ohio roundabout crashes using a two-step, explainable
workflow. Cluster Correspondence Analysis (CCA) identifies co-occurring factors
and yields four crash patterns. A tree-based severity model is then interpreted
with SHAP to quantify drivers of injury within and across patterns. Results
show higher severity when darkness, wet surfaces, and higher posted speeds
coincide with fixed-object or angle events, and lower severity in clear,
low-speed settings. Pattern-specific explanations highlight mechanisms at
entries (fail-to-yield, gap acceptance), within multi-lane circulation
(improper maneuvers), and during slow-downs (rear-end). The workflow links
pattern discovery with case-level explanations, supporting site screening,
countermeasure selection, and audit-ready reporting. The contribution to
Information Systems is a practical template for usable XAI in public safety
analytics.

</details>


### [15] [zELO: ELO-inspired Training Method for Rerankers and Embedding Models](https://arxiv.org/abs/2509.12541)
*Nicholas Pipitone,Ghita Houir Alami,Advaith Avadhanam,Anton Kaminskyi,Ashley Khoo*

Main category: cs.AI

TL;DR: zELO是一种新的训练方法，通过将排名任务静态等价于Thurstone模型来优化检索性能，基于此方法训练了zerank-1和zerank-1-small模型，在多个领域达到最先进的检索效果


<details>
  <summary>Details</summary>
Motivation: 开发一种新的训练方法，利用无监督数据训练高性能的重排序模型，以超越闭源专有模型在多个领域的检索性能

Method: 基于zELO训练方法，使用Thurstone模型将排名任务静态等价化，使用112,000个查询和每个查询100个文档的无标注数据进行端到端训练

Result: zerank-1和zerank-1-small模型在金融、法律、代码和STEM等多个领域获得最高检索分数，在NDCG@10和Recall指标上超越闭源专有重排序器，并在零样本设置下保持优异性能

Conclusion: zELO方法能够有效利用无监督数据训练出性能卓越的重排序模型，在多个专业领域和零样本场景下都表现出色，训练效率高（少于10,000 H100小时）

Abstract: We introduce a novel training methodology named zELO, which optimizes
retrieval performance via the analysis that ranking tasks are statically
equivalent to a Thurstone model. Based on the zELO method, we use unsupervised
data in order train a suite of state-of-the-art open-weight reranker models:
zerank-1 and zerank-1-small. These models achieve the highest retrieval scores
in multiple domains, including finance, legal, code, and STEM, outperforming
closed-source proprietary rerankers on both NDCG@10 and Recall. These models
also demonstrate great versatility, maintaining their 0-shot performance on
out-of-domain and private customer datasets. The training data included 112,000
queries and 100 documents per query, and was trained end-to-end from
unannotated queries and documents in less than 10,000 H100-hours.

</details>


### [16] [Human + AI for Accelerating Ad Localization Evaluation](https://arxiv.org/abs/2509.12543)
*Harshit Rajgarhia,Shivali Dalmia,Mengyang Zhao,Mukherji Abhishek,Kiran Ganesh*

Main category: cs.AI

TL;DR: 提出了一个结合自动组件和人工监督的结构化框架，用于解决广告本地化的复杂性，整合了场景文本检测、修复、机器翻译和文本重排技术。


<details>
  <summary>Details</summary>
Motivation: 多语言广告适配需要保持视觉一致性、空间对齐和风格完整性，而不仅仅是简单的文本翻译。

Method: 结合场景文本检测、修复、机器翻译和文本重排的自动化流程，并加入人工监督。

Result: 在六个地区的定性结果显示，该方法能产生语义准确且视觉一致的本地化广告，适合实际工作流部署。

Conclusion: 该框架是首个专门用于加速广告本地化评估工作流的集成解决方案，能有效处理多语言广告本地化的复杂需求。

Abstract: Adapting advertisements for multilingual audiences requires more than simple
text translation; it demands preservation of visual consistency, spatial
alignment, and stylistic integrity across diverse languages and formats. We
introduce a structured framework that combines automated components with human
oversight to address the complexities of advertisement localization. To the
best of our knowledge, this is the first work to integrate scene text
detection, inpainting, machine translation (MT), and text reimposition
specifically for accelerating ad localization evaluation workflows. Qualitative
results across six locales demonstrate that our approach produces semantically
accurate and visually coherent localized advertisements, suitable for
deployment in real-world workflows.

</details>


### [17] [Redefining CX with Agentic AI: Minerva CQ Case Study](https://arxiv.org/abs/2509.12589)
*Garima Agrawal,Riccardo De Maria,Kiran Davuluri,Daniele Spera,Charlie Read,Cosimo Spera,Jack Garrett,Don Miller*

Main category: cs.AI

TL;DR: 本文介绍了Minerva CQ，一种基于Agentic AI的实时客服助手系统，通过主动识别客户意图、触发模块化工作流和动态适应对话状态，显著提升了客服效率和客户体验。


<details>
  <summary>Details</summary>
Motivation: 传统客服系统存在处理时间长、首次解决率低和客户满意度差的问题，主要原因是客服人员面临认知负荷，需要操作碎片化系统并进行手动故障排除。现有的AI辅助工具多为被动响应，缺乏深度上下文推理能力。

Method: 采用Agentic AI方法，构建目标驱动、自主、使用工具的系统，实时主动支持客服人员。系统集成实时转录、意图和情感检测、实体识别、上下文检索、动态客户画像和部分对话摘要等功能。

Result: Minerva CQ作为AI副驾驶在生产环境中部署，在多个实施案例中实现了客服效率和客户体验的可衡量改进。

Conclusion: Agentic AI驱动的实时客服助手系统能够有效减轻客服认知负荷，通过主动工作流和持续上下文构建显著提升客服运营指标和客户满意度。

Abstract: Despite advances in AI for contact centers, customer experience (CX)
continues to suffer from high average handling time (AHT), low first-call
resolution, and poor customer satisfaction (CSAT). A key driver is the
cognitive load on agents, who must navigate fragmented systems, troubleshoot
manually, and frequently place customers on hold. Existing AI-powered
agent-assist tools are often reactive driven by static rules, simple prompting,
or retrieval-augmented generation (RAG) without deeper contextual reasoning. We
introduce Agentic AI goal-driven, autonomous, tool-using systems that
proactively support agents in real time. Unlike conventional approaches,
Agentic AI identifies customer intent, triggers modular workflows, maintains
evolving context, and adapts dynamically to conversation state. This paper
presents a case study of Minerva CQ, a real-time Agent Assist product deployed
in voice-based customer support. Minerva CQ integrates real-time transcription,
intent and sentiment detection, entity recognition, contextual retrieval,
dynamic customer profiling, and partial conversational summaries enabling
proactive workflows and continuous context-building. Deployed in live
production, Minerva CQ acts as an AI co-pilot, delivering measurable
improvements in agent efficiency and customer experience across multiple
deployments.

</details>


### [18] [Match Chat: Real Time Generative AI and Generative Computing for Tennis](https://arxiv.org/abs/2509.12592)
*Aaron Baughman,Gozde Akay,Eduardo Morales,Rahul Agarwal,Preetika Srivastava*

Main category: cs.AI

TL;DR: Match Chat是一个实时AI驱动的网球比赛助手，在2025年温网和美网中为约100万用户提供即时准确的比赛相关查询响应，准确率达92.83%，平均响应时间6.25秒。


<details>
  <summary>Details</summary>
Motivation: 提升网球球迷观赛体验，通过自然语言查询提供实时比赛洞察，解决传统数据访问方式的不便。

Method: 采用面向代理架构(AOA)，结合规则引擎、预测模型和代理技术预处理用户查询，再传递给GenAI组件，使用交互式提示设计引导96.08%的查询。

Result: 系统在120 RPS负载下保持92.83%的准确率和6.25秒平均响应时间，支持近100万用户，100%正常运行时间，无需技术背景即可使用。

Conclusion: 该工作展示了在动态环境中部署高性能代理系统的实用路径，为实时面向消费者的AI系统提供了强调速度、精确性和可用性的关键设计模式。

Abstract: We present Match Chat, a real-time, agent-driven assistant designed to
enhance the tennis fan experience by delivering instant, accurate responses to
match-related queries. Match Chat integrates Generative Artificial Intelligence
(GenAI) with Generative Computing (GenComp) techniques to synthesize key
insights during live tennis singles matches. The system debuted at the 2025
Wimbledon Championships and the 2025 US Open, where it provided about 1 million
users with seamless access to streaming and static data through natural
language queries. The architecture is grounded in an Agent-Oriented
Architecture (AOA) combining rule engines, predictive models, and agents to
pre-process and optimize user queries before passing them to GenAI components.
The Match Chat system had an answer accuracy of 92.83% with an average response
time of 6.25 seconds under loads of up to 120 requests per second (RPS). Over
96.08% of all queries were guided using interactive prompt design, contributing
to a user experience that prioritized clarity, responsiveness, and minimal
effort. The system was designed to mask architectural complexity, offering a
frictionless and intuitive interface that required no onboarding or technical
familiarity. Across both Grand Slam deployments, Match Chat maintained 100%
uptime and supported nearly 1 million unique users, underscoring the
scalability and reliability of the platform. This work introduces key design
patterns for real-time, consumer-facing AI systems that emphasize speed,
precision, and usability that highlights a practical path for deploying
performant agentic systems in dynamic environments.

</details>


### [19] [DaSAThco: Data-Aware SAT Heuristics Combinations Optimization via Large Language Models](https://arxiv.org/abs/2509.12602)
*Minyu Chen,Guoqiang Li*

Main category: cs.AI

TL;DR: DaSAThco是一个使用大型语言模型生成专门化启发式集成并通过自适应选择机制实现从实例特征到启发式配置的通用映射的框架，解决了SAT求解器配置的泛化问题。


<details>
  <summary>Details</summary>
Motivation: SAT问题的异质性使得单一最优配置不可行，而现有的数据集特定方法缺乏泛化能力且需要对新问题类型进行昂贵的重新优化。

Method: 使用大型语言模型在系统定义的问题原型指导下生成多样化的专门化启发式集成组合，然后学习自适应选择机制形成最终映射。

Result: 实验表明DaSAThco实现了优越性能，特别是在非自适应方法表现有限的领域外泛化方面表现出强大的鲁棒性。

Conclusion: 这项工作为复杂可配置系统的自动化算法设计建立了更可扩展和实用的路径。

Abstract: The performance of Conflict-Driven Clause Learning solvers hinges on internal
heuristics, yet the heterogeneity of SAT problems makes a single, universally
optimal configuration unattainable. While prior automated methods can find
specialized configurations for specific problem families, this dataset-specific
approach lacks generalizability and requires costly re-optimization for new
problem types. We introduce DaSAThco, a framework that addresses this challenge
by learning a generalizable mapping from instance features to tailored
heuristic ensembles, enabling a train-once, adapt-broadly model. Our framework
uses a Large Language Model, guided by systematically defined Problem
Archetypes, to generate a diverse portfolio of specialized heuristic ensembles
and subsequently learns an adaptive selection mechanism to form the final
mapping. Experiments show that DaSAThco achieves superior performance and, most
notably, demonstrates robust out-of-domain generalization where non-adaptive
methods show limitations. Our work establishes a more scalable and practical
path toward automated algorithm design for complex, configurable systems.

</details>


### [20] [Analogy-Driven Financial Chain-of-Thought (AD-FCoT): A Prompting Approach for Financial Sentiment Analysis](https://arxiv.org/abs/2509.12611)
*Anmol Singhal Navya Singhal*

Main category: cs.AI

TL;DR: 提出了AD-FCoT框架，通过类比推理和思维链提示相结合的方式，提升金融新闻情感分析的准确性和可解释性，无需额外训练数据即可实现更好的市场回报相关性。


<details>
  <summary>Details</summary>
Motivation: 现有金融情感分析方法难以捕捉复杂经济背景且缺乏透明推理，影响了可靠性。大型语言模型虽然具有强大文本理解能力，但在金融领域应用时仍需要更好的推理框架。

Method: AD-FCoT框架整合类比推理和思维链提示，引导LLM在新事件与已知结果的历史场景之间建立类比，并将这些类比嵌入结构化的逐步推理链中。纯提示方式，无需额外训练或微调。

Result: 在数千篇新闻文章上的实验显示，AD-FCoT在情感分类准确性上优于强基线方法，与市场回报的相关性显著更高。生成的解释与领域专业知识一致，提供可解释的洞察。

Conclusion: AD-FCoT是首个在金融领域明确将类比示例与思维链推理相结合的方法，通过结构化推理链生成类似人类分析的理性解释，适用于实际金融分析场景。

Abstract: Financial news sentiment analysis is crucial for anticipating market
movements. With the rise of AI techniques such as Large Language Models (LLMs),
which demonstrate strong text understanding capabilities, there has been
renewed interest in enhancing these systems. Existing methods, however, often
struggle to capture the complex economic context of news and lack transparent
reasoning, which undermines their reliability. We propose Analogy-Driven
Financial Chain-of-Thought (AD-FCoT), a prompting framework that integrates
analogical reasoning with chain-of-thought (CoT) prompting for sentiment
prediction on historical financial news. AD-FCoT guides LLMs to draw parallels
between new events and relevant historical scenarios with known outcomes,
embedding these analogies into a structured, step-by-step reasoning chain. To
our knowledge, this is among the first approaches to explicitly combine
analogical examples with CoT reasoning in finance. Operating purely through
prompting, AD-FCoT requires no additional training data or fine-tuning and
leverages the model's internal financial knowledge to generate rationales that
mirror human analytical reasoning. Experiments on thousands of news articles
show that AD-FCoT outperforms strong baselines in sentiment classification
accuracy and achieves substantially higher correlation with market returns. Its
generated explanations also align with domain expertise, providing
interpretable insights suitable for real-world financial analysis.

</details>


### [21] [GBV-SQL: Guided Generation and SQL2Text Back-Translation Validation for Multi-Agent Text2SQL](https://arxiv.org/abs/2509.12612)
*Daojun Chen,Xi Wang,Shenyuan Ren,Qingzhi Ma,Pengpeng Zhao,An Liu*

Main category: cs.AI

TL;DR: GBV-SQL是一个多代理框架，通过SQL2Text反向翻译验证来解决Text2SQL中的语义鸿沟问题，同时揭示了基准测试数据中的系统性质量问题。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在Text2SQL生成方面取得了显著进展，但存在语义鸿沟问题：语法有效的查询经常误解用户意图。当前评估还受到基准测试数据质量问题的严重影响。

Method: 提出GBV-SQL多代理框架，采用引导生成和SQL2Text反向翻译验证机制，使用专门代理将生成的SQL翻译回自然语言以验证其逻辑一致性。同时建立了"黄金错误"的形式化分类体系。

Result: 在BIRD基准上达到63.23%的执行准确率（绝对提升5.8%）。在去除有缺陷样本后，在Spider基准上达到96.5%（开发集）和97.6%（测试集）的执行准确率。

Conclusion: 该工作不仅提供了语义验证的鲁棒框架，还揭示了基准测试完整性的关键问题，强调了更严格数据集构建的必要性。

Abstract: While Large Language Models have significantly advanced Text2SQL generation,
a critical semantic gap persists where syntactically valid queries often
misinterpret user intent. To mitigate this challenge, we propose GBV-SQL, a
novel multi-agent framework that introduces Guided Generation with SQL2Text
Back-translation Validation. This mechanism uses a specialized agent to
translate the generated SQL back into natural language, which verifies its
logical alignment with the original question. Critically, our investigation
reveals that current evaluation is undermined by a systemic issue: the poor
quality of the benchmarks themselves. We introduce a formal typology for "Gold
Errors", which are pervasive flaws in the ground-truth data, and demonstrate
how they obscure true model performance. On the challenging BIRD benchmark,
GBV-SQL achieves 63.23% execution accuracy, a 5.8% absolute improvement. After
removing flawed examples, GBV-SQL achieves 96.5% (dev) and 97.6% (test)
execution accuracy on the Spider benchmark. Our work offers both a robust
framework for semantic validation and a critical perspective on benchmark
integrity, highlighting the need for more rigorous dataset curation.

</details>


### [22] [Mob-based cattle weight gain forecasting using ML models](https://arxiv.org/abs/2509.12615)
*Muhammad Riaz Hasib Hossain,Rafiqul Islam,Shawn R McGrath,Md Zahidul Islam,David Lamb*

Main category: cs.AI

TL;DR: 该研究提出了一种基于随机森林(RF)的牛群增重预测方法MB CWG，在包含天气和年龄因素的数据集上表现最佳(R²=0.973)，优于SVR和LSTM模型，并开发了自动化预处理工具。


<details>
  <summary>Details</summary>
Motivation: 预测牛群增重有助于大型畜牧场优化饲养策略、做出明智育种决策，并降低气候变异和市场波动的风险。

Method: 使用随机森林(RF)模型，与支持向量回归(SVR)和长短期记忆(LSTM)模型进行比较，利用756个样本数据和天气数据(降雨和温度)进行月度增重预测。

Result: RF模型在所有数据集上表现最佳，当包含天气和年龄因素时达到R²=0.973、RMSE=0.040、MAE=0.033。天气和年龄因素的加入显著提高了预测准确性。

Conclusion: RF模型是预测牛群增重的强大工具，年龄和气候因素对增重趋势有重要影响，研究还开发了公开的自动化预处理工具用于数据准备。

Abstract: Forecasting mob based cattle weight gain (MB CWG) may benefit large livestock
farms, allowing farmers to refine their feeding strategies, make educated
breeding choices, and reduce risks linked to climate variability and market
fluctuations. In this paper, a novel technique termed MB CWG is proposed to
forecast the one month advanced weight gain of herd based cattle using
historical data collected from the Charles Sturt University Farm. This research
employs a Random Forest (RF) model, comparing its performance against Support
Vector Regression (SVR) and Long Short Term Memory (LSTM) models for monthly
weight gain prediction. Four datasets were used to evaluate the performance of
models, using 756 sample data from 108 herd-based cattle, along with weather
data (rainfall and temperature) influencing CWG. The RF model performs better
than the SVR and LSTM models across all datasets, achieving an R^2 of 0.973,
RMSE of 0.040, and MAE of 0.033 when both weather and age factors were
included. The results indicate that including both weather and age factors
significantly improves the accuracy of weight gain predictions, with the RF
model outperforming the SVR and LSTM models in all scenarios. These findings
demonstrate the potential of RF as a robust tool for forecasting cattle weight
gain in variable conditions, highlighting the influence of age and climatic
factors on herd based weight trends. This study has also developed an
innovative automated pre processing tool to generate a benchmark dataset for MB
CWG predictive models. The tool is publicly available on GitHub and can assist
in preparing datasets for current and future analytical research..

</details>


### [23] [ECG-aBcDe: Overcoming Model Dependence, Encoding ECG into a Universal Language for Any LLM](https://arxiv.org/abs/2509.12625)
*Yong Xia,Jingxuan Li,YeTeng Sun,Jiarui Bu*

Main category: cs.AI

TL;DR: ECG-aBcDe是一种创新的ECG编码方法，将心电图信号转换为通用ECG语言，解决了LLM在心电图分析中的可迁移性、时间尺度信息学习和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法存在模型特定的ECG编码器阻碍跨LLM迁移、Transformer难以捕捉ECG关键时间尺度信息、以及黑盒性质限制临床应用等问题。

Method: 通过构建ECG语言和自然语言的混合数据集，将ECG信号转换为通用ECG语言，使预训练LLM无需架构修改即可直接微调，实现双向可转换性以提取注意力热图。

Result: 在ROUGE-L和METEOR上达到竞争性性能，BLEU-4指标在数据集内和跨数据集评估中分别提升2.8倍和3.9倍，达到42.58和30.76分。

Conclusion: ECG-aBcDe为ECG分析与LLM集成提供了新范式，证明了该方法的可行性，显著提升了性能指标和可解释性。

Abstract: Large Language Models (LLMs) hold significant promise for electrocardiogram
(ECG) analysis, yet challenges remain regarding transferability, time-scale
information learning, and interpretability. Current methods suffer from
model-specific ECG encoders, hindering transfer across LLMs. Furthermore, LLMs
struggle to capture crucial time-scale information inherent in ECGs due to
Transformer limitations. And their black-box nature limits clinical adoption.
To address these limitations, we introduce ECG-aBcDe, a novel ECG encoding
method that transforms ECG signals into a universal ECG language readily
interpretable by any LLM. By constructing a hybrid dataset of ECG language and
natural language, ECG-aBcDe enables direct fine-tuning of pre-trained LLMs
without architectural modifications, achieving "construct once, use anywhere"
capability. Moreover, the bidirectional convertibility between ECG and ECG
language of ECG-aBcDe allows for extracting attention heatmaps from ECG
signals, significantly enhancing interpretability. Finally, ECG-aBcDe
explicitly represents time-scale information, mitigating Transformer
limitations. This work presents a new paradigm for integrating ECG analysis
with LLMs. Compared with existing methods, our method achieves competitive
performance on ROUGE-L and METEOR. Notably, it delivers significant
improvements in the BLEU-4, with improvements of 2.8 times and 3.9 times in
in-dataset and cross-dataset evaluations, respectively, reaching scores of
42.58 and 30.76. These results provide strong evidence for the feasibility of
the new paradigm.

</details>


### [24] [Learn to Relax with Large Language Models: Solving Nonlinear Combinatorial Optimization Problems via Bidirectional Coevolution](https://arxiv.org/abs/2509.12643)
*Beidan Liu,Zhengqiu Zhu,Chen Gao,Yong Zhao,Wei Qi,Quanjun Yin*

Main category: cs.AI

TL;DR: AutoCO是一种端到端的自动化约束优化方法，利用大型语言模型生成约束松弛策略，结合进化算法和蒙特卡洛树搜索的双向协同进化机制，有效解决非线性组合优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统约束松弛方法依赖专家经验且缺乏系统性自动化，现有LLM优化方法主要作为被动约束验证器而非主动策略设计者，无法处理NCOPs中复杂的约束交互。

Method: 利用结构化LLM推理生成约束松弛策略，通过统一的三重表示方案动态演化算法原理和可执行代码；建立双向（全局-局部）协同进化机制，将进化算法与蒙特卡洛树搜索相结合。

Result: 在三个具有挑战性的NCOP基准测试上的综合实验验证了AutoCO的一致有效性和优于基线的性能表现。

Conclusion: AutoCO通过LLM学习松弛约束的方法革新了NCOPs的解决方式，在碎片化解空间中实现了强化和多样化的最优平衡。

Abstract: Nonlinear Combinatorial Optimization Problems (NCOPs) present a formidable
computational hurdle in practice, as their nonconvex nature gives rise to
multi-modal solution spaces that defy efficient optimization. Traditional
constraint relaxation approaches rely heavily on expert-driven, iterative
design processes that lack systematic automation and scalable adaptability.
While recent Large Language Model (LLM)-based optimization methods show promise
for autonomous problem-solving, they predominantly function as passive
constraint validators rather than proactive strategy architects, failing to
handle the sophisticated constraint interactions inherent to NCOPs.To address
these limitations, we introduce the first end-to-end \textbf{Auto}mated
\textbf{C}onstraint \textbf{O}ptimization (AutoCO) method, which revolutionizes
NCOPs resolution through learning to relax with LLMs.Specifically, we leverage
structured LLM reasoning to generate constraint relaxation strategies, which
are dynamically evolving with algorithmic principles and executable code
through a unified triple-representation scheme. We further establish a novel
bidirectional (global-local) coevolution mechanism that synergistically
integrates Evolutionary Algorithms for intensive local refinement with Monte
Carlo Tree Search for systematic global strategy space exploration, ensuring
optimal balance between intensification and diversification in fragmented
solution spaces. Finally, comprehensive experiments on three challenging NCOP
benchmarks validate AutoCO's consistent effectiveness and superior performance
over the baselines.

</details>


### [25] [Large Language Models Imitate Logical Reasoning, but at what Cost?](https://arxiv.org/abs/2509.12645)
*Lachlan McGinness,Peter Baumgartner*

Main category: cs.AI

TL;DR: 对前沿大语言模型在18个月期间的推理能力进行纵向研究，发现通过思维链提示和思维模型的引入显著提升了性能，并提出了一种神经符号架构来降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型随时间推移的推理能力变化，并探索更高效的推理方法以减少计算成本。

Method: 使用PrOntoQA数据集的真假问题测试三个领先模型，采用思维链提示和思维模型技术，并开发神经符号架构将问题转化为标准化形式后用Z3求解器处理。

Result: 从2023年到2024年性能提升归因于隐藏思维链提示，2024年到2025年思维模型带来显著改进。神经符号方法在保持近乎完美性能的同时显著降低了计算成本。

Conclusion: 大语言模型的推理能力随时间持续改进，神经符号架构提供了一种计算效率更高的替代方案，FLOPs估算公式在实际应用中准确度较高。

Abstract: We present a longitudinal study which evaluates the reasoning capability of
frontier Large Language Models over an eighteen month period. We measured the
accuracy of three leading models from December 2023, September 2024 and June
2025 on true or false questions from the PrOntoQA dataset and their
faithfulness to reasoning strategies provided through in-context learning. The
improvement in performance from 2023 to 2024 can be attributed to hidden Chain
of Thought prompting. The introduction of thinking models allowed for
significant improvement in model performance between 2024 and 2025.
  We then present a neuro-symbolic architecture which uses LLMs of less than 15
billion parameters to translate the problems into a standardised form. We then
parse the standardised forms of the problems into a program to be solved by Z3,
an SMT solver, to determine the satisfiability of the query. We report the
number of prompt and completion tokens as well as the computational cost in
FLOPs for open source models. The neuro-symbolic approach significantly reduces
the computational cost while maintaining near perfect performance. The common
approximation that the number of inference FLOPs is double the product of the
active parameters and total tokens was accurate within 10\% for all
experiments.

</details>


### [26] [Zero-shot Graph Reasoning via Retrieval Augmented Framework with LLMs](https://arxiv.org/abs/2509.12743)
*Hanqing Li,Kiran Sheena Jyothi,Henry Liang,Sharika Mahadevan,Diego Klabjan*

Main category: cs.AI

TL;DR: GRRAF是一种无需训练的图推理方法，利用检索增强生成(RAG)和LLM的代码生成能力，通过生成可执行代码查询图数据库来解决各种图推理任务，在大多数任务上达到100%准确率且具有良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有图推理方法通常需要大量微调或依赖预定义算法，存在灵活性不足和扩展性差的问题。GRRAF旨在通过结合RAG和LLM的代码生成能力，提供一种无需训练、可处理大规模图的通用解决方案。

Method: 将目标图存储在图形数据库中，提示LLM生成可执行代码查询来检索必要信息。采用错误反馈循环和超时机制确保正确性和效率，无需微调或预定义算法。

Result: 在GraphInstruct数据集上，GRRAF在大多数图推理任务（环检测、二分图检查、最短路径、最大流）上达到100%准确率，子图匹配性能也很高。可扩展到10,000个节点的大图，且token成本与图大小无关。

Conclusion: GRRAF证明了无需训练的检索增强方法在图推理任务中的有效性，提供了一种灵活、可扩展且准确的解决方案，特别适合处理大规模图数据。

Abstract: We propose a new, training-free method, Graph Reasoning via Retrieval
Augmented Framework (GRRAF), that harnesses retrieval-augmented generation
(RAG) alongside the code-generation capabilities of large language models
(LLMs) to address a wide range of graph reasoning tasks. In GRRAF, the target
graph is stored in a graph database, and the LLM is prompted to generate
executable code queries that retrieve the necessary information. This approach
circumvents the limitations of existing methods that require extensive
finetuning or depend on predefined algorithms, and it incorporates an error
feedback loop with a time-out mechanism to ensure both correctness and
efficiency. Experimental evaluations on the GraphInstruct dataset reveal that
GRRAF achieves 100% accuracy on most graph reasoning tasks, including cycle
detection, bipartite graph checks, shortest path computation, and maximum flow,
while maintaining consistent token costs regardless of graph sizes. Imperfect
but still very high performance is observed on subgraph matching. Notably,
GRRAF scales effectively to large graphs with up to 10,000 nodes.

</details>


### [27] [H$^2$R: Hierarchical Hindsight Reflection for Multi-Task LLM Agents](https://arxiv.org/abs/2509.12810)
*Shicheng Ye,Chao Yu,Kaiqiang Ke,Chengdong Xu,Yinqi Wei*

Main category: cs.AI

TL;DR: 提出分层记忆架构H²R，通过分离高层规划记忆和低层执行记忆实现细粒度知识迁移，提升LLM智能体在多任务场景中的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有方法将先验经验和知识视为整体单元，导致知识迁移效率低下且粒度粗糙，需要更细粒度的知识迁移机制

Method: 提出分层记忆架构，分离高层规划记忆和低层执行记忆；引入分层后见反思机制H²R，从过往交互中提炼可重用分层知识；测试时分别检索高低层记忆

Result: 在两个基准测试中，H²R提高了泛化能力和决策性能，优于Expel等先前基线方法

Conclusion: 分层记忆架构和H²R机制能够有效实现细粒度知识迁移，显著提升LLM智能体在多任务场景中的性能表现

Abstract: Large language model (LLM)-based agents have shown strong potential in
multi-task scenarios, owing to their ability to transfer knowledge across
diverse tasks. However, existing approaches often treat prior experiences and
knowledge as monolithic units, leading to inefficient and coarse-grained
knowledge transfer. In this work, we propose a novel hierarchical memory
architecture that enables fine-grained knowledge transfer by decoupling
high-level planning memory from low-level execution memory. To construct and
refine these hierarchical memories, we introduce Hierarchical Hindsight
Reflection (H$^2$R), a mechanism that distills reusable and hierarchical
knowledge from past agent-environment interactions. At test time, H$^2$R
performs retrievals of high-level and low-level memories separately, allowing
LLM-based agents to efficiently access and utilize task-relevant knowledge for
new tasks.Experimental results across two benchmarks demonstrate that H$^2$R
can improve generalization and decision-making performance, outperforming prior
baselines such as Expel.

</details>


### [28] [LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning](https://arxiv.org/abs/2509.12875)
*Jiaqi Wang,Binquan Ji,Haibo Luo,Yiyang Qi,Ruiting Li,Huiyan Wang,Yuantao Han,Cangyi Yang,jiaxu Zhang,Feiliang Ren*

Main category: cs.AI

TL;DR: LTA-Thinker是一个潜在思维增强训练框架，通过增加潜在思维向量的分布方差和引入基于分布的定向优化范式，提升大语言模型的复杂推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在复杂推理中存在"过度思考"问题，虽然Test-Time Scaling等方法在连续潜在空间推理中有效，但高质量潜在思维的高效生成和利用仍是核心瓶颈。SoftCoT++理论表明，生成的潜在思维分布方差越大，越接近真实分布。

Method: 1. 基于可学习先验构建潜在思维生成架构，增加生成潜在思维向量的分布方差；2. 引入基于分布的定向优化范式，联合约束分布局部性和分布规模，结合SFT损失、语义对齐损失（KL散度）和推理聚焦损失（对比学习机制）进行多目标协同训练。

Result: 实验表明LTA-Thinker在各种基线中实现了最先进的性能，表现出更高的性能上限和更好的扩展效果。

Conclusion: LTA-Thinker通过增强潜在思维分布的方差和引入多目标优化策略，有效提升了大语言模型的复杂推理能力，为解决过度思考问题提供了有效解决方案。

Abstract: Complex Reasoning in Large Language Models can be dynamically optimized using
Test-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut,
SoftCoT and its variant are effective in continuous latent space inference, the
core bottleneck still lies in the efficient generation and utilization of
high-quality Latent Thought. Drawing from the theory of SoftCoT++ that a larger
variance in the generated Latent Thought distribution more closely approximates
the golden truth distribution, we propose a Latent Thought-Augmented Training
Framework--LTA-Thinker, which improves distributional variance and enhances
reasoning performance from two perspectives. First, LTA-Thinker constructs a
Latent Thought generation architecture based on a learnable prior. This
architecture aims to increase the variance distribution of generated Latent
Thought Vectors in order to simplify the overall structure and raise the
performance ceiling. Second, LTA-Thinker introduces a distribution-based
directional optimization paradigm that jointly constrains both distribution
locality and distribution scale. This mechanism improves information efficiency
and computational cost through a multi-objective co-training strategy, which
combines standard Supervised Fine-Tuning (SFT) loss with two novel losses:
Semantic Alignment Loss, which utilizes KL divergence to ensure that the Latent
Thought is highly relevant to the semantics of the question; Reasoning Focus
Loss, which utilizes a contrastive learning mechanism to guide the model to
focus on the most critical reasoning steps. Experiments show that LTA-thinker
achieves state-of-the-art (SOTA) performance among various baselines and
demonstrates a higher performance ceiling and better scaling effects.

</details>


### [29] [Stochastic Streets: A Walk Through Random LLM Address Generation in four European Cities](https://arxiv.org/abs/2509.12914)
*Tairan Fu,David Campo-Nazareno,Javier Coronado-Blázquez,Javier Conde,Pedro Reviriego,Fabrizio Lombardi*

Main category: cs.AI

TL;DR: LLMs在解决复杂数学问题和回答各种难题方面表现出色，但生成欧洲城市随机街道地址的能力有待验证


<details>
  <summary>Details</summary>
Motivation: 验证大型语言模型在生成看似简单但实际上需要精确地理知识的随机街道地址任务上的表现

Method: 通过测试LLMs生成欧洲城市随机街道地址的能力，评估其在处理需要具体地理信息任务时的局限性

Result: LLMs虽然在复杂推理任务上表现优异，但在生成准确的随机街道地址方面可能存在困难

Conclusion: 即使是最先进的LLMs，在处理需要精确、具体地理知识的简单任务时仍存在局限性，这揭示了模型知识表示和生成能力的边界

Abstract: Large Language Models (LLMs) are capable of solving complex math problems or
answer difficult questions on almost any topic, but can they generate random
street addresses for European cities?

</details>


### [30] [Population Estimation using Deep Learning over Gandhinagar Urban Area](https://arxiv.org/abs/2509.12926)
*Jai Singla,Peal Jotania,Keivalya Pandya*

Main category: cs.AI

TL;DR: 使用高分辨率卫星影像、数字高程模型和深度学习技术进行人口估算的自动化方法，替代传统人口普查


<details>
  <summary>Details</summary>
Motivation: 传统人口普查方法成本高、耗时长且依赖大量人力资源，需要更高效、自动化的解决方案来进行人口估算和城市规划

Method: 结合卷积神经网络（CNN）进行建筑物分类（住宅/非住宅），使用人工神经网络（ANN）进行人口估算，整合0.3米分辨率卫星影像、0.5米DEM和矢量边界数据

Result: 在甘地讷格尔城市区域的4.8万栋建筑物上测试，总体F1分数达到0.9936，估算人口为278,954人

Conclusion: 该自动化框架为城市管理部门提供了可扩展和可复制的工具，展示了AI驱动的地理空间分析在数据驱动城市治理中的效率

Abstract: Population estimation is crucial for various applications, from resource
allocation to urban planning. Traditional methods such as surveys and censuses
are expensive, time-consuming and also heavily dependent on human resources,
requiring significant manpower for data collection and processing. In this
study a deep learning solution is proposed to estimate population using high
resolution (0.3 m) satellite imagery, Digital Elevation Models (DEM) of 0.5m
resolution and vector boundaries. Proposed method combines Convolution Neural
Network (CNN) architecture for classification task to classify buildings as
residential and non-residential and Artificial Neural Network (ANN)
architecture to estimate the population. Approx. 48k building footprints over
Gandhinagar urban area are utilized containing both residential and
non-residential, with residential categories further used for building-level
population estimation. Experimental results on a large-scale dataset
demonstrate the effectiveness of our model, achieving an impressive overall
F1-score of 0.9936. The proposed system employs advanced geospatial analysis
with high spatial resolution to estimate Gandhinagar population at 278,954. By
integrating real-time data updates, standardized metrics, and infrastructure
planning capabilities, this automated approach addresses critical limitations
of conventional census-based methodologies. The framework provides
municipalities with a scalable and replicable tool for optimized resource
management in rapidly urbanizing cities, showcasing the efficiency of AI-driven
geospatial analytics in enhancing data-driven urban governance.

</details>


### [31] [The Anatomy of Alignment: Decomposing Preference Optimization by Steering Sparse Features](https://arxiv.org/abs/2509.12934)
*Jeremias Ferrao,Matthijs van der Lende,Ilija Lichkovski,Clement Neo*

Main category: cs.AI

TL;DR: FSRL是一种透明的对齐框架，通过训练轻量级适配器来调节稀疏自编码器的可解释特征，实现与RLHF相当的效果，但机制分析显示偏好优化过程主要奖励风格特征而非明确的对齐概念。


<details>
  <summary>Details</summary>
Motivation: 当前RLHF方法导致参数变化分散且不透明，难以理解模型内部学到了什么，需要更透明的对齐方法来提升可解释性和安全性。

Method: 提出特征引导强化学习(FSRL)，训练轻量级适配器来调节稀疏自编码器(SAE)提取的可解释特征，实现透明对齐。

Result: FSRL在偏好优化方面与现有RLHF方法效果相当，机制分析发现适配器策略系统性地促进风格特征而非明确对齐概念，表明偏好优化过程将风格呈现作为质量的代理进行奖励。

Conclusion: FSRL为可解释模型控制和诊断对齐内部机制提供了工具，揭示了偏好优化过程中风格特征的重要性。

Abstract: Aligning large language models is critical for their usability and safety.
However, the prevailing approach of Reinforcement Learning from Human Feedback
(RLHF) induces diffuse, opaque parameter changes, making it difficult to
discern what the model has internalized. Hence, we introduce Feature Steering
with Reinforcement Learning (FSRL), a transparent alignment framework that
trains a lightweight adapter to steer behavior by modulating interpretable
features from a Sparse Autoencoder (SAE). First, we demonstrate that FSRL is an
effective method for preference optimization and is comparable with current
RLHF methods. We then perform mechanistic analysis on the trained adapter, and
find that its policy systematically promotes style features over explicit
alignment concepts, suggesting that the preference optimization process rewards
stylistic presentation as a proxy for quality. Ultimately, we hope that FSRL
provides a tool for both interpretable model control and diagnosing the
internal mechanisms of alignment.

</details>


### [32] [Black-box Model Merging for Language-Model-as-a-Service with Massive Model Repositories](https://arxiv.org/abs/2509.12951)
*Shilian Chen,Jie Zhou,Tianyu Huai,Yujiang Lu,Junsong Li,Bihao Zhan,Qianjun Pan,Yutao Yang,Xin Li,Qin Chen,Hang Yan,Liang He*

Main category: cs.AI

TL;DR: 提出Evo-Merging方法，通过进化算法实现黑盒大语言模型的合并，仅需推理API查询，无需访问模型权重


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法需要访问模型参数，但对于GPT-4等仅通过API提供的黑盒大模型，参数不可获取，因此需要解决黑盒模型合并的挑战

Method: 基于进化算法的无导数优化框架，包含稀疏性去噪和符号感知缩放两个关键组件，通过API查询实现模型合并

Result: 在多个任务上达到最先进性能，显著优于现有基线方法

Conclusion: Evo-Merging方法有效解决了黑盒大语言模型合并问题，为仅通过API访问的模型集成提供了可行方案

Abstract: Model merging refers to the process of integrating multiple distinct models
into a unified model that preserves and combines the strengths and capabilities
of the individual models. Most existing approaches rely on task vectors to
combine models, typically under the assumption that model parameters are
accessible. However, for extremely large language models (LLMs) such as GPT-4,
which are often provided solely as black-box services through API interfaces
(Language-Model-as-a-Service), model weights are not available to end users.
This presents a significant challenge, which we refer to as black-box model
merging (BMM) with massive LLMs. To address this challenge, we propose a
derivative-free optimization framework based on the evolutionary algorithm
(Evo-Merging) that enables effective model merging using only inference-time
API queries. Our method consists of two key components: (1) sparsity-based
denoising, designed to identify and filter out irrelevant or redundant
information across models, and (2) sign-aware scaling, which dynamically
computes optimal combination weights for the relevant models based on their
performance. We also provide a formal justification, along with a theoretical
analysis, for our asymmetric sparsification. Extensive experimental evaluations
demonstrate that our approach achieves state-of-the-art results on a range of
tasks, significantly outperforming existing strong baselines.

</details>


### [33] [Forget What's Sensitive, Remember What Matters: Token-Level Differential Privacy in Memory Sculpting for Continual Learning](https://arxiv.org/abs/2509.12958)
*Bihao Zhan,Jie Zhou,Junsong Li,Yutao Yang,Shilian Chen,Qianjun Pan,Xin Li,Wen Wu,Xingjiao Wu,Qin Chen,Hang Yan,Liang He*

Main category: cs.AI

TL;DR: 提出了隐私增强的持续学习框架PeCL，通过动态差分隐私策略和隐私引导的记忆雕刻模块，在保护敏感信息的同时保持模型性能


<details>
  <summary>Details</summary>
Motivation: 传统持续学习模型面临隐私挑战，统一的差分隐私预算会导致模型性能显著下降，阻碍在隐私敏感领域的部署

Method: 1. 基于语义敏感性的token级动态差分隐私策略 2. 隐私引导的记忆雕刻模块，智能遗忘敏感信息并保留任务不变的历史知识

Result: PeCL在隐私保护和模型效用之间实现了优越的平衡，在保持先前任务高精度的同时确保强大的隐私保护，优于基线模型

Conclusion: 该框架为隐私敏感的持续学习应用提供了有效的解决方案，通过自适应隐私预算分配和智能信息管理实现了隐私与性能的双重优化

Abstract: Continual Learning (CL) models, while adept at sequential knowledge
acquisition, face significant and often overlooked privacy challenges due to
accumulating diverse information. Traditional privacy methods, like a uniform
Differential Privacy (DP) budget, indiscriminately protect all data, leading to
substantial model utility degradation and hindering CL deployment in
privacy-sensitive areas. To overcome this, we propose a privacy-enhanced
continual learning (PeCL) framework that forgets what's sensitive and remembers
what matters. Our approach first introduces a token-level dynamic Differential
Privacy strategy that adaptively allocates privacy budgets based on the
semantic sensitivity of individual tokens. This ensures robust protection for
private entities while minimizing noise injection for non-sensitive, general
knowledge. Second, we integrate a privacy-guided memory sculpting module. This
module leverages the sensitivity analysis from our dynamic DP mechanism to
intelligently forget sensitive information from the model's memory and
parameters, while explicitly preserving the task-invariant historical knowledge
crucial for mitigating catastrophic forgetting. Extensive experiments show that
PeCL achieves a superior balance between privacy preserving and model utility,
outperforming baseline models by maintaining high accuracy on previous tasks
while ensuring robust privacy.

</details>


### [34] [Toward PDDL Planning Copilot](https://arxiv.org/abs/2509.12987)
*Yarin Benyamin,Argaman Mordoch,Shahaf S. Shperberg,Roni Stern*

Main category: cs.AI

TL;DR: Planning Copilot是一个集成多种规划工具的聊天机器人，通过自然语言指令调用外部规划工具，显著提升LLMs在长时程规划任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自主执行复杂任务时缺乏可靠的长期规划能力，需要外部工具支持来弥补这一缺陷。

Method: 基于Model Context Protocol (MCP)标准，将LLMs与外部规划工具连接，支持语法检查、规划器选择、计划生成验证和执行模拟等任务，无需领域特定微调。

Result: 实验表明Planning Copilot在使用三个开源LLMs时表现远超无工具支持的相同模型，且在使用较小LLM的情况下显著优于最新的GPT-5。

Conclusion: 专用规划工具是使LLMs有效执行规划任务的有效途径，MCP标准为实现LLM与外部工具的无缝集成提供了可行方案。

Abstract: Large Language Models (LLMs) are increasingly being used as autonomous agents
capable of performing complicated tasks. However, they lack the ability to
perform reliable long-horizon planning on their own. This paper bridges this
gap by introducing the Planning Copilot, a chatbot that integrates multiple
planning tools and allows users to invoke them through instructions in natural
language. The Planning Copilot leverages the Model Context Protocol (MCP), a
recently developed standard for connecting LLMs with external tools and
systems. This approach allows using any LLM that supports MCP without
domain-specific fine-tuning. Our Planning Copilot supports common planning
tasks such as checking the syntax of planning problems, selecting an
appropriate planner, calling it, validating the plan it generates, and
simulating their execution. We empirically evaluate the ability of our Planning
Copilot to perform these tasks using three open-source LLMs. The results show
that the Planning Copilot highly outperforms using the same LLMs without the
planning tools. We also conducted a limited qualitative comparison of our tool
against Chat GPT-5, a very recent commercial LLM. Our results shows that our
Planning Copilot significantly outperforms GPT-5 despite relying on a much
smaller LLM. This suggests dedicated planning tools may be an effective way to
enable LLMs to perform planning tasks.

</details>


### [35] [Data-driven Methods of Extracting Text Structure and Information Transfer](https://arxiv.org/abs/2509.12999)
*Shinichi Honna,Taichi Murayama,Akira Matsui*

Main category: cs.AI

TL;DR: 研究测试了安娜·卡列尼娜原则及其变体在不同媒介文本结构中的适用性，发现成功依赖于媒介特定的结构约束，而失败形式则多样化


<details>
  <summary>Details</summary>
Motivation: 验证安娜·卡列尼娜原则（AKP）及其反向模式在不同文本媒介中的适用性，探究成功与失败的结构模式差异

Method: 将文本表示为功能块序列，通过转换顺序和位置评估收敛性，分析小说、在线百科全书、研究论文和电影的结构模式

Result: 不同媒介呈现不同结构模式：小说在顺序上遵循反向AKP，维基百科结合AKP和有序模式，学术论文顺序上反向AKP但位置保持噪声，电影按类型分化

Conclusion: 成功依赖于每个媒介特定的结构约束，而失败在不同领域中呈现出不同的形态

Abstract: The Anna Karenina Principle (AKP) holds that success requires satisfying a
small set of essential conditions, whereas failure takes diverse forms. We test
AKP, its reverse, and two further patterns described as ordered and noisy
across novels, online encyclopedias, research papers, and movies. Texts are
represented as sequences of functional blocks, and convergence is assessed in
transition order and position. Results show that structural principles vary by
medium: novels follow reverse AKP in order, Wikipedia combines AKP with ordered
patterns, academic papers display reverse AKP in order but remain noisy in
position, and movies diverge by genre. Success therefore depends on structural
constraints that are specific to each medium, while failure assumes different
shapes across domains.

</details>


### [36] [A Visualized Framework for Event Cooperation with Generative Agents](https://arxiv.org/abs/2509.13011)
*Yuyang Tian,Shunqiang Mao,Wenchang Gao,Lanlan Qiu,Tianxing He*

Main category: cs.AI

TL;DR: MiniAgentPro是一个可视化平台，用于评估LLM代理在物理环境中的事件组织和协调能力，包含地图编辑器和模拟播放器，通过8个多样化事件场景测试发现基础设置表现良好但困难变体存在协调挑战。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理社会模拟框架缺乏系统的事件组织评估和物理环境可视化集成，限制了代理在空间中导航和与物品真实交互的能力。

Method: 开发MiniAgentPro可视化平台，包含直观的地图编辑器用于定制环境，以及带有平滑动画的模拟播放器；构建包含8个多样化事件场景（基础版和困难版）的综合测试集。

Result: 使用GPT-4o进行评估，在基础设置中表现出色，但在困难变体中凸显出协调挑战。

Conclusion: MiniAgentPro平台有效解决了现有框架在物理环境集成和系统评估方面的不足，为LLM代理的社会模拟提供了更全面的测试和可视化工具。

Abstract: Large Language Models (LLMs) have revolutionized the simulation of agent
societies, enabling autonomous planning, memory formation, and social
interactions. However, existing frameworks often overlook systematic
evaluations for event organization and lack visualized integration with
physically grounded environments, limiting agents' ability to navigate spaces
and interact with items realistically. We develop MiniAgentPro, a visualization
platform featuring an intuitive map editor for customizing environments and a
simulation player with smooth animations. Based on this tool, we introduce a
comprehensive test set comprising eight diverse event scenarios with basic and
hard variants to assess agents' ability. Evaluations using GPT-4o demonstrate
strong performance in basic settings but highlight coordination challenges in
hard variants.

</details>


### [37] [Reasoning with Preference Constraints: A Benchmark for Language Models in Many-to-One Matching Markets](https://arxiv.org/abs/2509.13131)
*Marylou Fauchard,Florian Carichon,Margarida Carvalho,Golnoosh Farnadi*

Main category: cs.AI

TL;DR: 本文提出了一个包含369个大学录取问题实例的新基准，用于评估大语言模型在匹配问题中的表现，发现LLMs在满足所有评估标准方面存在困难，推理模型表现优于传统模型，不同提示策略效果各异，迭代提示性能非单调。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在数学推理和组合优化方面表现出色，但在需要偏好和结构约束的匹配问题中的应用仍未被充分探索，因此需要建立一个专门的基准来评估LLMs在这类问题上的性能。

Method: 创建了包含369个大学录取问题实例的基准，评估多个开源LLMs在可行性、稳定性和最优性三个维度的表现，测试了包括思维链、上下文学习和基于角色的提示等多种提示策略，并进行迭代提示实验。

Result: LLMs能够满足某些约束但难以一致满足所有评估标准；推理模型（如QwQ和GPT-oss）显著优于传统模型（如Llama、Qwen、Mistral）；不同提示策略效果差异明显，无单一最优策略；迭代提示性能呈现非单调性，早期达到峰值后显著下降。

Conclusion: 该研究为模型推理性能和提示策略在具有偏好约束的组合优化问题中的有效性提供了新视角，揭示了LLMs在匹配问题中的局限性以及提示策略选择的重要性。

Abstract: Recent advances in reasoning with large language models (LLMs) have
demonstrated strong performance on complex mathematical tasks, including
combinatorial optimization. Techniques such as Chain-of-Thought and In-Context
Learning have further enhanced this capability, making LLMs both powerful and
accessible tools for a wide range of users, including non-experts. However,
applying LLMs to matching problems, which require reasoning under preferential
and structural constraints, remains underexplored. To address this gap, we
introduce a novel benchmark of 369 instances of the College Admission Problem,
a canonical example of a matching problem with preferences, to evaluate LLMs
across key dimensions: feasibility, stability, and optimality. We employ this
benchmark to assess the performance of several open-weight LLMs. Our results
first reveal that while LLMs can satisfy certain constraints, they struggle to
meet all evaluation criteria consistently. They also show that reasoning LLMs,
like QwQ and GPT-oss, significantly outperform traditional models such as
Llama, Qwen or Mistral, defined here as models used without any dedicated
reasoning mechanisms. Moreover, we observed that LLMs reacted differently to
the various prompting strategies tested, which include Chain-of-Thought,
In-Context Learning and role-based prompting, with no prompt consistently
offering the best performance. Finally, we report the performances from
iterative prompting with auto-generated feedback and show that they are not
monotonic; they can peak early and then significantly decline in later
attempts. Overall, this work offers a new perspective on model reasoning
performance and the effectiveness of prompting strategies in combinatorial
optimization problems with preferential constraints.

</details>


### [38] [Agentic AI for Financial Crime Compliance](https://arxiv.org/abs/2509.13137)
*Henrik Axelsen,Valdemar Licht,Jan Damsgaard*

Main category: cs.AI

TL;DR: 本文通过行动设计研究方法，开发了一个面向金融犯罪合规的代理AI系统，强调可解释性、可追溯性和合规性设计，实现了自动化工作流程和监管透明度。


<details>
  <summary>Details</summary>
Motivation: 金融犯罪合规成本不断上升但效果不佳，现有AI解决方案不透明且不符合监管要求，需要开发既高效又符合监管期望的AI系统。

Method: 采用行动设计研究(ADR)方法，与金融科技公司和监管机构合作，使用工件中心建模，为自主代理分配明确角色，实现任务特定模型路由和审计日志记录。

Result: 开发了参考架构和实际原型，展示了代理AI如何在监管约束下重构金融犯罪合规工作流程，支持透明度和制度信任。

Conclusion: 研究表明，在负责任的治理结构内嵌入自动化，可以在高风险监管环境中支持透明度和制度信任，为AI驱动的合规研究提供了新见解。

Abstract: The cost and complexity of financial crime compliance (FCC) continue to rise,
often without measurable improvements in effectiveness. While AI offers
potential, most solutions remain opaque and poorly aligned with regulatory
expectations. This paper presents the design and deployment of an agentic AI
system for FCC in digitally native financial platforms. Developed through an
Action Design Research (ADR) process with a fintech firm and regulatory
stakeholders, the system automates onboarding, monitoring, investigation, and
reporting, emphasizing explainability, traceability, and compliance-by-design.
Using artifact-centric modeling, it assigns clearly bounded roles to autonomous
agents and enables task-specific model routing and audit logging. The
contribution includes a reference architecture, a real-world prototype, and
insights into how Agentic AI can reconfigure FCC workflows under regulatory
constraints. Our findings extend IS literature on AI-enabled compliance by
demonstrating how automation, when embedded within accountable governance
structures, can support transparency and institutional trust in high-stakes,
regulated environments.

</details>


### [39] [G-CSEA: A Graph-Based Conflict Set Extraction Algorithm for Identifying Infeasibility in Pseudo-Boolean Models](https://arxiv.org/abs/2509.13203)
*Kanishk Garg,Saranya D.,Sanal Kumar,Saurabh Singh,Anupam Purwar*

Main category: cs.AI

TL;DR: 提出基于图的冲突集提取算法(G-CSEA)来解决伪布尔约束模型中不可行子集(IIS)的识别问题，相比传统方法减少了求解器调用次数


<details>
  <summary>Details</summary>
Motivation: 劳动力调度中存在大量基于规则的约束，这些约束可能相互冲突导致模型不可行。现有IIS提取方法如Additive Deletion和QuickXplain需要大量可行性检查，而双射线分析在伪布尔模型中可能失效

Method: 提出G-CSEA算法，受SAT求解器中冲突驱动子句学习(CDCL)启发，在约束传播过程中构建蕴含图，检测到冲突时追踪所有贡献约束，生成冲突集并可选择使用QuickXplain最小化为IIS

Result: 该方法能够有效提取冲突集，减少了求解器调用次数，解决了伪布尔约束模型中IIS识别的问题

Conclusion: G-CSEA为伪布尔约束模型提供了一种高效的冲突集提取方法，有助于快速识别和解决调度中的约束冲突问题

Abstract: Workforce scheduling involves a variety of rule-based constraints-such as
shift limits, staffing policies, working hour restrictions, and many similar
scheduling rules-which can interact in conflicting ways, leading to infeasible
models. Identifying the underlying causes of such infeasibility is critical for
resolving scheduling issues and restoring feasibility. A common diagnostic
approach is to compute Irreducible Infeasible Subsets (IISs): minimal sets of
constraints that are jointly infeasible but become feasible when any one is
removed. We consider models formulated using pseudo-Boolean constraints with
inequality relations over binary variables, which naturally encode scheduling
logic. Existing IIS extraction methods such as Additive Deletion and
QuickXplain rely on repeated feasibility checks, often incurring large numbers
of solver calls. Dual ray analysis, while effective for LP-based models, may
fail when the relaxed problem is feasible but the underlying pseudo-Boolean
model is not. To address these limitations, we propose Graph-based Conflict Set
Extraction Algorithm (G-CSEA) to extract a conflict set, an approach inspired
by Conflict-Driven Clause Learning (CDCL) in SAT solvers. Our method constructs
an implication graph during constraint propagation and, upon detecting a
conflict, traces all contributing constraints across both decision branches.
The resulting conflict set can optionally be minimized using QuickXplain to
produce an IIS.

</details>


### [40] [Simulating Clinical AI Assistance using Multimodal LLMs: A Case Study in Diabetic Retinopathy](https://arxiv.org/abs/2509.13234)
*Nadim Barakat,William Lotter*

Main category: cs.AI

TL;DR: 该研究评估了多模态大语言模型（GPT-4o和MedGemma）在糖尿病视网膜病变检测中的表现，发现MedGemma在基线评估中表现更优，而GPT-4o在结合MedGemma描述性输出时能达到高准确率，表明MLLMs可改善DR筛查流程并作为临床AI辅助的可扩展模拟器。


<details>
  <summary>Details</summary>
Motivation: 当前FDA批准的糖尿病视网膜病变筛查系统主要提供二元转诊输出，这种最小化输出可能限制临床信任和实用性。需要确定最有效的输出格式来增强临床医生与AI的协作性能。

Method: 在IDRiD和Messidor-2数据集上测试GPT-4o（通用MLLM）和MedGemma（开源医疗模型），包括：(1)基线评估，(2)使用合成预测的模拟AI辅助，(3)GPT-4o整合MedGemma输出的实际AI协作实验。

Result: MedGemma基线表现优于GPT-4o（更高敏感性和AUROC），GPT-4o特异性接近完美但敏感性低。在协作中，GPT-4o结合MedGemma描述性输出时即使无直接图像访问也能达到高AUROC（0.96）。

Conclusion: MLLMs可以改善DR筛查流程，并作为研究不同输出配置下临床AI辅助的可扩展模拟器。开源轻量级模型如MedGemma在资源有限环境中特别有价值，描述性输出可增强临床工作流程中的可解释性和医生信任。

Abstract: Diabetic retinopathy (DR) is a leading cause of blindness worldwide, and AI
systems can expand access to fundus photography screening. Current FDA-cleared
systems primarily provide binary referral outputs, where this minimal output
may limit clinical trust and utility. Yet, determining the most effective
output format to enhance clinician-AI performance is an empirical challenge
that is difficult to assess at scale. We evaluated multimodal large language
models (MLLMs) for DR detection and their ability to simulate clinical AI
assistance across different output types. Two models were tested on IDRiD and
Messidor-2: GPT-4o, a general-purpose MLLM, and MedGemma, an open-source
medical model. Experiments included: (1) baseline evaluation, (2) simulated AI
assistance with synthetic predictions, and (3) actual AI-to-AI collaboration
where GPT-4o incorporated MedGemma outputs. MedGemma outperformed GPT-4o at
baseline, achieving higher sensitivity and AUROC, while GPT-4o showed
near-perfect specificity but low sensitivity. Both models adjusted predictions
based on simulated AI inputs, but GPT-4o's performance collapsed with incorrect
ones, whereas MedGemma remained more stable. In actual collaboration, GPT-4o
achieved strong results when guided by MedGemma's descriptive outputs, even
without direct image access (AUROC up to 0.96). These findings suggest MLLMs
may improve DR screening pipelines and serve as scalable simulators for
studying clinical AI assistance across varying output configurations. Open,
lightweight models such as MedGemma may be especially valuable in low-resource
settings, while descriptive outputs could enhance explainability and clinician
trust in clinical workflows.

</details>


### [41] [A Scenario-Driven Cognitive Approach to Next-Generation AI Memory](https://arxiv.org/abs/2509.13235)
*Linyue Cai,Yuyang Cheng,Xiaoding Shao,Huiming Wang,Yong Zhao,Wei Zhang,Kang Li*

Main category: cs.AI

TL;DR: 提出了基于认知场景驱动的COLMA记忆架构，解决现有AI记忆系统适应性差、多模态整合不足等问题，为AGI发展提供结构化基础


<details>
  <summary>Details</summary>
Motivation: 随着AI向AGI发展，需要更鲁棒和类人的记忆系统。现有记忆架构存在适应性有限、多模态整合不足、无法支持持续学习等问题

Method: 采用场景驱动方法，从代表性认知场景中提取功能需求，提出统一设计原则，并构建认知分层记忆架构(COLMA)

Result: 提出了COLMA框架，将认知场景、记忆过程和存储机制整合为统一设计，为AI系统提供结构化基础

Conclusion: COLMA为开发具有终身学习和类人推理能力的AI系统提供了基础，有助于AGI的实用化发展

Abstract: As artificial intelligence advances toward artificial general intelligence
(AGI), the need for robust and human-like memory systems has become
increasingly evident. Current memory architectures often suffer from limited
adaptability, insufficient multimodal integration, and an inability to support
continuous learning. To address these limitations, we propose a scenario-driven
methodology that extracts essential functional requirements from representative
cognitive scenarios, leading to a unified set of design principles for
next-generation AI memory systems. Based on this approach, we introduce the
\textbf{COgnitive Layered Memory Architecture (COLMA)}, a novel framework that
integrates cognitive scenarios, memory processes, and storage mechanisms into a
cohesive design. COLMA provides a structured foundation for developing AI
systems capable of lifelong learning and human-like reasoning, thereby
contributing to the pragmatic development of AGI.

</details>


### [42] [RepIt: Representing Isolated Targets to Steer Language Models](https://arxiv.org/abs/2509.13281)
*Vincent Siu,Nathan W. Henry,Nicholas Crispino,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: RepIt是一个简单高效的概念向量提取框架，能够从少量数据中分离出特定概念的表示，实现对LLM行为的精准干预，同时避免对其他功能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的激活引导方法往往会产生超出预期的广泛影响，需要更纯净的概念向量来实现针对性干预，从而更细致地理解LLM的行为机制。

Method: 提出RepIt框架，通过数据高效的方式提取概念特定的表示向量，能够在少量示例（仅需十几个样本）和单块A6000 GPU上实现稳健的目标表示提取。

Result: 在五个前沿LLM上验证，RepIt能够精确抑制特定概念的拒绝行为，同时保持其他方面的拒绝能力；将修正信号定位到仅100-200个神经元；在标准安全基准上仍能保持安全评分。

Conclusion: RepIt展示了针对性干预可以抵消过度泛化问题，为更细粒度的模型行为控制奠定了基础，但也引发了关于使用有限计算和数据即可进行操纵的双重担忧。

Abstract: While activation steering in large language models (LLMs) is a growing area
of research, methods can often incur broader effects than desired. This
motivates isolation of purer concept vectors to enable targeted interventions
and understand LLM behavior at a more granular level. We present RepIt, a
simple and data-efficient framework for isolating concept-specific
representations. Across five frontier LLMs, RepIt enables precise
interventions: it selectively suppresses refusal on targeted concepts while
preserving refusal elsewhere, producing models that answer WMD-related
questions while still scoring as safe on standard benchmarks. We further show
that the corrective signal localizes to just 100-200 neurons and that robust
target representations can be extracted from as few as a dozen examples on a
single A6000. This efficiency raises a dual concern: manipulations can be
performed with modest compute and data to extend to underrepresented
data-scarce topics while evading existing benchmarks. By disentangling refusal
vectors with RepIt, this work demonstrates that targeted interventions can
counteract overgeneralization, laying the foundation for more granular control
of model behavior.

</details>


### [43] [Shapes of Cognition for Computational Cognitive Modeling](https://arxiv.org/abs/2509.13288)
*Marjorie McShane,Sergei Nirenburg,Sanjay Oruganti,Jesse English*

Main category: cs.AI

TL;DR: Shapes of cognition是一种新的计算认知建模范式，通过记忆化的知识星座来简化复杂现实，支持模式识别、类比推理和认知负载最小化。


<details>
  <summary>Details</summary>
Motivation: 为语言赋能的智能代理(LEIAs)开发一种能够像人类一样处理现实复杂性的计算认知模型，通过典型性预期和模式识别来最小化认知负荷。

Method: 基于形状的建模方法，整合感官、语言、概念、情景和程序知识，形成记忆化的知识星座，支持典型行为模式和异常恢复机制。

Result: 提出了一个具体的认知架构实现，能够处理广泛现象，构建可解释、可扩展且值得信赖的智能代理系统。

Conclusion: 形状认知范式不仅适用于LEIAs，其原则可以更广泛地应用于知识驱动和混合AI系统，为这些领域注入新的活力。

Abstract: Shapes of cognition is a new conceptual paradigm for the computational
cognitive modeling of Language-Endowed Intelligent Agents (LEIAs). Shapes are
remembered constellations of sensory, linguistic, conceptual, episodic, and
procedural knowledge that allow agents to cut through the complexity of real
life the same way as people do: by expecting things to be typical, recognizing
patterns, acting by habit, reasoning by analogy, satisficing, and generally
minimizing cognitive load to the degree situations permit. Atypical outcomes
are treated using shapes-based recovery methods, such as learning on the fly,
asking a human partner for help, or seeking an actionable, even if imperfect,
situational understanding. Although shapes is an umbrella term, it is not
vague: shapes-based modeling involves particular objectives, hypotheses,
modeling strategies, knowledge bases, and actual models of wide-ranging
phenomena, all implemented within a particular cognitive architecture. Such
specificity is needed both to vet our hypotheses and to achieve our practical
aims of building useful agent systems that are explainable, extensible, and
worthy of our trust, even in critical domains. However, although the LEIA
example of shapes-based modeling is specific, the principles can be applied
more broadly, giving new life to knowledge-based and hybrid AI.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [44] [Graph Coloring Below Guarantees via Co-Triangle Packing](https://arxiv.org/abs/2509.12347)
*Shyan Akmal,Tomohiro Koana*

Main category: cs.DS

TL;DR: 本文研究低于保证值的图着色问题，提出基于共三角形打包的算法框架，在给定共三角形自由调节器的情况下，以随机化O*(2^k)时间解决ℓ-着色问题，并改进了(n-k)-着色的时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究低于保证值的图着色问题，即测试n顶点图是否可以用g-k种颜色正确着色，其中g是平凡上界如n。旨在开发更高效的算法来处理这类问题。

Method: 引入基于共三角形(K3)打包的算法框架：贪婪查找共三角形并采用赢-赢分析。如果找到很多共三角形则返回YES，否则形成小的共三角形调节器，删除后使图变为共三角形自由。扩展了Gutin等人的工作，在给定大小为k的共三角形自由调节器时，以随机化O*(2^k)时间解决ℓ-着色问题。

Result: 获得了随机化O*(2^{3k/2})算法用于(n-k)-着色，改进了之前的O*(4^k)界限。针对更小的参数化问题(ω+μ-k)-着色，获得了随机化O*(2^{6k})算法，确立了其固定参数可处理性。

Conclusion: 证明了(ω+μ-k)-着色问题的固定参数可处理性，同时表明在标准复杂度假设下，(ω-k)-着色和(μ-k)-着色不存在固定参数可处理算法。

Abstract: In the $\ell$-Coloring Problem, we are given a graph on $n$ nodes, and tasked
with determining if its vertices can be properly colored using $\ell$ colors.
In this paper we study below-guarantee graph coloring, which tests whether an
$n$-vertex graph can be properly colored using $g-k$ colors, where $g$ is a
trivial upper bound such as $n$. We introduce an algorithmic framework that
builds on a packing of co-triangles $\overline{K_3}$ (independent sets of three
vertices): the algorithm greedily finds co-triangles and employs a win-win
analysis. If many are found, we immediately return YES; otherwise these
co-triangles form a small co-triangle modulator, whose deletion makes the graph
co-triangle-free.
  Extending the work of [Gutin et al., SIDMA 2021], who solved $\ell$-Coloring
(for any $\ell$) in randomized $O^*(2^{k})$ time when given a
$\overline{K_2}$-free modulator of size $k$, we show that this problem can
likewise be solved in randomized $O^*(2^{k})$ time when given a
$\overline{K_3}$-free modulator of size~$k$.
  This result in turn yields a randomized $O^{*}(2^{3k/2})$ algorithm for
$(n-k)$-Coloring (also known as Dual Coloring), improving the previous
$O^{*}(4^{k})$ bound. We then introduce a smaller parameterization,
$(\omega+\overline{\mu}-k)$-Coloring, where $\omega$ is the clique number and
$\overline{\mu}$ is the size of a maximum matching in the complement graph;
since $\omega+\overline{\mu}\le n$ for any graph, this problem is strictly
harder. Using the same co-triangle-packing argument, we obtain a randomized
$O^{*}(2^{6k})$ algorithm, establishing its fixed-parameter tractability for a
smaller parameter. Complementing this finding, we show that no fixed-parameter
tractable algorithm exists for $(\omega-k)$-Coloring or
$(\overline{\mu}-k)$-Coloring under standard complexity assumptions.

</details>


### [45] [Sublinear-Time Algorithms for Diagonally Dominant Systems and Applications to the Friedkin-Johnsen Model](https://arxiv.org/abs/2509.13112)
*Weiming Feng,Zelin Li,Pan Peng*

Main category: cs.DS

TL;DR: 提出了针对对角占优矩阵线性系统的亚线性时间随机算法，能够通过读取少量输入数据估计解的分量，并证明了匹配的下界


<details>
  <summary>Details</summary>
Motivation: 传统线性系统求解算法需要读取整个矩阵，对于大规模问题效率低下。需要开发亚线性时间算法，仅通过部分输入就能估计解的分量

Method: 基于分析解满足的简单概率递推关系，设计随机算法。算法仅需读取矩阵S和向量b的一小部分，时间复杂度与S_max线性相关

Result: 对于严格对角占优矩阵(δ>0)，算法时间复杂度为O(‖b‖∞²S_max/(δ³ε²)log(‖b‖∞/(δε)))，并证明S_max的线性依赖是最优的。算法适用于更一般的对角占优矩阵类型

Conclusion: 该工作首次为一般对角占优矩阵提供了亚线性时间算法，突破了之前仅限于对称非负对角占优矩阵的限制，并在意见估计模型中得到了应用

Abstract: We study sublinear-time algorithms for solving linear systems $Sz = b$, where
$S$ is a diagonally dominant matrix, i.e., $|S_{ii}| \geq \delta + \sum_{j \ne
i} |S_{ij}|$ for all $i \in [n]$, for some $\delta \geq 0$. We present
randomized algorithms that, for any $u \in [n]$, return an estimate $z_u$ of
$z^*_u$ with additive error $\varepsilon$ or $\varepsilon \lVert
z^*\rVert_\infty$, where $z^*$ is some solution to $Sz^* = b$, and the
algorithm only needs to read a small portion of the input $S$ and $b$. For
example, when the additive error is $\varepsilon$ and assuming $\delta>0$, we
give an algorithm that runs in time $O\left( \frac{\|b\|_\infty^2
S_{\max}}{\delta^3 \varepsilon^2} \log \frac{\| b \|_\infty}{\delta
\varepsilon} \right)$, where $S_{\max} = \max_{i \in [n]} |S_{ii}|$. We also
prove a matching lower bound, showing that the linear dependence on $S_{\max}$
is optimal. Unlike previous sublinear-time algorithms, which apply only to
symmetric diagonally dominant matrices with non-negative diagonal entries, our
algorithm works for general strictly diagonally dominant matrices ($\delta >
0$) and a broader class of non-strictly diagonally dominant matrices $(\delta =
0)$. Our approach is based on analyzing a simple probabilistic recurrence
satisfied by the solution. As an application, we obtain an improved
sublinear-time algorithm for opinion estimation in the Friedkin--Johnsen model.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [46] [Knowledge Graph Tokenization for Behavior-Aware Generative Next POI Recommendation](https://arxiv.org/abs/2509.12350)
*Ke Sun,Mayi Xu*

Main category: cs.IR

TL;DR: KGTB提出了一种基于知识图谱的令牌化方法，通过多行为学习来解决生成式POI推荐中的信息丢失和行为理解不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在令牌化过程中丢失异构信息，且只关注POI访问行为而忽略其他行为类型，导致对移动性理解不足。

Method: 使用知识图谱组织推荐数据，开发基于KG的令牌化器来保留异构信息，并通过多行为预测任务（POI、类别、区域访问行为）进行LLM微调。

Result: 在四个真实城市数据集上的实验表明KGTB具有优越性能。

Conclusion: KGTB通过知识图谱令牌化和多行为学习有效提升了生成式POI推荐的性能，解决了信息丢失和行为理解不足的问题。

Abstract: Generative paradigm, especially powered by Large Language Models (LLMs), has
emerged as a new solution to the next point-of-interest (POI) recommendation.
Pioneering studies usually adopt a two-stage pipeline, starting with a
tokenizer converting POIs into discrete identifiers that can be processed by
LLMs, followed by POI behavior prediction tasks to instruction-tune LLM for
next POI recommendation. Despite of remarkable progress, they still face two
limitations: (1) existing tokenizers struggle to encode heterogeneous signals
in the recommendation data, suffering from information loss issue, and (2)
previous instruction-tuning tasks only focus on users' POI visit behavior while
ignore other behavior types, resulting in insufficient understanding of
mobility. To address these limitations, we propose KGTB (Knowledge Graph
Tokenization for Behavior-aware generative next POI recommendation).
Specifically, KGTB organizes the recommendation data in a knowledge graph (KG)
format, of which the structure can seamlessly preserve the heterogeneous
information. Then, a KG-based tokenizer is developed to quantize each node into
an individual structural ID. This process is supervised by the KG's structure,
thus reducing the loss of heterogeneous information. Using generated IDs, KGTB
proposes multi-behavior learning that introduces multiple behavior-specific
prediction tasks for LLM fine-tuning, e.g., POI, category, and region visit
behaviors. Learning on these behavior tasks provides LLMs with comprehensive
insights on the target POI visit behavior. Experiments on four real-world city
datasets demonstrate the superior performance of KGTB.

</details>


### [47] [What News Recommendation Research Did (But Mostly Didn't) Teach Us About Building A News Recommender](https://arxiv.org/abs/2509.12361)
*Karl Higley,Robin Burke,Michael D. Ekstrand,Bart P. Knijnenburg*

Main category: cs.IR

TL;DR: 论文报告了构建新闻推荐系统POPROX的实际经验，发现当前研究文献与真实系统构建之间存在显著差距，并提出了使未来研究更具实践应用性的建议。


<details>
  <summary>Details</summary>
Motivation: 旨在将新闻推荐研究文献应用于构建真实的新闻推荐系统，检验研究成果在实际系统建设中的适用性和支持程度。

Method: 通过构建并运营POPROX这个真实的新闻推荐研究平台，收集实践经验，分析在实现个性化推荐功能时遇到的实际挑战。

Result: 发现了文献研究中未预料到的多个挑战，揭示了当前研究与实际系统需求之间的重要差距，特别是在新闻聚合商和出版商常见产品功能的实现方面。

Conclusion: 需要使未来的新闻推荐研究更加注重实际应用性，论文提供了从构建真实系统中获得的经验教训，并指出了提高研究实践影响力的机会。

Abstract: One of the goals of recommender systems research is to provide insights and
methods that can be used by practitioners to build real-world systems that
deliver high-quality recommendations to actual people grounded in their genuine
interests and needs. We report on our experience trying to apply the news
recommendation literature to build POPROX, a live platform for news
recommendation research, and reflect on the extent to which the current state
of research supports system-building efforts. Our experience highlights several
unexpected challenges encountered in building personalization features that are
commonly found in products from news aggregators and publishers, and shows how
those difficulties are connected to surprising gaps in the literature. Finally,
we offer a set of lessons learned from building a live system with a persistent
user base and highlight opportunities to make future news recommendation
research more applicable and impactful in practice.

</details>


### [48] [LEAF: Knowledge Distillation of Text Embedding Models with Teacher-Aligned Representations](https://arxiv.org/abs/2509.12539)
*Robin Vujanic,Thomas Rueckstiess*

Main category: cs.IR

TL;DR: LEAF是一个轻量级的知识蒸馏框架，用于文本嵌入模型，能够将小模型与教师模型对齐，支持信息检索中的非对称架构，并在BEIR和MTEB基准测试中达到同类尺寸模型的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决文本嵌入模型在信息检索等应用中需要高效部署小模型的需求，同时保持与大型教师模型的对齐性能，减少训练数据和基础设施要求。

Method: 使用知识蒸馏框架，将小模型（leaf模型）与教师模型对齐，支持非对称架构（文档用大模型编码，查询用小模型处理），无需显式训练即可继承教师模型的MRL和输出量化鲁棒性。

Result: leaf-ir模型（2300万参数）在BEIR基准测试中排名第一，leaf-mt模型在MTEB v2（英语）排行榜中同类尺寸排名第一，非对称模式下检索性能进一步提升。

Conclusion: LEAF框架有效实现了轻量级文本嵌入模型与教师模型的对齐，在多个基准测试中达到最佳性能，且训练要求低，适用于黑盒模型。

Abstract: We present LEAF ("Lightweight Embedding Alignment Framework"), a knowledge
distillation framework for text embedding models. A key distinguishing feature
is that our distilled leaf models are aligned to their teacher. In the context
of information retrieval, this allows for flexible asymmetric architectures
where documents are encoded with the larger teacher model, while queries can be
served with the smaller leaf models. We also show that leaf models
automatically inherit MRL and robustness to output quantization whenever these
properties are present in the teacher model, without explicitly training for
them. To demonstrate the capability of our framework we publish leaf-ir, a 23M
parameters information retrieval oriented text embedding model trained using
LEAF, which sets a new state-of-the-art (SOTA) on BEIR, ranking #1 on the
public leaderboard for this benchmark and for models of its size. When run in
asymmetric mode, its retrieval performance is further increased. Our scheme is
however not restricted to the information retrieval setting, and we demonstrate
its wider applicability by synthesizing the multi-task leaf-mt model. This also
sets a new SOTA, ranking #1 on the public MTEB v2 (English) leaderboard for its
size. LEAF is applicable to black-box models and in contrast to other embedding
model training frameworks, it does not require judgments nor hard negatives,
and training can be conducted using small batch sizes. Thus, dataset and
training infrastructure requirements for our framework are modest. We make our
models publicly available under a permissive Apache 2.0 license.

</details>


### [49] [InfoGain-RAG: Boosting Retrieval-Augmented Generation via Document Information Gain-based Reranking and Filtering](https://arxiv.org/abs/2509.12765)
*Zihan Wang,Zihan Liang,Zhou Shao,Yufei Ma,Huangyu Dai,Ben Chen,Lingtao Mao,Chenyi Lei,Yuqing Ding,Han Li*

Main category: cs.IR

TL;DR: 提出Document Information Gain (DIG)指标来量化检索文档对回答生成的贡献度，并基于此构建InfoGain-RAG框架，通过训练专门的重排器来筛选最有价值的文档，显著提升了RAG系统的性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前RAG框架在识别检索文档对答案生成的实际贡献时遇到的困难，避免无关或误导性内容对最终性能的负面影响。

Method: 设计DIG指标来量化文档价值（通过计算有无文档时LLM生成信心度的差异），并基于DIG分数训练专门的重排器，从准确区分和精确排序两个角度优先选择最有价值的文档。

Result: 在多个模型和测试集上显著超过现有方法，在NaturalQA上相比普通RAG、自反射RAG和现代排序RAG分别提升17.9%、4.5%、12.5%的准确率，在GPT-4o上均提升15.3%的性能。

Conclusion: InfoGain-RAG为RAG系统提供了可靠的解决方案，能够在多种应用场景中有效筛选有价值文档并显著提升回答质量。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach to
address key limitations of Large Language Models (LLMs), such as hallucination,
outdated knowledge, and lacking reference. However, current RAG frameworks
often struggle with identifying whether retrieved documents meaningfully
contribute to answer generation. This shortcoming makes it difficult to filter
out irrelevant or even misleading content, which notably impacts the final
performance. In this paper, we propose Document Information Gain (DIG), a novel
metric designed to quantify the contribution of retrieved documents to correct
answer generation. DIG measures a document's value by computing the difference
of LLM's generation confidence with and without the document augmented.
Further, we introduce InfoGain-RAG, a framework that leverages DIG scores to
train a specialized reranker, which prioritizes each retrieved document from
exact distinguishing and accurate sorting perspectives. This approach can
effectively filter out irrelevant documents and select the most valuable ones
for better answer generation. Extensive experiments across various models and
benchmarks demonstrate that InfoGain-RAG can significantly outperform existing
approaches, on both single and multiple retrievers paradigm. Specifically on
NaturalQA, it achieves the improvements of 17.9%, 4.5%, 12.5% in exact match
accuracy against naive RAG, self-reflective RAG and modern ranking-based RAG
respectively, and even an average of 15.3% increment on advanced proprietary
model GPT-4o across all datasets. These results demonstrate the feasibility of
InfoGain-RAG as it can offer a reliable solution for RAG in multiple
applications.

</details>


### [50] [DiffHash: Text-Guided Targeted Attack via Diffusion Models against Deep Hashing Image Retrieval](https://arxiv.org/abs/2509.12824)
*Zechao Liu,Zheng Zhou,Xiangkun Chen,Tao Liang,Dapeng Lang*

Main category: cs.IR

TL;DR: DiffHash是一种基于扩散模型的深度哈希目标攻击方法，通过LLM生成文本指导优化潜在表示，使用多空间哈希对齐网络和文本注意力机制生成语义对齐的对抗样本


<details>
  <summary>Details</summary>
Motivation: 现有深度哈希模型面临对抗样本的安全风险，传统目标攻击方法缺乏多模态指导、依赖标签信息且基于像素级操作，存在局限性

Method: 优化图像潜在表示而非直接修改像素，使用LLM生成目标图像的文本信息作为指导，设计多空间哈希对齐网络将高维图像和文本空间对齐到低维二进制哈希空间，采用文本引导注意力机制重构对抗样本

Result: 在广泛实验中优于最先进的目标攻击方法，具有更好的黑盒可迁移性和跨数据集的稳定性

Conclusion: DiffHash通过多模态指导和潜在空间优化有效解决了传统目标攻击方法的局限性，为深度哈希模型的安全提供了新思路

Abstract: Deep hashing models have been widely adopted to tackle the challenges of
large-scale image retrieval. However, these approaches face serious security
risks due to their vulnerability to adversarial examples. Despite the
increasing exploration of targeted attacks on deep hashing models, existing
approaches still suffer from a lack of multimodal guidance, reliance on
labeling information and dependence on pixel-level operations for attacks. To
address these limitations, we proposed DiffHash, a novel diffusion-based
targeted attack for deep hashing. Unlike traditional pixel-based attacks that
directly modify specific pixels and lack multimodal guidance, our approach
focuses on optimizing the latent representations of images, guided by text
information generated by a Large Language Model (LLM) for the target image.
Furthermore, we designed a multi-space hash alignment network to align the
high-dimension image space and text space to the low-dimension binary hash
space. During reconstruction, we also incorporated text-guided attention
mechanisms to refine adversarial examples, ensuring them aligned with the
target semantics while maintaining visual plausibility. Extensive experiments
have demonstrated that our method outperforms state-of-the-art (SOTA) targeted
attack methods, achieving better black-box transferability and offering more
excellent stability across datasets.

</details>


### [51] [A Learnable Fully Interacted Two-Tower Model for Pre-Ranking System](https://arxiv.org/abs/2509.12948)
*Chao Xiong,Xianwen Yu,Wei Xu,Lei Cheng,Chuan Yuan,Linjian Mo*

Main category: cs.IR

TL;DR: 提出FIT模型，通过可学习的元查询模块和轻量级相似度评分器，在保持推理效率的同时增强双塔模型的信息交互能力


<details>
  <summary>Details</summary>
Motivation: 传统双塔模型在预排序系统中效率与效果平衡良好，但由于用户和物品塔独立处理，缺乏信息交互导致效果受限

Method: FIT架构包含元查询模块（MQM）和轻量级相似度评分器（LSS）。MQM引入可学习的物品元矩阵实现早期交互，LSS实现后期交互

Result: 在多个公共数据集上的实验结果表明，FIT显著优于最先进的基线预排序模型

Conclusion: FIT模型成功解决了双塔模型信息交互不足的问题，在保持推理效率的同时显著提升了推荐效果

Abstract: Pre-ranking plays a crucial role in large-scale recommender systems by
significantly improving the efficiency and scalability within the constraints
of providing high-quality candidate sets in real time. The two-tower model is
widely used in pre-ranking systems due to a good balance between efficiency and
effectiveness with decoupled architecture, which independently processes user
and item inputs before calculating their interaction (e.g. dot product or
similarity measure). However, this independence also leads to the lack of
information interaction between the two towers, resulting in less
effectiveness. In this paper, a novel architecture named learnable Fully
Interacted Two-tower Model (FIT) is proposed, which enables rich information
interactions while ensuring inference efficiency. FIT mainly consists of two
parts: Meta Query Module (MQM) and Lightweight Similarity Scorer (LSS).
Specifically, MQM introduces a learnable item meta matrix to achieve expressive
early interaction between user and item features. Moreover, LSS is designed to
further obtain effective late interaction between the user and item towers.
Finally, experimental results on several public datasets show that our proposed
FIT significantly outperforms the state-of-the-art baseline pre-ranking models.

</details>


### [52] [Protecting participants or population? Comparison of k-anonymous Origin-Destination matrices](https://arxiv.org/abs/2509.12950)
*Pietro Armenante,Kai Huang,Nikhil Jha,Luca Vassio*

Main category: cs.IR

TL;DR: 本文提出了ODkAnon方法，用于生成保护隐私的OD矩阵，在保证调查参与者和整个人口k-匿名性的同时，还能保留丰富的社会人口统计信息。


<details>
  <summary>Details</summary>
Motivation: 传统OD矩阵在保护隐私时往往只关注调查参与者，而忽略了整个人口的隐私保护。本文旨在开发一种既能保护个体隐私又能保护整个人口隐私的匿名化框架。

Method: 提出了ODkAnon贪婪算法，通过地理区域泛化实现k-匿名性。与传统方法（ATG、OIGH、Mondrian）相比，新方法在速度和效果之间取得平衡，并考虑了整个人口的代表性。

Result: 开发了能够同时保护调查参与者和实际人口隐私的OD矩阵生成方法，相比传统方法在保护整个人口隐私方面有显著改进。

Conclusion: 基于人口视角的隐私保护方法为OD矩阵匿名化提供了新范式，ODkAnon方法在保护隐私的同时保持了数据的社会人口统计价值，为移动性研究提供了更好的隐私保护解决方案。

Abstract: Origin-Destination (OD) matrices are a core component of research on users'
mobility and summarize how individuals move between geographical regions. These
regions should be small enough to be representative of user mobility, without
incurring substantial privacy risks. There are two added values of the
NetMob2025 challenge dataset. Firstly, the data is extensive and contains a lot
of socio-demographic information that can be used to create multiple OD
matrices, based on the segments of the population. Secondly, a participant is
not merely a record in the data, but a statistically weighted proxy for a
segment of the real population. This opens the door to a fundamental shift in
the anonymization paradigm. A population-based view of privacy is central to
our contribution. By adjusting our anonymization framework to account for
representativeness, we are also protecting the inferred identity of the actual
population, rather than survey participants alone. The challenge addressed in
this work is to produce and compare OD matrices that are k-anonymous for survey
participants and for the whole population. We compare several traditional
methods of anonymization to k-anonymity by generalizing geographical areas.
These include generalization over a hierarchy (ATG and OIGH) and the classical
Mondrian. To this established toolkit, we add a novel method, i.e., ODkAnon, a
greedy algorithm aiming at balancing speed and quality. Unlike previous
approaches, which primarily address the privacy aspects of the given datasets,
we aim to contribute to the generation of privacy-preserving OD matrices
enriched with socio-demographic segmentation that achieves k-anonymity on the
actual population.

</details>


### [53] [Green Recommender Systems: Understanding and Minimizing the Carbon Footprint of AI-Powered Personalization](https://arxiv.org/abs/2509.13001)
*Lukas Wegmeth,Tobias Vente,Alan Said,Joeran Beel*

Main category: cs.IR

TL;DR: 推荐系统研究中深度学习模型的碳排放量是传统模型的42倍，单篇论文产生2909千克CO2当量，呼吁采用绿色AI原则


<details>
  <summary>Details</summary>
Motivation: 随着全球变暖加剧，需要评估和减少推荐系统的环境影响，但推荐系统社区对此缺乏认识和评估

Method: 复制2013和2023年ACM RecSys会议的79篇论文实验流程，使用硬件能量计测量能耗并转换为CO2当量，比较传统AI模型和深度学习模型

Result: 使用深度学习模型的论文碳排放量比传统模型高约42倍，平均单篇深度学习论文产生2909千克CO2当量

Conclusion: 推荐系统和机器学习社区迫切需要采用绿色AI原则，在算法进步和环境责任之间取得平衡，构建可持续的AI个性化未来

Abstract: As global warming soars, the need to assess and reduce the environmental
impact of recommender systems is becoming increasingly urgent. Despite this,
the recommender systems community hardly understands, addresses, and evaluates
the environmental impact of their work. In this study, we examine the
environmental impact of recommender systems research by reproducing typical
experimental pipelines. Based on our results, we provide guidelines for
researchers and practitioners on how to minimize the environmental footprint of
their work and implement green recommender systems - recommender systems
designed to minimize their energy consumption and carbon footprint. Our
analysis covers 79 papers from the 2013 and 2023 ACM RecSys conferences,
comparing traditional "good old-fashioned AI" models with modern deep learning
models. We designed and reproduced representative experimental pipelines for
both years, measuring energy consumption using a hardware energy meter and
converting it into CO2 equivalents. Our results show that papers utilizing deep
learning models emit approximately 42 times more CO2 equivalents than papers
using traditional models. On average, a single deep learning-based paper
generates 2,909 kilograms of CO2 equivalents - more than the carbon emissions
of a person flying from New York City to Melbourne or the amount of CO2
sequestered by one tree over 260 years. This work underscores the urgent need
for the recommender systems and wider machine learning communities to adopt
green AI principles, balancing algorithmic advancements and environmental
responsibility to build a sustainable future with AI-powered personalization.

</details>


### [54] [Efficient Cold-Start Recommendation via BPE Token-Level Embedding Initialization with LLM](https://arxiv.org/abs/2509.13179)
*Yushang Zhao,Xinyue Han,Qian Leng,Qianyi Sun,Haotian Lyu,Chengrui Zhou*

Main category: cs.IR

TL;DR: 提出基于BPE分词和预训练LLM嵌入的冷启动推荐方法，使用细粒度词元级向量作为语义先验，在零样本设置下实现即时推荐，无需用户-物品交互历史


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中的冷启动问题，特别是在新用户或新物品缺乏历史交互数据时，传统基于内容或混合方法在稀疏元数据环境中只能捕捉浅层模式

Method: 应用Byte Pair Encoding (BPE)分词和预训练大语言模型(LLM)嵌入，获得与BPE词汇表对齐的细粒度词元级向量，作为未见实体的密集语义先验

Result: 在基准数据集上的实验显示，BPE-LLM方法在Recall@k、NDCG@k和Hit Rate指标上优于标准基线，具有更好的泛化性和可解释性，计算性能充足

Conclusion: 词元级语义初始化作为轻量级但有效的扩展，可应用于现代推荐系统的零样本设置，在多语言和稀疏输入环境下表现优异

Abstract: The cold-start issue is the challenge when we talk about recommender systems,
especially in the case when we do not have the past interaction data of new
users or new items. Content-based features or hybrid solutions are common as
conventional solutions, but they can only work in a sparse metadata environment
with shallow patterns. In this paper, the efficient cold-start recommendation
strategy is presented, which is based on the sub word-level representations by
applying Byte Pair Encoding (BPE) tokenization and pre-trained Large Language
Model (LLM) embedding in the initialization procedure. We obtain fine-grained
token-level vectors that are aligned with the BPE vocabulary as opposed to
using coarse-grained sentence embeddings. Together, these token embeddings can
be used as dense semantic priors on unseen entities, making immediate
recommendation performance possible without user-item interaction history. Our
mechanism can be compared to collaborative filtering systems and tested over
benchmark datasets with stringent cold-start assumptions. Experimental findings
show that the given BPE-LLM method achieves higher Recall@k, NDCG@k, and Hit
Rate measurements compared to the standard baseline and displays the same
capability of sufficient computational performance. Furthermore, we demonstrate
that using subword-aware embeddings yields better generalizability and is more
interpretable, especially within a multilingual and sparse input setting. The
practical application of token-level semantic initialization as a lightweight,
but nevertheless effective extension to modern recommender systems in the
zero-shot setting is indicated within this work.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [55] [ScaleDoc: Scaling LLM-based Predicates over Large Document Collections](https://arxiv.org/abs/2509.12610)
*Hengrui Zhang,Yulong Hui,Yihao Liu,Huanchen Zhang*

Main category: cs.DB

TL;DR: ScaleDoc是一个针对大规模文档语义分析的高效系统，通过离线表示和在线过滤两阶段处理，减少LLM调用85%，实现2倍端到端加速


<details>
  <summary>Details</summary>
Motivation: 现代工作负载涉及大量非结构化文档，需要语义理解而非传统值谓词。虽然LLM具有强大的零样本能力，但其高推理成本导致不可接受的系统开销

Method: 1) 离线阶段使用LLM为文档生成语义表示；2) 在线阶段为每个查询训练轻量级代理模型过滤大部分文档，仅将模糊案例转发给LLM；3) 对比学习框架训练代理模型生成可靠预测分数；4) 自适应级联机制确定有效过滤策略

Result: 在三个数据集上的评估显示，ScaleDoc实现超过2倍的端到端加速，减少昂贵LLM调用达85%

Conclusion: ScaleDoc使大规模语义分析变得实用且高效，通过创新的两阶段处理和优化机制显著降低了LLM的使用成本

Abstract: Predicates are foundational components in data analysis systems. However,
modern workloads increasingly involve unstructured documents, which demands
semantic understanding, beyond traditional value-based predicates. Given
enormous documents and ad-hoc queries, while Large Language Models (LLMs)
demonstrate powerful zero-shot capabilities, their high inference cost leads to
unacceptable overhead. Therefore, we introduce \textsc{ScaleDoc}, a novel
system that addresses this by decoupling predicate execution into an offline
representation phase and an optimized online filtering phase. In the offline
phase, \textsc{ScaleDoc} leverages a LLM to generate semantic representations
for each document. Online, for each query, it trains a lightweight proxy model
on these representations to filter the majority of documents, forwarding only
the ambiguous cases to the LLM for final decision. Furthermore,
\textsc{ScaleDoc} proposes two core innovations to achieve significant
efficiency: (1) a contrastive-learning-based framework that trains the proxy
model to generate reliable predicating decision scores; (2) an adaptive cascade
mechanism that determines the effective filtering policy while meeting specific
accuracy targets. Our evaluations across three datasets demonstrate that
\textsc{ScaleDoc} achieves over a 2$\times$ end-to-end speedup and reduces
expensive LLM invocations by up to 85\%, making large-scale semantic analysis
practical and efficient.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [56] [Evaluation of Objective Image Quality Metrics for High-Fidelity Image Compression](https://arxiv.org/abs/2509.13150)
*Shima Mohammadi,Mohsen Jenadeleh,Jon Sneyers,Dietmar Saupe,João Ascenso*

Main category: cs.MM

TL;DR: 本文研究了高保真图像压缩场景下客观质量评估指标的性能，提出了新的评估方法并进行了全面评测，发现现有指标在检测细微压缩伪影方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 随着图像压缩技术向高保真质量范围发展，需要能够检测和量化细微压缩伪影的质量评估方法，但现有客观质量指标在此范围内的性能尚未得到充分研究。

Method: 使用JPEG AIC-3高保真图像压缩数据集，提出包含主观评分不确定性的Z-RMSE评估标准，应用新颖的统计测试方法，分析完整数据集及其高保真和中保真子集。

Result: 研究揭示了现有客观质量评估指标在检测Just Noticeable Difference阈值以下失真时的可靠性问题，并发布了包含基准和评估工具的公共数据集。

Conclusion: 在高保真图像压缩应用中，需要更精确的质量评估指标来检测细微伪影，本研究为后续相关研究提供了重要的评估框架和数据集支持。

Abstract: Nowadays, image compression solutions are increasingly designed to operate
within high-fidelity quality ranges, where preserving even the most subtle
details of the original image is essential. In this context, the ability to
detect and quantify subtle compression artifacts becomes critically important,
as even slight degradations can impact perceptual quality in professional or
quality sensitive applications, such as digital archiving, professional editing
and web delivery. However, the performance of current objective image quality
assessment metrics in this range has not been thoroughly investigated. In
particular, it is not well understood how reliably these metrics estimate
distortions at or below the threshold of Just Noticeable Difference (JND). This
study directly addresses this issue by proposing evaluation methodologies for
assessing the performance of objective quality metrics and performing a
comprehensive evaluation using the JPEG AIC-3 dataset which is designed for
high-fidelity image compression. Beyond conventional criteria, the study
introduces Z-RMSE to incorporate subjective score uncertainty and applies novel
statistical tests to assess significant differences between metrics. The
analysis spans the full JPEG AIC-3 range and its high- and medium-fidelity
subsets, examines the impact of cropping in subjective tests, and a public
dataset with benchmarks and evaluation tools is released to support further
research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [57] [Finite-Agent Stochastic Differential Games on Large Graphs: II. Graph-Based Architectures](https://arxiv.org/abs/2509.12484)
*Ruimeng Hu,Jihao Long,Haosheng Zhou*

Main category: cs.LG

TL;DR: 提出NTM神经网络架构，通过图引导的稀疏化处理，在保持性能的同时显著减少可训练参数数量，用于求解图结构随机微分博弈的纳什均衡


<details>
  <summary>Details</summary>
Motivation: 解决图结构多智能体系统中随机微分博弈的计算问题，这类系统在金融、机器人、能源和社会动力学中广泛存在，需要处理大规模稀疏设置下的计算效率问题

Method: 设计NTM架构，在前馈神经网络中嵌入与图拓扑对齐的固定不可训练组件，实现图引导的稀疏化，并将其集成到Direct Parameterization和Deep BSDE两种先进博弈求解器中

Result: 理论证明NTM在静态图博弈中具有通用逼近性质，数值实验表明NTM方法在三种随机微分博弈中性能与全可训练方法相当，但计算效率更高

Conclusion: NTM架构通过引入不可训练组件有效提升了图结构随机微分博弈求解的效率和可解释性，为大规模稀疏多智能体系统提供了实用的计算工具

Abstract: We propose a novel neural network architecture, called Non-Trainable
Modification (NTM), for computing Nash equilibria in stochastic differential
games (SDGs) on graphs. These games model a broad class of graph-structured
multi-agent systems arising in finance, robotics, energy, and social dynamics,
where agents interact locally under uncertainty. The NTM architecture imposes a
graph-guided sparsification on feedforward neural networks, embedding fixed,
non-trainable components aligned with the underlying graph topology. This
design enhances interpretability and stability, while significantly reducing
the number of trainable parameters in large-scale, sparse settings. We
theoretically establish a universal approximation property for NTM in static
games on graphs and numerically validate its expressivity and robustness
through supervised learning tasks. Building on this foundation, we incorporate
NTM into two state-of-the-art game solvers, Direct Parameterization and Deep
BSDE, yielding their sparse variants (NTM-DP and NTM-DBSDE). Numerical
experiments on three SDGs across various graph structures demonstrate that
NTM-based methods achieve performance comparable to their fully trainable
counterparts, while offering improved computational efficiency.

</details>


### [58] [PowerGrow: Feasible Co-Growth of Structures and Dynamics for Power Grid Synthesis](https://arxiv.org/abs/2509.12212)
*Xinyu He,Chenhan Xiao,Haoran Li,Ruizhong Qiu,Zhe Xu,Yang Weng,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: PowerGrow是一个联合生成电网结构和节点动态的框架，通过依赖分解方法降低计算开销，同时保持操作有效性，在保真度和多样性方面优于现有扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统日益动态化，但公开测试案例稀缺，需要生成工具来联合合成电网结构和节点动态，同时保持物理可行性和避免过高计算成本。

Method: 采用依赖分解方法，将复杂联合分布分解为可行电网拓扑、时间序列总线负载等条件分布链；使用分层图beta扩散过程进行结构合成，配合时间自编码器将时间序列数据嵌入紧凑潜在空间。

Result: 实验显示PowerGrow在保真度和多样性方面优于现有扩散模型，达到98.9%的潮流收敛率和改进的N-1事故恢复能力。

Conclusion: PowerGrow能够生成操作有效且现实的电网场景，为解决电力系统测试案例稀缺问题提供了有效解决方案。

Abstract: Modern power systems are becoming increasingly dynamic, with changing
topologies and time-varying loads driven by renewable energy variability,
electric vehicle adoption, and active grid reconfiguration. Despite these
changes, publicly available test cases remain scarce, due to security concerns
and the significant effort required to anonymize real systems. Such limitations
call for generative tools that can jointly synthesize grid structure and nodal
dynamics. However, modeling the joint distribution of network topology, branch
attributes, bus properties, and dynamic load profiles remains a major
challenge, while preserving physical feasibility and avoiding prohibitive
computational costs. We present PowerGrow, a co-generative framework that
significantly reduces computational overhead while maintaining operational
validity. The core idea is dependence decomposition: the complex joint
distribution is factorized into a chain of conditional distributions over
feasible grid topologies, time-series bus loads, and other system attributes,
leveraging their mutual dependencies. By constraining the generation process at
each stage, we implement a hierarchical graph beta-diffusion process for
structural synthesis, paired with a temporal autoencoder that embeds
time-series data into a compact latent space, improving both training stability
and sample fidelity. Experiments across benchmark settings show that PowerGrow
not only outperforms prior diffusion models in fidelity and diversity but also
achieves a 98.9\% power flow convergence rate and improved N-1 contingency
resilience. This demonstrates its ability to generate operationally valid and
realistic power grid scenarios.

</details>


### [59] [Scaling Up Data Parallelism in Decentralized Deep Learning](https://arxiv.org/abs/2509.12213)
*Bing Xie,Junqi Yin,Zhenyu Zhou,Sarp Oral,Feiyi Wang*

Main category: cs.LG

TL;DR: 本文提出了DBench基准测试框架和Ada自适应方法，研究发现去中心化学习在扩展性、模型精度与通信图连接数相关、对参数张量方差敏感，Ada方法能在1008 GPU规模上达到与中心化学习相当的精度


<details>
  <summary>Details</summary>
Motivation: 尽管去中心化学习在理论上被广泛探索，但由于在大规模DNN训练中缺乏稳定性、可扩展性和通用性，尚未在生产环境中广泛应用。本研究旨在探索去中心化学习在生产环境中的应用潜力

Method: 引入DBench基准测试框架，包含中心和去中心化DNN训练；提出基准测试方法分析模型精度与参数张量方差的相关性；基于观察结果提出Ada自适应方法，在训练迭代过程中动态调整通信图

Result: 发现去中心化训练存在扩展性和通用性问题；模型精度与通信图连接数相关；对参数张量方差异常敏感；Ada方法在所有样本应用中都能获得最佳收敛率，在1008 GPU规模上训练ResNet50达到与中心化学习相当的精度

Conclusion: Ada方法证明了去中心化学习在大规模DNN训练中的可行性，能够提供与中心化学习相当的性能，为生产环境应用提供了有力支持

Abstract: Although it has been extensively explored in theory, decentralized learning
is not yet green-lighted for production use, largely due to a lack of
stability, scalability, and generality in large scale DNN training. To shed
light on the production use of decentralized learning, this work studies
decentralized data parallel training at scale. To this end, we introduce a
benchmarking framework, namely DBench, to host both centralized and
decentralized DNN training. Building upon DBench, we introduce a benchmarking
methodology to uncover the correlations between model accuracy and the
variances of parameter tensors by varying communication graphs and training
scales. Based on the benchmarking results, we observe that, (1) Similar to
centralized learning, decentralized data parallel training also presents the
issues of scalability and generality when the training scales up; (2) The model
accuracy of decentralized learning is correlated to the number of connections
in a communication graph; (3) The model accuracy of decentralized learning is
surprisingly sensitive to the variance of parameter tensors across model
replicas. Built upon the observations, we propose Ada, a decentralized adaptive
approach that performs large scale DNN training following a decentralized SGD
method and adapting the communication graph in use dynamically throughout
training iterations. We apply Ada on large scale training and observe that Ada
can obtain the best convergence rates consistently in decentralized DNN
training, and delivers equally or comparably good model accuracy for all sample
applications as centralized learning does, even when training ResNet50 for
ImageNet-1K on the scale of 1008 GPUs.

</details>


### [60] [TimeCluster with PCA is Equivalent to Subspace Identification of Linear Dynamical Systems](https://arxiv.org/abs/2509.12895)
*Christian L. Hines,Samuel Spillard,Daniel P. Martin*

Main category: cs.LG

TL;DR: TimeCluster视觉分析技术与线性子空间识别方法在数学上等价，都通过滑动窗口矩阵和SVD/PCA提取时间序列的低维线性子空间结构


<details>
  <summary>Details</summary>
Motivation: 探索TimeCluster视觉分析技术与经典线性子空间识别方法之间的数学等价性，为时间序列分析提供理论支撑和新应用可能

Method: 通过数学推导证明TimeCluster的滑动窗口PCA方法与子空间识别（块汉克尔矩阵+SVD）在数学上等价，并用合成和真实动态信号实验验证

Result: 实验证实两种方法提取的嵌入坐标完全一致，证明了它们的数学等价性

Conclusion: TimeCluster与子空间识别方法本质相同，这一等价性为时间序列分析开辟了新方向，包括状态空间预测、在线扩展、外部输入整合和鲁棒趋势可视化等

Abstract: TimeCluster is a visual analytics technique for discovering structure in long
multivariate time series by projecting overlapping windows of data into a
low-dimensional space. We show that, when Principal Component Analysis (PCA) is
chosen as the dimensionality reduction technique, this procedure is
mathematically equivalent to classical linear subspace identification
(block-Hankel matrix plus Singular Vector Decomposition (SVD)). In both
approaches, the same low-dimensional linear subspace is extracted from the time
series data. We first review the TimeCluster method and the theory of subspace
system identification. Then we show that forming the sliding-window matrix of a
time series yields a Hankel matrix, so applying PCA (via SVD) to this matrix
recovers the same principal directions as subspace identification. Thus the
cluster coordinates from TimeCluster coincide with the subspace identification
methods. We present experiments on synthetic and real dynamical signals
confirming that the two embeddings coincide. Finally, we explore and discuss
future opportunities enabled by this equivalence, including forecasting from
the identified state space, streaming/online extensions, incorporating and
visualising external inputs and robust techniques for displaying underlying
trends in corrupted data.

</details>


### [61] [MEUV: Achieving Fine-Grained Capability Activation in Large Language Models via Mutually Exclusive Unlock Vectors](https://arxiv.org/abs/2509.12221)
*Xin Tong,Zhi Lin,Jingya Wang,Meng Han,Bo Jin*

Main category: cs.LG

TL;DR: MEUV框架通过主题对齐的互斥解锁向量，实现了细粒度的安全控制，在保持高攻击成功率的同时显著减少跨主题泄露，为安全敏感领域的LLM部署提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 传统LLM的安全对齐机制会一概拒绝所有恶意请求，但也阻碍了执法、国防等高风险领域的合法使用需求。现有的单一拒绝方向编辑方法缺乏语义控制，会无差别解锁所有危险话题。

Method: 提出互斥解锁向量(MEUV)框架，将单一拒绝方向分解为多个主题对齐、近乎正交的向量，每个向量专门针对一个敏感能力。通过多任务目标在单个epoch中学习，结合差异消融边界、跨主题和正交性惩罚等辅助项。

Result: 在双语恶意提示基准测试中，MEUV在Gemma-2-2B、LLaMA-3-8B和Qwen-7B上实现不低于87%的攻击成功率，同时相比最佳单方向基线减少高达90%的跨主题泄露。中英文向量可几乎无损跨语言迁移。

Conclusion: 研究表明通过细粒度的主题级能力激活可以实现最小效用损失，为安全敏感领域的受控LLM部署铺平了道路，揭示了语言无关的拒绝子空间存在。

Abstract: Large language models (LLMs) enforce safety alignment to reliably refuse
malicious requests, yet the same blanket safeguards also block legitimate uses
in policing, defense, and other high-stakes settings. Earlier
"refusal-direction" edits can bypass those layers, but they rely on a single
vector that indiscriminately unlocks all hazardous topics, offering no semantic
control. We introduce Mutually Exclusive Unlock Vectors (MEUV), a lightweight
framework that factorizes the monolithic refusal direction into topic-aligned,
nearly orthogonal vectors, each dedicated to one sensitive capability. MEUV is
learned in a single epoch with a multi-task objective that blends a
differential-ablation margin, cross-topic and orthogonality penalties, and
several auxiliary terms. On bilingual malicious-prompt benchmarks, MEUV
achieves an attack success rate of no less than 87% on Gemma-2-2B, LLaMA-3-8B,
and Qwen-7B, yet cuts cross-topic leakage by up to 90% compared with the best
single-direction baseline. Vectors trained in Chinese transfer almost unchanged
to English (and vice versa), suggesting a language-agnostic refusal subspace.
The results show that fine-grained, topic-level capability activation is
achievable with minimal utility loss, paving the way for controlled LLMs
deployment in security-sensitive domains.

</details>


### [62] [Accelerating Privacy-Preserving Federated Learning in Large-Scale LEO Satellite Systems](https://arxiv.org/abs/2509.12222)
*Binquan Guo,Junteng Cao,Marie Siew,Binbin Chen,Tony Q. S. Quek,Zhu Han*

Main category: cs.LG

TL;DR: 提出基于离散时序图的按需调度框架，用于卫星网络中的联邦学习加速，相比传统方法减少14.20%-41.48%的训练轮次时间


<details>
  <summary>Details</summary>
Motivation: 大规模低轨卫星系统支持广域数据交换和AI协同训练，但隐私约束下无法集中原始数据。联邦学习虽能保护隐私，但卫星网络的动态拓扑和有限带宽会阻碍参数聚合，导致训练时间延长

Method: 研究卫星网络中联邦学习调度问题，识别关键瓶颈，提出离散时序图按需调度框架，动态分配通信资源

Result: 仿真显示相比传统统计复用策略，该方法显著提升性能，训练轮次时间减少14.20%-41.48%，模型越大、客户端越多加速效果越明显

Conclusion: 所提方法具有良好可扩展性，特别适用于大规模模型和大量客户端的卫星联邦学习场景

Abstract: Large-scale low-Earth-orbit (LEO) satellite systems are increasingly valued
for their ability to enable rapid and wide-area data exchange, thereby
facilitating the collaborative training of artificial intelligence (AI) models
across geographically distributed regions. Due to privacy concerns and
regulatory constraints, raw data collected at remote clients cannot be
centrally aggregated, posing a major obstacle to traditional AI training
methods. Federated learning offers a privacy-preserving alternative by training
local models on distributed devices and exchanging only model parameters.
However, the dynamic topology and limited bandwidth of satellite systems will
hinder timely parameter aggregation and distribution, resulting in prolonged
training times. To address this challenge, we investigate the problem of
scheduling federated learning over satellite networks and identify key
bottlenecks that impact the overall duration of each training round. We propose
a discrete temporal graph-based on-demand scheduling framework that dynamically
allocates communication resources to accelerate federated learning. Simulation
results demonstrate that the proposed approach achieves significant performance
gains over traditional statistical multiplexing-based model exchange
strategies, reducing overall round times by 14.20% to 41.48%. Moreover, the
acceleration effect becomes more pronounced for larger models and higher
numbers of clients, highlighting the scalability of the proposed approach.

</details>


### [63] [TripOptimizer: Generative 3D Shape Optimization and Drag Prediction using Triplane VAE Networks](https://arxiv.org/abs/2509.12224)
*Parsa Vatani,Mohamed Elrefaie,Farhad Nazarpour,Faez Ahmed*

Main category: cs.LG

TL;DR: TripOptimizer是一个基于深度学习的可微分框架，用于从车辆点云数据直接进行快速气动分析和形状优化，通过变分自编码器和三平面隐式神经表示实现高保真3D几何重建和阻力系数预测。


<details>
  <summary>Details</summary>
Motivation: 传统计算流体动力学(CFD)的气动形状优化计算成本高昂，严重限制了设计空间探索，需要开发更高效的优化方法。

Method: 使用变分自编码器配合三平面隐式神经表示进行3D几何重建，在DrivAerNet++数据集（8000个车辆几何和对应阻力系数）上训练，通过修改编码器参数子集来实现形状优化。

Result: 优化设计实现了高达11.8%的阻力系数降低，结果通过超过1.5亿网格单元的高保真CFD模拟验证，对非水密网格具有鲁棒性。

Conclusion: 该框架提供了更敏捷的气动形状优化工作流程，减少了对计算密集型CFD模拟的依赖，特别适用于早期设计阶段。

Abstract: The computational cost of traditional Computational Fluid Dynamics-based
Aerodynamic Shape Optimization severely restricts design space exploration.
This paper introduces TripOptimizer, a fully differentiable deep learning
framework for rapid aerodynamic analysis and shape optimization directly from
vehicle point cloud data. TripOptimizer employs a Variational Autoencoder
featuring a triplane-based implicit neural representation for high-fidelity 3D
geometry reconstruction and a drag coefficient prediction head. Trained on
DrivAerNet++, a large-scale dataset of 8,000 unique vehicle geometries with
corresponding drag coefficients computed via Reynolds-Averaged Navier-Stokes
simulations, the model learns a latent representation that encodes
aerodynamically salient geometric features. We propose an optimization strategy
that modifies a subset of the encoder parameters to steer an initial geometry
towards a target drag value, and demonstrate its efficacy in case studies where
optimized designs achieved drag coefficient reductions up to 11.8\%. These
results were subsequently validated by using independent, high-fidelity
Computational Fluid Dynamics simulations with more than 150 million cells. A
key advantage of the implicit representation is its inherent robustness to
geometric imperfections, enabling optimization of non-watertight meshes, a
significant challenge for traditional adjoint-based methods. The framework
enables a more agile Aerodynamic Shape Optimization workflow, reducing reliance
on computationally intensive CFD simulations, especially during early design
stages.

</details>


### [64] [A Physics-Informed Neural Networks-Based Model Predictive Control Framework for $SIR$ Epidemics](https://arxiv.org/abs/2509.12226)
*Aiping Zhong,Baike She,Philip E. Paré*

Main category: cs.LG

TL;DR: 提出基于物理信息神经网络(PINNs)的模型预测控制(MPC)框架，用于SIR传染病模型的联合状态参数估计和最优控制，在仅知道恢复率或基本再生数的情况下处理噪声感染数据。


<details>
  <summary>Details</summary>
Motivation: 现有MPC流行病控制研究通常假设状态可测量或参数已知，本文旨在解决在仅知道部分参数的情况下，基于噪声感染数据实时联合估计状态和参数的问题。

Method: 提出三种PINNs算法：MPC-PINNs（带控制的SIR模型）、MPC-LS-PINNs（对数尺度损失函数增强抗噪性）、MPC-SI-PINNs（积分算子和状态耦合）。针对不同假设条件扩展算法框架。

Result: 实验结果表明所提方法在不同设置下均有效，能够同时估计流行病状态参数并生成最优控制策略。

Conclusion: 该PINNs-MPC框架成功解决了SIR模型在部分参数已知情况下的联合状态参数估计问题，为流行病控制提供了有效的实时决策支持工具。

Abstract: This work introduces a physics-informed neural networks (PINNs)-based model
predictive control (MPC) framework for susceptible-infected-recovered ($SIR$)
spreading models. Existing studies in MPC design for epidemic control often
assume either 1) measurable states of the dynamics, where the parameters are
learned, or 2) known parameters of the model, where the states are learned. In
this work, we address the joint real-time estimation of states and parameters
within the MPC framework using only noisy infected states, under the assumption
that 1) only the recovery rate is known, or 2) only the basic reproduction
number is known. Under the first assumption, we propose MPC-PINNs and two novel
PINNs algorithms, all of which are integrated into the MPC framework. First, we
introduce MPC-PINNs, which are designed for $SIR$ models with control. We then
propose log-scaled PINNs (MPC-LS-PINNs), which incorporate a log-scaled loss
function to improve robustness against noise. Next, we present split-integral
PINNs (MPC-SI-PINNs), which leverage integral operators and state coupling in
the neural network training process to effectively reconstruct the complete
epidemic state information. Building upon these methods, we further extend our
framework for the second assumption. We establish the necessary conditions and
extend our PINNs algorithms, where MPC-SI-PINNs are simplified as split-PINNs
(MPC-S-PINNs). By incorporating these algorithms into the MPC framework, we
simultaneously estimate the epidemic states and parameters while generating
optimal control strategies. Experiment results demonstrate the effectiveness of
the proposed methods under different settings.

</details>


### [65] [Research on Short-Video Platform User Decision-Making via Multimodal Temporal Modeling and Reinforcement Learning](https://arxiv.org/abs/2509.12269)
*Jinmeiyang Wang,Jing Dong,Li Zhou*

Main category: cs.LG

TL;DR: MT-DQN模型整合Transformer、时序图神经网络和深度Q网络，在短视频推荐中显著优于传统方法，F1-score提升10.97%，NDCG@5提升8.3%，相比Vanilla-DQN减少MSE 34.8%和MAE 26.5%，但存在计算成本和延迟敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 解决短视频环境中用户行为预测和推荐策略优化的挑战，传统方法在处理时序依赖和多模态特征方面存在局限。

Method: 提出MT-DQN模型，集成Transformer、时序图神经网络(TGNN)和深度Q网络(DQN)，用于捕捉用户行为的时序依赖和多模态特征，优化推荐策略。

Result: 实验显示MT-DQN显著优于传统拼接模型(Concat-Modal)和经典强化学习模型(Vanilla-DQN)，在F1-score、NDCG@5、MSE和MAE等指标上均有显著提升。

Conclusion: MT-DQN在短视频推荐任务中表现出色，但存在计算成本和在线推理延迟问题，未来需要通过架构优化解决部署挑战。

Abstract: This paper proposes the MT-DQN model, which integrates a Transformer,
Temporal Graph Neural Network (TGNN), and Deep Q-Network (DQN) to address the
challenges of predicting user behavior and optimizing recommendation strategies
in short-video environments. Experiments demonstrated that MT-DQN consistently
outperforms traditional concatenated models, such as Concat-Modal, achieving an
average F1-score improvement of 10.97% and an average NDCG@5 improvement of
8.3%. Compared to the classic reinforcement learning model Vanilla-DQN, MT-DQN
reduces MSE by 34.8% and MAE by 26.5%. Nonetheless, we also recognize
challenges in deploying MT-DQN in real-world scenarios, such as its
computational cost and latency sensitivity during online inference, which will
be addressed through future architectural optimization.

</details>


### [66] [Learning to Route: Per-Sample Adaptive Routing for Multimodal Multitask Prediction](https://arxiv.org/abs/2509.12227)
*Marzieh Ajirak,Oded Bein,Ellen Rose Bowen,Dora Kanellopoulos,Avital Falk,Faith M. Gunning,Nili Solomonov,Logan Grosenick*

Main category: cs.LG

TL;DR: 提出了一个用于多任务多模态预测的自适应路由框架，能够根据样本特性动态选择模态处理路径和任务共享策略，在心理治疗应用中表现出色


<details>
  <summary>Details</summary>
Motivation: 解决心理治疗应用中结构化评估和非结构化临床笔记共存、数据部分缺失和结果相关性的问题，需要处理数据异质性和任务交互变化的挑战

Method: 基于路由的架构，定义多个模态路径（包括原始和融合的文本与数值特征表示），学习为每个输入选择最信息丰富的专家组合，任务特定预测由共享或独立头根据路由决策产生，端到端训练

Result: 在合成数据和真实心理治疗笔记上评估，该方法 consistently 优于固定的多任务或单任务基线，学习到的路由策略提供了模态相关性和任务结构的可解释性洞察

Conclusion: 该框架通过实现考虑数据异质性和任务相关性的个性化信息处理，解决了个性化医疗中的关键挑战，在心理治疗应用中可改善心理健康结果、提高治疗分配精度和临床成本效益

Abstract: We propose a unified framework for adaptive routing in multitask, multimodal
prediction settings where data heterogeneity and task interactions vary across
samples. Motivated by applications in psychotherapy where structured
assessments and unstructured clinician notes coexist with partially missing
data and correlated outcomes, we introduce a routing-based architecture that
dynamically selects modality processing pathways and task-sharing strategies on
a per-sample basis. Our model defines multiple modality paths, including raw
and fused representations of text and numeric features and learns to route each
input through the most informative expert combination. Task-specific
predictions are produced by shared or independent heads depending on the
routing decision, and the entire system is trained end-to-end. We evaluate the
model on both synthetic data and real-world psychotherapy notes predicting
depression and anxiety outcomes. Our experiments show that our method
consistently outperforms fixed multitask or single-task baselines, and that the
learned routing policy provides interpretable insights into modality relevance
and task structure. This addresses critical challenges in personalized
healthcare by enabling per-subject adaptive information processing that
accounts for data heterogeneity and task correlations. Applied to
psychotherapy, this framework could improve mental health outcomes, enhance
treatment assignment precision, and increase clinical cost-effectiveness
through personalized intervention strategies.

</details>


### [67] [Profiling LoRA/QLoRA Fine-Tuning Efficiency on Consumer GPUs: An RTX 4060 Case Study](https://arxiv.org/abs/2509.12229)
*MSR Avinash*

Main category: cs.LG

TL;DR: 在8GB VRAM限制下对Qwen2.5-1.5B-Instruct模型进行LoRA/QLoRA微调的系统性能分析，显示分页优化器可提升25%吞吐量，bf16效率低于fp16，2048序列长度可行


<details>
  <summary>Details</summary>
Motivation: 探索在消费级GPU（特别是8GB VRAM限制下）进行参数高效微调的实际效率，为资源受限的研究者和从业者提供实用指导

Method: 使用NVIDIA RTX 4060对Qwen2.5-1.5B-Instruct模型进行控制性性能分析，系统变化批次大小、序列长度、优化器选择（AdamW vs PagedAdamW）和精度（fp16 vs bf16）

Result: 分页优化器提升吞吐量达25%（628 tok/s vs 500 tok/s基准），bf16效率低于fp16，在8GB限制下2048序列长度可行

Conclusion: 这是首个在消费级GPU上系统研究LLM微调效率的案例研究，提供了可复现的基准测试和为资源受限用户提供的实用指南

Abstract: Fine-tuning large language models (LLMs) with parameter-efficient techniques
such as LoRA and QLoRA has enabled adaptation of foundation models on modest
hardware. Yet the efficiency of such training on consumer-grade GPUs,
especially under strict 8 GB VRAM limits, remains underexplored. We present a
controlled profiling study of LoRA/QLoRA fine-tuning using the
Qwen2.5-1.5B-Instruct model on a single NVIDIA RTX 4060. Across three
representative configurations, we systematically vary batch size, sequence
length, optimizer choice (AdamW vs. PagedAdamW), and precision (fp16 vs. bf16).
We report throughput (tokens/s), time per 10k tokens, and VRAM footprint,
alongside energy estimates derived from GPU board power limits. Our results
show that paged optimizers improve throughput by up to 25% (628 tok/s vs. 500
tok/s baseline), while bf16 degrades efficiency relative to fp16. Despite 8 GB
constraints, sequence lengths up to 2048 tokens were feasible using
parameter-efficient strategies. To our knowledge, this is the first systematic
case study of LLM fine-tuning efficiency on consumer GPUs, providing
reproducible benchmarks and practical guidelines for resource-constrained
researchers and practitioners.

</details>


### [68] [Flexible Multimodal Neuroimaging Fusion for Alzheimer's Disease Progression Prediction](https://arxiv.org/abs/2509.12234)
*Benjamin Burns,Yuan Xue,Douglas W. Scharre,Xia Ning*

Main category: cs.LG

TL;DR: PerM-MoE是一种新型稀疏专家混合方法，使用独立的路由器处理每个模态，在阿尔茨海默病多模态神经影像数据缺失情况下显著提升预测性能


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在临床环境中经常面临模态缺失问题，导致预测准确性下降，需要提高多模态模型在高模态缺失率下的灵活性

Method: 提出PerM-MoE方法，为每个模态使用独立路由器替代传统的单一路由器，使用T1加权MRI、FLAIR、淀粉样蛋白PET和tau PET神经影像数据

Result: PerM-MoE在大多数模态缺失变化情况下优于最先进的Flex-MoE方法，并展示了比Flex-MoE更有效的专家利用

Conclusion: PerM-MoE方法能够有效处理多模态数据缺失问题，在阿尔茨海默病认知衰退预测中表现出优越性能

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disease with high
inter-patient variance in rate of cognitive decline. AD progression prediction
aims to forecast patient cognitive decline and benefits from incorporating
multiple neuroimaging modalities. However, existing multimodal models fail to
make accurate predictions when many modalities are missing during inference, as
is often the case in clinical settings. To increase multimodal model
flexibility under high modality missingness, we introduce PerM-MoE, a novel
sparse mixture-of-experts method that uses independent routers for each
modality in place of the conventional, single router. Using T1-weighted MRI,
FLAIR, amyloid beta PET, and tau PET neuroimaging data from the Alzheimer's
Disease Neuroimaging Initiative (ADNI), we evaluate PerM-MoE, state-of-the-art
Flex-MoE, and unimodal neuroimaging models on predicting two-year change in
Clinical Dementia Rating-Sum of Boxes (CDR-SB) scores under varying levels of
modality missingness. PerM-MoE outperforms the state of the art in most
variations of modality missingness and demonstrates more effective utility of
experts than Flex-MoE.

</details>


### [69] [RL Fine-Tuning Heals OOD Forgetting in SFT](https://arxiv.org/abs/2509.12235)
*Hangzhan Jin,Sitao Luan,Sicheng Lyu,Guillaume Rabusseau,Reihaneh Rabbany,Doina Precup,Mohammad Hamdaqa*

Main category: cs.LG

TL;DR: 研究发现SFT-RL两阶段微调中，SFT早期OOD性能最佳但随后下降，RL主要起OOD恢复作用而非创造新能力，奇异向量旋转是关键机制


<details>
  <summary>Details</summary>
Motivation: 探索SFT和RL两阶段微调范式背后的协同机制，挑战"SFT记忆、RL泛化"的简化观点

Method: 使用SVD分析参数矩阵，手动编辑参数并观察性能影响，分析不同训练阶段的OOD表现

Result: 发现OOD性能在SFT早期达到峰值后下降，RL主要恢复SFT丢失的OOD能力而非创造新能力，奇异向量旋转是影响OOD行为的关键因素

Conclusion: 重新定义了SFT和RL在两阶段微调中的角色，发现奇异向量旋转是影响模型OOD性能的关键机制，为理解微调过程提供了新视角

Abstract: The two-stage fine-tuning paradigm of Supervised Fine-Tuning (SFT) followed
by Reinforcement Learning (RL) has empirically shown better reasoning
performance than one-stage SFT for the post-training of Large Language Models
(LLMs). However, the evolution and mechanism behind the synergy of SFT and RL
are still under-explored and inconclusive. In our study, we find the well-known
claim "SFT memorizes, RL generalizes" is over-simplified, and discover that:
(1) OOD performance peaks at the early stage of SFT and then declines (OOD
forgetting), the best SFT checkpoint cannot be captured by training/test loss;
(2) the subsequent RL stage does not generate fundamentally better OOD
capability, instead it plays an \textbf{OOD restoration} role, recovering the
lost reasoning ability during SFT; (3) The recovery ability has boundaries,
\ie{} \textbf{if SFT trains for too short or too long, RL cannot recover the
lost OOD ability;} (4) To uncover the underlying mechanisms behind the
forgetting and restoration process, we employ SVD analysis on parameter
matrices, manually edit them, and observe their impacts on model performance.
Unlike the common belief that the shift of model capacity mainly results from
the changes of singular values, we find that they are actually quite stable
throughout fine-tuning. Instead, the OOD behavior strongly correlates with the
\textbf{rotation of singular vectors}. Our findings re-identify the roles of
SFT and RL in the two-stage fine-tuning and discover the rotation of singular
vectors as the key mechanism. %reversing the rotations induced by SFT, which
shows recovery from forgetting, whereas imposing the SFT parameter directions
onto a RL-tuned model results in performance degradation. Code is available at
https://github.com/xiaodanguoguo/RL_Heals_SFT

</details>


### [70] [Neural Diffeomorphic-Neural Operator for Residual Stress-Induced Deformation Prediction](https://arxiv.org/abs/2509.12237)
*Changqing Liu,Kaining Dai,Zhiwei Zhao,Tianyi Wu,Yingguang Li*

Main category: cs.LG

TL;DR: 提出了一种基于微分同胚嵌入神经算子的新框架NDNO，用于高效预测不同几何形状结构件的加工变形，解决了传统数值方法计算成本高和神经算子直接应用于变化几何域的限制问题。


<details>
  <summary>Details</summary>
Motivation: 结构件加工变形预测对保证尺寸精度和可靠性至关重要，但传统数值方法计算成本高，特别是处理不同几何形状时。神经算子虽能高效求解偏微分方程，但直接应用于变化几何域存在理论和实践限制。

Method: 通过受平滑性和可逆性约束的微分同胚神经网络将复杂三维几何显式映射到共同参考域，然后在参考域上训练神经算子学习残余应力引起的变形场。训练后的微分同胚神经网络和神经算子都能高效预测，快速适应不同几何形状。

Result: 该方法能准确预测主方向和多方向变形场，在不同几何形状（包括组件类型、尺寸和特征）的零件上实现了高精度和高效率。

Conclusion: NDNO框架为不同几何形状结构件的变形预测提供了有效且计算高效的解决方案，克服了传统方法和直接应用神经算子的局限性。

Abstract: Accurate prediction of machining deformation in structural components is
essential for ensuring dimensional precision and reliability. Such deformation
often originates from residual stress fields, whose distribution and influence
vary significantly with geometric complexity. Conventional numerical methods
for modeling the coupling between residual stresses and deformation are
computationally expensive, particularly when diverse geometries are considered.
Neural operators have recently emerged as a powerful paradigm for efficiently
solving partial differential equations, offering notable advantages in
accelerating residual stress-deformation analysis. However, their direct
application across changing geometric domains faces theoretical and practical
limitations. To address this challenge, a novel framework based on
diffeomorphic embedding neural operators named neural diffeomorphic-neural
operator (NDNO) is introduced. Complex three-dimensional geometries are
explicitly mapped to a common reference domain through a diffeomorphic neural
network constrained by smoothness and invertibility. The neural operator is
then trained on this reference domain, enabling efficient learning of
deformation fields induced by residual stresses. Once trained, both the
diffeomorphic neural network and the neural operator demonstrate efficient
prediction capabilities, allowing rapid adaptation to varying geometries. The
proposed method thus provides an effective and computationally efficient
solution for deformation prediction in structural components subject to varying
geometries. The proposed method is validated to predict both main-direction and
multi-direction deformation fields, achieving high accuracy and efficiency
across parts with diverse geometries including component types, dimensions and
features.

</details>


### [71] [Interpretable Data Mining of Follicular Thyroid Cancer Ultrasound Features Using Enhanced Association Rules](https://arxiv.org/abs/2509.12238)
*Songlin Zhou,Tao Zhou,Xin Li,Stephen Shing-Toung Yau*

Main category: cs.LG

TL;DR: 本研究基于改进的关联规则挖掘方法，分析滤泡性甲状腺癌的临床数据，识别出多个与恶性肿瘤强相关的临床指标，为术前诊断提供参考。


<details>
  <summary>Details</summary>
Motivation: 滤泡性甲状腺癌缺乏特异性超声征象，术前诊断比乳头状甲状腺癌更困难，相关临床研究较少。研究旨在通过数据挖掘方法识别有助于术前诊断的临床指标。

Method: 回顾性分析2010-2023年北京大学第三医院收集的病例数据，改进传统的关联规则挖掘方法，结合可解释机器学习中的SHAP方法思想，提出新的分析指标来反映临床指标与癌症之间的恶性关联。

Result: 数据集包含1673个结节（1414个良性，259个恶性），分析发现除了常见指标外，结节内结节模式、小梁模式、低TSH评分等指标具有强恶性关联，同时合并桥本甲状腺炎也可能具有强恶性关联。

Conclusion: 在疑似滤泡性甲状腺癌结节的术前诊断中，应考虑多个临床指标以提高诊断准确性。研究中发现的多样化恶性关联可为相关领域临床医生提供参考。

Abstract: Purpose: Thyroid cancer has been a common cancer. Papillary thyroid cancer
and follicular thyroid cancer are the two most common types of thyroid cancer.
Follicular thyroid cancer lacks distinctive ultrasound signs and is more
difficult to diagnose preoperatively than the more prevalent papillary thyroid
cancer, and the clinical studies associated with it are less well established.
We aimed to analyze the clinical data of follicular thyroid cancer based on a
novel data mining tool to identify some clinical indications that may help in
preoperative diagnosis. Methods: We performed a retrospective analysis based on
case data collected by the Department of General Surgery of Peking University
Third Hospital between 2010 and 2023. Unlike traditional statistical methods,
we improved the association rule mining, a classical data mining method, and
proposed new analytical metrics reflecting the malignant association between
clinical indications and cancer with the help of the idea of SHAP method in
interpretable machine learning. Results: The dataset was preprocessed to
contain 1673 cases (in terms of nodes rather than patients), of which 1414 were
benign and 259 were malignant nodes. Our analysis pointed out that in addition
to some common indicators (e.g., irregular or lobulated nodal margins, uneven
thickness halo, hypoechogenicity), there were also some indicators with strong
malignant associations, such as nodule-in-nodule pattern, trabecular pattern,
and low TSH scores. In addition, our results suggest that the combination of
Hashimoto's thyroiditis may also have a strong malignant association.
Conclusion: In the preoperative diagnosis of nodules suspected of follicular
thyroid cancer, multiple clinical indications should be considered for a more
accurate diagnosis. The diverse malignant associations identified in our study
may serve as a reference for clinicians in related fields.

</details>


### [72] [InJecteD: Analyzing Trajectories and Drift Dynamics in Denoising Diffusion Probabilistic Models for 2D Point Cloud Generation](https://arxiv.org/abs/2509.12239)
*Sanyam Jain,Khuram Naveed,Illia Oleksiienko,Alexandros Iosifidis,Ruben Pauwels*

Main category: cs.LG

TL;DR: InJecteD是一个用于解释DDPM去噪过程的框架，通过分析2D点云生成过程中的样本轨迹来增强模型透明度，支持人类-AI协作调试生成模型。


<details>
  <summary>Details</summary>
Motivation: 提高DDPM模型的可解释性，通过分析去噪过程中的轨迹特性来增强模型透明度，使从业者能够调试和改进生成模型。

Method: 使用简化的DDPM架构，分析三个数据集（牛眼图、恐龙图、圆形图）的去噪轨迹，量化位移、速度、聚类和漂移场动态等特性，采用Wasserstein距离和余弦相似度等统计指标。

Result: 实验揭示了不同的去噪阶段：初始噪声探索、快速形状形成和最终细化，不同数据集表现出特定行为（如牛眼图的同心收敛vs恐龙图的复杂轮廓形成）。傅里叶基嵌入提高了轨迹稳定性和重建质量。

Conclusion: InJecteD框架成功提供了DDPM模型的可解释性分析，通过轨迹分析揭示了去噪过程的动态特性，傅里叶嵌入方法显示出更好的性能，为生成模型的调试和改进提供了有效工具。

Abstract: This work introduces InJecteD, a framework for interpreting Denoising
Diffusion Probabilistic Models (DDPMs) by analyzing sample trajectories during
the denoising process of 2D point cloud generation. We apply this framework to
three datasets from the Datasaurus Dozen bullseye, dino, and circle using a
simplified DDPM architecture with customizable input and time embeddings. Our
approach quantifies trajectory properties, including displacement, velocity,
clustering, and drift field dynamics, using statistical metrics such as
Wasserstein distance and cosine similarity. By enhancing model transparency,
InJecteD supports human AI collaboration by enabling practitioners to debug and
refine generative models. Experiments reveal distinct denoising phases: initial
noise exploration, rapid shape formation, and final refinement, with
dataset-specific behaviors example, bullseyes concentric convergence vs. dinos
complex contour formation. We evaluate four model configurations, varying
embeddings and noise schedules, demonstrating that Fourier based embeddings
improve trajectory stability and reconstruction quality

</details>


### [73] [Why and How Auxiliary Tasks Improve JEPA Representations](https://arxiv.org/abs/2509.12249)
*Jiacan Yu,Siyi Chen,Mingrui Liu,Nono Horiuchi,Vladimir Braverman,Zicheng Xu,Dan Haramati,Randall Balestriero*

Main category: cs.LG

TL;DR: 本文从理论上分析了带有辅助回归头的JEPA架构，证明了在确定性MDP中，当潜在转移一致性和辅助回归损失都趋近于零时，非等效观测必须映射到不同的潜在表示，辅助任务锚定了表示必须保留的区分信息。


<details>
  <summary>Details</summary>
Motivation: Joint-Embedding Predictive Architecture (JEPA) 在视觉表示学习和基于模型的强化学习中应用日益广泛，但其行为机制仍缺乏理论理解，需要建立理论框架来分析其表示学习特性。

Method: 研究一个简单的实用JEPA变体，该变体具有与潜在动态联合训练的辅助回归头。在确定性MDP环境中进行理论分析，证明"无病态表示坍缩"定理，并通过计数环境中的受控消融实验验证理论。

Result: 证明了辅助回归头能够确保非等效观测映射到不同的潜在表示，实验表明联合训练JEPA模型和辅助头比分开训练能产生更丰富的表示。

Conclusion: 研究表明通过训练带有合适辅助函数的JEPA编码器，可以与转移动态一起编码正确的等价关系，这为提高JEPA编码器性能指明了方向。

Abstract: Joint-Embedding Predictive Architecture (JEPA) is increasingly used for
visual representation learning and as a component in model-based RL, but its
behavior remains poorly understood. We provide a theoretical characterization
of a simple, practical JEPA variant that has an auxiliary regression head
trained jointly with latent dynamics. We prove a No Unhealthy Representation
Collapse theorem: in deterministic MDPs, if training drives both the
latent-transition consistency loss and the auxiliary regression loss to zero,
then any pair of non-equivalent observations, i.e., those that do not have the
same transition dynamics or auxiliary label, must map to distinct latent
representations. Thus, the auxiliary task anchors which distinctions the
representation must preserve. Controlled ablations in a counting environment
corroborate the theory and show that training the JEPA model jointly with the
auxiliary head generates a richer representation than training them separately.
Our work indicates a path to improve JEPA encoders: training them with an
auxiliary function that, together with the transition dynamics, encodes the
right equivalence relations.

</details>


### [74] [Representation Learning on Large Non-Bipartite Transaction Networks using GraphSAGE](https://arxiv.org/abs/2509.12255)
*Mihir Tare,Clemens Rattasits,Yiming Wu,Euan Wielewski*

Main category: cs.LG

TL;DR: 本文展示了GraphSAGE图神经网络在银行交易网络分析中的实际应用，能够处理动态异构交易数据，生成可解释的节点嵌入，并在洗钱检测等下游任务中提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统图嵌入方法难以处理银行动态交易数据，金融机构需要可扩展的工具来分析复杂的交易网络。GraphSAGE的归纳式学习能力能够处理未见节点，适合随时间演变的交易数据。

Method: 使用匿名化的客户和商户交易数据构建交易网络，训练GraphSAGE模型生成节点嵌入。通过探索性分析发现嵌入中的可解释聚类，并将其应用于洗钱检测分类任务。

Result: 生成的节点嵌入显示出与地理和人口属性一致的可解释聚类。在洗钱检测模型中，使用这些嵌入提高了高风险账户的优先级排序性能。

Conclusion: GraphSAGE框架具有适应性、可扩展性和可解释性，为金融机构利用图机器学习从交易生态系统中获取可操作见解提供了蓝图。

Abstract: Financial institutions increasingly require scalable tools to analyse complex
transactional networks, yet traditional graph embedding methods struggle with
dynamic, real-world banking data. This paper demonstrates the practical
application of GraphSAGE, an inductive Graph Neural Network framework, to
non-bipartite heterogeneous transaction networks within a banking context.
Unlike transductive approaches, GraphSAGE scales well to large networks and can
generalise to unseen nodes which is critical for institutions working with
temporally evolving transactional data. We construct a transaction network
using anonymised customer and merchant transactions and train a GraphSAGE model
to generate node embeddings. Our exploratory work on the embeddings reveals
interpretable clusters aligned with geographic and demographic attributes.
Additionally, we illustrate their utility in downstream classification tasks by
applying them to a money mule detection model where using these embeddings
improves the prioritisation of high-risk accounts. Beyond fraud detection, our
work highlights the adaptability of this framework to banking-scale networks,
emphasising its inductive capability, scalability, and interpretability. This
study provides a blueprint for financial organisations to harness graph machine
learning for actionable insights in transactional ecosystems.

</details>


### [75] [Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) for Diabetes Risk Prediction](https://arxiv.org/abs/2509.12259)
*Kenneth G. Young II*

Main category: cs.LG

TL;DR: 量子启发的堆叠集成概念图模型(QISICGM)利用量子启发技术，在PIMA糖尿病数据集上实现了0.8933的F1分数和0.8699的AUC，优于传统方法，具备高效推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决糖尿病风险预测中的准确性和效率问题，通过量子启发技术提升机器学习模型的性能，同时关注AI的可信度、可解释性和开源可复现性。

Method: 使用PIMA印第安人糖尿病数据集（含2000个合成样本，共2768个样本），集成自改进概念图和堆叠集成学习（包括随机森林、极端树、变换器、CNN和FFNN），采用量子启发的相位特征映射和邻域序列建模技术。

Result: 实现了0.8933的OOF F1分数和0.8699的AUC，推理速度达到8.5行/秒，性能优于传统方法。

Conclusion: QISICGM为AI辅助临床分诊提供了潜在基准，强调了通过校准、可解释性和开源可复现性实现可信AI的重要性。

Abstract: The Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) is an
innovative machine learning framework that harnesses quantum-inspired
techniques to predict diabetes risk with exceptional accuracy and efficiency.
Utilizing the PIMA Indians Diabetes dataset augmented with 2,000 synthetic
samples to mitigate class imbalance (total: 2,768 samples, 1,949 positives),
QISICGM integrates a self-improving concept graph with a stacked ensemble
comprising Random Forests (RF), Extra Trees (ET), transformers, convolutional
neural networks (CNNs), and feed-forward neural networks (FFNNs). This approach
achieves an out-of-fold (OOF) F1 score of 0.8933 and an AUC of 0.8699,
outperforming traditional methods. Quantum inspired elements, such as phase
feature mapping and neighborhood sequence modeling, enrich feature
representations, enabling CPU-efficient inference at 8.5 rows per second. This
paper presents a detailed architecture, theoretical foundations, code insights,
and performance evaluations, including visualizations from the outputs
subfolder. The open-source implementation (v1.0.0) is available at
https://github.com/keninayoung/QISICGM, positioning QISICGM as a potential
benchmark for AI-assisted clinical triage in diabetes and beyond. Ultimately,
this work emphasizes trustworthy AI through calibration, interpretability, and
open-source reproducibility.

</details>


### [76] [Explainable Fraud Detection with GNNExplainer and Shapley Values](https://arxiv.org/abs/2509.12262)
*Ngoc Hieu Dao*

Main category: cs.LG

TL;DR: 本文提出开发可解释的欺诈检测器，以应对数字支付欺诈风险增加和监管对AI系统透明度要求的挑战


<details>
  <summary>Details</summary>
Motivation: 随着数字支付的普及，金融欺诈风险增加，监管机构和社会对AI欺诈检测系统的透明度提出了更高要求，欺诈分析师也需要简洁易懂的解释来提高调查效率

Method: 专注于开发可解释的欺诈检测器（具体方法未在摘要中详细说明）

Result: 未在摘要中提供具体结果

Conclusion: 需要开发可解释的欺诈检测系统来满足监管透明度和分析师调查需求

Abstract: The risk of financial fraud is increasing as digital payments are used more
and more frequently. Although the use of artificial intelligence systems for
fraud detection is widespread, society and regulators have raised the standards
for these systems' transparency for reliability verification purposes. To
increase their effectiveness in conducting fraud investigations, fraud analysts
also profit from having concise and understandable explanations. To solve these
challenges, the paper will concentrate on developing an explainable fraud
detector.

</details>


### [77] [Deriving the Scaled-Dot-Function via Maximum Likelihood Estimation and Maximum Entropy Approach](https://arxiv.org/abs/2509.12285)
*Jiyong Ma*

Main category: cs.LG

TL;DR: 本文提出了一种最大似然估计方法来确定Transformer模型中的值向量，将值向量、键向量和查询向量序列建模为高斯分布序列，为缩放点积函数和softmax函数提供了新的概率解释。


<details>
  <summary>Details</summary>
Motivation: 为Transformer架构中的缩放点积函数和softmax函数提供新的概率解释和理论基础，从最大似然估计和最大熵两个角度来理解这些核心组件的数学本质。

Method: 将值向量、键向量和查询向量序列建模为高斯分布序列，其中每个高斯分布的方差依赖于时间步、键向量和查询向量，均值依赖于时间步和值向量。同时采用最大熵方法，使用查询向量和键向量推导最大熵模型的特征函数。

Result: 提出了一个概率框架来解释Transformer中的注意力机制，为缩放点积函数和softmax函数提供了基于高斯分布和最大熵原理的理论基础。

Conclusion: 该分析为Transformer架构的核心组件提供了新的概率解释，从最大似然估计和最大熵的角度深化了对注意力机制数学本质的理解，可能为模型改进和理论分析提供新的思路。

Abstract: In this paper, we present a maximum likelihood estimation approach to
determine the value vector in transformer models. We model the sequence of
value vectors, key vectors, and the query vector as a sequence of Gaussian
distributions. The variance in each Gaussian distribution depends on the time
step, the corresponding key vector, and the query vector. The mean value in
each Gaussian distribution depends on the time step, and the corresponding
value vector. This analysis may offer a new explanation of the
scaled-dot-product function or softmax function used in transformer
architectures [1]. Another explanation, inspired by [4], is based on the
maximum entropy approach in natural language processing [5]. In this approach,
a query vector and key vectors are used to derive the feature functions for the
maximum entropy model.

</details>


### [78] [Prediction of Stocks Index Price using Quantum GANs](https://arxiv.org/abs/2509.12286)
*Sangram Deshpande,Gopal Ramesh Dahale,Sai Nandan Morapakula,Uday Wad*

Main category: cs.LG

TL;DR: 本研究探索量子生成对抗网络(QGANs)在股价预测中的应用，通过量子计算技术提升金融时间序列预测的准确性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 金融市场具有高度复杂性和波动性，传统模型难以有效捕捉其复杂模式。量子计算与生成模型的结合为金融预测提供了新的可能性。

Method: 使用定制化的QGAN模型，基于历史股价数据进行训练，采用AWS Braket SV1模拟器进行量子电路训练，并与传统LSTM和GAN模型进行对比。

Result: QGAN模型能够生成与真实市场行为高度相似的合成数据，在收敛速度和预测准确性方面均优于经典的LSTM和GAN模型。

Conclusion: 量子计算在金融预测领域展现出显著优势，为交易员、金融分析师和研究人员提供了更先进的市场分析工具，代表了量子计算在金融应用中的重要进展。

Abstract: This paper investigates the application of Quantum Generative Adversarial
Networks (QGANs) for stock price prediction. Financial markets are inherently
complex, marked by high volatility and intricate patterns that traditional
models often fail to capture. QGANs, leveraging the power of quantum computing,
offer a novel approach by combining the strengths of generative models with
quantum machine learning techniques. We implement a QGAN model tailored for
stock price prediction and evaluate its performance using historical stock
market data. Our results demonstrate that QGANs can generate synthetic data
closely resembling actual market behavior, leading to enhanced prediction
accuracy. The experiment was conducted using the Stocks index price data and
the AWS Braket SV1 simulator for training the QGAN circuits. The
quantum-enhanced model outperforms classical Long Short-Term Memory (LSTM) and
GAN models in terms of convergence speed and prediction accuracy. This research
represents a key step toward integrating quantum computing in financial
forecasting, offering potential advantages in speed and precision over
traditional methods. The findings suggest important implications for traders,
financial analysts, and researchers seeking advanced tools for market analysis.

</details>


### [79] [C3DE: Causal-Aware Collaborative Neural Controlled Differential Equation for Long-Term Urban Crowd Flow Prediction](https://arxiv.org/abs/2509.12289)
*Yuting Liu,Qiang Zhou,Hanzhe Li,Chenqi Gong,Jingjing Gu*

Main category: cs.LG

TL;DR: 提出C3DE模型，通过神经控制微分方程和因果感知机制解决长期城市人流预测中的累积采样误差和虚假相关性问题


<details>
  <summary>Details</summary>
Motivation: 长期城市人流预测面临累积采样误差问题，同时人流与POI分布之间存在多时间尺度异步动态和潜在虚假因果关系，传统NCDE方法难以有效处理

Method: 使用双路径NCDE作为主干网络捕捉多时间尺度协作信号，设计动态校正机制和反事实因果效应估计器量化POI对人流的因果影响，最小化虚假相关积累

Result: 在三个真实世界数据集上的实验表明C3DE具有优越性能，特别是在人流波动显著的城市中表现突出

Conclusion: C3DE通过结合神经控制微分方程和因果推理，有效解决了长期城市人流预测中的关键挑战，为时空预测任务提供了新思路

Abstract: Long-term urban crowd flow prediction suffers significantly from cumulative
sampling errors, due to increased sequence lengths and sampling intervals,
which inspired us to leverage Neural Controlled Differential Equations (NCDEs)
to mitigate this issue. However, regarding the crucial influence of Points of
Interest (POIs) evolution on long-term crowd flow, the multi-timescale
asynchronous dynamics between crowd flow and POI distribution, coupled with
latent spurious causality, poses challenges to applying NCDEs for long-term
urban crowd flow prediction. To this end, we propose Causal-aware Collaborative
neural CDE (C3DE) to model the long-term dynamic of crowd flow. Specifically,
we introduce a dual-path NCDE as the backbone to effectively capture the
asynchronous evolution of collaborative signals across multiple time scales.
Then, we design a dynamic correction mechanism with the counterfactual-based
causal effect estimator to quantify the causal impact of POIs on crowd flow and
minimize the accumulation of spurious correlations. Finally, we leverage a
predictor for long-term prediction with the fused collaborative signals of POI
and crowd flow. Extensive experiments on three real-world datasets demonstrate
the superior performance of C3DE, particularly in cities with notable flow
fluctuations.

</details>


### [80] [Spontaneous Kolmogorov-Arnold Geometry in Shallow MLPs](https://arxiv.org/abs/2509.12326)
*Michael Freedman,Michael Mulligan*

Main category: cs.LG

TL;DR: 研究发现传统单隐藏层神经网络训练中会自发产生Kolmogorov-Arnold几何结构，通过Jacobian矩阵的外幂统计特性进行量化分析


<details>
  <summary>Details</summary>
Motivation: 理解神经网络如何有机地学习准备输入数据以供下游处理，并研究KA几何结构的涌现机制以通过超参数干预加速学习

Method: 通过分析Jacobian矩阵J(x)的外幂统计特性，包括零行数量和子式统计量，来量化KA几何结构，测量J(x)的尺度和轴对齐

Result: 发现在训练普通单隐藏层神经网络时经常会产生KA几何结构，并对函数复杂度和模型超参数空间中KA几何出现的区域有了初步理解

Conclusion: 这项研究是KA-Networks（KANs）的"另一面"，不是将KA工程化到神经网络中，而是观察KA在浅层MLP中的自然涌现

Abstract: The Kolmogorov-Arnold (KA) representation theorem constructs universal, but
highly non-smooth inner functions (the first layer map) in a single
(non-linear) hidden layer neural network. Such universal functions have a
distinctive local geometry, a "texture," which can be characterized by the
inner function's Jacobian $J({\mathbf{x}})$, as $\mathbf{x}$ varies over the
data. It is natural to ask if this distinctive KA geometry emerges through
conventional neural network optimization. We find that indeed KA geometry often
is produced when training vanilla single hidden layer neural networks. We
quantify KA geometry through the statistical properties of the exterior powers
of $J(\mathbf{x})$: number of zero rows and various observables for the minor
statistics of $J(\mathbf{x})$, which measure the scale and axis alignment of
$J(\mathbf{x})$. This leads to a rough understanding for where KA geometry
occurs in the space of function complexity and model hyperparameters. The
motivation is first to understand how neural networks organically learn to
prepare input data for later downstream processing and, second, to learn enough
about the emergence of KA geometry to accelerate learning through a timely
intervention in network hyperparameters. This research is the "flip side" of
KA-Networks (KANs). We do not engineer KA into the neural network, but rather
watch KA emerge in shallow MLPs.

</details>


### [81] [Integrating Attention-Enhanced LSTM and Particle Swarm Optimization for Dynamic Pricing and Replenishment Strategies in Fresh Food Supermarkets](https://arxiv.org/abs/2509.12339)
*Xianchen Liu,Tianhui Zhang,Xinyu Zhang,Lingmin Hou,Zhen Guo,Yuanhao Tian,Yang Liu*

Main category: cs.LG

TL;DR: 结合LSTM和PSO优化生鲜超市定价补货策略，通过注意力机制增强预测，实现利润最大化和食品浪费减少


<details>
  <summary>Details</summary>
Motivation: 解决生鲜食品零售中动态定价和库存管理的挑战，平衡利润最大化和减少食品浪费的需求

Method: 使用带注意力机制的LSTM网络预测7天内的销量、价格趋势和损耗率，然后通过PSO算法优化定价和补货策略，结合成本加成定价实现动态调整

Result: 该方法不仅最大化利润，还减少食品浪费，提高决策准确性，为生鲜零售提供可扩展的解决方案

Conclusion: 该框架成功弥合了预测建模和优化之间的差距，为生鲜食品和其他易腐商品行业提供了有效的动态定价和库存管理方案

Abstract: This paper presents a novel approach to optimizing pricing and replenishment
strategies in fresh food supermarkets by combining Long Short-Term Memory
(LSTM) networks with Particle Swarm Optimization (PSO). The LSTM model,
enhanced with an attention mechanism, is used to predict sales volumes, pricing
trends, and spoilage rates over a seven-day period. The predictions generated
by the LSTM model serve as inputs for the PSO algorithm, which iteratively
optimizes pricing and replenishment strategies to maximize profitability while
adhering to inventory constraints. The integration of cost-plus pricing allows
for dynamic adjustments based on fixed and variable costs, ensuring real-time
adaptability to market fluctuations. The framework not only maximizes profits
but also reduces food waste, contributing to more sustainable supermarket
operations. The attention mechanism enhances the interpretability of the LSTM
model by identifying key time points and factors influencing sales, improving
decision-making accuracy. This methodology bridges the gap between predictive
modeling and optimization, offering a scalable solution for dynamic pricing and
inventory management in fresh food retail and other industries dealing with
perishable goods.

</details>


### [82] [FEDONet : Fourier-Embedded DeepONet for Spectrally Accurate Operator Learning](https://arxiv.org/abs/2509.12344)
*Arth Sojitra,Mrigank Dhingra,Omer San*

Main category: cs.LG

TL;DR: 提出了FEDONet，通过傅里叶嵌入主干网络来增强DeepONet的空间表示能力，在多个PDE数据集上相比传统DeepONet获得2-3倍的性能提升


<details>
  <summary>Details</summary>
Motivation: 标准DeepONet使用全连接线性层的主干网络在捕捉复杂空间结构方面存在局限，需要改进其空间表示能力

Method: 在DeepONet架构中引入傅里叶嵌入主干网络，利用随机傅里叶特征映射来丰富空间表示能力

Result: 在Poisson方程、Burgers方程、Lorenz-63混沌系统、Eikonal方程、Allen-Cahn方程、Kuramoto-Sivashinsky方程和Lorenz-96系统等PDE数据集上，FEDONet相比传统DeepONet平均相对L2性能提升2-3倍

Conclusion: 傅里叶嵌入能有效增强神经算子学习，为PDE代理建模提供了鲁棒且广泛适用的方法

Abstract: Deep Operator Networks (DeepONets) have recently emerged as powerful
data-driven frameworks for learning nonlinear operators, particularly suited
for approximating solutions to partial differential equations (PDEs). Despite
their promising capabilities, the standard implementation of DeepONets, which
typically employs fully connected linear layers in the trunk network, can
encounter limitations in capturing complex spatial structures inherent to
various PDEs. To address this, we introduce Fourier-embedded trunk networks
within the DeepONet architecture, leveraging random Fourier feature mappings to
enrich spatial representation capabilities. Our proposed Fourier-embedded
DeepONet, FEDONet demonstrates superior performance compared to the traditional
DeepONet across a comprehensive suite of PDE-driven datasets, including the
two-dimensional Poisson equation, Burgers' equation, the Lorenz-63 chaotic
system, Eikonal equation, Allen-Cahn equation, Kuramoto-Sivashinsky equation,
and the Lorenz-96 system. Empirical evaluations of FEDONet consistently show
significant improvements in solution reconstruction accuracy, with average
relative L2 performance gains ranging between 2-3x compared to the DeepONet
baseline. This study highlights the effectiveness of Fourier embeddings in
enhancing neural operator learning, offering a robust and broadly applicable
methodology for PDE surrogate modeling.

</details>


### [83] [Linear Dimensionality Reduction for Word Embeddings in Tabular Data Classification](https://arxiv.org/abs/2509.12346)
*Liam Ressel,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: 本文研究了在工程师薪资预测挑战中，使用PCA和LDA对词嵌入进行线性降维的方法，提出了Partitioned-LDA方法，在有限训练样本下显著提升了表格数据分类性能。


<details>
  <summary>Details</summary>
Motivation: 工程师薪资预测任务需要基于表格数据将薪资分类为三个类别。工作描述表示为300维词嵌入，大幅增加了维度，同时训练样本数量有限，使得分类具有挑战性。词嵌入的线性降维在表格数据分类中尚未得到充分探索。

Method: 研究主成分分析(PCA)和线性判别分析(LDA)。提出Partitioned-LDA方法，将词嵌入分成等大小的块，在每个块上分别执行LDA，从而减小协方差矩阵的大小。结合收缩正则化技术来改善性能。

Result: PCA在适当子空间维度下可以优于原始嵌入。未正则化的LDA由于协方差估计误差表现不佳，但应用收缩后性能显著提升。Partitioned-LDA优于常规LDA，结合收缩后在竞赛公开排行榜上达到前10准确率。

Conclusion: Partitioned-LDA方法有效提升了在有限训练样本下词嵌入在表格数据分类中的性能，为解决高维词嵌入与有限样本的矛盾提供了有效方案。

Abstract: The Engineers' Salary Prediction Challenge requires classifying salary
categories into three classes based on tabular data. The job description is
represented as a 300-dimensional word embedding incorporated into the tabular
features, drastically increasing dimensionality. Additionally, the limited
number of training samples makes classification challenging. Linear
dimensionality reduction of word embeddings for tabular data classification
remains underexplored. This paper studies Principal Component Analysis (PCA)
and Linear Discriminant Analysis (LDA). We show that PCA, with an appropriate
subspace dimension, can outperform raw embeddings. LDA without regularization
performs poorly due to covariance estimation errors, but applying shrinkage
improves performance significantly, even with only two dimensions. We propose
Partitioned-LDA, which splits embeddings into equal-sized blocks and performs
LDA separately on each, thereby reducing the size of the covariance matrices.
Partitioned-LDA outperforms regular LDA and, combined with shrinkage, achieves
top-10 accuracy on the competition public leaderboard. This method effectively
enhances word embedding performance in tabular data classification with limited
training samples.

</details>


### [84] [Unsupervised Atomic Data Mining via Multi-Kernel Graph Autoencoders for Machine Learning Force Fields](https://arxiv.org/abs/2509.12358)
*Hong Sun,Joshua A. Vita,Amit Samanta,Vincenzo Lordi*

Main category: cs.LG

TL;DR: 提出MEAGraph模型，一种无监督方法，用于分析原子数据集并有效去除采样偏差，通过多核变换和注意力机制实现原子环境的有效聚类和剪枝。


<details>
  <summary>Details</summary>
Motivation: 在计算化学和材料科学中，传统数据集生成技术容易在势能面上过采样，且高维原子描述符使得传统聚类和剪枝方法难以有效识别不同区域，导致信息丢失或偏差去除不彻底。

Method: 开发Multi-kernel Edge Attention-based Graph Autoencoder (MEAGraph)模型，结合多线性核变换和基于注意力的消息传递，无需标签或大量训练即可捕获几何敏感性并进行有效数据集剪枝。

Result: 在铌、钽和铁数据集上的应用表明，MEAGraph能有效分组相似原子环境，使基础剪枝技术能够成功去除采样偏差。

Conclusion: MEAGraph为表示学习和聚类提供了有效方法，可用于数据分析、异常检测和数据集优化，解决了高维原子描述符带来的聚类挑战。

Abstract: Constructing a chemically diverse dataset while avoiding sampling bias is
critical to training efficient and generalizable force fields. However, in
computational chemistry and materials science, many common dataset generation
techniques are prone to oversampling regions of the potential energy surface.
Furthermore, these regions can be difficult to identify and isolate from each
other or may not align well with human intuition, making it challenging to
systematically remove bias in the dataset. While traditional clustering and
pruning (down-sampling) approaches can be useful for this, they can often lead
to information loss or a failure to properly identify distinct regions of the
potential energy surface due to difficulties associated with the high
dimensionality of atomic descriptors. In this work, we introduce the
Multi-kernel Edge Attention-based Graph Autoencoder (MEAGraph) model, an
unsupervised approach for analyzing atomic datasets. MEAGraph combines multiple
linear kernel transformations with attention-based message passing to capture
geometric sensitivity and enable effective dataset pruning without relying on
labels or extensive training. Demonstrated applications on niobium, tantalum,
and iron datasets show that MEAGraph efficiently groups similar atomic
environments, allowing for the use of basic pruning techniques for removing
sampling bias. This approach provides an effective method for representation
learning and clustering that can be used for data analysis, outlier detection,
and dataset optimization.

</details>


### [85] [Enhancing Smart Farming Through Federated Learning: A Secure, Scalable, and Efficient Approach for AI-Driven Agriculture](https://arxiv.org/abs/2509.12363)
*Ritesh Janga,Rushit Dave*

Main category: cs.LG

TL;DR: 提出基于联邦学习的智能农业框架，用于明尼苏达州农场的作物病害检测，在保护数据隐私的同时实现高精度分类


<details>
  <summary>Details</summary>
Motivation: 农业领域对数据驱动决策的需求日益增长，但农场对数据隐私的担忧阻碍了数据共享。需要一种既能利用先进机器学习技术又能保护农场敏感数据隐私的解决方案

Method: 采用联邦学习框架，包括：从明尼苏达州农场收集数据、应用本地深度学习算法、迁移学习、中央聚合服务器进行模型精炼

Result: 预期实现：病害检测精度提升、良好的农业场景泛化能力、降低通信和训练时间成本、早期病害识别和干预

Conclusion: 该框架填补了先进机器学习技术与农民实际隐私需求之间的差距，利用联邦学习优势为智能农业系统提供安全高效的病害检测方法

Abstract: The agricultural sector is undergoing a transformation with the integration
of advanced technologies, particularly in data-driven decision-making. This
work proposes a federated learning framework for smart farming, aiming to
develop a scalable, efficient, and secure solution for crop disease detection
tailored to the environmental and operational conditions of Minnesota farms. By
maintaining sensitive farm data locally and enabling collaborative model
updates, our proposed framework seeks to achieve high accuracy in crop disease
classification without compromising data privacy. We outline a methodology
involving data collection from Minnesota farms, application of local deep
learning algorithms, transfer learning, and a central aggregation server for
model refinement, aiming to achieve improved accuracy in disease detection,
good generalization across agricultural scenarios, lower costs in communication
and training time, and earlier identification and intervention against diseases
in future implementations. We outline a methodology and anticipated outcomes,
setting the stage for empirical validation in subsequent studies. This work
comes in a context where more and more demand for data-driven interpretations
in agriculture has to be weighed with concerns about privacy from farms that
are hesitant to share their operational data. This will be important to provide
a secure and efficient disease detection method that can finally revolutionize
smart farming systems and solve local agricultural problems with data
confidentiality. In doing so, this paper bridges the gap between advanced
machine learning techniques and the practical, privacy-sensitive needs of
farmers in Minnesota and beyond, leveraging the benefits of federated learning.

</details>


### [86] [Explainable Unsupervised Multi-Anomaly Detection and Temporal Localization in Nuclear Times Series Data with a Dual Attention-Based Autoencoder](https://arxiv.org/abs/2509.12372)
*Konstantinos Vasili,Zachery T. Dahm,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: 提出基于LSTM自编码器和双重注意力机制的无监督方法，用于核反应堆辐射监测系统中的异常事件检测与定位


<details>
  <summary>Details</summary>
Motivation: 下一代核反应堆规模更小但产生大量多变量时间序列数据，需要开发远程自主控制系统，而准确诊断模块是关键第一步。现有ML/DL方法缺乏可解释性、真实数据和异常事件稀缺等问题

Method: 使用LSTM自编码器结合特征注意力和时间注意力机制，特征注意力为异常模式的辐射传感器分配权重，时间注意力突出异常发生的时间步，实现异常检测和定位

Result: 在PUR-1研究反应堆的真实数据集上进行评估，框架能够识别受影响的传感器和每个异常的持续时间

Conclusion: 该框架通过双重注意力机制提供了异常检测和定位的统一解决方案，解决了核安全关键领域ML/DL方法可解释性不足的问题

Abstract: The nuclear industry is advancing toward more new reactor designs, with
next-generation reactors expected to be smaller in scale and power output.
These systems have the potential to produce large volumes of information in the
form of multivariate time-series data, which could be used for enhanced
real-time monitoring and control. In this context, the development of remote
autonomous or semi-autonomous control systems for reactor operation has gained
significant interest. A critical first step toward such systems is an accurate
diagnostics module capable of detecting and localizing anomalies within the
reactor system. Recent studies have proposed various ML and DL approaches for
anomaly detection in the nuclear domain. Despite promising results, key
challenges remain, including limited to no explainability, lack of access to
real-world data, and scarcity of abnormal events, which impedes benchmarking
and characterization. Most existing studies treat these methods as black boxes,
while recent work highlights the need for greater interpretability of ML/DL
outputs in safety-critical domains. Here, we propose an unsupervised
methodology based on an LSTM autoencoder with a dual attention mechanism for
characterization of abnormal events in a real-world reactor radiation area
monitoring system. The framework includes not only detection but also
localization of the event and was evaluated using real-world datasets of
increasing complexity from the PUR-1 research reactor. The attention mechanisms
operate in both the feature and temporal dimensions, where the feature
attention assigns weights to radiation sensors exhibiting abnormal patterns,
while time attention highlights the specific timesteps where irregularities
occur, thus enabling localization. By combining the results, the framework can
identify both the affected sensors and the duration of each anomaly within a
single unified network.

</details>


### [87] [Diffusion-Based Generation and Imputation of Driving Scenarios from Limited Vehicle CAN Data](https://arxiv.org/abs/2509.12375)
*Julian Ripper,Ousama Esbel,Rafael Fietzek,Max Mühlhäuser,Thomas Kreutz*

Main category: cs.LG

TL;DR: 本文提出了一种混合生成方法，结合自回归和非自回归技术，使用DDPM模型生成汽车CAN总线时间序列数据，既能生成高质量合成数据，又能修复损坏样本。


<details>
  <summary>Details</summary>
Motivation: 解决在小样本且包含损坏数据的时间序列数据集上训练深度学习的挑战，特别是汽车时间序列数据的生成和修复问题。

Method: 采用去噪扩散概率模型(DDPM)，结合自回归和非自回归技术的混合生成方法，对两种DDPM架构进行改进，并提出三个评估指标来量化物理正确性和测试轨道遵循度。

Result: 最佳模型在物理正确性方面甚至优于训练数据，显示出合理的驾驶行为，并成功修复了训练数据中物理上不合理的区域，提高了数据质量。

Conclusion: DDPM模型能有效生成真实的时间序列数据并修复损坏样本，在汽车CAN数据集上表现出色，为小样本时间序列数据生成提供了有效解决方案。

Abstract: Training deep learning methods on small time series datasets that also
include corrupted samples is challenging. Diffusion models have shown to be
effective to generate realistic and synthetic data, and correct corrupted
samples through imputation. In this context, this paper focuses on generating
synthetic yet realistic samples of automotive time series data. We show that
denoising diffusion probabilistic models (DDPMs) can effectively solve this
task by applying them to a challenging vehicle CAN-dataset with long-term data
and a limited number of samples. Therefore, we propose a hybrid generative
approach that combines autoregressive and non-autoregressive techniques. We
evaluate our approach with two recently proposed DDPM architectures for time
series generation, for which we propose several improvements. To evaluate the
generated samples, we propose three metrics that quantify physical correctness
and test track adherence. Our best model is able to outperform even the
training data in terms of physical correctness, while showing plausible driving
behavior. Finally, we use our best model to successfully impute physically
implausible regions in the training data, thereby improving the data quality.

</details>


### [88] [Causal-Symbolic Meta-Learning (CSML): Inducing Causal World Models for Few-Shot Generalization](https://arxiv.org/abs/2509.12387)
*Mohamed Zayaan S*

Main category: cs.LG

TL;DR: CSML是一个新颖的因果符号元学习框架，通过感知、因果归纳和推理三个模块学习任务分布的潜在因果结构，在需要真正因果推理的任务上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型依赖虚假相关性，泛化能力差且需要大量数据。人类智能的关键在于理解因果机制，实现鲁棒且样本高效的学习

Method: 包含三个核心模块：感知模块（将原始输入映射到解耦符号表示）、可微分因果归纳模块（发现符号间的因果图）、基于图的推理模块（利用因果图进行预测）。通过元学习跨任务分布的共享因果世界模型

Result: 在CausalWorld新基准测试中，CSML显著优于最先进的元学习和神经符号基线方法，特别是在需要真正因果推理的任务上表现突出

Conclusion: CSML框架通过整合因果推理和元学习，能够从少量样本快速适应新任务，包括需要干预和反事实推理的任务，为实现人类水平的样本高效学习提供了有前景的方向

Abstract: Modern deep learning models excel at pattern recognition but remain
fundamentally limited by their reliance on spurious correlations, leading to
poor generalization and a demand for massive datasets. We argue that a key
ingredient for human-like intelligence-robust, sample-efficient learning-stems
from an understanding of causal mechanisms. In this work, we introduce
Causal-Symbolic Meta-Learning (CSML), a novel framework that learns to infer
the latent causal structure of a task distribution. CSML comprises three key
modules: a perception module that maps raw inputs to disentangled symbolic
representations; a differentiable causal induction module that discovers the
underlying causal graph governing these symbols and a graph-based reasoning
module that leverages this graph to make predictions. By meta-learning a shared
causal world model across a distribution of tasks, CSML can rapidly adapt to
novel tasks, including those requiring reasoning about interventions and
counterfactuals, from only a handful of examples. We introduce CausalWorld, a
new physics-based benchmark designed to test these capabilities. Our
experiments show that CSML dramatically outperforms state-of-the-art
meta-learning and neuro-symbolic baselines, particularly on tasks demanding
true causal inference.

</details>


### [89] [Evaluating the printability of stl files with ML](https://arxiv.org/abs/2509.12392)
*Janik Henn,Adrian Hauptmannl,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: 开发AI模型检测3D模型中的常见打印问题，帮助新手用户在打印前识别可能导致失败的几何特征


<details>
  <summary>Details</summary>
Motivation: 3D打印技术从专业领域向大众市场发展，但新手用户缺乏经验识别模型中的问题几何特征，导致打印失败。需要开发智能工具来辅助用户预先检测问题

Method: 训练人工智能模型来分析和检测3D模型中可能导致打印失败的常见几何问题特征

Result: 提出了一个新颖的AI支持层，能够在切片软件中预先识别3D模型的潜在打印问题

Conclusion: 该AI模型为3D打印新手用户提供了重要的辅助工具，通过在打印开始前检测几何问题，显著减少打印失败率，使3D打印技术对更广泛的用户群体更加友好和可靠

Abstract: 3D printing has long been a technology for industry professionals and
enthusiasts willing to tinker or even build their own machines. This stands in
stark contrast to today's market, where recent developments have prioritized
ease of use to attract a broader audience. Slicing software nowadays has a few
ways to sanity check the input file as well as the output gcode. Our approach
introduces a novel layer of support by training an AI model to detect common
issues in 3D models. The goal is to assist less experienced users by
identifying features that are likely to cause print failures due to difficult
to print geometries before printing even begins.

</details>


### [90] [Adaptive Spatial Goodness Encoding: Advancing and Scaling Forward-Forward Learning Without Backpropagation](https://arxiv.org/abs/2509.12394)
*Qingchun Gong,Robert Bogdan Staszewski,Kai Xu*

Main category: cs.LG

TL;DR: 提出了自适应空间优良性编码(ASGE)框架，作为前向-前向算法的改进版本，专门针对CNN设计，解决了通道维度爆炸问题，在多个基准测试中取得了优于其他无反向传播方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的前向-前向算法扩展虽然增强了原始算法并适应了卷积神经网络，但存在表示能力有限和在大规模数据集上扩展性差的问题，主要是由于通道维度爆炸导致的。

Method: 提出自适应空间优良性编码(ASGE)框架，利用特征图计算每层的空间感知优良性表示，实现逐层监督，并将分类复杂度与通道维度解耦。

Result: 在多个基准测试中表现优异：MNIST 99.65%、FashionMNIST 93.41%、CIFAR-10 90.62%、CIFAR-100 65.42%，并首次成功应用于ImageNet（Top-1 26.21%、Top-5 47.49%）。

Conclusion: ASGE框架完全消除了反向传播，显著缩小了与BP训练模型的性能差距，为可扩展的无反向传播CNN训练奠定了可行基础。

Abstract: The Forward-Forward (FF) algorithm offers a promising alternative to
backpropagation (BP). Despite advancements in recent FF-based extensions, which
have enhanced the original algorithm and adapted it to convolutional neural
networks (CNNs), they often suffer from limited representational capacity and
poor scalability to large-scale datasets, primarily due to exploding channel
dimensionality. In this work, we propose adaptive spatial goodness encoding
(ASGE), a new FF-based training framework tailored for CNNs. ASGE leverages
feature maps to compute spatially-aware goodness representations at each layer,
enabling layer-wise supervision. Crucially, this approach decouples
classification complexity from channel dimensionality, thereby addressing the
issue of channel explosion and achieving competitive performance compared to
other BP-free methods. ASGE outperforms all other FF-based approaches across
multiple benchmarks, delivering test accuracies of 99.65% on MNIST, 93.41% on
FashionMNIST, 90.62% on CIFAR-10, and 65.42% on CIFAR-100. Moreover, we present
the first successful application of FF-based training to ImageNet, with Top-1
and Top-5 accuracies of 26.21% and 47.49%. By entirely eliminating BP and
significantly narrowing the performance gap with BP-trained models, the ASGE
framework establishes a viable foundation toward scalable BP-free CNN training.

</details>


### [91] [Bayesian Parametric Matrix Models: Principled Uncertainty Quantification for Spectral Learning](https://arxiv.org/abs/2509.12406)
*Mohammad Nooraiepour*

Main category: cs.LG

TL;DR: 提出贝叶斯参数矩阵模型（B-PMMs），为科学机器学习中的谱方法提供不确定性量化，解决了传统确定性方法在安全关键应用中缺乏置信度估计的问题。


<details>
  <summary>Details</summary>
Motivation: 当前谱学习方法仅提供点估计而缺乏不确定性量化，限制了其在需要预测置信度的安全关键应用中的使用。参数矩阵模型虽然性能优异，但其确定性特性阻碍了在不确定性量化应用中的部署。

Method: 开发了贝叶斯参数矩阵模型框架，包括：(i) 具有正则化矩阵扰动界的自适应谱分解，(ii) 使用流形感知矩阵变量高斯后验的结构化变分推理算法，(iii) 具有谱间隙和问题条件明确依赖关系的有限样本校准保证。

Result: 在5x5到500x500矩阵维度上的实验验证显示完美收敛率，B-PMMs实现了优异的不确定性校准（ECE < 0.05），同时保持良好的扩展性，在谱病态条件下表现优雅退化。

Conclusion: 该框架支持在不确定性关键领域进行稳健的谱学习，为更广泛的贝叶斯谱机器学习奠定了基础，提供了可靠的不确定性估计，即使在近退化状态下也能工作。

Abstract: Scientific machine learning increasingly uses spectral methods to understand
physical systems. Current spectral learning approaches provide only point
estimates without uncertainty quantification, limiting their use in
safety-critical applications where prediction confidence is essential.
Parametric matrix models have emerged as powerful tools for scientific machine
learning, achieving exceptional performance by learning governing equations.
However, their deterministic nature limits deployment in uncertainty
quantification applications. We introduce Bayesian parametric matrix models
(B-PMMs), a principled framework that extends PMMs to provide uncertainty
estimates while preserving their spectral structure and computational
efficiency. B-PMM addresses the fundamental challenge of quantifying
uncertainty in matrix eigenvalue problems where standard Bayesian methods fail
due to the geometric constraints of spectral decomposition. The theoretical
contributions include: (i) adaptive spectral decomposition with regularized
matrix perturbation bounds that characterize eigenvalue uncertainty
propagation, (ii) structured variational inference algorithms using
manifold-aware matrix-variate Gaussian posteriors that respect Hermitian
constraints, and (iii) finite-sample calibration guarantees with explicit
dependence on spectral gaps and problem conditioning. Experimental validation
across matrix dimensions from 5x5 to 500x500 with perfect convergence rates
demonstrates that B-PMMs achieve exceptional uncertainty calibration (ECE <
0.05) while maintaining favorable scaling. The framework exhibits graceful
degradation under spectral ill-conditioning and provides reliable uncertainty
estimates even in near-degenerate regimes. The proposed framework supports
robust spectral learning in uncertainty-critical domains and lays the
groundwork for broader Bayesian spectral machine learning.

</details>


### [92] [Surrogate Representation Inference for Noisy Text and Image Annotations](https://arxiv.org/abs/2509.12416)
*Kentaro Nakamura*

Main category: cs.LG

TL;DR: 提出Surrogate Representation Inference (SRI)方法，通过神经网络学习低维表示来校正机器学习标注中的偏差，在中等预测精度下可将标准误差降低50%以上，并能处理人工标注中的非差分测量误差。


<details>
  <summary>Details</summary>
Motivation: 现有方法在机器学习模型和LLM标注非结构化数据时存在标准误差大、需要无误差人工标注的问题，需要开发更有效的偏差校正方法。

Method: 提出SRI方法，假设非结构化数据完全中介人工标注与结构化变量之间的关系，设计神经网络架构学习满足代理假设的低维表示，建立识别条件和半参数有效估计策略。

Result: 模拟研究和实际应用表明，SRI在机器学习预测精度中等时可将标准误差降低50%以上，即使在人工标注存在非差分测量误差时也能提供有效推断。

Conclusion: SRI方法通过低维表示学习有效解决了机器学习标注中的偏差问题，显著降低了标准误差，对处理非结构化数据标注具有重要价值。

Abstract: As researchers increasingly rely on machine learning models and LLMs to
annotate unstructured data, such as texts or images, various approaches have
been proposed to correct bias in downstream statistical analysis. However,
existing methods tend to yield large standard errors and require some
error-free human annotation. In this paper, I introduce Surrogate
Representation Inference (SRI), which assumes that unstructured data fully
mediate the relationship between human annotations and structured variables.
The assumption is guaranteed by design provided that human coders rely only on
unstructured data for annotation. Under this setting, I propose a neural
network architecture that learns a low-dimensional representation of
unstructured data such that the surrogate assumption remains to be satisfied.
When multiple human annotations are available, SRI can further correct
non-differential measurement errors that may exist in human annotations.
Focusing on text-as-outcome settings, I formally establish the identification
conditions and semiparametric efficient estimation strategies that enable
learning and leveraging such a low-dimensional representation. Simulation
studies and a real-world application demonstrate that SRI reduces standard
errors by over 50% when machine learning prediction accuracy is moderate and
provides valid inference even when human annotations contain non-differential
measurement errors.

</details>


### [93] [On the Regularity and Fairness of Combinatorial Multi-Armed Bandit](https://arxiv.org/abs/2509.12457)
*Xiaoyi Wu,Bin Li*

Main category: cs.LG

TL;DR: 提出了一种参数化的正则公平学习算法，用于组合多臂老虎机问题，同时实现累积奖励最大化、公平性保证和奖励规律性


<details>
  <summary>Details</summary>
Motivation: 受无线网络应用启发，需要同时最大化累积奖励、保证各臂的公平性（最小平均奖励要求）和确保奖励规律性（各臂获得奖励的频率）

Method: 算法线性组合虚拟队列长度（跟踪公平性违规）、时间-自上次奖励（TSLR）指标和上置信界（UCB）估计，利用Lyapunov函数进行理论分析

Result: 理论分析表明算法实现了零累积公平性违规、良好的奖励规律性和累积遗憾性能，并通过两个真实数据集验证

Conclusion: 所提算法能有效平衡探索与利用，同时满足公平性和规律性约束，在无线网络等应用中具有重要价值

Abstract: The combinatorial multi-armed bandit model is designed to maximize cumulative
rewards in the presence of uncertainty by activating a subset of arms in each
round. This paper is inspired by two critical applications in wireless
networks, where it's not only essential to maximize cumulative rewards but also
to guarantee fairness among arms (i.e., the minimum average reward required by
each arm) and ensure reward regularity (i.e., how often each arm receives the
reward). In this paper, we propose a parameterized regular and fair learning
algorithm to achieve these three objectives. In particular, the proposed
algorithm linearly combines virtual queue-lengths (tracking the fairness
violations), Time-Since-Last-Reward (TSLR) metrics, and Upper Confidence Bound
(UCB) estimates in its weight measure. Here, TSLR is similar to
age-of-information and measures the elapsed number of rounds since the last
time an arm received a reward, capturing the reward regularity performance, and
UCB estimates are utilized to balance the tradeoff between exploration and
exploitation in online learning. By exploring a key relationship between
virtual queue-lengths and TSLR metrics and utilizing several non-trivial
Lyapunov functions, we analytically characterize zero cumulative fairness
violation, reward regularity, and cumulative regret performance under our
proposed algorithm. These theoretical outcomes are verified by simulations
based on two real-world datasets.

</details>


### [94] [Nonlocal Neural Tangent Kernels via Parameter-Space Interactions](https://arxiv.org/abs/2509.12467)
*Sriram Nagaraj,Vishakh Hari*

Main category: cs.LG

TL;DR: 本文提出了非局部神经正切核（NNTK），通过用参数空间中的非局部交互近似替代局部梯度，将NTK理论扩展到非光滑函数、随机估计器和更广泛的模型类别。


<details>
  <summary>Details</summary>
Motivation: 传统的神经正切核（NTK）框架依赖于网络对参数可微的假设，这一假设在处理非光滑目标函数或表现出不可微行为的参数化模型时会失效。

Method: 提出非局部神经正切核（NNTK），用非局部梯度替代标准梯度，探索了固定核和基于注意力的非局部算子公式，并通过数值研究进行验证。

Result: NNTK允许将NTK理论扩展到更广泛的函数类别，包括非光滑函数和随机估计器，为更广泛的模型家族提供了理论分析框架。

Conclusion: 非局部神经正切核框架成功扩展了传统NTK理论的应用范围，使其能够处理非光滑性和更复杂的模型行为，为深度学习的理论分析提供了新的工具。

Abstract: The Neural Tangent Kernel (NTK) framework has provided deep insights into the
training dynamics of neural networks under gradient flow. However, it relies on
the assumption that the network is differentiable with respect to its
parameters, an assumption that breaks down when considering non-smooth target
functions or parameterized models exhibiting non-differentiable behavior. In
this work, we propose a Nonlocal Neural Tangent Kernel (NNTK) that replaces the
local gradient with a nonlocal interaction-based approximation in parameter
space. Nonlocal gradients are known to exist for a wider class of functions
than the standard gradient. This allows NTK theory to be extended to nonsmooth
functions, stochastic estimators, and broader families of models. We explore
both fixed-kernel and attention-based formulations of this nonlocal operator.
We illustrate the new formulation with numerical studies.

</details>


### [95] [Comparative Analysis of Wave Scattering Numerical Modeling Using the Boundary Element Method and Physics-Informed Neural Networks](https://arxiv.org/abs/2509.12483)
*Oscar Rincón-Cardeno,Gregorio Pérez Bernal,Silvana Montoya Noguera,Nicolás Guarín-Zapata*

Main category: cs.LG

TL;DR: 本研究比较了边界元法(BEM)和物理信息神经网络(PINNs)在二维Helmholtz方程波散射问题中的性能，发现在相同精度下PINNs训练时间比BEM长42倍，但评估速度快204倍，且泛化能力较差。


<details>
  <summary>Details</summary>
Motivation: 比较BEM和PINNs两种方法在相同条件下求解Helmholtz方程波散射问题的性能，为波传播问题的研究方法选择提供定量数据支持。

Method: 使用BEM进行边界离散化求解，使用PINNs通过最小化控制方程和边界条件的残差进行训练，并通过超参数优化确定最佳配置。评估指标包括求解精度、计算时间和泛化能力。

Result: 在相同精度下，PINNs训练时间比BEM长42倍，但评估速度快204倍。在训练域外泛化时，PINNs相对误差从7.46×10⁻²增加到8.22，而BEM在扩展区域保持相似误差水平。

Conclusion: PINNs在评估速度上有优势但训练耗时且泛化能力有限，BEM在计算效率和泛化性方面表现更稳定。该研究为波传播问题的计算方法选择提供了重要参考。

Abstract: Purpose - This study compares the Boundary Element Method (BEM) and
Physics-Informed Neural Networks (PINNs) for solving the two-dimensional
Helmholtz equation in wave scattering problems. The objective is to evaluate
the performance of both methods under the same conditions.
  Design/methodology/approach - We solve the Helmholtz equation using BEM and
PINNs for the same scattering problem. The PINNs are trained by minimizing the
residual of the governing equations and boundary conditions, with their
configuration determined through hyperparameter optimization, while the BEM is
applied using boundary discretization. Both methods are evaluated in terms of
solution accuracy, computation time, and generalization capacity.
  Findings - Numerical experiments were conducted by varying the number of
integration points for BEM and the number of layers and neurons per layer for
PINNs. Hyperparameter tuning provided further insight into suitable
configurations for wave scattering problems. At comparable accuracy, PINNs
produced consistent solutions but required training times approximately 42
times longer than BEM. However, once trained, PINNs achieved evaluation times
up to 204 times faster. The generalization capacity was also assessed outside
the PINN training domain, where the relative error increased from $7.46 \times
10^{-2}$ to 8.22, while BEM maintained a similar error level in the extended
region.
  Originality/value - This work presents a direct comparison between PINNs and
BEM for the Helmholtz equation. The analysis provides quantitative data on the
performance of both methods, supporting their selection in future research on
wave propagation problems and establishing future challenges and directions.

</details>


### [96] [Prediction and Causality of functional MRI and synthetic signal using a Zero-Shot Time-Series Foundation Model](https://arxiv.org/abs/2509.12497)
*Alessandro Crimi,Andrea Brovelli*

Main category: cs.LG

TL;DR: 基础模型在fMRI脑活动预测和因果关系发现中表现竞争力，特别是零样本预测能力显著，并提供更精确的因果交互检测


<details>
  <summary>Details</summary>
Motivation: 评估基础模型与传统方法在脑科学时间序列预测和因果关系发现中的性能对比，探索基础模型在零样本设置下的应用潜力

Method: 使用基础模型进行fMRI时间序列预测（零样本和微调设置），并通过类似Granger因果性的估计与标准Granger因果性进行比较，使用逻辑映射耦合和Ornstein-Uhlenbeck过程生成的合成时间序列进行验证

Result: 基础模型在零样本fMRI预测中获得竞争力结果（平均绝对百分比误差控制组0.55，患者0.27），虽然标准Granger因果性未显示清晰数量差异，但基础模型提供了更精确的因果交互检测

Conclusion: 基础模型具有多用性、强大的零样本性能和在时间序列数据预测和因果关系发现中的潜在应用价值

Abstract: Time-series forecasting and causal discovery are central in neuroscience, as
predicting brain activity and identifying causal relationships between neural
populations and circuits can shed light on the mechanisms underlying cognition
and disease. With the rise of foundation models, an open question is how they
compare to traditional methods for brain signal forecasting and causality
analysis, and whether they can be applied in a zero-shot setting. In this work,
we evaluate a foundation model against classical methods for inferring
directional interactions from spontaneous brain activity measured with
functional magnetic resonance imaging (fMRI) in humans. Traditional approaches
often rely on Wiener-Granger causality. We tested the forecasting ability of
the foundation model in both zero-shot and fine-tuned settings, and assessed
causality by comparing Granger-like estimates from the model with standard
Granger causality. We validated the approach using synthetic time series
generated from ground-truth causal models, including logistic map coupling and
Ornstein-Uhlenbeck processes. The foundation model achieved competitive
zero-shot forecasting fMRI time series (mean absolute percentage error of 0.55
in controls and 0.27 in patients). Although standard Granger causality did not
show clear quantitative differences between models, the foundation model
provided a more precise detection of causal interactions.
  Overall, these findings suggest that foundation models offer versatility,
strong zero-shot performance, and potential utility for forecasting and causal
discovery in time-series data.

</details>


### [97] [Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time](https://arxiv.org/abs/2509.12521)
*Yifan Lan,Yuanpu Cao,Weitong Zhang,Lu Lin,Jinghui Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为Preference Hijacking (Phi)的新方法，通过精心优化的图像来操纵多模态大语言模型(MLLM)的输出偏好，揭示了MLLMs的安全风险。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在各领域获得广泛应用，但其安全性问题日益突出。研究发现MLLMs的输出偏好可以被恶意图像操纵，产生上下文相关但有偏见的响应，这种攻击难以检测。

Method: 提出了Preference Hijacking (Phi)方法，在推理时通过偏好劫持图像来操纵MLLM响应偏好，无需修改模型。还引入了通用劫持扰动，可嵌入不同图像中劫持MLLM响应至攻击者指定的偏好。

Result: 在各种任务上的实验结果表明该方法具有有效性，能够成功操纵MLLMs的输出偏好。

Conclusion: 该研究揭示了MLLMs存在新的安全风险，提出的Phi方法能够有效操纵模型输出偏好，为MLLM安全性研究提供了重要洞见。

Abstract: Recently, Multimodal Large Language Models (MLLMs) have gained significant
attention across various domains. However, their widespread adoption has also
raised serious safety concerns. In this paper, we uncover a new safety risk of
MLLMs: the output preference of MLLMs can be arbitrarily manipulated by
carefully optimized images. Such attacks often generate contextually relevant
yet biased responses that are neither overtly harmful nor unethical, making
them difficult to detect. Specifically, we introduce a novel method, Preference
Hijacking (Phi), for manipulating the MLLM response preferences using a
preference hijacked image. Our method works at inference time and requires no
model modifications. Additionally, we introduce a universal hijacking
perturbation -- a transferable component that can be embedded into different
images to hijack MLLM responses toward any attacker-specified preferences.
Experimental results across various tasks demonstrate the effectiveness of our
approach. The code for Phi is accessible at https://github.com/Yifan-Lan/Phi.

</details>


### [98] [Selective Risk Certification for LLM Outputs via Information-Lift Statistics: PAC-Bayes, Robustness, and Skeleton Design](https://arxiv.org/abs/2509.12527)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.LG

TL;DR: 本文提出了信息提升证书的理论框架，通过PAC-Bayes子伽马分析和骨架敏感性定理，为大语言模型的选择性分类提供形式化保证，在多个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常产生看似合理但错误的输出，现有启发式方法缺乏形式化保证，需要开发具有理论保证的选择性分类方法。

Method: 开发了信息提升证书的综合理论，包括PAC-Bayes子伽马分析、骨架敏感性定理、假设违反下的故障模式保证，以及骨架构建的变分方法。

Result: 在六个数据集和多个模型家族上验证了假设，在相同风险下减少12-15%的弃权率，运行时开销保持在20%以下（通过批处理进一步降低）。

Conclusion: 该理论框架为大语言模型的选择性分类提供了首个全面的形式化保证，显著提升了性能并保持了计算效率。

Abstract: Large language models often produce plausible but incorrect outputs. Existing
heuristics such as HallBayes lack formal guarantees. We develop the first
comprehensive theory of \emph{information-lift certificates} under selective
classification. Our contributions are: (i) a PAC-Bayes \emph{sub-gamma}
analysis extending beyond standard Bernstein bounds; (ii) explicit skeleton
sensitivity theorems quantifying robustness to misspecification; (iii)
failure-mode guarantees under assumption violations; and (iv) a principled
variational method for skeleton construction. Across six datasets and multiple
model families, we validate assumptions empirically, reduce abstention by
12--15\% at the same risk, and maintain runtime overhead below 20\% (further
reduced via batching).

</details>


### [99] [Graph Homophily Booster: Rethinking the Role of Discrete Features on Heterophilic Graphs](https://arxiv.org/abs/2509.12530)
*Ruizhong Qiu,Ting-Wei Li,Gaotang Li,Hanghang Tong*

Main category: cs.LG

TL;DR: GRAPHITE是一个新颖的图神经网络框架，通过直接增加图同质性来解决异质图学习问题，而不是传统的架构设计方法。


<details>
  <summary>Details</summary>
Motivation: 现有的GNN在异质图上表现不佳，甚至不如简单的MLP，需要从根本上解决图异质性问题。

Method: 通过精心设计的图变换直接提高图同质性，创建特征节点来促进具有相似特征节点之间的同质消息传递。

Result: 在具有挑战性的数据集上显著优于现有最先进方法，在异质图上表现优异，在同质图上也能达到可比精度。

Conclusion: GRAPHITE提供了一种新的范式，通过直接转换图结构来解决图异质性问题，是第一个明确通过图变换提高同质性的方法。

Abstract: Graph neural networks (GNNs) have emerged as a powerful tool for modeling
graph-structured data. However, existing GNNs often struggle with heterophilic
graphs, where connected nodes tend to have dissimilar features or labels. While
numerous methods have been proposed to address this challenge, they primarily
focus on architectural designs without directly targeting the root cause of the
heterophily problem. These approaches still perform even worse than the
simplest MLPs on challenging heterophilic datasets. For instance, our
experiments show that 21 latest GNNs still fall behind the MLP on the Actor
dataset. This critical challenge calls for an innovative approach to addressing
graph heterophily beyond architectural designs. To bridge this gap, we propose
and study a new and unexplored paradigm: directly increasing the graph
homophily via a carefully designed graph transformation. In this work, we
present a simple yet effective framework called GRAPHITE to address graph
heterophily. To the best of our knowledge, this work is the first method that
explicitly transforms the graph to directly improve the graph homophily.
Stemmed from the exact definition of homophily, our proposed GRAPHITE creates
feature nodes to facilitate homophilic message passing between nodes that share
similar features. Furthermore, we both theoretically and empirically show that
our proposed GRAPHITE significantly increases the homophily of originally
heterophilic graphs, with only a slight increase in the graph size. Extensive
experiments on challenging datasets demonstrate that our proposed GRAPHITE
significantly outperforms state-of-the-art methods on heterophilic graphs while
achieving comparable accuracy with state-of-the-art methods on homophilic
graphs.

</details>


### [100] [Cross-Modal Deep Metric Learning for Time Series Anomaly Detection](https://arxiv.org/abs/2509.12540)
*Wei Li,Zheze Yang*

Main category: cs.LG

TL;DR: 提出基于跨模态深度度量学习的时间序列异常检测方法，通过特征聚类模型和vMF分布提高检测灵敏度和效率


<details>
  <summary>Details</summary>
Motivation: 解决时间序列异常检测中灵敏度低和耗时高的问题

Method: 构建跨模态深度度量学习特征聚类模型，包含输入层、三元组选择层和损失函数计算层，使用平方欧氏距离计算聚类中心距离，采用随机梯度下降优化模型，利用vMF分布描述时间序列数据方向特征

Result: 实验结果表明该方法能准确分类不同属性的时间序列数据，对异常具有高灵敏度，检测精度高、速度快、鲁棒性强

Conclusion: 该方法有效提升了时间序列异常检测的性能，在灵敏度、准确度和效率方面都有显著改善

Abstract: To effectively address the issues of low sensitivity and high time
consumption in time series anomaly detection, we propose an anomaly detection
method based on cross-modal deep metric learning. A cross-modal deep metric
learning feature clustering model is constructed, composed of an input layer, a
triplet selection layer, and a loss function computation layer. The squared
Euclidean distances between cluster centers are calculated, and a stochastic
gradient descent strategy is employed to optimize the model and classify
different time series features. The inner product of principal component
direction vectors is used as a metric for anomaly measurement. The von
Mises-Fisher (vMF) distribution is applied to describe the directional
characteristics of time series data, and historical data is used to train and
obtain evaluation parameters. By comparing the principal component direction
vector of actual time series data with the threshold, anomaly detection is
performed. Experimental results demonstrate that the proposed method accurately
classifies time series data with different attributes, exhibits high
sensitivity to anomalies, and achieves high detection accuracy, fast detection
speed, and strong robustness.

</details>


### [101] [iCD: A Implicit Clustering Distillation Mathod for Structural Information Mining](https://arxiv.org/abs/2509.12553)
*Xiang Xue,Yatu Ji,Qing-dao-er-ji Ren,Bao Shi,Min Lu,Nier Wu,Xufei Zhuang,Haiteng Xu,Gan-qi-qi-ge Cha*

Main category: cs.LG

TL;DR: 提出了iCD方法，通过Gram矩阵挖掘logits中的可解释结构知识，无需真实标签或特征对齐，在细粒度分类任务上取得显著提升


<details>
  <summary>Details</summary>
Motivation: Logit知识蒸馏虽然简单且无需中间特征对齐，但其决策过程可解释性有限，需要一种能够挖掘和传递可解释结构知识的方法

Method: iCD利用解耦的局部logit表示的Gram矩阵，使学生模型能够学习潜在的语义结构模式，无需真实标签或特征空间对齐

Result: 在基准数据集上的广泛实验显示iCD在不同师生架构中均有效，特别是在细粒度分类任务上相比基线最高提升5.08%

Conclusion: iCD是一种简单有效的知识蒸馏方法，能够从logits中挖掘可解释的结构知识，显著提升模型性能，特别是在细粒度分类任务中

Abstract: Logit Knowledge Distillation has gained substantial research interest in
recent years due to its simplicity and lack of requirement for intermediate
feature alignment; however, it suffers from limited interpretability in its
decision-making process. To address this, we propose implicit Clustering
Distillation (iCD): a simple and effective method that mines and transfers
interpretable structural knowledge from logits, without requiring ground-truth
labels or feature-space alignment. iCD leverages Gram matrices over decoupled
local logit representations to enable student models to learn latent semantic
structural patterns. Extensive experiments on benchmark datasets demonstrate
the effectiveness of iCD across diverse teacher-student architectures, with
particularly strong performance in fine-grained classification tasks --
achieving a peak improvement of +5.08% over the baseline. The code is available
at: https://github.com/maomaochongaa/iCD.

</details>


### [102] [No Need for "Learning" to Defer? A Training Free Deferral Framework to Multiple Experts through Conformal Prediction](https://arxiv.org/abs/2509.12573)
*Tim Bary,Benoît Macq,Louis Petit*

Main category: cs.LG

TL;DR: 提出了一种基于共形预测的训练免费、模型无关的专家延迟框架，通过预测集识别标签不确定性并选择最具区分度的专家，在CIFAR10-H和ImageNet16-H上达到99%+准确率，减少专家工作量达11倍。


<details>
  <summary>Details</summary>
Motivation: 现有学习延迟(L2D)方法对专家组成变化敏感且需要大量重新训练，需要一种无需训练、模型无关的专家延迟解决方案。

Method: 使用共形预测器生成的预测集来识别标签特定不确定性，通过可分离性标准选择最能区分剩余可能标签的专家。

Result: 在CIFAR10-H和ImageNet16-H上分别达到99.57±0.10%和99.40±0.52%准确率，专家工作量减少高达11倍，在专家性能下降时保持稳健。

Conclusion: 该方法为现实世界人机协作提供了可扩展、无需重新训练的L2D替代方案，在低信息设置下性能逐渐下降但仍保持良好表现。

Abstract: AI systems often fail to deliver reliable predictions across all inputs,
prompting the need for hybrid human-AI decision-making. Existing Learning to
Defer (L2D) approaches address this by training deferral models, but these are
sensitive to changes in expert composition and require significant retraining
if experts change. We propose a training-free, model- and expert-agnostic
framework for expert deferral based on conformal prediction. Our method uses
the prediction set generated by a conformal predictor to identify
label-specific uncertainty and selects the most discriminative expert using a
segregativity criterion, measuring how well an expert distinguishes between the
remaining plausible labels. Experiments on CIFAR10-H and ImageNet16-H show that
our method consistently outperforms both the standalone model and the strongest
expert, with accuracies attaining $99.57\pm0.10\%$ and $99.40\pm0.52\%$, while
reducing expert workload by up to a factor of $11$. The method remains robust
under degraded expert performance and shows a gradual performance drop in
low-information settings. These results suggest a scalable, retraining-free
alternative to L2D for real-world human-AI collaboration.

</details>


### [103] [Exploring Training Data Attribution under Limited Access Constraints](https://arxiv.org/abs/2509.12581)
*Shiyuan Zhang,Junwei Deng,Juhan Bae,Jiaqi Ma*

Main category: cs.LG

TL;DR: 本研究系统分析了在模型访问受限和计算资源有限条件下进行训练数据归因(TDA)的可行性，提出了使用代理模型等解决方案，并证明即使模型未在目标数据集上训练，其归因分数仍然具有信息价值。


<details>
  <summary>Details</summary>
Motivation: 现有的梯度基TDA方法(如影响函数)虽然性能优越，但在实际应用中受到完全模型访问需求和高计算成本的限制，特别是在商业模型不公开且计算资源有限的情况下，这阻碍了TDA技术的广泛应用。

Method: 通过系统研究不同访问和资源约束下的TDA方法，探索使用适当设计的解决方案(如代理模型)在不同访问约束级别下执行TDA的可行性，并验证从未在目标数据集上训练的模型获得的归因分数的有效性。

Result: 研究表明，即使在模型访问受限和计算资源有限的情况下，通过代理模型等方法仍能有效执行TDA，且从未训练目标数据集的模型获得的归因分数在各种任务中仍具有信息价值。

Conclusion: 该研究为在现实环境中部署TDA提供了实用指导，提高了在有限访问条件下的可行性和效率，有助于推动TDA技术在实际应用中的更广泛采用。

Abstract: Training data attribution (TDA) plays a critical role in understanding the
influence of individual training data points on model predictions.
Gradient-based TDA methods, popularized by \textit{influence function} for
their superior performance, have been widely applied in data selection, data
cleaning, data economics, and fact tracing. However, in real-world scenarios
where commercial models are not publicly accessible and computational resources
are limited, existing TDA methods are often constrained by their reliance on
full model access and high computational costs. This poses significant
challenges to the broader adoption of TDA in practical applications.
  In this work, we present a systematic study of TDA methods under various
access and resource constraints. We investigate the feasibility of performing
TDA under varying levels of access constraints by leveraging appropriately
designed solutions such as proxy models. Besides, we demonstrate that
attribution scores obtained from models without prior training on the target
dataset remain informative across a range of tasks, which is useful for
scenarios where computational resources are limited. Our findings provide
practical guidance for deploying TDA in real-world environments, aiming to
improve feasibility and efficiency under limited access.

</details>


### [104] [A Multimodal Foundation Model to Enhance Generalizability and Data Efficiency for Pan-cancer Prognosis Prediction](https://arxiv.org/abs/2509.12600)
*Huajun Zhou,Fengtao Zhou,Jiabo Ma,Yingxue Xu,Xi Wang,Xiuming Zhang,Li Liang,Zhenhui Li,Hao Chen*

Main category: cs.LG

TL;DR: MICE是一个多模态基础模型，通过协作专家机制整合病理图像、临床报告和基因组数据，在泛癌预后预测中表现出卓越的泛化能力和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型难以充分利用多模态数据的丰富信息，提取的表示泛化性差，需要开发能够有效整合异质信息的多模态模型来改善肿瘤微环境的整体理解。

Method: 采用功能多样化的专家模块替代传统多专家模块，全面捕捉跨癌种和癌种特异性信息；结合对比学习和监督学习，使用11,799名患者的数据（涵盖30种癌症类型）增强模型泛化性。

Result: MICE在内部队列中C-index提升3.8%-11.2%，在独立队列中提升5.8%-8.8%，显著优于单模态和最先进的多专家多模态模型，并展现出卓越的数据效率。

Conclusion: MICE为泛癌预后预测建立了有效且可扩展的基础框架，具有个性化定制治疗和改善治疗结果的强大潜力。

Abstract: Multimodal data provides heterogeneous information for a holistic
understanding of the tumor microenvironment. However, existing AI models often
struggle to harness the rich information within multimodal data and extract
poorly generalizable representations. Here we present MICE (Multimodal data
Integration via Collaborative Experts), a multimodal foundation model that
effectively integrates pathology images, clinical reports, and genomics data
for precise pan-cancer prognosis prediction. Instead of conventional
multi-expert modules, MICE employs multiple functionally diverse experts to
comprehensively capture both cross-cancer and cancer-specific insights.
Leveraging data from 11,799 patients across 30 cancer types, we enhanced MICE's
generalizability by coupling contrastive and supervised learning. MICE
outperformed both unimodal and state-of-the-art multi-expert-based multimodal
models, demonstrating substantial improvements in C-index ranging from 3.8% to
11.2% on internal cohorts and 5.8% to 8.8% on independent cohorts,
respectively. Moreover, it exhibited remarkable data efficiency across diverse
clinical scenarios. With its enhanced generalizability and data efficiency,
MICE establishes an effective and scalable foundation for pan-cancer prognosis
prediction, holding strong potential to personalize tailored therapies and
improve treatment outcomes.

</details>


### [105] [High-Energy Concentration for Federated Learning in Frequency Domain](https://arxiv.org/abs/2509.12630)
*Haozhi Shi,Weiying Xie,Hangyu Ye,Daixun Li,Jitao Ma,Leyuan Fang*

Main category: cs.LG

TL;DR: 提出FedFD方法，通过频域高能量集中特性过滤冗余高频信息，在降低联邦学习通信成本的同时提升性能


<details>
  <summary>Details</summary>
Motivation: 现有基于数据集蒸馏的联邦学习方法在空间域设计中存在冗余信息和噪声问题，增加了通信负担，需要更高效的频域处理方法

Method: 基于离散余弦变换的高能量集中特性，设计二进制掩码保留低频分量，通过频域分布对齐和真实数据驱动的合成分类损失来优化低频分量质量

Result: 在5个图像和语音数据集上优于现有方法，在CIFAR-10数据集上通信成本降低37.78%，性能提升10.88%

Conclusion: FedFD通过频域处理有效解决了联邦学习中的通信效率和性能优化问题，证明了频域高能量集中特性在减少冗余信息和噪声方面的有效性

Abstract: Federated Learning (FL) presents significant potential for collaborative
optimization without data sharing. Since synthetic data is sent to the server,
leveraging the popular concept of dataset distillation, this FL framework
protects real data privacy while alleviating data heterogeneity. However, such
methods are still challenged by the redundant information and noise in entire
spatial-domain designs, which inevitably increases the communication burden. In
this paper, we propose a novel Frequency-Domain aware FL method with
high-energy concentration (FedFD) to address this problem. Our FedFD is
inspired by the discovery that the discrete cosine transform predominantly
distributes energy to specific regions, referred to as high-energy
concentration. The principle behind FedFD is that low-energy like
high-frequency components usually contain redundant information and noise, thus
filtering them helps reduce communication costs and optimize performance. Our
FedFD is mathematically formulated to preserve the low-frequency components
using a binary mask, facilitating an optimal solution through frequency-domain
distribution alignment. In particular, real data-driven synthetic
classification is imposed into the loss to enhance the quality of the
low-frequency components. On five image and speech datasets, FedFD achieves
superior performance than state-of-the-art methods while reducing communication
costs. For example, on the CIFAR-10 dataset with Dirichlet coefficient $\alpha
= 0.01$, FedFD achieves a minimum reduction of 37.78\% in the communication
cost, while attaining a 10.88\% performance gain.

</details>


### [106] [Leveraging Intermediate Representations of Time Series Foundation Models for Anomaly Detection](https://arxiv.org/abs/2509.12650)
*Chan Sik Han,Keon Myung Lee*

Main category: cs.LG

TL;DR: TimeRep是一种新颖的时间序列异常检测方法，利用时间序列基础模型的中间层表示而非最终层表示来计算异常分数，通过距离度量实现检测，并包含核心集策略和概念漂移适应机制。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖时间序列基础模型的最终层表示，通过重构或预测误差计算异常分数，但忽略了中间层可能包含的更丰富信息。

Method: 选择预训练TSFM中最具信息量的中间层和补丁标记位置，构建训练数据的中间表示参考集，应用核心集策略缩减规模，推理时通过距离计算异常分数，并集成适应机制处理概念漂移。

Result: 在UCR Anomaly Archive的250个单变量时间序列数据集上，TimeRep持续优于包括非深度学习、深度学习和基础模型方法在内的多种最先进基线方法。

Conclusion: TimeRep通过利用时间序列基础模型的中间层表示，结合核心集策略和适应机制，在异常检测任务中表现出卓越性能，为时间序列异常检测提供了新的有效方法。

Abstract: Detecting anomalies in time series data is essential for the reliable
operation of many real-world systems. Recently, time series foundation models
(TSFMs) have emerged as a powerful tool for anomaly detection. However,
existing methods typically rely on the final layer's representations of TSFMs,
computing the anomaly score as a reconstruction or forecasting error via a
task-specific head. Instead, we propose TimeRep, a novel anomaly detection
approach that leverages the intermediate layer's representations of TSFMs,
computing the anomaly score as the distance between these representations.
Given a pre-trained TSFM, TimeRep selects the intermediate layer and
patch-token position that yield the most informative representation. TimeRep
forms a reference collection of intermediate representations from the training
data and applies a core-set strategy to reduce its size while maintaining
distributional coverage. During inference, TimeRep computes the anomaly score
for incoming data by measuring the distance between its intermediate
representations and those of the collection. To address concept drift, TimeRep
integrates an adaptation mechanism that, at inference time, augments the
collection exclusively with non-redundant intermediate representations from
incoming data. We conducted extensive experiments on the UCR Anomaly Archive,
which contains 250 univariate time series. TimeRep consistently outperforms a
broad spectrum of state-of-the-art baselines, including non-DL, DL, and
foundation model-based methods.

</details>


### [107] [Instance-level Randomization: Toward More Stable LLM Evaluations](https://arxiv.org/abs/2509.12678)
*Yiyang Li,Yonghuang Wu,Ying Luo,Liangtai Sun,Zishu Qin,Lin Qiu,Xuezhi Cao,Xunliang Cai*

Main category: cs.LG

TL;DR: 提出实例级随机化(ILR)方法来解决大语言模型评估中的不稳定性问题，通过为每个实例随机化所有影响因素并多次实验取平均，减少方差并提高模型比较的公平性


<details>
  <summary>Details</summary>
Motivation: 大语言模型评估存在不稳定性，随机因素的微小变化会导致评分剧烈波动和模型排名变化，固定随机因素设置可能导致不公平的模型比较

Method: 提出实例级随机化(ILR)方法，为每个实例随机化所有影响评估分数的因素，进行多次实验并报告平均分数

Result: 理论分析和实证结果表明ILR能减少随机因素引起的方差和不公平比较，且以不到一半的计算成本达到与先前方法相似的鲁棒性水平

Conclusion: ILR方法能有效缓解大语言模型评估的波动性，提高评估的稳定性和公平性，同时降低计算成本

Abstract: Evaluations of large language models (LLMs) suffer from instability, where
small changes of random factors such as few-shot examples can lead to drastic
fluctuations of scores and even model rankings. Moreover, different LLMs can
have different preferences for a certain setting of random factors. As a
result, using a fixed setting of random factors, which is often adopted as the
paradigm of current evaluations, can lead to potential unfair comparisons
between LLMs. To mitigate the volatility of evaluations, we first theoretically
analyze the sources of variance induced by changes in random factors. Targeting
these specific sources, we then propose the instance-level randomization (ILR)
method to reduce variance and enhance fairness in model comparisons. Instead of
using a fixed setting across the whole benchmark in a single experiment, we
randomize all factors that affect evaluation scores for every single instance,
run multiple experiments and report the averaged score. Theoretical analyses
and empirical results demonstrate that ILR can reduce the variance and unfair
comparisons caused by random factors, as well as achieve similar robustness
level with less than half computational cost compared with previous methods.

</details>


### [108] [Large Language Model Scaling Laws for Neural Quantum States in Quantum Chemistry](https://arxiv.org/abs/2509.12679)
*Oliver Knitter,Dan Zhao,Stefan Leichenauer,Shravan Veerapaneni*

Main category: cs.LG

TL;DR: 本文研究了基于transformer的神经量子态在量子化学应用中的缩放规律，发现模型大小与训练时间的关系高度依赖于损失度量和波函数形式，与语言模型的线性关系不同。


<details>
  <summary>Details</summary>
Motivation: 随着神经量子态越来越多地采用基于大语言模型的组件，需要理解NQS的缩放规律，以揭示其可扩展性和最优性能-资源权衡。

Method: 识别基于transformer的NQS在二次量子化量子化学应用中的缩放规律，通过计算约束优化获得的参数曲线进行分析。

Result: 发现模型大小与训练时间的关系高度依赖于损失度量和波函数形式，与语言模型的近似线性关系不同。

Conclusion: 神经量子态的缩放规律与语言模型存在显著差异，这为优化量子计算资源分配提供了重要见解。

Abstract: Scaling laws have been used to describe how large language model (LLM)
performance scales with model size, training data size, or amount of
computational resources. Motivated by the fact that neural quantum states (NQS)
has increasingly adopted LLM-based components, we seek to understand NQS
scaling laws, thereby shedding light on the scalability and optimal
performance--resource trade-offs of NQS ansatze. In particular, we identify
scaling laws that predict the performance, as measured by absolute error and
V-score, for transformer-based NQS as a function of problem size in
second-quantized quantum chemistry applications. By performing analogous
compute-constrained optimization of the obtained parametric curves, we find
that the relationship between model size and training time is highly dependent
on loss metric and ansatz, and does not follow the approximately linear
relationship found for language models.

</details>


### [109] [ZTree: A Subgroup Identification Based Decision Tree Learning Framework](https://arxiv.org/abs/2509.12688)
*Eric Cheng,Jie Cheng*

Main category: cs.LG

TL;DR: ZTree是一种基于假设检验的新型决策树学习框架，用统计原理的子组识别替代传统CART的纯度分裂方法，通过交叉验证控制多重检验，仅需一个参数即可调节树复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统CART决策树基于纯度分裂缺乏统计理论基础，ZTree旨在提供统计上更严谨的决策树学习方法，提高模型的可解释性和灵活性。

Method: 在每个节点使用假设检验（z检验、t检验、Mann-Whitney U、log-rank等）评估候选子组与补集的差异显著性，采用交叉验证方法处理多重检验问题，通过z阈值控制树复杂度。

Result: 在五个大规模UCI数据集上的实验表明，ZTree在低数据量情况下表现优异，相比CART能生成更简单的树而不牺牲性能。

Conclusion: ZTree为决策树学习提供了统计基础更坚实的替代方案，通过假设检验和交叉验证方法实现了高效灵活的框架，参数调节直观简单。

Abstract: Decision trees are a commonly used class of machine learning models valued
for their interpretability and versatility, capable of both classification and
regression. We propose ZTree, a novel decision tree learning framework that
replaces CART's traditional purity based splitting with statistically
principled subgroup identification. At each node, ZTree applies hypothesis
testing (e.g., z-tests, t-tests, Mann-Whitney U, log-rank) to assess whether a
candidate subgroup differs meaningfully from the complement. To adjust for the
complication of multiple testing, we employ a cross-validation-based approach
to determine if further node splitting is needed. This robust stopping
criterion eliminates the need for post-pruning and makes the test threshold
(z-threshold) the only parameter for controlling tree complexity. Because of
the simplicity of the tree growing procedure, once a detailed tree is learned
using the most lenient z-threshold, all simpler trees can be derived by simply
removing nodes that do not meet the larger z-thresholds. This makes parameter
tuning intuitive and efficient. Furthermore, this z-threshold is essentially a
p-value, allowing users to easily plug in appropriate statistical tests into
our framework without adjusting the range of parameter search. Empirical
evaluation on five large-scale UCI datasets demonstrates that ZTree
consistently delivers strong performance, especially at low data regimes.
Compared to CART, ZTree also tends to grow simpler trees without sacrificing
performance. ZTree introduces a statistically grounded alternative to
traditional decision tree splitting by leveraging hypothesis testing and a
cross-validation approach to multiple testing correction, resulting in an
efficient and flexible framework.

</details>


### [110] [Soft Graph Transformer for MIMO Detection](https://arxiv.org/abs/2509.12694)
*Jiadong Hong,Lei Liu,Xinyu Bian,Wenjie Wang,Zhaoyang Zhang*

Main category: cs.LG

TL;DR: 提出了Soft Graph Transformer (SGT)用于MIMO检测，结合图注意力机制和消息传递，支持软输入软输出，接近最大似然性能。


<details>
  <summary>Details</summary>
Motivation: 传统最大似然检测计算复杂度过高，消息传递算法依赖大系统渐近假设，现有Transformer检测器无法利用MIMO因子图结构和解码器软信息。

Method: 将消息传递直接集成到图感知注意力机制中，通过软输入嵌入支持解码器信息更新，实现有效的软输出生成。

Result: 作为独立检测器，SGT接近最大似然性能，超越现有基于Transformer的方法。

Conclusion: SGT成功克服了现有方法的局限性，在保持计算效率的同时实现了优异的检测性能。

Abstract: We propose the Soft Graph Transformer (SGT), a Soft-Input-Soft-Output neural
architecture tailored for MIMO detection. While Maximum Likelihood (ML)
detection achieves optimal accuracy, its prohibitive exponential complexity
renders it impractical for real-world systems. Conventional message passing
algorithms offer tractable alternatives but rely on large-system asymptotics
and random matrix assumptions, both of which break down under practical
implementations. Prior Transformer-based detectors, on the other hand, fail to
incorporate the MIMO factor graph structure and cannot utilize decoder-side
soft information, limiting their standalone performance and their applicability
in iterative detection-decoding (IDD). To overcome these limitations, SGT
integrates message passing directly into a graph-aware attention mechanism and
supports decoder-informed updates through soft-input embeddings. This design
enables effective soft-output generation while preserving computational
efficiency. As a standalone detector, SGT closely approaches ML performance and
surpasses prior Transformer-based approaches.

</details>


### [111] [Bi-level Personalization for Federated Foundation Models: A Task-vector Aggregation Approach](https://arxiv.org/abs/2509.12697)
*Yiyuan Yang,Guodong Long,Qinghua Lu,Liming Zhu,Jing Jiang*

Main category: cs.LG

TL;DR: 提出了一种双层个性化联邦学习框架，用于在基础模型上进行联邦微调，通过客户端个性化微调和基于相似用户的服务器端个性化聚合来解决小规模用户群体的个性化与联邦学习之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 联邦基础模型需要在小规模新用户或专业场景下进行微调，这些场景数据有限，个性化与联邦学习之间的权衡变得敏感，需要新的方法来平衡个性化需求和联邦协作。

Method: 采用双层个性化框架：客户端使用私有数据进行个性化微调，服务器端使用基于客户端特定任务向量测量的相似用户进行个性化聚合，从而获得群体个性化信息并减少非IID数据中无关或利益冲突客户的干扰。

Result: 在基准数据集上的广泛实验分析证明了所提出算法的有效性。

Conclusion: 该双层个性化框架能够有效解决联邦基础模型在小规模用户群体微调时的个性化与联邦协作平衡问题，通过客户端和服务器端的双重个性化处理提升了模型性能。

Abstract: Federated foundation models represent a new paradigm to jointly fine-tune
pre-trained foundation models across clients. It is still a challenge to
fine-tune foundation models for a small group of new users or specialized
scenarios, which typically involve limited data compared to the large-scale
data used in pre-training. In this context, the trade-off between
personalization and federation becomes more sensitive. To tackle these, we
proposed a bi-level personalization framework for federated fine-tuning on
foundation models. Specifically, we conduct personalized fine-tuning on the
client-level using its private data, and then conduct a personalized
aggregation on the server-level using similar users measured by client-specific
task vectors. Given the personalization information gained from client-level
fine-tuning, the server-level personalized aggregation can gain group-wise
personalization information while mitigating the disturbance of irrelevant or
interest-conflict clients with non-IID data. The effectiveness of the proposed
algorithm has been demonstrated by extensive experimental analysis in benchmark
datasets.

</details>


### [112] [NORA: A Nephrology-Oriented Representation Learning Approach Towards Chronic Kidney Disease Classification](https://arxiv.org/abs/2509.12704)
*Mohammad Abdul Hafeez Khan,Twisha Bhattacharyya,Omar Khan,Noorah Khan,Alina Aziz Fatima Khan,Mohammed Qutub Khan,Sujoy Ghosh Hajra*

Main category: cs.LG

TL;DR: 提出NORA方法，结合监督对比学习和随机森林分类器，利用常规非肾脏临床变量进行慢性肾病分类，在早期检测方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 慢性肾病早期检测困难，特别是在门诊环境中缺乏实验室肾脏生物标志物，需要探索常规收集的非肾脏临床变量的预测潜力。

Method: NORA方法：监督对比学习+非线性随机森林分类器，首先从表格化电子健康记录数据中提取判别性患者表示，然后用于下游CKD分类。

Result: NORA提高了类别可分性和整体分类性能，特别增强了早期CKD的F1分数，在Riverside Nephrology Physicians数据集和UCI CKD数据集上都表现出良好的泛化能力。

Conclusion: NORA方法能够有效利用常规非肾脏临床变量进行慢性肾病风险分层，为不同患者群体的CKD早期检测提供了有效解决方案。

Abstract: Chronic Kidney Disease (CKD) affects millions of people worldwide, yet its
early detection remains challenging, especially in outpatient settings where
laboratory-based renal biomarkers are often unavailable. In this work, we
investigate the predictive potential of routinely collected non-renal clinical
variables for CKD classification, including sociodemographic factors, comorbid
conditions, and urinalysis findings. We introduce the Nephrology-Oriented
Representation leArning (NORA) approach, which combines supervised contrastive
learning with a nonlinear Random Forest classifier. NORA first derives
discriminative patient representations from tabular EHR data, which are then
used for downstream CKD classification. We evaluated NORA on a clinic-based EHR
dataset from Riverside Nephrology Physicians. Our results demonstrated that
NORA improves class separability and overall classification performance,
particularly enhancing the F1-score for early-stage CKD. Additionally, we
assessed the generalizability of NORA on the UCI CKD dataset, demonstrating its
effectiveness for CKD risk stratification across distinct patient cohorts.

</details>


### [113] [Spatio-temporal DeepKriging in PyTorch: A Supplementary Application to Precipitation Data for Interpolation and Probabilistic Forecasting](https://arxiv.org/abs/2509.12708)
*Pratik Nag*

Main category: cs.LG

TL;DR: 提出了一个基于PyTorch的时空深度克里金（STDK）框架，用于欧洲降水数据的插值和多步预测，能够处理时空不规则性并生成高分辨率结果。


<details>
  <summary>Details</summary>
Motivation: 需要开发一个能够有效处理欧洲降水数据时空不规则性的插值和预测框架，为气候数据集提供可复现的解决方案。

Method: 使用PyTorch平台实现时空深度克里金框架，开发了独立的插值和预测代码模块，能够处理时空不规则性并生成高分辨率结果。

Result: 通过对每日降水测量的广泛评估，证明了该方法在预测性能和鲁棒性方面的有效性。

Conclusion: 提出的STDK框架成功实现了对欧洲降水数据的高质量插值和多步预测，提供了可复现的代码实现，便于在类似气候数据集上的广泛应用。

Abstract: A detailed analysis of precipitation data over Europe is presented, with a
focus on interpolation and forecasting applications. A Spatio-temporal
DeepKriging (STDK) framework has been implemented using the PyTorch platform to
achieve these objectives. The proposed model is capable of handling
spatio-temporal irregularities while generating high-resolution interpolations
and multi-step forecasts. Reproducible code modules have been developed as
standalone PyTorch implementations for the
interpolation\footnote[2]{Interpolation -
https://github.com/pratiknag/Spatio-temporalDeepKriging-Pytorch.git} and
forecasting\footnote[3]{Forecasting -
https://github.com/pratiknag/pytorch-convlstm.git}, facilitating broader
application to similar climate datasets. The effectiveness of this approach is
demonstrated through extensive evaluation on daily precipitation measurements,
highlighting predictive performance and robustness.

</details>


### [114] [Unbiased Online Curvature Approximation for Regularized Graph Continual Learning](https://arxiv.org/abs/2509.12727)
*Jie Yin,Ke Sun,Han Wu*

Main category: cs.LG

TL;DR: 本文提出了一个基于Fisher信息矩阵的图持续学习正则化框架，并开发了一种新的在线曲率近似方法，显著优于现有正则化方法。


<details>
  <summary>Details</summary>
Motivation: 现有的图持续学习方法（如EWC及其变体）使用对角近似的经验Fisher信息矩阵，存在局限性。需要更好的方法来防止灾难性遗忘，同时保持学习新知识的能力。

Method: 建立基于Fisher信息矩阵曲率参数空间的一般正则化框架，提出基于模型当前学习状态的无偏在线曲率近似方法，直接在线估计正则化项而不需要显式计算和存储Fisher信息矩阵。

Result: 在三个图数据集上的大量实验表明，该方法显著优于现有的基于正则化的方法，在稳定性（保留旧知识）和可塑性（获取新知识）之间实现了更好的平衡。

Conclusion: 提出的在线曲率近似方法能够更好地捕捉学习新任务时的损失景观，同时保留从先前任务中学到的知识，为图持续学习提供了有效的正则化解决方案。

Abstract: Graph continual learning (GCL) aims to learn from a continuous sequence of
graph-based tasks. Regularization methods are vital for preventing catastrophic
forgetting in GCL, particularly in the challenging replay-free,
class-incremental setting, where each task consists of a set of unique classes.
In this work, we first establish a general regularization framework for GCL
based on the curved parameter space induced by the Fisher information matrix
(FIM). We show that the dominant Elastic Weight Consolidation (EWC) and its
variants are a special case within this framework, using a diagonal
approximation of the empirical FIM based on parameters from previous tasks. To
overcome their limitations, we propose a new unbiased online curvature
approximation of the full FIM based on the model's current learning state. Our
method directly estimates the regularization term in an online manner without
explicitly evaluating and storing the FIM itself. This enables the model to
better capture the loss landscape during learning new tasks while retaining the
knowledge learned from previous tasks. Extensive experiments on three graph
datasets demonstrate that our method significantly outperforms existing
regularization-based methods, achieving a superior trade-off between stability
(retaining old knowledge) and plasticity (acquiring new knowledge).

</details>


### [115] [A Graph Machine Learning Approach for Detecting Topological Patterns in Transactional Graphs](https://arxiv.org/abs/2509.12730)
*Francesco Zola,Jon Ander Medina,Andrea Venturi,Amaia Gil,Raul Orduna*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于图机器学习和网络分析的方法，用于检测金融交易图中的犯罪模式，解决传统规则系统无法检测复杂协同犯罪行为的问题。


<details>
  <summary>Details</summary>
Motivation: 数字化生态系统中金融犯罪手段日益复杂化，传统规则系统缺乏适应性，需要分析交易者互动来发现可疑活动和操作手段。

Method: 首先提出四步预处理框架：提取图结构、考虑时序性、检测社区、自动标签生成。然后使用图自动编码器（GAE）的三种变体区分常见拓扑模式。

Result: 初步结果显示，这种以模式为中心、拓扑驱动的方法能够有效检测复杂的金融犯罪套路。

Conclusion: 该方法为检测复杂金融犯罪提供了有前景的替代方案，比传统规则系统更加适应现代金融犯罪的复杂性。

Abstract: The rise of digital ecosystems has exposed the financial sector to evolving
abuse and criminal tactics that share operational knowledge and techniques both
within and across different environments (fiat-based, crypto-assets, etc.).
Traditional rule-based systems lack the adaptability needed to detect
sophisticated or coordinated criminal behaviors (patterns), highlighting the
need for strategies that analyze actors' interactions to uncover suspicious
activities and extract their modus operandi. For this reason, in this work, we
propose an approach that integrates graph machine learning and network analysis
to improve the detection of well-known topological patterns within
transactional graphs. However, a key challenge lies in the limitations of
traditional financial datasets, which often provide sparse, unlabeled
information that is difficult to use for graph-based pattern analysis.
Therefore, we firstly propose a four-step preprocessing framework that involves
(i) extracting graph structures, (ii) considering data temporality to manage
large node sets, (iii) detecting communities within, and (iv) applying
automatic labeling strategies to generate weak ground-truth labels. Then, once
the data is processed, Graph Autoencoders are implemented to distinguish among
the well-known topological patterns. Specifically, three different GAE variants
are implemented and compared in this analysis. Preliminary results show that
this pattern-focused, topology-driven method is effective for detecting complex
financial crime schemes, offering a promising alternative to conventional
rule-based detection systems.

</details>


### [116] [A Novel Recurrent Neural Network Framework for Prediction and Treatment of Oncogenic Mutation Progression](https://arxiv.org/abs/2509.12732)
*Rishab Parthasarathy,Achintya Bhowmik*

Main category: cs.LG

TL;DR: 提出基于AI的端到端癌症通路分析框架，结合时间序列机器学习预测癌症严重程度和突变进展，推荐治疗方案，准确率超60%


<details>
  <summary>Details</summary>
Motivation: 癌症是第二大死因，传统通路分析依赖耗时的手工湿实验数据，需要更高效、经济的分析方法

Method: 从TCGA数据库提取突变序列，通过预处理算法筛选关键突变，使用RNN预测癌症严重程度，结合多药物靶点数据库预测未来突变并推荐治疗

Result: ROC曲线准确率超过60%，预处理成功识别出每个癌症阶段约数百个关键驱动突变，生成基因频率热图突出关键突变

Conclusion: 首次提出不依赖昂贵湿实验的高效端到端框架，可预测癌症进展并提供治疗方案，成本效益显著

Abstract: Despite significant medical advancements, cancer remains the second leading
cause of death, with over 600,000 deaths per year in the US. One emerging
field, pathway analysis, is promising but still relies on manually derived wet
lab data, which is time-consuming to acquire. This work proposes an efficient,
effective end-to-end framework for Artificial Intelligence (AI) based pathway
analysis that predicts both cancer severity and mutation progression, thus
recommending possible treatments. The proposed technique involves a novel
combination of time-series machine learning models and pathway analysis. First,
mutation sequences were isolated from The Cancer Genome Atlas (TCGA) Database.
Then, a novel preprocessing algorithm was used to filter key mutations by
mutation frequency. This data was fed into a Recurrent Neural Network (RNN)
that predicted cancer severity. Then, the model probabilistically used the RNN
predictions, information from the preprocessing algorithm, and multiple
drug-target databases to predict future mutations and recommend possible
treatments. This framework achieved robust results and Receiver Operating
Characteristic (ROC) curves (a key statistical metric) with accuracies greater
than 60%, similar to existing cancer diagnostics. In addition, preprocessing
played an instrumental role in isolating important mutations, demonstrating
that each cancer stage studied may contain on the order of a few-hundred key
driver mutations, consistent with current research. Heatmaps based on predicted
gene frequency were also generated, highlighting key mutations in each cancer.
Overall, this work is the first to propose an efficient, cost-effective
end-to-end framework for projecting cancer progression and providing possible
treatments without relying on expensive, time-consuming wet lab work.

</details>


### [117] [Similarity-Distance-Magnitude Activations](https://arxiv.org/abs/2509.12760)
*Allen Schmaltz*

Main category: cs.LG

TL;DR: 提出SDM激活函数，在标准softmax基础上增加相似性和距离感知，提高神经网络的鲁棒性和可解释性，特别适用于选择性分类任务。


<details>
  <summary>Details</summary>
Motivation: 标准softmax激活函数在面对协变量偏移和分布外输入时缺乏鲁棒性，且可解释性有限。需要一种能同时考虑输出幅度、相似性和距离感知的激活函数来提高模型性能。

Method: 在softmax基础上引入相似性感知（正确预测的深度匹配）和距离感知（与训练分布的距离），形成相似性-距离-幅度（SDM）激活函数。通过密集匹配实现基于范例的可解释性。

Result: SDM激活函数比softmax对协变量偏移和分布外输入更具鲁棒性，能够对类别经验CDF进行分区以保护选择性分类中的类别召回率，在选择性分类任务中表现更优。

Conclusion: SDM激活函数通过整合相似性、距离和幅度感知，提供了比标准softmax更好的鲁棒性和可解释性，是选择性分类任务的优选方案，即使与后校准方法相比也具有优势。

Abstract: We introduce a more robust and interpretable formulation of the standard
softmax activation function commonly used with neural networks by adding
Similarity (i.e., correctly predicted depth-matches into training) awareness
and Distance-to-training-distribution awareness to the existing output
Magnitude (i.e., decision-boundary) awareness. When used as the final-layer
activation with language models, the resulting Similarity-Distance-Magnitude
(SDM) activation function is more robust than the softmax function to
co-variate shifts and out-of-distribution inputs in high-probability regions,
and provides interpretability-by-exemplar via dense matching. Complementing the
prediction-conditional estimates, the SDM activation enables a partitioning of
the class-wise empirical CDFs to guard against low class-wise recall among
selective classifications. These properties make it preferable for selective
classification, even when considering post-hoc calibration methods over the
softmax.

</details>


### [118] [EmbeddedML: A New Optimized and Fast Machine Learning Library](https://arxiv.org/abs/2509.12774)
*Halil Hüseyin Çalışkan,Talha Koruk*

Main category: cs.LG

TL;DR: EmbeddedML是一个经过数学重写和优化的机器学习库，相比scikit-learn在训练速度上有显著提升，在保持准确率的同时，线性回归速度提升约倍，SVM在小数据集上训练时间减少2倍，大数据集上减少800倍，逻辑回归减少4倍。


<details>
  <summary>Details</summary>
Motivation: 解决传统机器学习库（如scikit-learn）在处理大型数据集时训练时间过长、效率低下的问题，通过数学优化提升训练速度。

Method: 对多种机器学习算法进行数学重写和优化，包括多重线性回归、逻辑回归和支持向量机（SVM），采用训练时间优化的数学增强方法。

Result: 在回归模型中速度相比scikit-learn显著提升且无准确率损失；分类模型中SVM小数据集训练时间减少2倍，大数据集减少800倍；逻辑回归训练时间减少4倍。

Conclusion: EmbeddedML库提供了经过数学重写和优化的回归、分类、聚类和降维算法，能够显著减少训练时间，是高效的机器学习解决方案。

Abstract: Machine learning models and libraries can train datasets of different sizes
and perform prediction and classification operations, but machine learning
models and libraries cause slow and long training times on large datasets. This
article introduces EmbeddedML, a training-time-optimized and mathematically
enhanced machine learning library. The speed was increased by approximately
times compared to scikit-learn without any loss in terms of accuracy in
regression models such as Multiple Linear Regression. Logistic Regression and
Support Vector Machines (SVM) algorithms have been mathematically rewritten to
reduce training time and increase accuracy in classification models. With the
applied mathematical improvements, training time has been reduced by
approximately 2 times for SVM on small datasets and by around 800 times on
large datasets, and by approximately 4 times for Logistic Regression, compared
to the scikit-learn implementation. In summary, the EmbeddedML library offers
regression, classification, clustering, and dimensionality reduction algorithms
that are mathematically rewritten and optimized to reduce training time.

</details>


### [119] [Energy-Efficient Quantized Federated Learning for Resource-constrained IoT devices](https://arxiv.org/abs/2509.12814)
*Wilfrid Sougrinoma Compaoré,Yaya Etiabi,El Mehdi Amhoud,Mohamad Assaad*

Main category: cs.LG

TL;DR: 提出了一种面向物联网的联邦学习框架，整合有限块长传输、模型量化和错误感知聚合机制，显著提升能效和通信可靠性


<details>
  <summary>Details</summary>
Motivation: 物联网设备资源受限，面临能量有限、通信不可靠以及无限块长传输不切实际的挑战，需要设计高效的联邦学习方案

Method: 集成有限块长传输、模型量化和错误感知聚合机制，优化上行传输功率以平衡能耗和模型性能

Result: 相比标准联邦学习模型，能耗降低高达75%，同时保持稳健的模型精度

Conclusion: 该框架为实际物联网部署中高效可靠的联邦学习实现提供了可行解决方案

Abstract: Federated Learning (FL) has emerged as a promising paradigm for enabling
collaborative machine learning while preserving data privacy, making it
particularly suitable for Internet of Things (IoT) environments. However,
resource-constrained IoT devices face significant challenges due to limited
energy,unreliable communication channels, and the impracticality of assuming
infinite blocklength transmission. This paper proposes a federated learning
framework for IoT networks that integrates finite blocklength transmission,
model quantization, and an error-aware aggregation mechanism to enhance energy
efficiency and communication reliability. The framework also optimizes uplink
transmission power to balance energy savings and model performance. Simulation
results demonstrate that the proposed approach significantly reduces energy
consumption by up to 75\% compared to a standard FL model, while maintaining
robust model accuracy, making it a viable solution for FL in real-world IoT
scenarios with constrained resources. This work paves the way for efficient and
reliable FL implementations in practical IoT deployments. Index Terms:
Federated learning, IoT, finite blocklength, quantization, energy efficiency.

</details>


### [120] [Safe Reinforcement Learning using Action Projection: Safeguard the Policy or the Environment?](https://arxiv.org/abs/2509.12833)
*Hannah Markgraf,Shamburaj Sawant,Hanna Krasowski,Lukas Schäfer,Sebastien Gros,Matthias Althoff*

Main category: cs.LG

TL;DR: 本文对两种基于投影的安全强化学习方法（SE-RL和SP-RL）进行了理论比较，重点分析了动作别名效应对策略梯度的影响，并提出了改进策略。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，基于投影的安全过滤器被广泛用于强化学习，但对其两种主要集成策略（SE-RL和SP-RL）的理论差异缺乏深入理解。

Method: 建立了统一的actor-critic算法形式化框架，理论分析了两种方法的策略梯度估计，重点研究了动作别名效应的影响，并比较了多种缓解策略。

Result: 实证结果表明动作别名效应对SP-RL的影响更大，但通过适当的改进策略，SP-RL可以在多种环境中达到或超越改进后的SE-RL性能。

Conclusion: 研究结果为根据任务特性选择和优化基于投影的安全RL方法提供了实用指导，SP-RL在适当改进后具有竞争优势。

Abstract: Projection-based safety filters, which modify unsafe actions by mapping them
to the closest safe alternative, are widely used to enforce safety constraints
in reinforcement learning (RL). Two integration strategies are commonly
considered: Safe environment RL (SE-RL), where the safeguard is treated as part
of the environment, and safe policy RL (SP-RL), where it is embedded within the
policy through differentiable optimization layers. Despite their practical
relevance in safety-critical settings, a formal understanding of their
differences is lacking. In this work, we present a theoretical comparison of
SE-RL and SP-RL. We identify a key distinction in how each approach is affected
by action aliasing, a phenomenon in which multiple unsafe actions are projected
to the same safe action, causing information loss in the policy gradients. In
SE-RL, this effect is implicitly approximated by the critic, while in SP-RL, it
manifests directly as rank-deficient Jacobians during backpropagation through
the safeguard. Our contributions are threefold: (i) a unified formalization of
SE-RL and SP-RL in the context of actor-critic algorithms, (ii) a theoretical
analysis of their respective policy gradient estimates, highlighting the role
of action aliasing, and (iii) a comparative study of mitigation strategies,
including a novel penalty-based improvement for SP-RL that aligns with
established SE-RL practices. Empirical results support our theoretical
predictions, showing that action aliasing is more detrimental for SP-RL than
for SE-RL. However, with appropriate improvement strategies, SP-RL can match or
outperform improved SE-RL across a range of environments. These findings
provide actionable insights for choosing and refining projection-based safe RL
methods based on task characteristics.

</details>


### [121] [Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use](https://arxiv.org/abs/2509.12867)
*Yabo Zhang,Yihan Zeng,Qingyun Li,Zhen Hu,Kavin Han,Wangmeng Zuo*

Main category: cs.LG

TL;DR: Tool-R1是一个强化学习框架，通过生成可执行Python代码使大语言模型能够进行通用、组合和多步骤的工具使用，在GAIA基准测试中比强基线提升约10%的准确率


<details>
  <summary>Details</summary>
Motivation: 大语言模型在语言理解和推理方面表现出色，但在处理需要最新知识、精确操作或专业工具使用的现实任务时仍有限制

Method: 提出基于强化学习的框架，支持用户自定义工具和标准库集成，通过跨步骤变量共享构建连贯工作流，使用基于结果的奖励函数（结合LLM答案判断和代码执行成功）指导策略优化，并维护动态样本队列缓存高质量轨迹以提高训练效率

Result: 在GAIA基准测试中，Tool-R1显著提高了准确性和鲁棒性，比强基线提升约10%，在复杂的多步骤任务上提升更大

Conclusion: Tool-R1具有在现实应用中实现可靠高效工具增强推理的潜力

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
language understanding and reasoning, yet they remain limited when tackling
real-world tasks that require up-to-date knowledge, precise operations, or
specialized tool use. To address this, we propose Tool-R1, a reinforcement
learning framework that enables LLMs to perform general, compositional, and
multi-step tool use by generating executable Python code. Tool-R1 supports
integration of user-defined tools and standard libraries, with variable sharing
across steps to construct coherent workflows. An outcome-based reward function,
combining LLM-based answer judgment and code execution success, guides policy
optimization. To improve training efficiency, we maintain a dynamic sample
queue to cache and reuse high-quality trajectories, reducing the overhead of
costly online sampling. Experiments on the GAIA benchmark show that Tool-R1
substantially improves both accuracy and robustness, achieving about 10\% gain
over strong baselines, with larger improvements on complex multi-step tasks.
These results highlight the potential of Tool-R1 for enabling reliable and
efficient tool-augmented reasoning in real-world applications. Our code will be
available at https://github.com/YBYBZhang/Tool-R1.

</details>


### [122] [Reversible Deep Equilibrium Models](https://arxiv.org/abs/2509.12917)
*Sam McCallum,Kamran Arora,James Foster*

Main category: cs.LG

TL;DR: Reversible Deep Equilibrium Models (RevDEQs) 通过引入可逆性解决了传统DEQs梯度计算近似的问题，实现了精确梯度计算，在语言建模和图像分类任务上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 传统Deep Equilibrium Models (DEQs) 虽然通过隐式固定点定义实现了优于显式模型的性能，但其梯度计算是近似的，导致训练不稳定，需要正则化或大量函数评估来修正

Method: 引入Reversible Deep Equilibrium Models (RevDEQs)，通过可逆性设计使得梯度计算变得精确，无需正则化，且大幅减少了函数评估次数

Result: RevDEQs在语言建模和图像分类任务上实现了最先进的性能，超越了可比较的隐式和显式模型

Conclusion: 可逆深度平衡模型通过精确梯度计算解决了传统DEQs的训练稳定性问题，在保持高性能的同时显著降低了计算成本

Abstract: Deep Equilibrium Models (DEQs) are an interesting class of implicit model
where the model output is implicitly defined as the fixed point of a learned
function. These models have been shown to outperform explicit (fixed-depth)
models in large-scale tasks by trading many deep layers for a single layer that
is iterated many times. However, gradient calculation through DEQs is
approximate. This often leads to unstable training dynamics and requires
regularisation or many function evaluations to fix. Here, we introduce
Reversible Deep Equilibrium Models (RevDEQs) that allow for exact gradient
calculation, no regularisation and far fewer function evaluations than DEQs. We
show that RevDEQs achieve state-of-the-art performance on language modelling
and image classification tasks against comparable implicit and explicit models.

</details>


### [123] [Soft Gradient Boosting with Learnable Feature Transforms for Sequential Regression](https://arxiv.org/abs/2509.12920)
*Huseyin Karaca,Suleyman Serdar Kozat*

Main category: cs.LG

TL;DR: 提出了一种软梯度提升框架，在提升过程中嵌入可学习的线性特征变换，特别适用于高维数据稀缺场景，能同时优化特征选择和提升过程。


<details>
  <summary>Details</summary>
Motivation: 解决高维数据稀缺场景下传统梯度提升方法在特征选择和表示学习方面的不足，通过端到端优化提升模型性能并避免过拟合。

Method: 在每个提升迭代中训练软决策树并同时学习线性输入特征变换Q，可扩展到可微分非线性变换，实现特征选择/变换与提升的联合优化。

Result: 在合成和真实数据集上验证了方法的有效性，能够高效提升性能，避免过拟合，代码已公开共享。

Conclusion: 该软梯度提升框架通过嵌入可学习特征变换，在高维数据稀缺场景下显著提升了模型性能，为相关研究提供了可复现的基础。

Abstract: We propose a soft gradient boosting framework for sequential regression that
embeds a learnable linear feature transform within the boosting procedure. At
each boosting iteration, we train a soft decision tree and learn a linear input
feature transform Q together. This approach is particularly advantageous in
high-dimensional, data-scarce scenarios, as it discovers the most relevant
input representations while boosting. We demonstrate, using both synthetic and
real-world datasets, that our method effectively and efficiently increases the
performance by an end-to-end optimization of feature selection/transform and
boosting while avoiding overfitting. We also extend our algorithm to
differentiable non-linear transforms if overfitting is not a problem. To
support reproducibility and future work, we share our code publicly.

</details>


### [124] [Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety](https://arxiv.org/abs/2509.12936)
*Denis Janiak,Julia Moska,Dawid Motyka,Karolina Seweryn,Paweł Walkowiak,Bartosz Żuk,Arkadiusz Janz*

Main category: cs.LG

TL;DR: 本文提出了一个统一的评估框架，比较了PPO、DPO、ORPO、KTO等LLM对齐方法在事实性、安全性、简洁性、主动性和多样性五个维度上的表现，发现不同方法在不同维度上各有优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单一技术或特定维度，缺乏对LLM对齐方法内在权衡的整体评估，需要建立一个全面的评估框架来比较不同对齐方法的综合表现。

Method: 使用统一的评估框架，在分布内和分布外数据集上比较PPO、DPO、ORPO、KTO等对齐方法，采用经过人工研究验证的LLM-as-Judge提示方法进行评估。

Result: DPO和KTO在事实准确性方面表现最佳，PPO和DPO在安全性方面领先，PPO在简洁性和主动性之间取得了最佳平衡。

Conclusion: 研究结果揭示了常见对齐方法的内在权衡，为开发更平衡和可靠的大型语言模型提供了指导。

Abstract: Large language models (LLMs) require careful alignment to balance competing
objectives - factuality, safety, conciseness, proactivity, and diversity.
Existing studies focus on individual techniques or specific dimensions, lacking
a holistic assessment of the inherent trade-offs. We propose a unified
evaluation framework that compares LLM alignment methods (PPO, DPO, ORPO, KTO)
across these five axes, using both in-distribution and out-of-distribution
datasets. Leveraging a specialized LLM-as-Judge prompt, validated through human
studies, we reveal that DPO and KTO excel in factual accuracy, PPO and DPO lead
in safety, and PPO best balances conciseness with proactivity. Our findings
provide insights into trade-offs of common alignment methods, guiding the
development of more balanced and reliable LLMs.

</details>


### [125] [Sy-FAR: Symmetry-based Fair Adversarial Robustness](https://arxiv.org/abs/2509.12939)
*Haneen Najjar,Eyal Ronen,Mahmood Sharif*

Main category: cs.LG

TL;DR: 本文提出Sy-FAR方法，通过追求对称性而非完美公平性来提升对抗性鲁棒性，在面部识别等安全关键任务中实现更公平的对抗攻击防御。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗性鲁棒性方法通常导致不公平的防御效果，某些类别更容易受到攻击。在现实世界中实现完美公平性往往不可行，因此需要寻找更可行的对称性解决方案。

Method: 开发Sy-FAR技术，通过鼓励对称性（即从类别i到j的攻击成功率与从j到i的攻击成功率相同）来同时优化对抗性鲁棒性和公平性。

Result: 在五个数据集和三种模型架构上的广泛评估显示，Sy-FAR相比最先进方法显著提高了公平对抗鲁棒性，运行速度更快且结果更一致。

Conclusion: 对称性是实现公平对抗鲁棒性的更可行方法，Sy-FAR技术不仅能改善传统的不公平问题，还能减少对抗样本被错误分类到特定目标类别的脆弱性。

Abstract: Security-critical machine-learning (ML) systems, such as face-recognition
systems, are susceptible to adversarial examples, including real-world
physically realizable attacks. Various means to boost ML's adversarial
robustness have been proposed; however, they typically induce unfair
robustness: It is often easier to attack from certain classes or groups than
from others. Several techniques have been developed to improve adversarial
robustness while seeking perfect fairness between classes. Yet, prior work has
focused on settings where security and fairness are less critical. Our insight
is that achieving perfect parity in realistic fairness-critical tasks, such as
face recognition, is often infeasible -- some classes may be highly similar,
leading to more misclassifications between them. Instead, we suggest that
seeking symmetry -- i.e., attacks from class $i$ to $j$ would be as successful
as from $j$ to $i$ -- is more tractable. Intuitively, symmetry is a desirable
because class resemblance is a symmetric relation in most domains.
Additionally, as we prove theoretically, symmetry between individuals induces
symmetry between any set of sub-groups, in contrast to other fairness notions
where group-fairness is often elusive. We develop Sy-FAR, a technique to
encourage symmetry while also optimizing adversarial robustness and extensively
evaluate it using five datasets, with three model architectures, including
against targeted and untargeted realistic attacks. The results show Sy-FAR
significantly improves fair adversarial robustness compared to state-of-the-art
methods. Moreover, we find that Sy-FAR is faster and more consistent across
runs. Notably, Sy-FAR also ameliorates another type of unfairness we discover
in this work -- target classes that adversarial examples are likely to be
classified into become significantly less vulnerable after inducing symmetry.

</details>


### [126] [Spatiotemporal graph neural process for reconstruction, extrapolation, and classification of cardiac trajectories](https://arxiv.org/abs/2509.12953)
*Jaume Banus,Augustin C. Ogier,Roger Hullin,Philippe Meyer,Ruud B. van Heeswijk,Jonas Richiardi*

Main category: cs.LG

TL;DR: 提出了一个概率框架，用于从稀疏观测中建模结构化时空动力学，特别关注心脏运动。该方法整合了神经ODE、图神经网络和神经过程，能够捕捉不确定性、时间连续性和解剖结构。


<details>
  <summary>Details</summary>
Motivation: 需要从稀疏的医学观测数据中准确建模心脏等生物医学系统的结构化时空动力学，同时处理不确定性和实现插值外推。

Method: 使用时空多重图表示动态系统，通过GNN参数化的向量场建模潜在轨迹。基于节点和边级别的稀疏上下文观测，推断潜在初始状态和控制变量的分布。

Result: 在三个合成动力系统和两个真实心脏影像数据集上验证，能够准确重建轨迹并从单个观测周期外推未来心脏周期。在ACDC分类任务上达到99%准确率，在UK Biobank房颤检测上达到67%准确率。

Conclusion: 该方法为分析心脏运动提供了灵活方法，为基于图学习的结构化生物医学时空时间序列数据分析奠定了基础。

Abstract: We present a probabilistic framework for modeling structured spatiotemporal
dynamics from sparse observations, focusing on cardiac motion. Our approach
integrates neural ordinary differential equations (NODEs), graph neural
networks (GNNs), and neural processes into a unified model that captures
uncertainty, temporal continuity, and anatomical structure. We represent
dynamic systems as spatiotemporal multiplex graphs and model their latent
trajectories using a GNN-parameterized vector field. Given the sparse context
observations at node and edge levels, the model infers a distribution over
latent initial states and control variables, enabling both interpolation and
extrapolation of trajectories. We validate the method on three synthetic
dynamical systems (coupled pendulum, Lorenz attractor, and Kuramoto
oscillators) and two real-world cardiac imaging datasets - ACDC (N=150) and UK
Biobank (N=526) - demonstrating accurate reconstruction, extrapolation, and
disease classification capabilities. The model accurately reconstructs
trajectories and extrapolates future cardiac cycles from a single observed
cycle. It achieves state-of-the-art results on the ACDC classification task (up
to 99% accuracy), and detects atrial fibrillation in UK Biobank subjects with
competitive performance (up to 67% accuracy). This work introduces a flexible
approach for analyzing cardiac motion and offers a foundation for graph-based
learning in structured biomedical spatiotemporal time-series data.

</details>


### [127] [Bridging Performance Gaps for Foundation Models: A Post-Training Strategy for ECGFounder](https://arxiv.org/abs/2509.12991)
*Ya Zhou,Yujie Yang,Xiaohan Fan,Wei Zhao*

Main category: cs.LG

TL;DR: 提出了一种简单有效的后训练策略来增强ECG基础模型ECGFounder的性能，在PTB-XL基准测试中显著提升了分类性能指标


<details>
  <summary>Details</summary>
Motivation: ECG基础模型虽然在各种任务中具有适应性，但其临床适用性往往受到性能限制，即使在大规模ECG数据集上进行预训练和目标数据微调后，仍存在性能差距，这主要是缺乏有效的后训练策略

Method: 提出了一种后训练方法，包括关键组件如随机深度(stochastic depth)和预览线性探测(preview linear probing)，用于增强ECGFounder模型

Result: 在PTB-XL基准测试中，相比基线微调策略，宏观AUROC提升1.2%-3.3%，宏观AUPRC提升5.3%-20.9%；使用仅10%训练数据时，宏观AUROC提升9.1%，宏观AUPRC提升34.9%；优于多个最先进方法

Conclusion: 后训练策略具有显著提升ECG基础模型性能的潜力，这项工作有助于推动ECG领域基础模型的持续发展

Abstract: ECG foundation models are increasingly popular due to their adaptability
across various tasks. However, their clinical applicability is often limited by
performance gaps compared to task-specific models, even after pre-training on
large ECG datasets and fine-tuning on target data. This limitation is likely
due to the lack of an effective post-training strategy. In this paper, we
propose a simple yet effective post-training approach to enhance ECGFounder, a
state-of-the-art ECG foundation model pre-trained on over 7 million ECG
recordings. Experiments on the PTB-XL benchmark show that our approach improves
the baseline fine-tuning strategy by 1.2%-3.3% in macro AUROC and 5.3%-20.9% in
macro AUPRC. Additionally, our method outperforms several recent
state-of-the-art approaches, including task-specific and advanced
architectures. Further evaluation reveals that our method is more stable and
sample-efficient compared to the baseline, achieving a 9.1% improvement in
macro AUROC and a 34.9% improvement in macro AUPRC using just 10% of the
training data. Ablation studies identify key components, such as stochastic
depth and preview linear probing, that contribute to the enhanced performance.
These findings underscore the potential of post-training strategies to improve
ECG foundation models, and we hope this work will contribute to the continued
development of foundation models in the ECG domain.

</details>


### [128] [BAPFL: Exploring Backdoor Attacks Against Prototype-based Federated Learning](https://arxiv.org/abs/2509.12964)
*Honghong Zeng,Jiong Lou,Zhe Wang,Hefeng Zhou,Chentao Wu,Wei Zhao,Jie Li*

Main category: cs.LG

TL;DR: 本文提出了BAPFL，这是首个专门针对原型联邦学习(PFL)框架的后门攻击方法，通过原型投毒策略和触发器优化机制，在保持主任务精度的同时将攻击成功率提高了35%-75%。


<details>
  <summary>Details</summary>
Motivation: 原型联邦学习(PFL)虽然能有效处理数据异构性问题，但其对后门攻击的鲁棒性尚未被充分研究。作者发现PFL对现有后门攻击具有天然抵抗力，因此需要专门设计针对PFL的攻击方法。

Method: BAPFL结合了原型投毒策略和触发器优化机制。原型投毒策略操纵全局原型轨迹来误导良性客户端的原型训练；触发器优化机制为每个目标标签学习独特且隐蔽的触发器，使触发样本原型与目标标签全局原型对齐。

Result: 在多个数据集和PFL变体上的实验表明，BAPFL相比传统后门攻击方法，攻击成功率提高了35%-75%，同时保持了主任务精度。

Conclusion: BAPFL证明了PFL框架对后门攻击并非免疫，该方法具有高效性、隐蔽性和适应性，为PFL安全性研究提供了重要见解。

Abstract: Prototype-based federated learning (PFL) has emerged as a promising paradigm
to address data heterogeneity problems in federated learning, as it leverages
mean feature vectors as prototypes to enhance model generalization. However,
its robustness against backdoor attacks remains largely unexplored. In this
paper, we identify that PFL is inherently resistant to existing backdoor
attacks due to its unique prototype learning mechanism and local data
heterogeneity. To further explore the security of PFL, we propose BAPFL, the
first backdoor attack method specifically designed for PFL frameworks. BAPFL
integrates a prototype poisoning strategy with a trigger optimization
mechanism. The prototype poisoning strategy manipulates the trajectories of
global prototypes to mislead the prototype training of benign clients, pushing
their local prototypes of clean samples away from the prototypes of
trigger-embedded samples. Meanwhile, the trigger optimization mechanism learns
a unique and stealthy trigger for each potential target label, and guides the
prototypes of trigger-embedded samples to align closely with the global
prototype of the target label. Experimental results across multiple datasets
and PFL variants demonstrate that BAPFL achieves a 35\%-75\% improvement in
attack success rate compared to traditional backdoor attacks, while preserving
main task accuracy. These results highlight the effectiveness, stealthiness,
and adaptability of BAPFL in PFL.

</details>


### [129] [FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning](https://arxiv.org/abs/2509.13160)
*Liang Hu,Jianpeng Jiao,Jiashuo Liu,Yanle Ren,Zhoufutu Wen,Kaiyuan Zhang,Xuanliang Zhang,Xiang Gao,Tianci He,Fei Hu,Yali Liao,Zaiyuan Wang,Chenghao Yang,Qianyu Yang,Mingren Yin,Zhiyuan Zeng,Ge Zhang,Xinyi Zhang,Xiying Zhao,Zhenwei Zhu,Hongseok Namkoong,Wenhao Huang,Yuwen Tang*

Main category: cs.LG

TL;DR: FinSearchComp是首个开源的金融搜索代理基准测试，包含三个真实金融分析师工作流程任务，由70位金融专家标注，评估了21个模型在635个问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有开源金融数据集缺乏对端到端代理数据搜索能力的评估，金融领域需要处理时间敏感、领域特定的复杂多步搜索，是评估搜索能力和知识推理的理想场景。

Method: 构建包含三个任务（时间敏感数据获取、简单历史查询、复杂历史调查）的基准测试，聘请70位金融专家进行标注，实施严格的多阶段质量保证流程，涵盖全球和大中华区市场。

Result: Grok 4在全局子集上表现最佳，接近专家级准确率；DouBao在大中华区子集领先。实验表明配备网络搜索和金融插件的代理性能显著提升，模型和工具的国家来源对性能影响显著。

Conclusion: FinSearchComp通过与真实分析师任务对齐并提供端到端评估，为复杂金融搜索和推理提供了专业、高难度的测试平台。

Abstract: Search has emerged as core infrastructure for LLM-based agents and is widely
viewed as critical on the path toward more general intelligence. Finance is a
particularly demanding proving ground: analysts routinely conduct complex,
multi-step searches over time-sensitive, domain-specific data, making it ideal
for assessing both search proficiency and knowledge-grounded reasoning. Yet no
existing open financial datasets evaluate data searching capability of
end-to-end agents, largely because constructing realistic, complicated tasks
requires deep financial expertise and time-sensitive data is hard to evaluate.
We present FinSearchComp, the first fully open-source agent benchmark for
realistic, open-domain financial search and reasoning. FinSearchComp comprises
three tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and
Complex Historical Investigation -- closely reproduce real-world financial
analyst workflows. To ensure difficulty and reliability, we engage 70
professional financial experts for annotation and implement a rigorous
multi-stage quality-assurance pipeline. The benchmark includes 635 questions
spanning global and Greater China markets, and we evaluate 21 models (products)
on it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.
DouBao (web) leads on the Greater China subset. Experimental analyses show that
equipping agents with web search and financial plugins substantially improves
results on FinSearchComp, and the country origin of models and tools impact
performance significantly.By aligning with realistic analyst tasks and
providing end-to-end evaluation, FinSearchComp offers a professional,
high-difficulty testbed for complex financial search and reasoning.

</details>


### [130] [Causal Discovery via Quantile Partial Effect](https://arxiv.org/abs/2509.12981)
*Yikang Chen,Xingzhe Sun,Dehui Du*

Main category: cs.LG

TL;DR: 该论文提出了基于分位数部分效应(QPE)的因果发现方法，通过分析观测分布的形状不对称性来识别因果方向，无需考虑噪声机制或马尔可夫假设，并在多变量因果发现中利用Fisher信息确定因果顺序。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法通常需要假设噪声机制或函数形式，限制了实际应用。本文旨在开发一种直接从观测数据分布形状特征中识别因果关系的更通用方法。

Method: 利用分位数回归中的QPE统计量，假设因果效应位于有限线性空间中，通过基函数检验估计的QPE来区分因果方向。在多变量情况下，利用QPE与得分函数的联系，使用Fisher信息作为统计量确定因果顺序。

Result: 在大量双变量因果发现数据集上的实验表明该方法有效。在多变量因果发现中，使用Fisher信息识别因果顺序的方法在合成和真实数据集上都验证了可行性。

Conclusion: QPE方法提供了一种新的因果发现框架，直接从观测分布的形状不对称性中识别因果关系，避免了传统方法对噪声机制和函数形式的强假设，具有更好的通用性和实用性。

Abstract: Quantile Partial Effect (QPE) is a statistic associated with conditional
quantile regression, measuring the effect of covariates at different levels.
Our theory demonstrates that when the QPE of cause on effect is assumed to lie
in a finite linear span, cause and effect are identifiable from their
observational distribution. This generalizes previous identifiability results
based on Functional Causal Models (FCMs) with additive, heteroscedastic noise,
etc. Meanwhile, since QPE resides entirely at the observational level, this
parametric assumption does not require considering mechanisms, noise, or even
the Markov assumption, but rather directly utilizes the asymmetry of shape
characteristics in the observational distribution. By performing basis function
tests on the estimated QPE, causal directions can be distinguished, which is
empirically shown to be effective in experiments on a large number of bivariate
causal discovery datasets. For multivariate causal discovery, leveraging the
close connection between QPE and score functions, we find that Fisher
Information is sufficient as a statistical measure to determine causal order
when assumptions are made about the second moment of QPE. We validate the
feasibility of using Fisher Information to identify causal order on multiple
synthetic and real-world multivariate causal discovery datasets.

</details>


### [131] [On the Correlation between Individual Fairness and Predictive Accuracy in Probabilistic Models](https://arxiv.org/abs/2509.13165)
*Alessandro Antonucci,Eric Rossetto,Ivan Duvnjak*

Main category: cs.LG

TL;DR: 本文研究生成式概率分类器的个体公平性，通过分析后验推断对私有特征扰动的鲁棒性，发现鲁棒性与预测准确性之间存在相关性，并提出用马尔可夫随机场重新表述问题以降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究生成式概率分类器中个体公平性问题，探索后验推断对私有特征扰动的鲁棒性，旨在理解鲁棒性与预测准确性之间的关系，以缓解公平性与准确性之间的传统权衡。

Method: 使用14个具有公平性关注的数据集作为基准，采用贝叶斯网络作为基础生成模型。为了解决贝叶斯网络中多个私有特征鲁棒性分析的计算复杂度问题，将问题重新表述为辅助马尔可夫随机场中的最可能解释任务。

Result: 实验证实了鲁棒性与预测准确性之间存在相关性的假设，即表现出更大鲁棒性的实例更可能被准确分类。

Conclusion: 研究结果为缓解公平性与准确性之间的传统权衡提供了新的方向，表明通过分析鲁棒性可以同时改善分类器的公平性和准确性表现。

Abstract: We investigate individual fairness in generative probabilistic classifiers by
analysing the robustness of posterior inferences to perturbations in private
features. Building on established results in robustness analysis, we
hypothesise a correlation between robustness and predictive accuracy,
specifically, instances exhibiting greater robustness are more likely to be
classified accurately. We empirically assess this hypothesis using a benchmark
of fourteen datasets with fairness concerns, employing Bayesian networks as the
underlying generative models. To address the computational complexity
associated with robustness analysis over multiple private features with
Bayesian networks, we reformulate the problem as a most probable explanation
task in an auxiliary Markov random field. Our experiments confirm the
hypothesis about the correlation, suggesting novel directions to mitigate the
traditional trade-off between fairness and accuracy.

</details>


### [132] [Is Meta-Learning Out? Rethinking Unsupervised Few-Shot Classification with Limited Entropy](https://arxiv.org/abs/2509.13185)
*Yunchuan Guan,Yu Liu,Ke Zhou,Zhiqi Shen,Jenq-Neng Hwang,Serge Belongie,Lei Li*

Main category: cs.LG

TL;DR: 本文通过理论分析和实验验证，证明元学习在有限熵监督设置下比全类训练具有更紧的泛化边界，对标签噪声和异构任务更鲁棒，并提出了MINO元学习框架来提升无监督性能。


<details>
  <summary>Details</summary>
Motivation: 针对近期研究表明全类训练策略在少样本分类任务中能达到与元学习相当的性能，本文旨在证明元学习的价值，通过建立公平比较的熵限制监督设置来展示元学习的优势。

Method: 建立熵限制监督设置进行理论分析和实验验证；提出MINO元学习框架，使用DBSCAN自适应聚类算法和动态头进行无监督任务构建，以及基于稳定性的元缩放器来增强对标签噪声的鲁棒性。

Result: 理论分析和实验验证表明元学习具有更紧的泛化边界，在有限熵下更高效，对标签噪声和异构任务更鲁棒；MINO框架在多个无监督少样本和零样本任务中表现出有效性。

Conclusion: 元学习在有限熵设置下相比全类训练具有显著优势，特别是在无监督任务中；提出的MINO框架成功利用了这些优势，在无监督少样本和零shot任务中取得了优异性能。

Abstract: Meta-learning is a powerful paradigm for tackling few-shot tasks. However,
recent studies indicate that models trained with the whole-class training
strategy can achieve comparable performance to those trained with meta-learning
in few-shot classification tasks. To demonstrate the value of meta-learning, we
establish an entropy-limited supervised setting for fair comparisons. Through
both theoretical analysis and experimental validation, we establish that
meta-learning has a tighter generalization bound compared to whole-class
training. We unravel that meta-learning is more efficient with limited entropy
and is more robust to label noise and heterogeneous tasks, making it
well-suited for unsupervised tasks. Based on these insights, We propose MINO, a
meta-learning framework designed to enhance unsupervised performance. MINO
utilizes the adaptive clustering algorithm DBSCAN with a dynamic head for
unsupervised task construction and a stability-based meta-scaler for robustness
against label noise. Extensive experiments confirm its effectiveness in
multiple unsupervised few-shot and zero-shot tasks.

</details>


### [133] [Ensemble Visualization With Variational Autoencoder](https://arxiv.org/abs/2509.13000)
*Cenyang Wu,Qinhan Yu,Liang Zhou*

Main category: cs.LG

TL;DR: 提出一种通过构建结构化概率潜在空间来可视化数据集合的新方法，使用变分自编码器将空间特征转换为遵循多元标准高斯分布的潜在空间，从而支持置信区间分析和密度估计


<details>
  <summary>Details</summary>
Motivation: 传统的数据集合可视化方法难以有效处理高维空间特征和概率分布分析，需要一种能够将复杂空间数据转换为结构化概率表示的方法，以便进行更精确的统计分析和可视化

Method: 使用变分自编码器（VAE）进行特征空间转换和无监督学习，将数据集合的空间特征映射到潜在空间，确保潜在空间遵循多元标准高斯分布，从而支持解析计算置信区间和概率密度估计

Result: 在天气预报数据集合上的初步实验结果表明，该方法具有有效性和通用性，能够成功构建结构化概率潜在空间并进行相关统计分析

Conclusion: 该方法为数据集合的可视化和概率分析提供了一种有效的解决方案，特别是在处理高维空间数据和需要概率推断的应用场景中表现出良好潜力

Abstract: We present a new method to visualize data ensembles by constructing
structured probabilistic representations in latent spaces, i.e.,
lower-dimensional representations of spatial data features. Our approach
transforms the spatial features of an ensemble into a latent space through
feature space conversion and unsupervised learning using a variational
autoencoder (VAE). The resulting latent spaces follow multivariate standard
Gaussian distributions, enabling analytical computation of confidence intervals
and density estimation of the probabilistic distribution that generates the
data ensemble. Preliminary results on a weather forecasting ensemble
demonstrate the effectiveness and versatility of our method.

</details>


### [134] [B-TGAT: A Bi-directional Temporal Graph Attention Transformer for Clustering Multivariate Spatiotemporal Data](https://arxiv.org/abs/2509.13202)
*Francis Ndikum Nji,Vandana Janaja,Jianwu Wang*

Main category: cs.LG

TL;DR: 提出了一种结合双向时序图注意力变换器(B-TGAT)的时间分布混合U-Net自编码器，用于高维多变量时空气候数据的高效时序聚类


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法在处理高维时空气候数据时难以同时捕捉局部和全局时序关系并保持空间上下文，需要新的方法来处理复杂的时间依赖性、演化的空间交互和非平稳动态

Method: 使用ConvLSTM2D模块提取联合时空特征，通过U-Net跳跃连接保持多尺度空间细节，在瓶颈处集成基于图的空间建模和注意力驱动的时序编码(B-TGAT)，实现自适应时序邻域加权和长短程依赖捕获

Result: 在三个不同的时空气候数据集上实验表明，相比最先进的基线方法，该方法在聚类可分性、时序稳定性和与已知气候转变的对齐方面表现优异

Conclusion: ConvLSTM2D、U-Net跳跃连接和B-TGAT的集成提升了时序聚类性能，同时为复杂时空变异性提供了可解释的见解，推动了方法学发展和气候科学应用

Abstract: Clustering high-dimensional multivariate spatiotemporal climate data is
challenging due to complex temporal dependencies, evolving spatial
interactions, and non-stationary dynamics. Conventional clustering methods,
including recurrent and convolutional models, often struggle to capture both
local and global temporal relationships while preserving spatial context. We
present a time-distributed hybrid U-Net autoencoder that integrates a
Bi-directional Temporal Graph Attention Transformer (B-TGAT) to guide efficient
temporal clustering of multidimensional spatiotemporal climate datasets. The
encoder and decoder are equipped with ConvLSTM2D modules that extract joint
spatial--temporal features by modeling localized dynamics and spatial
correlations over time, and skip connections that preserve multiscale spatial
details during feature compression and reconstruction. At the bottleneck,
B-TGAT integrates graph-based spatial modeling with attention-driven temporal
encoding, enabling adaptive weighting of temporal neighbors and capturing both
short and long-range dependencies across regions. This architecture produces
discriminative latent embeddings optimized for clustering. Experiments on three
distinct spatiotemporal climate datasets demonstrate superior cluster
separability, temporal stability, and alignment with known climate transitions
compared to state-of-the-art baselines. The integration of ConvLSTM2D, U-Net
skip connections, and B-TGAT enhances temporal clustering performance while
providing interpretable insights into complex spatiotemporal variability,
advancing both methodological development and climate science applications.

</details>


### [135] [ReTrack: Data Unlearning in Diffusion Models through Redirecting the Denoising Trajectory](https://arxiv.org/abs/2509.13007)
*Qitan Shi,Cheng Jin,Jiawei Zhang,Yuantao Gu*

Main category: cs.LG

TL;DR: ReTrack是一种针对扩散模型的高效数据遗忘方法，通过重要性采样和保留主导项来构建优化损失函数，将去噪轨迹重定向到k近邻，在保持生成质量的同时实现有效遗忘。


<details>
  <summary>Details</summary>
Motivation: 扩散模型存在训练数据记忆化问题，带来隐私和安全风险。数据遗忘技术可以在不重新训练的情况下移除特定数据的影响，但需要高效且保持生成质量的方法。

Method: 使用重要性采样构建更高效的微调损失函数，通过保留主导项来近似该损失，得到一个可解释的目标函数，将去噪轨迹重定向到k近邻。

Result: 在MNIST T-Shirt、CelebA-HQ、CIFAR-10和Stable Diffusion上的实验表明，ReTrack达到了最先进的性能，在遗忘强度和生成质量保持之间取得了最佳平衡。

Conclusion: ReTrack是一种快速有效的数据遗忘方法，能够有效解决扩散模型的隐私和安全问题，同时保持高质量的生成能力。

Abstract: Diffusion models excel at generating high-quality, diverse images but suffer
from training data memorization, raising critical privacy and safety concerns.
Data unlearning has emerged to mitigate this issue by removing the influence of
specific data without retraining from scratch. We propose ReTrack, a fast and
effective data unlearning method for diffusion models. ReTrack employs
importance sampling to construct a more efficient fine-tuning loss, which we
approximate by retaining only dominant terms. This yields an interpretable
objective that redirects denoising trajectories toward the $k$-nearest
neighbors, enabling efficient unlearning while preserving generative quality.
Experiments on MNIST T-Shirt, CelebA-HQ, CIFAR-10, and Stable Diffusion show
that ReTrack achieves state-of-the-art performance, striking the best trade-off
between unlearning strength and generation quality preservation.

</details>


### [136] [Single-stream Policy Optimization](https://arxiv.org/abs/2509.13232)
*Zhongwen Xu,Zihan Ding*

Main category: cs.LG

TL;DR: SPO是一种单流策略优化方法，通过持久化KL自适应值跟踪器和全局优势归一化，解决了传统分组方法(如GRPO)的退化组问题和可扩展性限制，在数学推理任务上实现了更稳定高效的收敛和更高的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的分组方法(如GRPO)虽然通过即时基线减少方差，但存在关键缺陷：频繁的退化组会消除学习信号，同步障碍阻碍可扩展性。需要一种更稳定、高效的策略优化方法。

Method: SPO采用单流设计，用持久化的KL自适应值跟踪器替代每组的基线，并在整个批次中全局归一化优势，为每个样本提供稳定、低方差的学习信号。无分组设计支持更高吞吐量和可变生成时间的场景。

Result: 在Qwen3-8B上的实验显示，SPO比GRPO收敛更平滑且准确率更高。在五个数学基准测试中，平均maj@32提高了3.4个百分点，在BRUMO 25上提升7.3pp，AIME 25上提升4.4pp，HMMT 25上提升3.3pp。

Conclusion: SPO的成功挑战了RL算法中添加附带复杂性的趋势，表明通过基本原则而非架构变通可以推动LLM推理的下一个进步浪潮，提供了更稳健高效的LLM推理优化路径。

Abstract: We revisit policy-gradient optimization for Large Language Models (LLMs) from
a single-stream perspective. Prevailing group-based methods like GRPO reduce
variance with on-the-fly baselines but suffer from critical flaws: frequent
degenerate groups erase learning signals, and synchronization barriers hinder
scalability. We introduce Single-stream Policy Optimization (SPO), which
eliminates these issues by design. SPO replaces per-group baselines with a
persistent, KL-adaptive value tracker and normalizes advantages globally across
the batch, providing a stable, low-variance learning signal for every sample.
Being group-free, SPO enables higher throughput and scales effectively in
long-horizon or tool-integrated settings where generation times vary.
Furthermore, the persistent value tracker naturally enables an adaptive
curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO
converges more smoothly and attains higher accuracy than GRPO, while
eliminating computation wasted on degenerate groups. Ablation studies confirm
that SPO's gains stem from its principled approach to baseline estimation and
advantage normalization, offering a more robust and efficient path for LLM
reasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the
average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial
absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,
+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain
in pass@$k$ across the evaluated $k$ values. SPO's success challenges the
prevailing trend of adding incidental complexity to RL algorithms, highlighting
a path where fundamental principles, not architectural workarounds, drive the
next wave of progress in LLM reasoning.

</details>


### [137] [Spiking Vocos: An Energy-Efficient Neural Vocoder](https://arxiv.org/abs/2509.13049)
*Yukun Chen,Zhaoxi Mu,Andong Li,Peilin Li,Xinyu Yang*

Main category: cs.LG

TL;DR: 提出Spiking Vocos，一种基于SNN的超低能耗神经声码器，通过Spiking ConvNeXt模块、振幅快捷路径和自架构蒸馏策略，在保持性能的同时仅消耗14.7%的能量


<details>
  <summary>Details</summary>
Motivation: 传统神经声码器在边缘设备上能耗过高，而SNN因其事件驱动特性具有高能效优势，但存在信息瓶颈问题需要解决

Method: 基于Vocos框架构建SNN声码器，设计Spiking ConvNeXt减少MAC操作，加入振幅快捷路径保持信号动态，采用自架构蒸馏策略和轻量级时序移位模块

Result: 性能与ANN相当（UTMOS 3.74，PESQ 3.45），能耗仅为ANN的14.7%

Conclusion: Spiking Vocos成功实现了高性能与超低能耗的平衡，为边缘设备部署提供了可行的解决方案

Abstract: Despite the remarkable progress in the synthesis speed and fidelity of neural
vocoders, their high energy consumption remains a critical barrier to practical
deployment on computationally restricted edge devices. Spiking Neural Networks
(SNNs), widely recognized for their high energy efficiency due to their
event-driven nature, offer a promising solution for low-resource scenarios. In
this paper, we propose Spiking Vocos, a novel spiking neural vocoder with
ultra-low energy consumption, built upon the efficient Vocos framework. To
mitigate the inherent information bottleneck in SNNs, we design a Spiking
ConvNeXt module to reduce Multiply-Accumulate (MAC) operations and incorporate
an amplitude shortcut path to preserve crucial signal dynamics. Furthermore, to
bridge the performance gap with its Artificial Neural Network (ANN)
counterpart, we introduce a self-architectural distillation strategy to
effectively transfer knowledge. A lightweight Temporal Shift Module is also
integrated to enhance the model's ability to fuse information across the
temporal dimension with negligible computational overhead. Experiments
demonstrate that our model achieves performance comparable to its ANN
counterpart, with UTMOS and PESQ scores of 3.74 and 3.45 respectively, while
consuming only 14.7% of the energy. The source code is available at
https://github.com/pymaster17/Spiking-Vocos.

</details>


### [138] [Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise Behaviors](https://arxiv.org/abs/2509.13237)
*Aniket Didolkar,Nicolas Ballas,Sanjeev Arora,Anirudh Goyal*

Main category: cs.LG

TL;DR: 该论文提出了一种将重复推理片段转换为可重用"行为"的方法，通过LLM的元认知分析创建行为手册，在推理时提供相关行为来减少token使用并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决多步问题时经常重复推导相同的中间步骤，导致token使用和延迟增加，上下文窗口饱和限制了探索能力。

Method: 通过模型的元认知分析将重复推理片段转换为简洁可重用的"行为"（名称+指令），存储在行为手册中，在推理时提供相关行为或通过监督微调蒸馏到参数中。

Result: 在三种设置中取得改进：1)行为条件推理减少46%推理token同时保持或提高准确性；2)行为引导自改进比基线提高10%准确性；3)行为条件SFT比普通SFT更有效将非推理模型转换为推理模型。

Conclusion: 将缓慢推导转换为快速程序提示使LLM能够记住如何推理，而不仅仅是记住结论，提高了推理效率和效果。

Abstract: Large language models (LLMs) now solve multi-step problems by emitting
extended chains of thought. During the process, they often re-derive the same
intermediate steps across problems, inflating token usage and latency. This
saturation of the context window leaves less capacity for exploration. We study
a simple mechanism that converts recurring reasoning fragments into concise,
reusable "behaviors" (name + instruction) via the model's own metacognitive
analysis of prior traces. These behaviors are stored in a "behavior handbook"
which supplies them to the model in-context at inference or distills them into
parameters via supervised fine-tuning. This approach achieves improved
test-time reasoning across three different settings - 1) Behavior-conditioned
inference: Providing the LLM relevant behaviors in-context during reasoning
reduces number of reasoning tokens by up to 46% while matching or improving
baseline accuracy; 2) Behavior-guided self-improvement: Without any parameter
updates, the model improves its own future reasoning by leveraging behaviors
from its own past problem solving attempts. This yields up to 10% higher
accuracy than a naive critique-and-revise baseline; and 3) Behavior-conditioned
SFT: SFT on behavior-conditioned reasoning traces is more effective at
converting non-reasoning models into reasoning models as compared to vanilla
SFT. Together, these results indicate that turning slow derivations into fast
procedural hints enables LLMs to remember how to reason, not just what to
conclude.

</details>


### [139] [Traces Propagation: Memory-Efficient and Scalable Forward-Only Learning in Spiking Neural Networks](https://arxiv.org/abs/2509.13053)
*Lorenzo Pes,Bojian Yin,Sander Stuijk,Federico Corradi*

Main category: cs.LG

TL;DR: 提出了一种名为Traces Propagation (TP)的前向传播、内存高效的完全局部学习规则，用于脉冲神经网络训练，解决了时空信用分配问题，无需辅助矩阵，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决SNN训练中的时空信用分配问题，BPTT方法不符合生物神经系统的局部性且计算内存需求高，现有局部学习规则需要辅助矩阵导致内存开销大和可扩展性差。

Method: 结合资格迹和逐层对比损失的前向传播学习规则TP，无需辅助矩阵，实现完全局部学习。

Result: 在NMNIST和SHD数据集上优于其他完全局部学习规则，在DVS-GESTURE和DVS-CIFAR10等复杂数据集上表现竞争性，可扩展到VGG-9等深层架构，内存扩展性优于先前方法。

Conclusion: TP是一种高效、可扩展的局部学习规则，适用于边缘设备上的实际微调任务，为边缘高效学习铺平道路。

Abstract: Spiking Neural Networks (SNNs) provide an efficient framework for processing
dynamic spatio-temporal signals and for investigating the learning principles
underlying biological neural systems. A key challenge in training SNNs is to
solve both spatial and temporal credit assignment. The dominant approach for
training SNNs is Backpropagation Through Time (BPTT) with surrogate gradients.
However, BPTT is in stark contrast with the spatial and temporal locality
observed in biological neural systems and leads to high computational and
memory demands, limiting efficient training strategies and on-device learning.
Although existing local learning rules achieve local temporal credit assignment
by leveraging eligibility traces, they fail to address the spatial credit
assignment without resorting to auxiliary layer-wise matrices, which increase
memory overhead and hinder scalability, especially on embedded devices. In this
work, we propose Traces Propagation (TP), a forward-only, memory-efficient,
scalable, and fully local learning rule that combines eligibility traces with a
layer-wise contrastive loss without requiring auxiliary layer-wise matrices. TP
outperforms other fully local learning rules on NMNIST and SHD datasets. On
more complex datasets such as DVS-GESTURE and DVS-CIFAR10, TP showcases
competitive performance and scales effectively to deeper SNN architectures such
as VGG-9, while providing favorable memory scaling compared to prior fully
local scalable rules, for datasets with a significant number of classes.
Finally, we show that TP is well suited for practical fine-tuning tasks, such
as keyword spotting on the Google Speech Commands dataset, thus paving the way
for efficient learning at the edge.

</details>


### [140] [JANUS: A Dual-Constraint Generative Framework for Stealthy Node Injection Attacks](https://arxiv.org/abs/2509.13266)
*Jiahao Zhang,Xiaobing Pei,Zhaokun Zhong,Wenqiang Hao,Zhenghao Tang*

Main category: cs.LG

TL;DR: JANUS是一个双约束隐密节点注入攻击框架，通过局部特征流形对齐和全局语义模式一致性来提升攻击的隐蔽性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络对抗攻击方法在隐蔽性方面存在局限，要么依赖间接代理指标，要么只关注局部结构模仿，导致局部短视问题，无法有效避免检测。

Method: 提出JANUS框架：局部层面使用特征流形对齐策略实现几何一致性；全局层面引入结构化潜变量并最大化与生成结构的互信息，确保注入结构与原始图语义模式一致；将注入攻击建模为序列决策过程，使用强化学习优化。

Result: 在多个标准数据集上的实验表明，JANUS框架在攻击效果和隐蔽性方面均显著优于现有方法。

Conclusion: JANUS通过同时考虑局部和全局结构一致性，有效解决了节点注入攻击中的隐蔽性问题，为图神经网络安全提供了新的攻击范式。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable performance across
various applications, yet they are vulnerable to sophisticated adversarial
attacks, particularly node injection attacks. The success of such attacks
heavily relies on their stealthiness, the ability to blend in with the original
graph and evade detection. However, existing methods often achieve stealthiness
by relying on indirect proxy metrics, lacking consideration for the fundamental
characteristics of the injected content, or focusing only on imitating local
structures, which leads to the problem of local myopia. To overcome these
limitations, we propose a dual-constraint stealthy node injection framework,
called Joint Alignment of Nodal and Universal Structures (JANUS). At the local
level, we introduce a local feature manifold alignment strategy to achieve
geometric consistency in the feature space. At the global level, we incorporate
structured latent variables and maximize the mutual information with the
generated structures, ensuring the injected structures are consistent with the
semantic patterns of the original graph. We model the injection attack as a
sequential decision process, which is optimized by a reinforcement learning
agent. Experiments on multiple standard datasets demonstrate that the JANUS
framework significantly outperforms existing methods in terms of both attack
effectiveness and stealthiness.

</details>


### [141] [When Inverse Data Outperforms: Exploring the Pitfalls of Mixed Data in Multi-Stage Fine-Tuning](https://arxiv.org/abs/2509.13079)
*Mengyi Deng,Xin Li,Tingyu Zhu,Zhicheng Yang,Zhijiang Guo,Wei Wang*

Main category: cs.LG

TL;DR: 通过反转前向推理数据构建高质量反向推理数据集r1k，研究发现双向SFT会削弱方向区分性，DPO虽能部分恢复但会抑制非偏好推理路径，表明需要方向感知的对齐策略


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单向监督微调(SFT)，忽视了不同推理模式之间的复杂相互作用，需要研究双向推理目标下SFT和DPO对模型对齐的影响

Method: 构建r1k反向推理数据集（通过反转s1k中的1000个前向示例），比较SFT和直接偏好优化(DPO)在双向推理目标下的对齐效果

Result: 在r1k上进行SFT比s1k在评估基准上准确率提升1.6%-6.8%；混合前向和反向数据的SFT会削弱方向区分性；DPO能部分恢复区分性但会抑制非偏好推理路径

Conclusion: 混合推理数据会引入冲突的监督信号，强调了需要开发鲁棒且方向感知的对齐策略来处理双向推理任务

Abstract: Existing work has shown that o1-level performance can be achieved with
limited data distillation, but most existing methods focus on unidirectional
supervised fine-tuning (SFT), overlooking the intricate interplay between
diverse reasoning patterns. In this paper, we construct r1k, a high-quality
reverse reasoning dataset derived by inverting 1,000 forward examples from s1k,
and examine how SFT and Direct Preference Optimization (DPO) affect alignment
under bidirectional reasoning objectives. SFT on r1k yields a 1.6%--6.8%
accuracy improvement over s1k across evaluated benchmarks. However, naively
mixing forward and reverse data during SFT weakens the directional distinction.
Although DPO can partially recover this distinction, it also suppresses less
preferred reasoning paths by shifting the probability mass toward irrelevant
outputs. These findings suggest that mixed reasoning data introduce conflicting
supervision signals, underscoring the need for robust and direction-aware
alignment strategies.

</details>


### [142] [Discovering Mathematical Equations with Diffusion Language Model](https://arxiv.org/abs/2509.13136)
*Xiaoxu Han,Chengzhen Ning,Jinghui Zhong,Fubiao Yang,Yu Wang,Xin Mu*

Main category: cs.LG

TL;DR: DiffuSR是一个基于连续状态扩散语言模型的符号回归预训练框架，通过扩散过程将离散数学符号映射到连续潜在空间，利用交叉注意力机制注入数值数据指导，在标准基准测试中达到与最先进自回归方法竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 符号回归在科学发现中至关重要，但由于搜索空间巨大以及精度与复杂度之间的权衡，该任务仍然具有挑战性。现有方法存在局限性，需要新的框架来更有效地建模方程分布。

Method: DiffuSR采用可训练嵌入层在扩散过程中将离散数学符号映射到连续潜在空间，通过迭代去噪将初始噪声序列转换为符号方程，并使用交叉注意力机制注入数值数据。还设计了有效的推理策略，将logit先验注入遗传编程。

Result: 在标准符号回归基准测试中，DiffuSR达到了与最先进自回归方法竞争的性能，并生成更具可解释性和多样性的数学表达式。

Conclusion: DiffuSR提供了一个有效的符号回归预训练框架，通过扩散模型和连续潜在空间表示，在保持竞争性能的同时提高了生成表达式的可解释性和多样性。

Abstract: Discovering valid and meaningful mathematical equations from observed data
plays a crucial role in scientific discovery. While this task, symbolic
regression, remains challenging due to the vast search space and the trade-off
between accuracy and complexity. In this paper, we introduce DiffuSR, a
pre-training framework for symbolic regression built upon a continuous-state
diffusion language model. DiffuSR employs a trainable embedding layer within
the diffusion process to map discrete mathematical symbols into a continuous
latent space, modeling equation distributions effectively. Through iterative
denoising, DiffuSR converts an initial noisy sequence into a symbolic equation,
guided by numerical data injected via a cross-attention mechanism. We also
design an effective inference strategy to enhance the accuracy of the
diffusion-based equation generator, which injects logit priors into genetic
programming. Experimental results on standard symbolic regression benchmarks
demonstrate that DiffuSR achieves competitive performance with state-of-the-art
autoregressive methods and generates more interpretable and diverse
mathematical expressions.

</details>


### [143] [Curriculum Learning for Mesh-based simulations](https://arxiv.org/abs/2509.13138)
*Paul Garnier,Vincent Lannelongue,Elie Hachem*

Main category: cs.LG

TL;DR: 提出一种从粗到细的课程学习方法，通过先在粗网格上训练GNN，再逐步引入中高分辨率网格，显著加速收敛并减少50%训练时间，同时保持相当的泛化精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法在高分辨率非结构化网格（多达30万个节点）上训练图神经网络成本过高，需要寻找更高效的训练策略来降低计算开销。

Method: 采用课程学习策略，模型结构保持不变，仅改变训练数据的精度：先在粗网格上训练，然后逐步引入中等和高分辨率网格数据。

Result: 在保持相当泛化精度的同时，总训练时间减少高达50%，且在模型容量不足的数据集上能够突破训练平台期。

Conclusion: 粗到细课程学习是一种有效的GNN训练加速方法，无需改变模型架构即可显著提升训练效率，特别适用于计算流体动力学中的高分辨率网格问题。

Abstract: Graph neural networks (GNNs) have emerged as powerful surrogates for
mesh-based computational fluid dynamics (CFD), but training them on
high-resolution unstructured meshes with hundreds of thousands of nodes remains
prohibitively expensive. We study a \emph{coarse-to-fine curriculum} that
accelerates convergence by first training on very coarse meshes and then
progressively introducing medium and high resolutions (up to \(3\times10^5\)
nodes). Unlike multiscale GNN architectures, the model itself is unchanged;
only the fidelity of the training data varies over time. We achieve comparable
generalization accuracy while reducing total wall-clock time by up to 50\%.
Furthermore, on datasets where our model lacks the capacity to learn the
underlying physics, using curriculum learning enables it to break through
plateaus.

</details>


### [144] [Learning from Heterophilic Graphs: A Spectral Theory Perspective on the Impact of Self-Loops and Parallel Edges](https://arxiv.org/abs/2509.13139)
*Kushal Bose,Swagatam Das*

Main category: cs.LG

TL;DR: 该论文研究了图异质性对消息传递图神经网络性能的影响，通过添加自循环和平行边来更新异质图，分析了图拉普拉斯特征值变化与GCN性能趋势的关系，建立了图谱与低通滤波器性能之间的联系。


<details>
  <summary>Details</summary>
Motivation: 图异质性对消息传递图神经网络（MP-GNNs）的性能构成重大挑战，特别是低通滤波器如图卷积网络（GCNs）在异质图上性能下降的问题需要深入分析。

Method: 通过向异质图添加自循环和平行边来更新图结构，观察图拉普拉斯特征值的变化，并在多个基准异质网络上研究GCN的性能趋势，建立图谱与性能之间的关系。

Result: 研究发现添加自循环和平行边会分别降低和增加图拉普拉斯特征值，GCN性能呈现相应的增加或减少趋势，成功建立了图谱特性与低通滤波器性能之间的关联。

Conclusion: 该工作能够通过观察低通滤波器的性能趋势来无缝评估图谱和属性，无需进行昂贵的特征值分解，为理解图异质性对GNN性能的影响提供了新的分析框架。

Abstract: Graph heterophily poses a formidable challenge to the performance of
Message-passing Graph Neural Networks (MP-GNNs). The familiar low-pass filters
like Graph Convolutional Networks (GCNs) face performance degradation, which
can be attributed to the blending of the messages from dissimilar neighboring
nodes. The performance of the low-pass filters on heterophilic graphs still
requires an in-depth analysis. In this context, we update the heterophilic
graphs by adding a number of self-loops and parallel edges. We observe that
eigenvalues of the graph Laplacian decrease and increase respectively by
increasing the number of self-loops and parallel edges. We conduct several
studies regarding the performance of GCN on various benchmark heterophilic
networks by adding either self-loops or parallel edges. The studies reveal that
the GCN exhibited either increasing or decreasing performance trends on adding
self-loops and parallel edges. In light of the studies, we established
connections between the graph spectra and the performance trends of the
low-pass filters on the heterophilic graphs. The graph spectra characterize the
essential intrinsic properties of the input graph like the presence of
connected components, sparsity, average degree, cluster structures, etc. Our
work is adept at seamlessly evaluating graph spectrum and properties by
observing the performance trends of the low-pass filters without pursuing the
costly eigenvalue decomposition. The theoretical foundations are also discussed
to validate the impact of adding self-loops and parallel edges on the graph
spectrum.

</details>


### [145] [CoVariance Filters and Neural Networks over Hilbert Spaces](https://arxiv.org/abs/2509.13178)
*Claudio Battiloro,Andrea Cavallo,Elvin Isufi*

Main category: cs.LG

TL;DR: 该论文提出了针对无限维希尔伯特空间信号的卷积学习框架，基于协方差算子构建希尔伯特协方差滤波器和网络，并证明了其能够恢复功能主成分分析。


<details>
  <summary>Details</summary>
Motivation: 现有的协方差神经网络主要针对有限维希尔伯特空间，对于无限维空间的鲁棒性和可迁移性研究不足，需要扩展相关理论框架。

Method: 构建希尔伯特协方差滤波器(HVFs)和网络(HVNs)，采用基于经验协方差算子的卷积操作，包含非线性激活的滤波器堆栈，并提出离散化方法。

Result: 理论证明经验HVFs能够恢复滤波信号的功能主成分分析(FPCA)，在合成和真实时间序列分类任务中表现出比MLP和FPCA分类器更鲁棒的性能。

Conclusion: 该工作成功将协方差神经网络扩展到无限维希尔伯特空间，建立了理论框架并验证了实际应用效果，为处理无限维信号提供了新工具。

Abstract: CoVariance Neural Networks (VNNs) perform graph convolutions on the empirical
covariance matrix of signals defined over finite-dimensional Hilbert spaces,
motivated by robustness and transferability properties. Yet, little is known
about how these arguments extend to infinite-dimensional Hilbert spaces. In
this work, we take a first step by introducing a novel convolutional learning
framework for signals defined over infinite-dimensional Hilbert spaces,
centered on the (empirical) covariance operator. We constructively define
Hilbert coVariance Filters (HVFs) and design Hilbert coVariance Networks (HVNs)
as stacks of HVF filterbanks with nonlinear activations. We propose a
principled discretization procedure, and we prove that empirical HVFs can
recover the Functional PCA (FPCA) of the filtered signals. We then describe the
versatility of our framework with examples ranging from multivariate
real-valued functions to reproducing kernel Hilbert spaces. Finally, we
validate HVNs on both synthetic and real-world time-series classification
tasks, showing robust performance compared to MLP and FPCA-based classifiers.

</details>


### [146] [TRUST-FS: Tensorized Reliable Unsupervised Multi-View Feature Selection for Incomplete Data](https://arxiv.org/abs/2509.13192)
*Minghui Lu,Yanyong Huang,Minbo Ma,Dongjie Wang,Xiuwen Yi,Tianrui Li*

Main category: cs.LG

TL;DR: TRUST-FS是一种针对不完整多视图数据的无监督特征选择方法，通过张量分解统一处理特征选择、缺失值填补和视图权重学习，利用主观逻辑获取可信的跨视图相似性信息。


<details>
  <summary>Details</summary>
Motivation: 现有多视图无监督特征选择方法存在三个主要问题：1)只能处理视图缺失，无法处理更一般的变量缺失情况；2)填补和特征选择过程分离，忽略了二者的交互；3)缺失数据导致相似性图不准确，影响特征选择性能。

Method: 提出TRUST-FS方法，采用自适应加权CP分解，在统一的张量分解框架中同时进行特征选择、缺失变量填补和视图权重学习。利用主观逻辑获取可信的跨视图相似性信息，构建可靠的相似性图来指导特征选择和填补。

Result: 综合实验结果表明，该方法在效果和性能上优于现有的最先进方法。

Conclusion: TRUST-FS通过统一的张量分解框架有效解决了不完整多视图数据中的特征选择问题，实现了特征选择、缺失填补和视图权重学习的协同优化。

Abstract: Multi-view unsupervised feature selection (MUFS), which selects informative
features from multi-view unlabeled data, has attracted increasing research
interest in recent years. Although great efforts have been devoted to MUFS,
several challenges remain: 1) existing methods for incomplete multi-view data
are limited to handling missing views and are unable to address the more
general scenario of missing variables, where some features have missing values
in certain views; 2) most methods address incomplete data by first imputing
missing values and then performing feature selection, treating these two
processes independently and overlooking their interactions; 3) missing data can
result in an inaccurate similarity graph, which reduces the performance of
feature selection. To solve this dilemma, we propose a novel MUFS method for
incomplete multi-view data with missing variables, termed Tensorized Reliable
UnSupervised mulTi-view Feature Selection (TRUST-FS). TRUST-FS introduces a new
adaptive-weighted CP decomposition that simultaneously performs feature
selection, missing-variable imputation, and view weight learning within a
unified tensor factorization framework. By utilizing Subjective Logic to
acquire trustworthy cross-view similarity information, TRUST-FS facilitates
learning a reliable similarity graph, which subsequently guides feature
selection and imputation. Comprehensive experimental results demonstrate the
effectiveness and superiority of our method over state-of-the-art methods.

</details>


### [147] [HAM: Hierarchical Adapter Merging for Scalable Continual Learning](https://arxiv.org/abs/2509.13211)
*Eric Nuertey Coleman,Luigi Quarantiello,Samrat Mukherjee,Julio Hurtado,Vincenzo Lomonaco*

Main category: cs.LG

TL;DR: HAM是一种新颖的分层适配器合并框架，通过动态组合不同任务的适配器来解决持续学习中的灾难性遗忘问题，在视觉基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中灾难性遗忘问题，特别是当面对新数据分布时，现有参数高效微调方法（如LoRA）在动态学习场景和长任务序列中面临扩展性挑战和干扰问题。

Method: 提出分层适配器合并（HAM）框架，维护固定组集来分层整合新适配器。为每个任务训练低秩适配器和重要性标量，基于适配器相似性动态分组任务，在组内进行适配器修剪、缩放和合并。

Result: 在三个视觉基准测试上的广泛实验表明，HAM显著优于最先进方法，特别是在任务数量增加时表现更加突出。

Conclusion: HAM框架能够有效扩展，管理比竞争基线更多的任务，并通过促进相关任务间的迁移学习来提高效率，为解决持续学习中的灾难性遗忘问题提供了有效方案。

Abstract: Continual learning is an essential capability of human cognition, yet it
poses significant challenges for current deep learning models. The primary
issue is that new knowledge can interfere with previously learned information,
causing the model to forget earlier knowledge in favor of the new, a phenomenon
known as catastrophic forgetting. Although large pre-trained models can
partially mitigate forgetting by leveraging their existing knowledge and
over-parameterization, they often struggle when confronted with novel data
distributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,
enable efficient adaptation to new knowledge. However, they still face
challenges in scaling to dynamic learning scenarios and long sequences of
tasks, as maintaining one adapter per task introduces complexity and increases
the potential for interference. In this paper, we introduce Hierarchical
Adapters Merging (HAM), a novel framework that dynamically combines adapters
from different tasks during training. This approach enables HAM to scale
effectively, allowing it to manage more tasks than competing baselines with
improved efficiency. To achieve this, HAM maintains a fixed set of groups that
hierarchically consolidate new adapters. For each task, HAM trains a low-rank
adapter along with an importance scalar, then dynamically groups tasks based on
adapter similarity. Within each group, adapters are pruned, scaled and merge,
facilitating transfer learning between related tasks. Extensive experiments on
three vision benchmarks show that HAM significantly outperforms
state-of-the-art methods, particularly as the number of tasks increases.

</details>


### [148] [Density-Aware Farthest Point Sampling](https://arxiv.org/abs/2509.13213)
*Paolo Climaco,Jochen Garcke*

Main category: cs.LG

TL;DR: 提出了一种新的密度感知最远点采样方法(DA-FPS)，用于在标注数据有限的情况下选择训练集，通过最小化加权填充距离来降低回归模型的预测误差上界


<details>
  <summary>Details</summary>
Motivation: 在机器学习回归任务中，由于计算约束或标注成本高昂，标注训练数据有限，需要从无标注数据中选择合适的训练集来平衡性能与效率

Method: 推导了Lipschitz连续回归模型预测误差的上界，该上界线性依赖于训练集的加权填充距离。提出了DA-FPS采样方法，该方法能够近似最小化数据驱动的加权填充距离估计

Result: 在三个数据集上使用两种回归模型进行实验，结果表明DA-FPS相比其他采样策略显著降低了平均绝对预测误差

Conclusion: DA-FPS是一种有效的被动式、模型无关的采样方法，仅基于数据特征表示就能选择出高质量的训练集，在标注数据有限的情况下能够显著提升回归模型的性能

Abstract: We focus on training machine learning regression models in scenarios where
the availability of labeled training data is limited due to computational
constraints or high labeling costs. Thus, selecting suitable training sets from
unlabeled data is essential for balancing performance and efficiency. For the
selection of the training data, we focus on passive and model-agnostic sampling
methods that only consider the data feature representations. We derive an upper
bound for the expected prediction error of Lipschitz continuous regression
models that linearly depends on the weighted fill distance of the training set,
a quantity we can estimate simply by considering the data features. We
introduce "Density-Aware Farthest Point Sampling" (DA-FPS), a novel sampling
method. We prove that DA-FPS provides approximate minimizers for a data-driven
estimation of the weighted fill distance, thereby aiming at minimizing our
derived bound. We conduct experiments using two regression models across three
datasets. The results demonstrate that DA-FPS significantly reduces the mean
absolute prediction error compared to other sampling strategies.

</details>


### [149] [FOSSIL: Regret-minimizing weighting for robust learning under imbalance and small data](https://arxiv.org/abs/2509.13218)
*J. Cha,J. Lee,J. Cho,J. Shin*

Main category: cs.LG

TL;DR: FOSSIL是一个统一的样本权重框架，通过单一可解释公式整合类别不平衡校正、难度感知课程、增强惩罚和预热动态，在数据不平衡和小样本场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决不平衡和小数据场景（如罕见疾病成像、基因组学）中标记样本稀缺且传统增强方法容易引入伪影的问题，现有方法如过采样、焦点损失等存在脆弱性或复杂性。

Method: 提出FOSSIL框架，通过样本敏感重要性学习，将类别不平衡校正、难度感知课程学习、增强惩罚和预热动态统一到一个可解释的权重公式中，无需改变模型架构。

Result: 在合成和真实数据集上相比ERM、课程学习和元加权基线方法获得了一致的经验增益，并提供了基于遗憾的理论保证。

Conclusion: FOSSIL提供了一个理论上有保证且实用的统一框架，有效解决了不平衡和小数据学习中的多个挑战，优于现有的隔离解决方案。

Abstract: Imbalanced and small data regimes are pervasive in domains such as rare
disease imaging, genomics, and disaster response, where labeled samples are
scarce and naive augmentation often introduces artifacts. Existing solutions
such as oversampling, focal loss, or meta-weighting address isolated aspects of
this challenge but remain fragile or complex. We introduce FOSSIL (Flexible
Optimization via Sample Sensitive Importance Learning), a unified weighting
framework that seamlessly integrates class imbalance correction,
difficulty-aware curricula, augmentation penalties, and warmup dynamics into a
single interpretable formula. Unlike prior heuristics, the proposed framework
provides regret-based theoretical guarantees and achieves consistent empirical
gains over ERM, curriculum, and meta-weighting baselines on synthetic and
real-world datasets, while requiring no architectural changes.

</details>


### [150] [On the Out-of-Distribution Backdoor Attack for Federated Learning](https://arxiv.org/abs/2509.13219)
*Jiahao Xu,Zikai Zhang,Rui Hu*

Main category: cs.LG

TL;DR: 提出了一种新型联邦学习后门攻击OBA，使用分布外数据作为毒化样本和触发器，并开发了SoDa方法提高隐蔽性。同时提出了BNGuard防御机制，利用批归一化层统计偏差检测恶意模型更新。


<details>
  <summary>Details</summary>
Motivation: 传统后门攻击需要可见触发器和物理修改目标对象，应用场景受限。为了扩展联邦学习后门攻击的实用性，需要开发更隐蔽的攻击方法。

Method: OBA攻击使用分布外(OOD)数据同时作为毒化样本和触发器；SoDa方法通过正则化恶意本地模型的幅度和方向来增强隐蔽性；BNGuard防御通过检测批归一化层运行统计的显著偏差来识别恶意模型更新。

Result: 实验证明OBA能有效绕过最先进的防御机制，同时保持主任务的高准确率；BNGuard在各种设置下都能有效防御SoDa攻击。

Conclusion: 该工作提出了更实用的联邦学习后门攻击方法，并开发了相应的防御机制，增强了联邦学习系统的后门鲁棒性。

Abstract: Traditional backdoor attacks in federated learning (FL) operate within
constrained attack scenarios, as they depend on visible triggers and require
physical modifications to the target object, which limits their practicality.
To address this limitation, we introduce a novel backdoor attack prototype for
FL called the out-of-distribution (OOD) backdoor attack ($\mathtt{OBA}$), which
uses OOD data as both poisoned samples and triggers simultaneously. Our
approach significantly broadens the scope of backdoor attack scenarios in FL.
To improve the stealthiness of $\mathtt{OBA}$, we propose $\mathtt{SoDa}$,
which regularizes both the magnitude and direction of malicious local models
during local training, aligning them closely with their benign versions to
evade detection. Empirical results demonstrate that $\mathtt{OBA}$ effectively
circumvents state-of-the-art defenses while maintaining high accuracy on the
main task.
  To address this security vulnerability in the FL system, we introduce
$\mathtt{BNGuard}$, a new server-side defense method tailored against
$\mathtt{SoDa}$. $\mathtt{BNGuard}$ leverages the observation that OOD data
causes significant deviations in the running statistics of batch normalization
layers. This allows $\mathtt{BNGuard}$ to identify malicious model updates and
exclude them from aggregation, thereby enhancing the backdoor robustness of FL.
Extensive experiments across various settings show the effectiveness of
$\mathtt{BNGuard}$ on defending against $\mathtt{SoDa}$. The code is available
at https://github.com/JiiahaoXU/SoDa-BNGuard.

</details>


### [151] [Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning](https://arxiv.org/abs/2509.13240)
*Bo Yin,Xingyi Yang,Xinchao Wang*

Main category: cs.LG

TL;DR: NoRA是首个直接调整预训练Transformer模型中非线性激活函数的参数高效微调框架，通过可学习的有理函数替换固定激活函数，在极少的参数更新下（0.4%）达到或超过全参数微调效果。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法主要调整权重矩阵而保持激活函数固定，作者认为激活函数也应该作为模型适配的一等公民进行优化。

Method: 用可学习的有理函数替换固定激活函数，对分子和分母系数应用结构化低秩更新，采用分组设计实现局部适配并提高稳定性。

Result: 在CIFAR数据集上，NoRA仅更新0.4%参数就达到或超过全微调效果；与LoRA结合（NoRA++）在相同训练预算下表现更优；在LLaMA3-8B指令微调中持续提升生成质量，MMLU平均提升0.3%-0.8%。

Conclusion: 激活空间调优是权重基PEFT的补充性高效替代方案，激活函数应作为模型适配的一等对象，NoRA将适配约束到低维函数子空间，隐式正则化更新幅度和方向。

Abstract: Existing parameter-efficient fine-tuning (PEFT) methods primarily adapt
weight matrices while keeping activation functions fixed. We introduce
\textbf{NoRA}, the first PEFT framework that directly adapts nonlinear
activation functions in pretrained transformer-based models. NoRA replaces
fixed activations with learnable rational functions and applies structured
low-rank updates to numerator and denominator coefficients, with a group-wise
design that localizes adaptation and improves stability at minimal cost. On
vision transformers trained on CIFAR-10 and CIFAR-100, NoRA matches or exceeds
full fine-tuning while updating only 0.4\% of parameters (0.02M), achieving
accuracy gains of +0.17\% and +0.27\%. When combined with LoRA
(\textbf{NoRA++}), it outperforms LoRA and DoRA under matched training budgets
by adding fewer trainable parameters. On LLaMA3-8B instruction tuning, NoRA++
consistently improves generation quality, yielding average MMLU gains of
+0.3\%--0.8\%, including +1.6\% on STEM (Alpaca) and +1.3\% on OpenOrca. We
further show that NoRA constrains adaptation to a low-dimensional functional
subspace, implicitly regularizing update magnitude and direction. These results
establish activation-space tuning as a complementary and highly
parameter-efficient alternative to weight-based PEFT, positioning activation
functions as first-class objects for model adaptation.

</details>


### [152] [Post-Hoc Split-Point Self-Consistency Verification for Efficient, Unified Quantification of Aleatoric and Epistemic Uncertainty in Deep Learning](https://arxiv.org/abs/2509.13262)
*Zhizhong Zhao,Ke Chen*

Main category: cs.LG

TL;DR: 提出了一种后处理单前向传播框架SPC-UQ，通过分割点分析(SPA)同时捕捉偶然和认知不确定性，无需重新训练预训练模型，在回归和分类任务中表现优异且计算开销小。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法要么计算成本高（如贝叶斯或集成方法），要么只能提供部分任务特定的估计（如单前向传播技术），需要一种高效且全面的不确定性量化解决方案。

Method: 使用分割点分析(SPA)将预测残差分解为上下子集，计算平均绝对残差(MARs)，通过自一致性差异分数(SDS)进行认知不确定性估计。回归任务中使用分位数回归获得预测区间，分类任务中基于SPA校准softmax输出后计算预测熵。

Result: 在多个回归和分类基准测试中，该方法匹配或超越了多种最先进的不确定性量化方法，同时保持了最小的计算开销。

Conclusion: SPC-UQ框架提供了一种高效、全面的不确定性量化解决方案，能够同时处理偶然和认知不确定性，适用于各种深度学习任务，具有实际应用价值。

Abstract: Uncertainty quantification (UQ) is vital for trustworthy deep learning, yet
existing methods are either computationally intensive, such as Bayesian or
ensemble methods, or provide only partial, task-specific estimates, such as
single-forward-pass techniques. In this paper, we propose a post-hoc
single-forward-pass framework that jointly captures aleatoric and epistemic
uncertainty without modifying or retraining pretrained models. Our method
applies \emph{Split-Point Analysis} (SPA) to decompose predictive residuals
into upper and lower subsets, computing \emph{Mean Absolute Residuals} (MARs)
on each side. We prove that, under ideal conditions, the total MAR equals the
harmonic mean of subset MARs; deviations define a novel \emph{Self-consistency
Discrepancy Score} (SDS) for fine-grained epistemic estimation across
regression and classification. For regression, side-specific quantile
regression yields prediction intervals with improved empirical coverage, which
are further calibrated via SDS. For classification, when calibration data are
available, we apply SPA-based calibration identities to adjust the softmax
outputs and then compute predictive entropy on these calibrated probabilities.
Extensive experiments on diverse regression and classification benchmarks
demonstrate that our framework matches or exceeds several state-of-the-art UQ
methods while incurring minimal overhead.
  Our source code is available at https://github.com/zzz0527/SPC-UQ.

</details>


### [153] [LLMs for energy and macronutrients estimation using only text data from 24-hour dietary recalls: a parameter-efficient fine-tuning experiment using a 10-shot prompt](https://arxiv.org/abs/2509.13268)
*Rodrigo M Carrillo-Larco*

Main category: cs.LG

TL;DR: 使用开源大语言模型通过文本描述预测食物营养成分，经过参数高效微调后准确率显著提升，可用于低负担的饮食监测工具


<details>
  <summary>Details</summary>
Motivation: 现有AI营养估算工具多依赖图像输入，研究探索大语言模型能否仅基于文本描述准确预测营养成分，以简化饮食监测过程

Method: 使用NHANES青少年24小时饮食回顾数据，采用10-shot思维链提示的开源量化LLM预测能量和五种宏量营养素，并进行参数高效微调(PEFT)

Result: 原始LLM预测效果差（能量MAE 652.08，Lin's CCC <0.46），微调后模型性能显著改善（能量MAE 171.34-190.90，Lin's CCC >0.89）

Conclusion: 经过思维链提示和PEFT微调的开源LLM能够仅基于文本输入准确预测饮食回顾中的能量和宏量营养素值，为低负担的文本基饮食监测工具提供了可能

Abstract: BACKGROUND: Most artificial intelligence tools used to estimate nutritional
content rely on image input. However, whether large language models (LLMs) can
accurately predict nutritional values based solely on text descriptions of
foods consumed remains unknown. If effective, this approach could enable
simpler dietary monitoring without the need for photographs. METHODS: We used
24-hour dietary recalls from adolescents aged 12-19 years in the National
Health and Nutrition Examination Survey (NHANES). An open-source quantized LLM
was prompted using a 10-shot, chain-of-thought approach to estimate energy and
five macronutrients based solely on text strings listing foods and their
quantities. We then applied parameter-efficient fine-tuning (PEFT) to evaluate
whether predictive accuracy improved. NHANES-calculated values served as the
ground truth for energy, proteins, carbohydrates, total sugar, dietary fiber
and total fat. RESULTS: In a pooled dataset of 11,281 adolescents (49.9% male,
mean age 15.4 years), the vanilla LLM yielded poor predictions. The mean
absolute error (MAE) was 652.08 for energy and the Lin's CCC <0.46 across
endpoints. In contrast, the fine-tuned model performed substantially better,
with energy MAEs ranging from 171.34 to 190.90 across subsets, and Lin's CCC
exceeding 0.89 for all outcomes. CONCLUSIONS: When prompted using a
chain-of-thought approach and fine-tuned with PEFT, open-source LLMs exposed
solely to text input can accurately predict energy and macronutrient values
from 24-hour dietary recalls. This approach holds promise for low-burden,
text-based dietary monitoring tools.

</details>


### [154] [WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning](https://arxiv.org/abs/2509.13305)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Rui Ye,Yida Zhao,Liwen Zhang,Litu Ou,Dingchu Zhang,Xixi Wu,Jialong Wu,Xinyu Wang,Zile Qiao,Zhen Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.LG

TL;DR: WebSailor是一种后训练方法，通过生成高不确定性任务和代理强化学习训练，使开源模型在复杂信息搜索任务中达到专有代理的性能水平


<details>
  <summary>Details</summary>
Motivation: 超越人类认知限制是LLM训练的关键前沿。专有代理系统在复杂信息搜索基准上表现出超人类能力，而开源模型缺乏系统处理极端不确定性的推理模式

Method: 通过结构化采样和信息模糊化生成新颖的高不确定性任务，采用RFT冷启动和高效的代理强化学习算法DUPO进行训练

Result: WebSailor在复杂信息搜索任务中显著优于所有开源代理，匹配专有代理的性能，缩小了能力差距

Conclusion: 该方法成功地将专有代理系统的关键能力移植到开源模型中，证明了通过系统化训练可以弥补开源与专有模型在复杂推理任务上的性能差距

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all open-source agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [155] [Exploring the entropic region](https://arxiv.org/abs/2509.12439)
*Laszlo Csirmaz*

Main category: cs.IT

TL;DR: 本文探讨了三种获取新熵不等式的方法及其变体和局限性，重点分析了Copy Lemma及其迭代版本、最大熵原理方法、以及Ahlswede-Körner方法，发现Copy Lemma是最强的方法。


<details>
  <summary>Details</summary>
Motivation: 研究如何获得新的熵不等式，特别是非Shannon不等式，以深入理解信息论中的基本限制和关系。

Method: 系统分析三种方法：1) Copy Lemma及其迭代版本和对称化效果；2) 基于最大熵原理的方法；3) Ahlswede-Körner方法，并比较它们的生成能力和局限性。

Result: 发现Copy Lemma是最强大的方法，其迭代版本能生成最多不等式；最大熵原理方法是Copy Lemma的特例；Ahlswede-Körner方法实际上隐含使用了Copy Lemma，单独使用无法生成新不等式。

Conclusion: Copy Lemma是生成新熵不等式的最有效方法，其他方法要么是其特例，要么依赖于它。文章以教程形式呈现，并提出了开放性问题供进一步研究。

Abstract: The paper explores three known methods, their variants and limitations, that
can be used to obtain new entropy inequalities. The Copy Lemma was distilled
from the original Zhang-Yeung construction which produced the first non-Shannon
inequality. Its iterated version, effects of symmetrizations, and connections
with polyhedral vertex enumeration are discussed. Another method, derived from
the principle of maximum entropy, has the Copy Lemma as a special case.
Nevertheless, none of the two presented variants is known to generate more
inequalities than the iterated Copy Lemma. Finally, the Ahlswede-K\"orner
method is shown to employ a hidden application of the Copy Lemma - the
underlying lemma alone cannot generate new inequalities -, which makes this
method strictly weaker than the Copy Lemma. The paper is written in a tutorial
style and concludes with a list of open questions and research problems.

</details>


### [156] [Channel Estimation for Rydberg Atomic Quantum Receivers](https://arxiv.org/abs/2509.12586)
*Jian Xiao,Ji Wang,Ming Zeng,Hongbo Xu,Xingwang Li,Arumugam Nallanathan*

Main category: cs.IT

TL;DR: 提出URformer，一种基于Transformer展开的深度学习框架，用于解决Rydberg原子量子接收器的非线性信道估计问题，显著优于传统迭代算法和黑盒神经网络


<details>
  <summary>Details</summary>
Motivation: Rydberg原子量子接收器(RAQRs)具有独特的基于偏置相位检索的非线性信号模型，传统迭代算法在低信噪比下表现不佳且无法捕捉复杂系统特性

Method: 通过展开稳定化的期望最大化Gerchberg-Saxton(EM-GS)算法，构建Transformer架构URformer，包含可学习滤波器、自适应门控机制和高效信道Transformer块三个可训练模块

Result: 数值结果表明URformer显著优于经典迭代算法和传统黑盒神经网络，且需要更少的导频开销

Conclusion: URformer为RAQRs信道估计提供了有效的深度学习解决方案，能够处理非线性信号模型并提升估计性能

Abstract: The advent of Rydberg atomic quantum receivers (RAQRs) offers a new solution
for the evolution of wireless transceiver architecture, promising unprecedented
sensitivity and immunity to thermal noise. However, RAQRs introduce a unique
non-linear signal model based on biased phase retrieval, which complicates
fundamental channel estimation tasks. Traditional iterative algorithms often
struggle in low signal-to-noise regimes and fail to capture complex and
non-ideal system characteristics. To address this, we propose a novel
model-driven deep learning framework for channel estimation in RAQRs.
Specifically, we propose a Transformer-based unrolling architecture, termed
URformer, which is derived by unrolling a stabilized variant of the
expectation-maximization Gerchberg-Saxton (EM-GS) algorithm. Specifically, each
layer of the proposed URformer incorporates three trainable modules: 1) a
learnable filter implemented by a neural network that replaces the fixed Bessel
function ratio in the classic EM-GS algorithm; 2) a trainable gating mechanism
that adaptively combines classic and model-based updates to ensure training
stability; and 3) a efficient channel Transformer block that learns to correct
residual errors by capturing non-local dependencies across the channel matrix.
Numerical results demonstrate that the proposed URformer significantly
outperforms classic iterative algorithms and conventional black-box neural
networks with less pilot overhead.

</details>


### [157] [Three Classes of Twisted Gabidulin Codes with Different Twists](https://arxiv.org/abs/2509.12693)
*Ran Li,Fang-Wei Fu,Weijun Fang*

Main category: cs.IT

TL;DR: 本文研究了三种不同类型的扭曲Gabidulin码，建立了它们成为最大秩距离(MRD)码的充要条件，确定了非MRD码的条件，并通过扭曲Gabidulin码构造了多类MRD码。同时分析了这些码在汉明度量下的MDS特性以及覆盖半径和深洞问题。


<details>
  <summary>Details</summary>
Motivation: 扭曲Gabidulin码作为Gabidulin码的扩展近年来受到广泛关注，但对其不同扭曲类型、MRD特性、MDS特性以及覆盖性质的系统研究尚不充分。

Method: 研究三类具有不同扭曲的扭曲Gabidulin码，建立数学条件分析其MRD和MDS特性，通过理论推导确定充要条件，并构造新的MRD码类。

Result: 确定了三类扭曲Gabidulin码成为MRD码的充要条件，发现了非MRD码的条件，成功构造了多个MRD码类，并给出了在汉明度量下成为MDS、几乎MDS或近MDS码的条件，同时研究了覆盖半径和深洞特性。

Conclusion: 扭曲Gabidulin码具有丰富的代数结构和编码特性，通过适当的扭曲设计可以构造出性能优良的MRD码和MDS码，为编码理论提供了新的构造方法和理论结果。

Abstract: Twisted Gabidulin codes are an extension of Gabidulin codes and have recently
attracted great attention. In this paper, we study three classes of twisted
Gabidulin codes with different twists. Moreover, we establish necessary and
sufficient conditions for them to be maximum rank distance (MRD) codes,
determine the conditions under which they are not MRD codes, and construct
several classes of MRD codes via twisted Gabidulin codes. In addition,
considering these codes in the Hamming metric, we provide necessary and
sufficient conditions for them to be maximum distance separable (MDS), almost
MDS, or near MDS. Finally, we investigate the covering radii and deep holes of
twisted Gabidulin codes.

</details>


### [158] [Linear Complexity Computation of Code Distance and Minimum Size of Trapping Sets for LDPC Codes with Bounded Treewidth](https://arxiv.org/abs/2509.13040)
*Qingqing Peng,Ke Liu,Guiying Yan,Guanghui Wang*

Main category: cs.IT

TL;DR: 本文证明了对于具有有界树宽的二进制线性码，寻找最小a的(a,b)陷阱集问题可以在线性复杂度内解决，并提供了具体算法。


<details>
  <summary>Details</summary>
Motivation: 在二进制线性码中寻找最小a的(a,b)陷阱集是NP难问题，但对于具有有界树宽的码，可以找到更高效的解决方案。

Method: 利用树分解技术，针对树宽有界的二进制线性码，设计了线性复杂度的算法来计算最小a值和对应的(a,b)陷阱集数量。

Result: 提出的算法具有线性复杂度，仿真实验验证了算法的正确性。

Conclusion: 对于树宽有界的二进制线性码，原本NP难的最小陷阱集寻找问题可以在线性时间内解决，这为相关编码理论问题提供了新的计算途径。

Abstract: It is well known that, given \(b\ge 0\), finding an $(a,b)$-trapping set with
the minimum \(a\) in a binary linear code is NP-hard. In this paper, we
demonstrate that this problem can be solved with linear complexity with respect
to the code length for codes with bounded treewidth. Furthermore, suppose a
tree decomposition corresponding to the treewidth of the binary linear code is
known. In that case, we also provide a specific algorithm to compute the
minimum \(a\) and the number of the corresponding \((a, b)\)-trapping sets for
a given \(b\) with linear complexity. Simulation experiments are presented to
verify the correctness of the proposed algorithm.

</details>
