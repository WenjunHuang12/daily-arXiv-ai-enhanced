<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 11]
- [cs.IT](#cs.IT) [Total: 40]
- [cs.DB](#cs.DB) [Total: 13]
- [cs.DS](#cs.DS) [Total: 14]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.IR](#cs.IR) [Total: 20]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Temporal Fair Division of Indivisible Goods with Scheduling](https://arxiv.org/abs/2601.12835)
*Kui Wang Choi,Minming LI*

Main category: cs.GT

TL;DR: 研究多轮分配中的时间公平分割，探讨TEF1、TEFX、α-TEFX和TMMS等概念，在受限设置和引入调度机制下分析可能性和不可能性边界。


<details>
  <summary>Details</summary>
Motivation: 针对标准设置中已知的不可能性结果，研究时间公平分割在受限场景下的可行性，通过引入调度机制探索实现公平分配的边界条件。

Method: 采用理论分析方法，研究无调度和有调度两种情况下的时间公平分割。在无调度情况下分析常数因子α-TEFX的可行性；在有调度情况下研究调度缓冲区大小对实现TEF1的影响。

Result: 1) 无调度时，一般情况常数因子α-TEFX不可能，但广义二元估值和相同天数两智能体可实现1/2近似；2) 有调度时，大小至少n/2的缓冲区可实现相同天数下的TEF1；3) TEFX和TMMS即使在有调度或受限域中仍基本不可能。

Conclusion: 严格时间公平具有内在困难，需要权衡近似保证。调度机制能改善某些公平概念，但TEFX和TMMS等严格标准即使在受限条件下也难以实现。

Abstract: We study temporal fair division, where agents receive goods over multiple rounds and cumulative fairness is required. We investigate Temporal Envy-Freeness Up to One Good (TEF1) and Up to Any Good (TEFX), its approximation $α$-TEFX, and Temporal Maximin Share (TMMS). Motivated by known impossibilities in standard settings, we consider the model in various restricted settings and extend it by introducing scheduling.
  Our main contributions draw the boundary between possibility and impossibility. First, regarding temporal fair division without scheduling, we prove that while constant-factor $α$-TEFX is impossible in general, a $1/2$-approximation is achievable for generalized binary valuations and identical days with two agents. Second, regarding temporal fair division with scheduling, we demonstrate that a scheduling buffer of size at least $n/2$ enables TEF1 for identical days. However, we establish that TEFX and TMMS remain largely impossible even with scheduling or restricted domains. These results highlight the inherent difficulty of strict temporal fairness and quantify the trade-offs required to achieve approximation guarantees.

</details>


### [2] [The Cost of EFX: Generalized-Mean Welfare and Complexity Dichotomies with Few Surplus Items](https://arxiv.org/abs/2601.12849)
*Eugene Lim,Tzeh Yuan Neoh,Nicholas Teh*

Main category: cs.GT

TL;DR: EFX公平分配与p-均值福利的复杂性分析：p>0时NP难，p≤0时可多项式时间优化，并量化了公平性代价


<details>
  <summary>Details</summary>
Motivation: 研究在少量剩余物品（最多3个）情况下，EFX公平分配与广义均值福利（包括功利主义、纳什、平等主义）之间的计算复杂性和效率权衡

Method: 建立复杂性二分法：分析不同p值下EFX分配与p-均值福利优化的计算复杂性，使用价格公平性框架量化福利损失，并研究EFX与帕累托最优的组合复杂性

Result: p>0时EFX优化是NP难的，p≤0时可多项式时间优化；p>0时福利损失随代理数线性增长，p≤0时损失有界；EFX与帕累托最优组合是NP难的

Conclusion: 在少量剩余物品设置中，EFX的计算成本与福利最大化之间的对齐程度取决于p值：p≤0时结构对齐且可高效计算，p>0时计算困难且福利损失显著

Abstract: Envy-freeness up to any good (EFX) is a central fairness notion for allocating indivisible goods, yet its existence is unresolved in general. In the setting with few surplus items, where the number of goods exceeds the number of agents by a small constant (at most three), EFX allocations are guaranteed to exist, shifting the focus from existence to efficiency and computation. We study how EFX interacts with generalized-mean ($p$-mean) welfare, which subsumes commonly-studied utilitarian ($p=1$), Nash ($p=0$), and egalitarian ($p \rightarrow -\infty$) objectives. We establish sharp complexity dichotomies at $p=0$: for any fixed $p \in (0,1]$, both deciding whether EFX can attain the global $p$-mean optimum and computing an EFX allocation maximizing $p$-mean welfare are NP-hard, even with at most three surplus goods; in contrast, for any fixed $p \leq 0$, we give polynomial-time algorithms that optimize $p$-mean welfare within the space of EFX allocations and efficiently certify when EFX attains the global optimum. We further quantify the welfare loss of enforcing EFX via the price of fairness framework, showing that for $p > 0$, the loss can grow linearly with the number of agents, whereas for $p \leq 0$, it is bounded by a constant depending on the surplus (and for Nash welfare it vanishes asymptotically). Finally we show that requiring Pareto-optimality alongside EFX is NP-hard (and becomes $Σ_2^P$-complete for a stronger variant of EFX). Overall, our results delineate when EFX is computationally costly versus structurally aligned with welfare maximization in the setting with few surplus items.

</details>


### [3] [The Cost of Failure: On The Complexity of Recampaigning under Fixed Districts](https://arxiv.org/abs/2601.13246)
*Michael C. Chavrimootoo,Aidan Jeansonne*

Main category: cs.GT

TL;DR: 研究选举重划选区后，失败政党如何通过战略性地重新分配候选人到不同选区来改变选举结果，称为"再竞选"问题


<details>
  <summary>Details</summary>
Motivation: 现有计算社会选择研究主要关注如何划定"适当"选区，但本研究关注失败政党在选区已确定的情况下，是否还能通过战略性地重新分配候选人到特定选区来赢得选举

Method: 将再竞选建模为计算问题，考虑自然变体，通过多项式时间多一归约、分离/坍塌（无条件与公理充分）、最坏情况和参数化复杂度等角度进行研究

Result: 未在摘要中明确说明具体结果，但研究框架已建立，包括多种复杂性分析方法

Conclusion: 提出了选举重划选区问题的新视角，关注失败政党的战略选择空间，为计算社会选择领域提供了新的研究方向

Abstract: Redistricting efforts have gathered contemporary attention in both quotidian and scholarly debates, particularly in the United States where efforts to redraw congressional districts to favor either of the two major parties in 12 states -- such as California, Texas, and Ohio -- have captured the public eye. The treatment of redistricting in computational social choice has essentially focused on the process of determining "appropriate" districts. In this work, we are interested in understanding the gamut of options left for the "losing" party, and so we consider the flip side of the problem: Given fixed/predetermined districts, can a given party still make their candidates win by strategically placing them in certain districts? We dub this as "recampaigning" to capture the intuition that a party would redirect their campaigning efforts from one district to another. We model recampaigning as a computational problem, consider natural variations of the model, and study those new models through the lens of (1) (polynomial-time many-one) interreducibilities, (2) separations/collapses (both unconditional and axiomatic-sufficient), and (3) both worst-case and parametrized complexity.

</details>


### [4] [Tight Asymptotic Bounds for Fair Division With Externalities](https://arxiv.org/abs/2601.13287)
*Frank Connor,Max Dupré la Tour,Vishnu V. Narayan,Šimon Schierreich*

Main category: cs.GT

TL;DR: 该论文研究了具有外部性的不可分割物品分配问题，证明了EF1分配不一定存在，但EF-O(√n)分配总是存在且可多项式时间找到，并给出了匹配的下界。


<details>
  <summary>Details</summary>
Motivation: 传统公平分配模型假设代理人只关心自己获得的物品，但在现实场景中，代理人可能对分配给其他代理人的物品产生正或负的效用（外部性）。由于精确的无嫉妒性无法保证，先前研究关注其松弛形式，但两个核心问题未解决：是否存在EF1分配？如果不能，最优的EF-k松弛是什么？

Method: 作者通过推导消除嫉妒所需物品数量的紧渐近界来解决问题。他们证明了对于任意n个代理人的实例，总是存在一个EF-O(√n)分配，并且可以在多项式时间内找到。同时证明了匹配的Ω(√n)下界，即使对于二元估值也成立。

Result: 主要结果：1) EF1分配在具有外部性的情况下不一定存在；2) 总是存在EF-O(√n)分配且可多项式时间计算；3) 证明了匹配的Ω(√n)下界，表明结果是最优的；4) 该下界即使在二元估值下也成立。

Conclusion: 该研究完全解决了具有外部性的公平分配中的两个核心问题：EF1分配不一定存在，最优的松弛是EF-Θ(√n)。这为具有外部性的公平分配理论提供了完整的渐近刻画。

Abstract: We study the problem of allocating a set of indivisible items among agents whose preferences include externalities. Unlike the standard fair division model, agents may derive positive or negative utility not only from items allocated directly to them, but also from items allocated to other agents. Since exact envy-freeness cannot be guaranteed, prior work has focused on its relaxations. However, two central questions remained open: does there always exist an allocation that is envy-free up to one item (EF1), and if not, what is the optimal relaxation EF-$k$ that can always be attained?
  We settle both questions by deriving tight asymptotic bounds on the number of items sufficient to eliminate envy. We show that for any instance with $n$ agents, an allocation that is envy-free up to $O(\sqrt{n})$ items always exists and can be found in polynomial time, and we prove a matching $Ω(\sqrt{n})$ lower bound showing that this result is tight even for binary valuations, which rules out the existence of EF1 allocations when agents have externalities.

</details>


### [5] [Bridging the Gap Between Estimated and True Regret Towards Reliable Regret Estimation in Deep Learning based Mechanism Design](https://arxiv.org/abs/2601.13489)
*Shuyuan You,Zhiqiang Zhuang,Kewen Wang,Zhe Wang*

Main category: cs.GT

TL;DR: 现有基于深度学习的拍卖机制（如RegretNet等）通过近似方法评估激励相容性，但实验发现这些方法系统性低估了实际后悔值，导致对IC和收益的夸大。本文提出了一种后悔值下界和高效的项目级后悔近似方法，并设计了引导优化过程来显著提高后悔估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的多物品拍卖机制（如RegretNet、ALGnet、RegretFormer、CITransNet）通过放松激励相容性约束并使用事后后悔值来衡量IC违反程度。然而，这些后悔估计的真实准确性尚不清楚。由于计算精确后悔值在计算上不可行，现有模型依赖梯度优化器，其结果严重依赖于超参数选择，导致对IC和收益的评估可能存在偏差。

Method: 1. 通过大量实验揭示现有方法系统性低估实际后悔值的问题；2. 推导后悔值的下界；3. 提出高效的项目级后悔近似方法；4. 基于此设计引导优化过程，显著提高后悔估计的准确性并降低计算成本。

Result: 实验发现现有方法严重低估实际后悔值（在某些模型中，真实后悔值比报告值大数百倍），导致对激励相容性和收益的夸大。提出的方法能够提供更可靠的后悔估计，为基于深度学习的拍卖机制评估提供了更坚实的基础。

Conclusion: 本文揭示了现有深度学习拍卖机制在后悔估计方面的系统性偏差问题，提出了更准确的后悔估计方法。研究强调了重新评估该领域先前性能声明的必要性，并为评估拍卖机制的激励相容性提供了更可靠的基础。

Abstract: Recent advances, such as RegretNet, ALGnet, RegretFormer and CITransNet, use deep learning to approximate optimal multi item auctions by relaxing incentive compatibility (IC) and measuring its violation via ex post regret. However, the true accuracy of these regret estimates remains unclear. Computing exact regret is computationally intractable, and current models rely on gradient based optimizers whose outcomes depend heavily on hyperparameter choices. Through extensive experiments, we reveal that existing methods systematically underestimate actual regret (In some models, the true regret is several hundred times larger than the reported regret), leading to overstated claims of IC and revenue. To address this issue, we derive a lower bound on regret and introduce an efficient item wise regret approximation. Building on this, we propose a guided refinement procedure that substantially improves regret estimation accuracy while reducing computational cost. Our method provides a more reliable foundation for evaluating incentive compatibility in deep learning based auction mechanisms and highlights the need to reassess prior performance claims in this area.

</details>


### [6] [Concurrent Permissive Strategy Templates](https://arxiv.org/abs/2601.13500)
*Ashwani Anand,Christel Baier,Calvin Chau,Sascha Klüppelholz,Ali Mirzaei,Satya Prakash Nayak,Anne-Kathrin Schmuck*

Main category: cs.GT

TL;DR: 提出并发策略模板(ConSTels)，用于在并发游戏中紧凑表示无限随机化获胜策略，支持离线和在线适应


<details>
  <summary>Details</summary>
Motivation: 并发游戏语义能自然捕捉网络物理系统的同步交互特性，但在CPS设计中应用有限；需要一种能表示策略集合并支持适应的表示方法

Method: 基于回合制游戏的许可策略模板(PeSTels)，引入并发策略模板(ConSTels)，支持安全、Büchi和Co-Büchi目标；利用组合性实现增量合成，支持运行时概率调整

Result: 实现了ConSTel合成和适应的原型工具，实验展示了其潜力

Conclusion: ConSTels为并发游戏提供了一种紧凑的策略表示方法，支持离线和在线适应，有助于网络物理系统的设计和验证

Abstract: Two-player games on finite graphs provide a rigorous foundation for modeling the strategic interaction between reactive systems and their environment. While concurrent game semantics naturally capture the synchronous interactions characteristic of many cyber-physical systems (CPS), their adoption in CPS design remains limited. Building on the concept of permissive strategy templates (PeSTels) for turn-based games, we introduce concurrent (permissive) strategy templates (ConSTels) -- a novel representation for sets of randomized winning strategies in concurrent games with Safety, Büchi, and Co-Büchi objectives. ConSTels compactly encode infinite families of strategies, thereby supporting both offline and online adaptation. Offline, we exploit compositionality to enable incremental synthesis: combining ConSTels for simpler objectives into non-conflicting templates for more complex combined objectives. Online, we demonstrate how ConSTels facilitate runtime adaptation, adjusting action probabilities in response to observed opponent behavior to optimize performance while preserving correctness. We implemented ConSTel synthesis and adaptation in a prototype tool and experimentally show its potential.

</details>


### [7] [Stochastic Dynamic Pricing of Electric Vehicle Charging with Heterogeneous User Behavior: A Stackelberg Game Framework](https://arxiv.org/abs/2601.13571)
*Yongqi Zhang,Dong Ngoduy,Li Duan,Mingchang Zhu,Zhuo Chen*

Main category: cs.GT

TL;DR: 提出一个考虑行为异质性的随机动态定价框架，用于电动汽车充电站管理，通过双层Stackelberg博弈优化系统效用，避免网络均衡约束以提高可扩展性。


<details>
  <summary>Details</summary>
Motivation: 电动汽车快速普及带来了复杂的时空需求管理挑战，传统动态定价模型过度简化用户行为且缺乏可扩展性，无法有效处理需求不平衡、行为异质性和系统不确定性。

Method: 采用双层Stackelberg博弈框架：上层优化时变定价以最大化系统效用；下层通过多项式logit选择模型模拟分散的EV用户行为，考虑价格敏感性、电池老化、风险态度和网络旅行成本。避免网络均衡约束，使用排队论近似表示拥堵效应。采用滚动时域方法结合动态概率敏感性分析引导的交叉熵方法和连续平均法求解大规模优化问题。

Result: 在墨尔本Clayton地区22个充电站的真实案例验证表明，相比固定定价和分时定价，所提机制显著减少了排队惩罚并提高了用户效用。

Conclusion: 该框架为战略性电动汽车充电管理提供了稳健、可扩展的工具，在现实性和计算效率之间取得了平衡。

Abstract: The rapid adoption of electric vehicles (EVs) introduces complex spatiotemporal demand management challenges for charging station operators (CSOs), exacerbated by demand imbalances, behavioral heterogeneity, and system uncertainty. Traditional dynamic pricing models, often relying on deterministic EV-CS pairings and network equilibrium assumptions, frequently oversimplify user behavior and lack scalability. This study proposes a stochastic, behaviorally heterogeneous dynamic pricing framework formulated as a bi-level Stackelberg game. The upper level optimizes time-varying pricing to maximize system-wide utility, while the lower level models decentralized EV users via a multinomial logit (MNL) choice model incorporating price sensitivity, battery aging, risk attitudes, and network travel costs. Crucially, the model avoids network equilibrium constraints to enhance scalability, with congestion effects represented via queuing-theoretic approximations. To efficiently solve the resulting large-scale optimization problem, a rolling-horizon approach combining the Dynamic Probabilistic Sensitivity Analysis-guided Cross-Entropy Method (PSA-CEM) with the Method of Successive Averages (MSA) is implemented. A real-world case study in Clayton, Melbourne, validates the framework using 22 charging stations. Simulation results demonstrate that the proposed mechanism substantially reduces queuing penalties and improves user utility compared to fixed and time-of-use pricing. The framework provides a robust, scalable tool for strategic EV charging management, balancing realism with computational efficiency.

</details>


### [8] [Asymmetric regularization mechanism for GAN training with Variational Inequalities](https://arxiv.org/abs/2601.13920)
*Spyridon C. Giagtzoglou,Mark H. M. Winands,Barbara Franci*

Main category: cs.GT

TL;DR: 将GAN训练表述为纳什均衡寻求问题，提出基于Tikhonov步长和零中心梯度惩罚的非对称正则化机制，在特定条件下获得显式Lipschitz和强单调常数，证明EFTP方法的最后迭代线性收敛。


<details>
  <summary>Details</summary>
Motivation: 生成对抗网络（GAN）训练通常不稳定，难以收敛到纳什均衡。现有方法缺乏理论保证，需要一种能够稳定训练并确保收敛到均衡的正则化机制。

Method: 提出非对称正则化机制，结合经典Tikhonov步长和新型零中心梯度惩罚。在由高斯-牛顿格拉姆矩阵诱导的光滑性和局部可识别性条件下，推导正则化算子的显式Lipschitz和强单调常数。使用单次调用的"从过去外推"（EFTP）方法。

Result: 获得显式Lipschitz和强单调常数，确保EFTP方法的最后迭代线性收敛。即使在无法实现强单调性的情况下，非对称正则化仍能收敛到均衡并稳定轨迹。

Conclusion: 将GAN训练表述为纳什均衡寻求问题，提出的非对称正则化机制能够稳定训练过程并确保收敛到纳什均衡，为GAN训练提供了理论保证和实用方法。

Abstract: We formulate the training of generative adversarial networks (GANs) as a Nash equilibrium seeking problem. To stabilize the training process and find a Nash equilibrium, we propose an asymmetric regularization mechanism based on the classic Tikhonov step and on a novel zero-centered gradient penalty. Under smoothness and a local identifiability condition induced by a Gauss-Newton Gramian, we obtain explicit Lipschitz and (strong)-monotonicity constants for the regularized operator. These constants ensure last-iterate linear convergence of a single-call Extrapolation-from-the-Past (EFTP) method. Empirical simulations on an academic example show that, even when strong monotonicity cannot be achieved, the asymmetric regularization is enough to converge to an equilibrium and stabilize the trajectory.

</details>


### [9] [BallotRank: A Condorcet Completion Method for Graphs](https://arxiv.org/abs/2601.14015)
*Ismar Volic,Jason Douglas Todd*

Main category: cs.GT

TL;DR: BallotRank是一种基于改进PageRank算法的排名偏好聚合方法，具有Condorcet一致性且无需阻尼，能提供完整的候选人排名。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够识别Condorcet胜者并提供完整候选人排名的偏好聚合方法，同时满足多种社会选择标准。

Method: 基于修改的PageRank算法开发BallotRank方法，这是一种无阻尼的Condorcet一致性方法，通过分析近2000个排名选择选举和超过20000个互联网投票进行实证验证。

Result: BallotRank在常规阻尼参数值下总能识别Condorcet胜者，并证明该方法满足与其他知名Condorcet完成方法相同的多种社会选择标准。

Conclusion: BallotRank是一种自然的社交福利函数，能够提供候选人的完整排名，具有作为Condorcet完成方法的优势。

Abstract: We introduce BallotRank, a ranked preference aggregation method derived from a modified PageRank algorithm. It is a Condorcet-consistent method without damping, and empirical examination of nearly 2,000 ranked choice elections and over 20,000 internet polls confirms that BallotRank always identifies the Condorcet winner at conventional values of the damping parameter. We also prove that the method satisfies many of the same social choice criteria as other well-known Condorcet completion methods, but it has the advantage of being a natural social welfare function that provides a full ranking of the candidates.

</details>


### [10] [Collective intelligence in science: direct elicitation of diverse information from experts with unknown information structure](https://arxiv.org/abs/2601.14047)
*Alexey V. Osipov,Nikolay N. Osipov*

Main category: cs.GT

TL;DR: 提出一种基于游戏货币预测市场与聊天系统相结合的新机制，用于集体分析复杂科学问题，实现专家私人信息的高效聚合


<details>
  <summary>Details</summary>
Motivation: 解决开放科学问题需要整合大量互不相识专家的多样化私人信息，传统方法难以处理这种信息聚合挑战

Method: 设计自解析的游戏货币预测市场与聊天系统纠缠机制，专家通过聊天分享信息并在市场交易，系统能自动达到均衡状态

Result: 该机制能使专家直接分享私人信息，市场交易如同假设真实情况已知，实现信息高效聚合且完全可解释

Conclusion: 通过游戏货币奖励转换为真实资产，为大规模协作研究提供创新资助方式，适用于任何类型研究

Abstract: Suppose we need a deep collective analysis of an open scientific problem: there is a complex scientific hypothesis and a large online group of mutually unrelated experts with relevant private information of a diverse and unpredictable nature. This information may be results of experts' individual experiments, original reasoning of some of them, results of AI systems they use, etc. We propose a simple mechanism based on a self-resolving play-money prediction market entangled with a chat. We show that such a system can easily be brought to an equilibrium where participants directly share their private information on the hypothesis through the chat and trade as if the market were resolved in accordance with the truth of the hypothesis. This approach will lead to efficient aggregation of relevant information in a completely interpretable form even if the ground truth cannot be established and experts initially know nothing about each other and cannot perform complex Bayesian calculations. Finally, by rewarding the experts with some real assets proportionally to the play money they end up with, we can get an innovative way to fund large-scale collaborative studies of any type.

</details>


### [11] [A Minimax Perspective on Almost-Stable Matchings](https://arxiv.org/abs/2601.14195)
*Frederik Glitzner,David Manlove*

Main category: cs.GT

TL;DR: 该论文提出了一种基于最小最大原则的公平稳定性匹配方法，旨在最小化任何个体面临的最大阻塞对数量，而非传统上最小化总阻塞对数量，以解决稳定性匹配中的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 在现实匹配市场（如医院住院医师分配、室友分配）中，完全稳定性往往无法实现或会导致大量参与者无法匹配。现有"近似稳定"匹配方法关注聚合指标（最小化总阻塞对数量），但这可能导致少数个体承受不成比例的稳定性负担，引发公平性和激励问题。

Method: 引入基于最小最大原则的公平稳定性概念：寻找最小化任何个体面临的最大阻塞对数量的匹配。等价于最小化任何人对他人产生合理嫉妒的最大数量。研究在不同基础匹配设置下的计算复杂性。

Result: 发现即使非常温和的保证也是计算难解的：当偏好列表长度有常数界时，判断是否存在每个个体最多只在一个阻塞对中的匹配是NP完全的。这适用于稳定室友问题和最大基数稳定婚姻问题。正面结果：当个体最多只对两个其他个体排序时，有多项式时间算法，并提供了近似算法和整数规划。

Conclusion: 该研究绘制了算法图景，揭示了分布保证与计算可行性之间的基本权衡。最小最大公平稳定性方法提供了保护最弱势个体免受不成比例稳定性负担的框架，但计算复杂性限制了其实际应用范围。

Abstract: Stability is crucial in matching markets, yet in many real-world settings - from hospital residency allocations to roommate assignments - full stability is either impossible to achieve or can come at the cost of leaving many agents unmatched. When stability cannot be achieved, algorithmicists and market designers face a critical question: how should instability be measured and distributed among participants? Existing approaches to "almost-stable" matchings focus on aggregate measures, minimising either the total number of blocking pairs or the count of agents involved in blocking pairs. However, such aggregate objectives can result in concentrated instability on a few individual agents, raising concerns about fairness and incentives to deviate. We introduce a fairness-oriented approach to approximate stability based on the minimax principle: we seek matchings that minimise the maximum number of blocking pairs any agent is in. Equivalently, we minimise the maximum number of agents that anyone has justified envy towards. This distributional objective protects the worst-off agents from a disproportionate amount of instability. We characterise the computational complexity of this notion across fundamental matching settings. Surprisingly, even very modest guarantees prove computationally intractable: we show that it is NP-complete to decide whether a matching exists in which no agent is in more than one blocking pair, even when preference lists have constant-bounded length. This hardness applies to both Stable Roommates and maximum-cardinality Stable Marriage. On the positive side, we provide polynomial-time algorithms when agents rank at most two others, and present approximation algorithms and integer programs. Our results map the algorithmic landscape and reveal fundamental trade-offs between distributional guarantees and computational feasibility.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [12] [Age-Based Scheduling for a Memory-Constrained Quantum Switch](https://arxiv.org/abs/2601.11698)
*Stavros Mitrolaris,Subhankar Banerjee,Sennur Ulukus*

Main category: cs.IT

TL;DR: 研究量子交换机中多部分纠缠请求的调度问题，考虑有限量子内存、概率性纠缠生成与交换、以及单时隙退相干，提出基于纠缠建立年龄（AoEE）的新性能度量，开发并比较了三种低复杂度调度策略。


<details>
  <summary>Details</summary>
Motivation: 随着量子网络的发展，需要在具有有限量子内存的量子交换机中有效调度多部分纠缠请求。现有研究缺乏考虑实际约束（如概率性纠缠生成、交换和退相干）的调度策略性能评估方法。

Method: 1) 引入纠缠建立年龄（AoEE）作为新的性能度量；2) 开发两个策略家族并推导其AoEE闭式表达式；3) 优化每个家族得到两个策略；4) 提出第三种低复杂度策略并提供性能保证；5) 数值比较三种策略性能。

Result: 获得了两个策略家族的闭式AoEE表达式，通过优化得到两个具体策略，并提出了第三个具有性能保证的策略。数值比较显示了不同策略在量子交换机调度中的性能差异。

Conclusion: 该研究为量子网络中的多部分纠缠调度提供了理论框架和实用策略，提出的AoEE度量能有效评估调度性能，三种低复杂度策略为实际量子交换机设计提供了可行方案。

Abstract: In a time-slotted system, we study the problem of scheduling multipartite entanglement requests in a quantum switch with a finite number of quantum memory registers. Specifically, we consider probabilistic link-level entanglement (LLE) generation for each user, probabilistic entanglement swapping, and one-slot decoherence. To evaluate the performance of the proposed scheduling policies, we introduce a novel age-based metric, coined age of entanglement establishment (AoEE). We consider two families of low-complexity policies for which we obtain closed-form expressions for their corresponding AoEE performance. Optimizing over each family, we obtain two policies. Further, we propose one more low-complexity policy and provide its performance guarantee. Finally, we numerically compare the performance of the proposed policies.

</details>


### [13] [Asymptotically Optimal Tests for One- and Two-Sample Problems](https://arxiv.org/abs/2601.11727)
*Arick Grootveld,Biao Chen,Venkata Gandikota*

Main category: cs.IT

TL;DR: 本文重新审视了单样本和双样本检验问题，为Hoeffding似然比检验的渐近最优性提供了更简洁的证明，并将其自然扩展到双样本检验，证明了类似形式的检验也是渐近最优的。


<details>
  <summary>Details</summary>
Motivation: 本文旨在重新审视假设检验中的单样本和双样本问题，特别是在一个或两个分布未知的情况下。作者希望为Hoeffding似然比检验的渐近最优性提供更直观、更简洁的证明，并将该方法扩展到双样本检验场景。

Method: 对于单样本检验，作者提供了Hoeffding似然比检验渐近最优性的更简洁证明，该检验等价于经验分布与名义分布之间相对熵的阈值检验。然后将这种方法自然扩展到双样本检验，提出了两个经验分布之间相对熵的阈值检验形式。

Result: 证明了单样本检验中Hoeffding似然比检验的渐近最优性，并成功将其扩展到双样本检验，证明了类似形式的检验也是渐近最优的。同时获得了双样本检验的强逆定理。

Conclusion: 本文为单样本和双样本检验问题提供了统一的理论框架，通过相对熵阈值检验的方法实现了渐近最优性，为未知分布情况下的假设检验提供了有效的解决方案。

Abstract: In this work, we revisit the one- and two-sample testing problems: binary hypothesis testing in which one or both distributions are unknown. For the one-sample test, we provide a more streamlined proof of the asymptotic optimality of Hoeffding's likelihood ratio test, which is equivalent to the threshold test of the relative entropy between the empirical distribution and the nominal distribution. The new proof offers an intuitive interpretation and naturally extends to the two-sample test where we show that a similar form of Hoeffding's test, namely a threshold test of the relative entropy between the two empirical distributions is also asymptotically optimal. A strong converse for the two-sample test is also obtained.

</details>


### [14] [The Noisy Quantitative Group Testing Problem](https://arxiv.org/abs/2601.11797)
*Tenghao Li,Neha Sangwan,Xiaxin Li,Arya Mazumdar*

Main category: cs.IT

TL;DR: 本文研究了定量群检测问题，分析了三种模型（无噪声、加性高斯噪声、噪声Z信道）和两种算法（基于相关分数的线性估计器、最小二乘估计器），推导了精确恢复所需测试次数的上界，并给出了信息论下界。


<details>
  <summary>Details</summary>
Motivation: 研究定量群检测问题，分析不同噪声模型下的算法性能，为精确恢复提供理论保证。

Method: 分析了三种模型：无噪声模型、加性高斯噪声模型、噪声Z信道模型；研究了两种算法：基于相关分数的线性估计器和最小二乘估计器。

Result: 推导了精确恢复所需测试次数的上界，并给出了信息论下界；在加性高斯噪声设置中，上下界在阶数上匹配。

Conclusion: 为定量群检测问题提供了理论分析框架，在加性高斯噪声模型中获得了紧致的上下界结果。

Abstract: In this paper, we study the problem of quantitative group testing (QGT) and analyze the performance of three models: the noiseless model, the additive Gaussian noise model, and the noisy Z-channel model. For each model, we analyze two algorithmic approaches: a linear estimator based on correlation scores, and a least squares estimator (LSE). We derive upper bounds on the number of tests required for exact recovery with vanishing error probability, and complement these results with information-theoretic lower bounds. In the additive Gaussian noise setting, our lower and upper bounds match in order.

</details>


### [15] [Bayesian ICA for Causal Discovery](https://arxiv.org/abs/2601.11815)
*Joe Suzuki*

Main category: cs.IT

TL;DR: 提出一个基于信息论和贝叶斯ICA的因果顺序估计框架，可处理任意混杂因素，通过量化噪声变量间的多元互信息来评估混杂程度，而非假设噪声完全独立。


<details>
  <summary>Details</summary>
Motivation: 传统LiNGAM方法假设存在一个因果顺序使得噪声项完全独立，但这个强假设在有混杂因素时经常被违反。需要建立一个更通用的框架来处理任意混杂情况下的因果发现。

Method: 将因果顺序选择建模为排列上的模型选择问题，使用贝叶斯边际似然估计噪声变量间的多元互信息，将互信息分解为沿因果顺序的互信息项之和。采用非高斯预测模型（如多元t分布）避免高斯噪声导致的不可识别性，通过MCMC评估边际似然。

Result: 提出的贝叶斯互信息估计器在标准正则条件下具有一致性，冗余度为O(log n)。该方法在无混杂时恢复经典LiNGAM和DirectLiNGAM，在有混杂时提供因果顺序的排序，建立了统一的、考虑混杂的、基于信息论的ICA因果发现扩展。

Conclusion: 建立了一个通用的信息论框架，将因果顺序估计扩展到存在任意混杂的情况，通过量化混杂程度而非假设完全独立，为ICA-based因果发现提供了理论基础和实用方法。

Abstract: Causal discovery based on Independent Component Analysis (ICA) has achieved remarkable success through the LiNGAM framework, which exploits non-Gaussianity and independence of noise variables to identify causal order. However, classical LiNGAM methods rely on the strong assumption that there exists an ordering under which the noise terms are exactly independent, an assumption that is often violated in the presence of confounding. In this paper, we propose a general information-theoretic framework for causal order estimation that remains applicable under arbitrary confounding. Rather than imposing independence as a hard constraint, we quantify the degree of confounding by the multivariate mutual information among the noise variables. This quantity is decomposed into a sum of mutual information terms along a causal order and is estimated using Bayesian marginal likelihoods. The resulting criterion can be interpreted as Bayesian ICA for causal discovery, where causal order selection is formulated as a model selection problem over permutations. Under standard regularity conditions, we show that the proposed Bayesian mutual information estimator is consistent, with redundancy of order $O(\log n)$. To avoid non-identifiability caused by Gaussian noise, we employ non-Gaussian predictive models, including multivariate $t$ distributions, whose marginal likelihoods can be evaluated via MCMC. The proposed method recovers classical LiNGAM and DirectLiNGAM as limiting cases in the absence of confounding, while providing a principled ranking of causal orders when confounding is present. This establishes a unified, confounding-aware, and information-theoretically grounded extension of ICA-based causal discovery.

</details>


### [16] [On the Rényi Rate-Distortion-Perception Function and Functional Representations](https://arxiv.org/abs/2601.11862)
*Jiahui Wei,Marios Kountouris*

Main category: cs.IT

TL;DR: 将率失真感知框架扩展到Rényi信息论体系，利用Sibson α-互信息刻画失真和感知约束下的基本极限，针对高斯信源得到闭式解，并揭示了最优函数表示的相变现象。


<details>
  <summary>Details</summary>
Motivation: 传统率失真感知框架基于香农互信息，本文旨在将其推广到更一般的Rényi信息论体系，以探索在广义互信息概念下压缩共享随机性的新特性。

Method: 使用Sibson α-互信息作为信息度量，推导Rényi率失真感知函数，针对标量高斯信源获得闭式表达式，并建立Rényi广义强函数表示引理。

Result: 发现感知约束导致再生方差的可行区间，揭示了最优函数表示的相变：当0.5<α<1时编码成本有界且需要重尾码本；当α>1时表示坍缩为有限支撑集。

Conclusion: Rényi率失真感知框架为广义互信息下的压缩问题提供了新视角，揭示了编码复杂度的相变现象，对共享随机性压缩有重要理论意义。

Abstract: We extend the Rate-Distortion-Perception (RDP) framework to the Rényi information-theoretic regime, utilizing Sibson's $α$-mutual information to characterize the fundamental limits under distortion and perception constraints. For scalar Gaussian sources, we derive closed-form expressions for the Rényi RDP function, showing that the perception constraint induces a feasible interval for the reproduction variance. Furthermore, we establish a Rényi-generalized version of the Strong Functional Representation Lemma. Our analysis reveals a phase transition in the complexity of optimal functional representations: for $0.5<α< 1$, the coding cost is bounded by the $α$-divergence of order $α+1$, necessitating a codebook with heavy-tailed polynomial decay; conversely, for $α> 1$, the representation collapses to one with finite support, offering new insights into the compression of shared randomness under generalized notions of mutual information.

</details>


### [17] [Rate-Distortion-Classification Representation Theory for Bernoulli Sources](https://arxiv.org/abs/2601.11919)
*Nam Nguyen,Thinh Nguyen,Bella Bose*

Main category: cs.IT

TL;DR: 研究任务导向的有损压缩，通过率-失真-分类表示视角，针对伯努利源、汉明失真和二元对称分类模型，推导闭式表征并分析通用编码器的性能边界。


<details>
  <summary>Details</summary>
Motivation: 研究任务导向的有损压缩问题，探索如何在压缩过程中同时优化失真和分类性能，为实际应用中的智能压缩系统提供理论框架。

Method: 采用率-失真-分类表示方法，基于单次公共随机性公式推导闭式表征，通过线性规划分析固定表示下的失真-分类区域，并研究支持多种操作点的通用编码器。

Result: 推导了单次RDC和DRC权衡的闭式表征，通过线性规划获得了固定表示下的失真-分类区域边界，并为通用编码器的最小渐近速率提供了可计算的上界。

Conclusion: 建立了任务导向压缩的理论框架，为同时优化压缩失真和分类性能提供了系统方法，并通过数值示例验证了所提方法的有效性。

Abstract: We study task-oriented lossy compression through the lens of rate-distortion-classification (RDC) representations. The source is Bernoulli, the distortion measure is Hamming, and the binary classification variable is coupled to the source via a binary symmetric model. Building on the one-shot common-randomness formulation, we first derive closed-form characterizations of the one-shot RDC and the dual distortion-rate-classification (DRC) tradeoffs. We then use a representation-based viewpoint and characterize the achievable distortion-classification (DC) region induced by a fixed representation by deriving its lower boundary via a linear program. Finally, we study universal encoders that must support a family of DC operating points and derive computable lower and upper bounds on the minimum asymptotic rate required for universality, thereby yielding bounds on the corresponding rate penalty. Numerical examples are provided to illustrate the achievable regions and the resulting universal RDC/DRC curves.

</details>


### [18] [Exact Redundancy for Symmetric Rate-Distortion](https://arxiv.org/abs/2601.11927)
*Sharang M. Sriramu,Aaron B. Wagner*

Main category: cs.IT

TL;DR: 该论文研究了在几乎确定失真约束下的变长编码冗余问题，证明了对于满足特定对称条件的均匀信源，log n/(2n) 是最优可达的冗余率，且即使将失真约束放宽为期望约束也无法改进。


<details>
  <summary>Details</summary>
Motivation: Zhang等人先前的研究表明，对于离散信源，在几乎确定失真约束下，冗余的上界为log n/n，下界（在大多数情况下）为log n/(2n)。本文旨在进一步探究对于特定类型的信源（均匀信源），这个下界是否可达，以及约束条件放宽后是否能有改进。

Method: 针对满足特定对称条件的均匀信源，作者分析了在几乎确定失真约束下的变长编码问题。通过理论推导，证明了log n/(2n)的冗余率是可达的，并且进一步证明了即使将失真约束从几乎确定放宽为期望约束，这个结果也无法改进。

Result: 对于满足对称条件的均匀信源，log n/(2n)的冗余率是最优可达的。更重要的是，即使将失真约束条件从几乎确定放宽为期望约束，也无法获得比log n/(2n)更好的冗余率，这表明该界限具有鲁棒性。

Conclusion: 该研究确定了在几乎确定失真约束下，对于特定类型的均匀信源，变长编码的最优冗余率为log n/(2n)，并且这个界限即使在更宽松的期望失真约束下也无法改进，为相关编码理论提供了重要的理论边界。

Abstract: For variable-length coding with an almost-sure distortion constraint, Zhang et al. show that for discrete sources the redundancy is upper bounded by $\log n/n$ and lower bounded (in most cases) by $\log n/(2n)$, ignoring lower order terms. For a uniform source with a distortion measure satisfying certain symmetry conditions, we show that $\log n/(2n)$ is achievable and that this cannot be improved even if one relaxes the distortion constraint to be in expectation rather than with probability one.

</details>


### [19] [Small-Error Cascaded Group Testing](https://arxiv.org/abs/2601.11945)
*Daniel McMorrow,Nikhil Karamchandani,Sidharth Jaggi*

Main category: cs.IT

TL;DR: 该论文研究级联群测试模型，其中测试结果指示按顺序排列的测试中第一个缺陷项，建立了多种恢复准则下的可实现性界限。


<details>
  <summary>Details</summary>
Motivation: 传统群测试模型使用二进制测试结果（仅指示测试中是否存在缺陷项），而级联群测试模型通过测试结果指示按顺序排列的测试中第一个缺陷项，提供了更丰富的测试信息，能够更有效地识别缺陷项。

Method: 研究级联群测试模型，建立多种恢复准则下的可实现性界限，包括非自适应测试设计和自适应（少量阶段）测试设计两种方法。

Result: 为级联群测试模型建立了多种恢复准则下的可实现性界限，证明了在不同测试设计策略下的性能表现。

Conclusion: 级联群测试模型相比传统二进制测试模型提供了更丰富的测试信息，通过适当的测试设计策略可以实现更高效的缺陷项恢复，为群测试问题提供了新的研究方向。

Abstract: Group testing concerns itself with the accurate recovery of a set of "defective" items from a larger population via a series of tests. While most works in this area have considered the classical group testing model, where tests are binary and indicate the presence of at least one defective item in the test, we study the cascaded group testing model. In cascaded group testing, tests admit an ordering, and test outcomes indicate the first defective item in the test under this ordering. Under this model, we establish various achievability bounds for several different recovery criteria using both non-adaptive and adaptive (with "few" stages) test designs.

</details>


### [20] [Generalizing the Fano inequality further](https://arxiv.org/abs/2601.12027)
*Raghav Bongole,Tobias J. Oechtering,Mikael Skoglund*

Main category: cs.IT

TL;DR: 提出了一种改进的交互式统计决策下界框架，通过随机化单比特统计量推广交互式Fano方法，可推导有界损失的条件风险价值下界


<details>
  <summary>Details</summary>
Motivation: 现有交互式统计决策的信息论下界主要针对期望风险，对尾部敏感目标（如CVaR）的研究不足，需要开发更通用的下界框架

Method: 将交互式Fano框架中的硬成功事件替换为表示损失任意有界变换的随机化单比特统计量，推导Bernoulli f-散度不等式，通过反演得到变换的双侧区间

Result: 建立了适用于有界损失条件风险价值（CVaR）的下界，特别针对KL散度与混合参考分布的情况，通过Pinsker不等式得到互信息的显式表达式

Conclusion: 提出的随机化单比特统计量方法扩展了交互式Fano框架，为尾部敏感目标提供了更通用的信息论下界分析工具

Abstract: Interactive statistical decision making (ISDM) features algorithm-dependent data generated through interaction. Existing information-theoretic lower bounds in ISDM largely target expected risk, while tail-sensitive objectives are less developed. We generalize the interactive Fano framework of Chen et al. by replacing the hard success event with a randomized one-bit statistic representing an arbitrary bounded transform of the loss. This yields a Bernoulli f-divergence inequality, which we invert to obtain a two-sided interval for the transform, recovering the previous result as a special case. Instantiating the transform with a bounded hinge and using the Rockafellar-Uryasev representation, we derive lower bounds on the prior-predictive (Bayesian) CVaR of bounded losses. For KL divergence with the mixture reference distribution, the bound becomes explicit in terms of mutual information via Pinsker's inequality.

</details>


### [21] [Function Computation Over Multiple Access Channels via Hierarchical Constellations](https://arxiv.org/abs/2601.12050)
*Saeed Razavikia,Mohammad Kazemi,Deniz Gündüz,Carlo Fischione*

Main category: cs.IT

TL;DR: 提出基于分层星座设计的空中计算框架，支持单信道使用计算多个函数输出，通过屏蔽机制提高可靠性，实现低延迟、信道无关的函数计算。


<details>
  <summary>Details</summary>
Motivation: 研究高斯多址信道上的函数计算问题，多个发射机需要在公共接收机处计算其值的函数。现有方法在计算效率和可靠性方面存在限制，需要设计支持多函数输出、高可靠性的低延迟计算框架。

Method: 提出基于分层星座设计的编码调制框架，支持单信道使用计算多个函数输出。引入基于变长分组编码的屏蔽机制，减轻噪声引起的跨星座层级错误传播，同时保持多址信道的叠加结构。

Result: 所提分层星座框架的计算速率与最优速率之间的差距按O(log₂(1/ε)/K)缩放，随网络规模增大而消失。屏蔽机制将差距改进为O(log₂ln(1/ε))，与源分布无关，显著提高可靠性。

Conclusion: 该工作确定了未编码或轻编码空中计算在信息论上最优的机制，为低延迟、信道无关的函数计算提供了统一框架，分层星座设计和屏蔽机制相结合实现了高效可靠的多函数计算。

Abstract: We study function computation over a Gaussian multiple-access channel (MAC), where multiple transmitters aim at computing a function of their values at a common receiver. To this end, we propose a novel coded-modulation framework for over-the-air computation (OAC) based on hierarchical constellation design, which supports reliable computation of multiple function outputs using a single channel use. Moreover, we characterize the achievable computation rate and show that the proposed hierarchical constellations can compute R output functions with decoding error probability epsilon while the gap to the optimal computation rate scales as O(\log_2(1/ε)/K) for independent source symbols, where K denotes the number of transmitters. Consequently, this gap vanishes as the network size grows, and the optimal rate is asymptotically attained.
  Furthermore, we introduce a shielding mechanism based on variable-length block coding that mitigates noise-induced error propagation across constellation levels while preserving the superposition structure of the MAC. We show that the shielding technique improves reliability, yielding a gap that scales optimally as O(\log_2\ln{(1/ε)}), regardless of the source distribution. Together, these results identify the regimes in which uncoded or lightly coded OAC is information-theoretically optimal, providing a unified framework for low-latency, channel-agnostic function computation.

</details>


### [22] [On the Construction and Correlation Properties of Permutation-Interleaved Zadoff-Chu Sequences](https://arxiv.org/abs/2601.12107)
*Qin Yuan,Chunlei Li,Xiangyong Zeng*

Main category: cs.IT

TL;DR: 本文提出了一种通过置换多项式交错ZC序列生成CAZAC序列的新方法，证明了所得序列与现有序列不等价，并验证了Berggren和Popović猜想的充分性。


<details>
  <summary>Details</summary>
Motivation: CAZAC序列在雷达和通信系统中应用广泛。受Berggren和Popović近期工作的启发，本文进一步研究通过置换多项式交错ZC序列来生成CAZAC序列的方法。

Method: 提出一类在整数环Z_N上的高阶置换多项式，利用这些多项式及其逆函数交错ZC序列来构造CAZAC序列。同时评估了来自二次多项式的ZC序列的非周期自相关特性。

Result: 所获得的CAZAC序列与ZC序列及通过二次置换多项式及其逆函数交错的ZC序列的等价类不同，并证明了Berggren和Popović猜想的充分性。

Conclusion: 通过高阶置换多项式交错ZC序列可以生成新的CAZAC序列，这些序列与现有序列不等价，为CAZAC序列的构造提供了新方法。

Abstract: Constant amplitude zero auto-correlation (CAZAC) sequences are widely applied in waveforms for radar and communication systems. Motivated by a recent work [Berggren and Popović, IEEE Trans. Inf. Theory 70(8), 6068-6075 (2024)], this paper further investigates the approach to generating CAZAC sequences by interleaving Zadoff-Chu (ZC) sequences with permutation polynomials (PPs). We propose one class of high-degree PPs over the integer ring Z N , and utilize them and their inverses to interleave ZC sequences for constructing CAZAC sequences. It is known that a CAZAC sequence can be extended to an equivalence class by five basic opertations. We further show that the obtained CAZAC sequences are not covered by the equivalence classes of ZC sequences and interleaved ZC sequences by quadratic PPs and their inverses, and prove the sufficiency of the conjecture by Berggren and Popović in the aforementioned work. In addition, we also evaluate the aperiodic auto-correlation of certain ZC sequences from quadratic PPs.

</details>


### [23] [Coherent Comparison as Information Cost: A Cost-First Ledger Framework for Discrete Dynamics](https://arxiv.org/abs/2601.12194)
*Sebastian Pardo-Guerra,Megan Simons,Anil Thapa,Jonathan Washburn*

Main category: cs.IT

TL;DR: 提出基于比率比较成本的信息论框架，推导出唯一互易成本函数，应用于有向图上的离散账本系统，在最小化假设下得到原子时间戳和平衡双分录，最终连接比率散度、保守图流和离散势理论。


<details>
  <summary>Details</summary>
Motivation: 建立离散动力学的信息论框架，通过比率比较来量化偏离平衡的程度，为有向图上的识别事件提供最小化无损编码的基础。

Method: 1) 定义比率比较成本函数，要求满足乘法链的相干组合、归一化和二次校准；2) 推导出唯一的互易成本函数J(x)；3) 将J(x)应用于有向图上的离散账本系统；4) 在确定性更新语义和最小化假设下推导原子时间戳；5) 引入结构假设（守恒、无源汇、成对局部性、量子化）强制平衡双分录；6) 通过时间聚合循环闭合获得标量势。

Result: 1) 证明了唯一的互易成本函数J(x)=½(x+x⁻¹)-1；2) 在最小化假设下得到原子时间戳（每时间单位最多一个事件）；3) 结构假设强制平衡双分录和离散账本单位；4) 时间聚合循环闭合等价于路径无关性，允许在连通分量上定义唯一标量势；5) 在超立方图Q_d上，原子性强制2^d时间单位的最小周期，并在d=3时给出格雷码实现。

Conclusion: 该框架通过相干强制成本结构，将基于比率的散度、保守图流和离散势理论统一起来，为离散动力学提供了信息论基础，并在有向图账本系统中实现了最小化无损编码和原子时间戳。

Abstract: We develop an information-theoretic framework for discrete dynamics grounded in a comparison-cost functional on ratios. Given two quantities compared via their ratio \(x=a/b\), we assign a cost \(F(x)\) measuring deviation from equilibrium (\(x=1\)). Requiring coherent composition under multiplicative chaining imposes a d'Alembert functional equation; together with normalization (\(F(1)=0\)) and quadratic calibration at unity, this yields a unique reciprocal cost functional (proved in a companion paper): \[ J(x) = \tfrac{1}{2}\bigl(x + x^{-1}\bigr) - 1. \] This cost exhibits reciprocity \(J(x)=J(x^{-1})\), vanishes only at \(x=1\), and diverges at boundary regimes \(x\to 0^+\) and \(x\to\infty\), excluding ``nothingness'' configurations. Using \(J\) as input, we introduce a discrete ledger as a minimal lossless encoding of recognition events on directed graphs. Under deterministic update semantics and minimality (no intra-tick ordering metadata), we derive atomic ticks (at most one event per tick). Explicit structural assumptions (conservation, no sources/sinks, pairwise locality, quantization in \(δ\mathbb{Z}\)) force balanced double-entry postings and discrete ledger units. To obtain scalar potentials on graphs with cycles while retaining single-edge impulses per tick, we impose time-aggregated cycle closure (no-arbitrage/clearing over finite windows). Under this hypothesis, cycle closure is equivalent to path-independence, and the cleared cumulative flow admits a unique scalar potential on each connected component (up to additive constant), via a discrete Poincaré lemma. On hypercube graphs \(Q_d\), atomicity imposes a \(2^d\)-tick minimal period, with explicit Gray-code realization at \(d=3\). The framework connects ratio-based divergences, conservative graph flows, and discrete potential theory through a coherence-forced cost structure.

</details>


### [24] [Classical-Quantum Channel Resolvability Using Matrix Multiplicative Weight Update Algorithm](https://arxiv.org/abs/2601.12230)
*Koki Takahashi,Shun Watanabe*

Main category: cs.IT

TL;DR: 首次提出使用确定性编码（而非随机编码）证明经典-量子信道可分辨性，采用矩阵乘法权重更新算法


<details>
  <summary>Details</summary>
Motivation: 经典-量子信道可分辨性在文献中仅通过随机编码证明，需要探索确定性编码方法

Method: 将确定性编码方法扩展到经典-量子信道，采用矩阵乘法权重更新算法

Result: 首次实现了经典-量子信道可分辨性的确定性编码证明

Conclusion: 成功将确定性编码方法应用于经典-量子信道可分辨性，为量子信息理论提供了新的编码方法

Abstract: We study classical-quantum (C-Q) channel resolvability. C-Q channel resolvability has been proved by only random coding in the literature. In our previous study, we proved channel resolvability by deterministic coding, using multiplicative weight update algorithm. We extend this approach to C-Q channels and prove C-Q channel resolvability by deterministic coding, using the matrix multiplicative weight update algorithm. This is the first approach to C-Q channel resolvability using deterministic coding.

</details>


### [25] [On the Minimum Length of Functional Batch Codes with Small Recovery Sets](https://arxiv.org/abs/2601.12302)
*Kristiina Oksner,Henk D. L. Hollmann,Ago-Erik Riet,Vitaly Skachek*

Main category: cs.IT

TL;DR: 本文研究具有小查询复杂度的线性功能批处理码，推导了此类码的最小长度界限，并通过数值计算进行了评估。


<details>
  <summary>Details</summary>
Motivation: 功能批处理码在分布式数据存储系统中具有负载均衡和私有信息检索的潜在应用价值。与标准批处理码只能查询信息符号不同，功能批处理码允许用户查询信息符号的线性组合，这在实际应用中更为灵活。

Method: 研究线性功能批处理码，特别关注每个查询只需使用少量编码符号的特性。通过理论分析推导此类码的最小长度界限，并采用数值计算方法对结果进行评估。

Result: 获得了具有小查询复杂度的线性功能批处理码的最小长度界限，并通过数值计算验证了这些界限的有效性。

Conclusion: 本文为具有小查询复杂度的线性功能批处理码建立了理论界限，为分布式存储系统中的高效查询设计提供了理论基础。

Abstract: Batch codes are of potential use for load balancing and private information retrieval in distributed data storage systems. Recently, a special case of batch codes, termed functional batch codes, was proposed in the literature. In functional batch codes, users can query linear combinations of the information symbols, and not only the information symbols themselves, as is the case for standard batch codes. In this work, we consider linear functional batch codes with the additional property that every query is answered by using only a small number of coded symbols. We derive bounds on the minimum length of such codes, and evaluate the results by numerical computations.

</details>


### [26] [$2$-quasi-perfect Lee codes and abelian Ramanujan graphs: a new construction and relationship](https://arxiv.org/abs/2601.12393)
*Shohei Satake*

Main category: cs.IT

TL;DR: 提出了一种基于阿贝尔（几乎）拉马努金图生成集的新显式2-准完美Lee码族，长度可任意大


<details>
  <summary>Details</summary>
Motivation: 构建具有良好纠错性能的Lee码，特别是2-准完美Lee码，这些码在通信和编码理论中有重要应用

Method: 利用Forey、Fresán、Kowalski和Wigderson获得的阿贝尔（几乎）拉马努金图的生成集来构造新码族

Result: 获得了长度可任意大的新显式2-准完美Lee码族，并建立了某些阿贝尔拉马努金图与Mesnager、Tang和Qi获得的2-准完美Lee码之间的关系

Conclusion: 通过阿贝尔拉马努金图的生成集成功构造了新的2-准完美Lee码族，为Lee码的设计提供了新的数学工具和理论框架

Abstract: In this paper, we obtain a new explicit family of $2$-quasi-perfect Lee codes of arbitrarily large length. Our construction is based on generating sets of abelian (almost) Ramanujan graphs obtained by Forey, Fresán, Kowalski and Wigderson. Also, we develop a relationship between certain abelian Ramanujan graphs and $2$-quasi-perfect Lee codes obtained by Mesnager, Tang and Qi.

</details>


### [27] [Privacy via Modulation Rotation and Inter-Symbol Interference](https://arxiv.org/abs/2601.12394)
*Morteza Varasteh,Pegah Sharifi*

Main category: cs.IT

TL;DR: 提出两种物理层机制实现用户侧差分隐私：基于BPSK调制相位旋转和故意引入ISI，无需显式噪声注入即可提供隐私保证


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私机制依赖人工添加噪声，带来额外能量或通信成本。本文探索利用通信系统中固有的硬件非理想性和实现误差（如相位旋转、ISI）来实现隐私，避免显式数据扰动

Method: 1. 旋转BPSK：发射机应用确定性相位旋转，接收机不知旋转角度，通过减小判决距离实现隐私；2. ISI方案：故意引入确定性定时偏移产生ISI，接收机不知定时偏移参数。两种方法都无需显式噪声注入

Result: 旋转BPSK通过控制BER增加实现隐私；ISI方案的隐私损失依赖于输入数据分布，当二进制输入符号等概率时隐私损失最大。两种结构化修改都能提供差分隐私保证

Conclusion: 可以利用通信系统中固有的硬件非理想性（如相位旋转、ISI）实现差分隐私，无需显式噪声注入，避免额外的隐私成本。这为利用设备固有缺陷实现隐私保护提供了新思路

Abstract: Two physical-layer mechanisms for achieving user-side differential privacy in communication systems are proposed. Focusing on binary phase-shift keying (BPSK) modulation, differential privacy (DP) is first studied under a deterministic phase rotation applied on the BPSK modulation at the transmitter, while the receiver is assumed to be unaware of the rotation angle. In this setting, privacy is achieved through an effective reduction in the decision distance, resulting in a controlled increase in the bit error rate (BER) without explicit noise injection. Next, a BPSK transmission scheme with intentionally induced inter-symbol interference (ISI) is studied, where the receiver is likewise unaware of the deterministic timing offset that generates the ISI. Unlike the rotated BPSK scheme, the DP obtained via ISI is shown to depend explicitly on the input data distribution. In particular, numerical results demonstrate that, for a fixed ISI parameter, the privacy loss is maximized when the binary input symbols are equiprobable. While conventional DP mechanisms rely on artificially added noise, often incurring additional energy or communication costs, it is shown that structured modifications, such as modulation rotation or induced ISI inherent to realistic communication channels can itself provide DP guarantees. While the analysis focuses on deterministic transmitter modifications unknown to the receiver, it is noted that real-world devices naturally introduce unintentional rotations or ISI due to hardware nonidealities and implementation errors. These effects can therefore provide a level of privacy without requiring explicit noise injection. Hence, it is possible to avoid deliberately perturbing the data, instead leveraging inherent device imperfections to achieve privacy guarantees with no additional privacy cost.

</details>


### [28] [Counterexamples, Constructions, and Nonexistence Results for Optimal Ternary Cyclic Codes](https://arxiv.org/abs/2601.12427)
*Jingjun Bao,Hanlin Zou*

Main category: cs.IT

TL;DR: 该论文研究了三元循环码的开放问题，为第三和第四个问题提供了首个反例，并在特定条件下构造了两类最优码，部分解决了第三个问题。同时研究了特定指数形式的循环码，对a≡3(mod 4)的情况构造了两类新的最优码，对a≡1(mod 4)的情况得到了几个不存在性结果。


<details>
  <summary>Details</summary>
Motivation: 2013年Ding和Helleseth提出了关于最优三元循环码的九个开放问题，目前只有前两个和第六个问题被完全解决，其他问题仍然开放。本文旨在推进第三和第四个问题的研究，并为特定形式的循环码提供新的构造和不存在性结果。

Method: 1. 为第三和第四个开放问题提供首个反例；2. 在特定条件下构造两类最优码；3. 研究指数形式为e(3^h±1)≡(3^m-a)/2 (mod 3^m-1)且a为奇数的循环码；4. 对a≡3(mod 4)的情况构造新的最优码；5. 对a≡1(mod 4)的情况分析不存在性条件。

Result: 1. 首次为第三和第四个开放问题提供了反例；2. 部分解决了第三个问题；3. 对a≡3(mod 4)的情况构造了两类新的最优三元循环码，参数为[3^m-1, 3^m-1-2m, 4]；4. 对a≡1(mod 4)的情况得到了几个最优码不存在的结果，揭示了此类码的约束条件。

Conclusion: 本文在Ding和Helleseth提出的三元循环码开放问题研究上取得了重要进展，为第三和第四个问题提供了首个反例，并部分解决了第三个问题。同时扩展了特定指数形式的最优码构造，为a≡3(mod 4)的情况提供了新的最优码族，为a≡1(mod 4)的情况揭示了不存在性条件，深化了对三元循环码结构的理解。

Abstract: Cyclic codes are an important subclass of linear codes with wide applications in communication systems and data storage systems. In 2013, Ding and Helleseth presented nine open problems on optimal ternary cyclic codes $\mathcal{C}_{(1,e)}$. While the first two and the sixth problems have been fully solved, others remain open. In this paper, we advance the study of the third and fourth open problems by providing the first counterexamples to both and constructing two families of optimal codes under certain conditions, thereby partially solving the third problem. Furthermore, we investigate the cyclic codes $\mathcal{C}_{(1,e)}$ where $e(3^h\pm 1)\equiv\frac{3^m-a}{2}\pmod{3^m-1}$ and $a$ is odd. For $a\equiv 3\pmod{4}$, we present two new families of optimal codes with parameters $[3^m-1,3^m-1-2m,4]$, generalizing known constructions. For $a\equiv 1\pmod{4}$, we obtain several nonexistence results on optimal codes $\mathcal{C}_{(1,e)}$ with the aforementioned parameters revealing the constraints of such codes.

</details>


### [29] [The Origin of the Inaccessible Game](https://arxiv.org/abs/2601.12576)
*Neil D. Lawrence*

Main category: cs.IT

TL;DR: 论文提出了一个基于信息几何的"不可及游戏"框架，通过最大熵产生和边际熵守恒研究信息损失动力学。在量子版本中，使用冯·诺依曼熵允许存在具有零联合熵和正边际熵的"原点"状态，并推导了约束梯度流和熵时间参数化。


<details>
  <summary>Details</summary>
Motivation: 研究信息损失动力学中的几何框架，解决经典香农熵框架下无法表示零联合熵与正边际熵共存的问题，建立量子信息几何理论以描述边际熵守恒条件下的最大熵产生过程。

Method: 采用Baez-Fritz-Leinster-Parzygnat范畴框架，将香农熵替换为冯·诺依曼熵，定义LME（局部最大纠缠）原点状态，推导约束梯度流，引入熵时间参数化，分析动力学分解为对称耗散和可逆分量。

Result: 量子框架允许存在全局纯态与最大混合边际的"原点"，边际熵守恒成为二阶几何条件，约束梯度流在矩阵指数族中可解，熵时间参数化将无限仿射时间映射到有限区间，动力学分解为SEA实现和幺正演化。

Conclusion: 量子信息几何框架成功解决了经典熵理论的限制，建立了边际熵守恒的几何表述，为研究非平衡热力学中的信息损失动力学提供了新的理论工具，将局部模哈密顿量期望守恒与热力学固定能量约束联系起来。

Abstract: The inaccessible game is an information-geometric framework where dynamics of information loss emerge from maximum entropy production under marginal-entropy conservation.
  We study the game's starting state, the origin. Classical Shannon entropy forbids a representation with zero joint entropy and positive marginal entropies: non-negativity of conditional entropy rules this out. Replacing Shannon with von Neumann entropy within the Baez Fritz Leinster Parzygnat categorical framework removes this obstruction and admits a well-defined origin: a globally pure state with maximally mixed marginals, selected up to local-unitary equivalence. At this LME origin, marginal-entropy conservation becomes a second-order geometric condition. Because the marginal-entropy sum is saturated termwise, the constraint gradient vanishes and first-order tangency is vacuous; admissible directions are selected by the kernel of the constraint Hessian, characterised by the marginal-preserving tangent space.
  We derive the constrained gradient flow in the matrix exponential family and show that, as the origin is approached, the affine time parameter degenerates. This motivates an axiomatically distinguished reparametrisation, entropy time $t$, defined by $dH/dt = c$ for fixed constant $c>0$. In this parametrisation, the infinite affine-time approach to the boundary maps to a finite entropy-time interval. The constrained dynamics split into a symmetric dissipative component realising SEA and a reversible component represented as unitary evolution.
  As in the classical game, marginal-entropy conservation is equivalent to conservation of a sum of local modular Hamiltonian expectations, a state-dependent "modular energy"; in Gibbs regimes where local modular generators become approximately parameter-invariant, this reduces to familiar fixed-energy constraints from nonequilibrium thermodynamics.

</details>


### [30] [Beyond Identification: Computing Boolean Functions via Channels](https://arxiv.org/abs/2601.12640)
*Jingge Zhu,Matthias Frey*

Main category: cs.IT

TL;DR: 该论文研究了一种通信系统，其中发送方持有长度为m的二进制消息，接收方需要恢复该消息的某个布尔函数值，而该函数对发送方未知但来自已知函数类F。论文探讨了m和n之间的渐近关系，定义了计算容量，并对特定函数类（由汉明权重表征）给出了可达性和逆界结果。


<details>
  <summary>Details</summary>
Motivation: 研究在发送方不知道接收方具体需要计算哪个布尔函数的情况下，如何设计通信系统使得接收方能够可靠地恢复该函数值。这扩展了Ahlswede和Dueck提出的通过信道识别框架，探索更一般的函数计算问题。

Method: 定义了计算容量的概念，针对由汉明权重表征的函数类F，推导了可达性（存在性）和逆界（不可能性）结果。通过分析消息长度m和码字长度n之间的渐近关系来研究该问题。

Result: 对于论文中考虑的所有函数类F，所获得的结果在缩放行为意义上是紧的（即可达界和逆界匹配）。具体给出了不同函数类下m和n之间的渐近关系。

Conclusion: 该论文成功地将识别通过信道框架推广到更一般的函数计算问题，针对特定函数类建立了紧的计算容量结果，为未知函数计算通信系统提供了理论基础。

Abstract: Consider a point-to-point communication system in which the transmitter holds a binary message of length $m$ and transmits a corresponding codeword of length $n$. The receiver's goal is to recover a Boolean function of that message, where the function is unknown to the transmitter, but chosen from a known class $F$. We are interested in the asymptotic relationship of $m$ and $n$: given $n$, how large can $m$ be (asymptotically), such that the value of the Boolean function can be recovered reliably? This problem generalizes the identification-via-channels framework introduced by Ahlswede and Dueck. We formulate the notion of computation capacity, and derive achievability and converse results for selected classes of functions $F$, characterized by the Hamming weight of functions. Our obtained results are tight in the sense of the scaling behavior for all cases of $F$ considered in the paper.

</details>


### [31] [Explicit Entropic Constructions for Coverage, Facility Location, and Graph Cuts](https://arxiv.org/abs/2601.12724)
*Rishabh Iyer*

Main category: cs.IT

TL;DR: 论文证明了许多实际应用中常用的单调次模函数（如集合覆盖、设施选址、图割等）都可以精确表示为香农熵，从而建立了组合信息度量与经典信息论之间的直接桥梁。


<details>
  <summary>Details</summary>
Motivation: 次模函数和组合信息度量（SIMs）已被广泛应用于数据子集选择、主动学习等领域，但一个基本问题是：实践中常用的单调次模函数是否都是熵的？香农熵是次模函数，但并非所有次模函数都是熵的。本文旨在回答这个基础问题。

Method: 为广泛使用的多拟阵函数提供明确的熵构造，包括：集合覆盖和覆盖函数、设施选址、饱和覆盖、通过截断的凹-模函数、单调图割型目标。证明这些函数可以精确表示为适当构造的随机变量的香农熵。

Result: 对于这些函数类，次模互信息与经典互信息一致，条件增益特化为条件熵，次模条件互信息简化为标准的条件互信息。这些函数确实可以表示为香农熵。

Conclusion: 论文肯定地回答了基础问题：实践中常用的许多单调次模函数确实是熵的。这建立了组合信息度量与经典信息论之间的直接联系，为这些函数在信息论框架下的应用提供了理论基础。

Abstract: Shannon entropy is a polymatroidal set function and lies at the foundation of information theory, yet the class of entropic polymatroids is strictly smaller than the class of all submodular functions. In parallel, submodular and combinatorial information measures (SIMs) have recently been proposed as a principled framework for extending entropy, mutual information, and conditional mutual information to general submodular functions, and have been used extensively in data subset selection, active learning, domain adaptation, and representation learning. This raises a natural and fundamental question: are the monotone submodular functions most commonly used in practice entropic?
  In this paper, we answer this question in the affirmative for a broad class of widely used polymatroid functions. We provide explicit entropic constructions for set cover and coverage functions, facility location, saturated coverage, concave-over-modular functions via truncations, and monotone graph-cut-type objectives. Our results show that these functions can be realized exactly as Shannon entropies of appropriately constructed random variables. As a consequence, for these functions, submodular mutual information coincides with classical mutual information, conditional gain specializes to conditional entropy, and submodular conditional mutual information reduces to standard conditional mutual information in the entropic sense. These results establish a direct bridge between combinatorial information measures and classical information theory for many of the most common submodular objectives used in applications.

</details>


### [32] [Extended Gabidulin-Kronecker Product Codes and Their Application to Cryptosystems](https://arxiv.org/abs/2601.12780)
*Zhe Sun,Terry Shue Chien Lau,Mengying Zhao,Zimeng Zhou,Fang-Wei Fu*

Main category: cs.IT

TL;DR: 本文研究了具有Kronecker乘积结构的扩展Gabidulin码，提出了三种增强版RQC密码系统变体，包括新的MRD码族、EGK码解码算法，以及更小公钥尺寸的密码方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于秩的密码系统存在公钥尺寸大、解密失败概率非零等缺陷，需要开发更高效、更安全的方案。通过研究Gabidulin-Kronecker乘积码的结构特性，可以构建新的MRD码族和改进的密码系统。

Method: 1) 建立Gabidulin-Kronecker乘积码的最小秩距离精确界；2) 提出扩展Gabidulin-Kronecker乘积(EGK)码及其直接恢复码字的解码算法；3) 基于EGK码设计三种RQC密码系统变体，平衡安全性与效率。

Result: 1) 发现新的MRD码族，不同于经典Gabidulin码；2) EGK码解码算法实现零解码失败概率；3) 三种RQC变体在128位安全级别下，相比Multi-UR-AG显著减小公钥尺寸，同时确保零解密失败概率。

Conclusion: Gabidulin-Kronecker乘积结构为构建新型MRD码和改进秩基密码系统提供了有效框架，EGK码及其解码算法以及三种RQC变体在安全性和效率方面均有显著提升，为零解密失败概率的密码方案设计开辟了新途径。

Abstract: In this paper, we initiate the study of Extended Gabidulin codes with a Kronecker product structure and propose three enhanced variants of the Rank Quasi-Cyclic (RQC) (Melchor et.al., IEEE IT, 2018) cryptosystem. First, we establish precise bounds on the minimum rank distance of Gabidulin-Kronecker product codes under two distinct parameter regimes. Specifically, when $n_{1}=k_{1}$ and $n_{2}=m<n_{1}n_{2}$, the minimum rank distance is exactly $n_{2}-k_{2}+1$. This yields a new family of Maximum Rank Distance (MRD) codes, which are distinct from classical Gabidulin codes. For the case of $k_{1}\leq n_{1},k_{2}\leq n_{2},n_{1}n_{2}\leq m$, the minimum rank distance $d$ of Gabidulin-Kronecker product codes satisfies a tight upper and lower bound, i.e., $n_{2}-k_{2}+1 \leq d \leq (n_{1}-k_{1}+1)(n_{2}-k_{2}+1)$. Second, we introduce a new class of decodable rank-metric codes, namely Extended Gabidulin-Kronecker product (EGK) codes, which generalize the structure of Gabidulin-Kronecker product (GK) codes. We also propose a decoding algorithm that directly retrieves the codeword without recovering the error vector, thus improving efficiency. This algorithm achieves zero decoding failure probability when the error weight is within its correction capability. Third, we propose three enhanced variants of the RQC cryptosystem based on EGK codes, each offering a distinct trade-off between security and efficiency. For 128-bit security, all variants achieve significant reductions in public key size compared to the Multi-UR-AG (Bidoux et.al., IEEE IT, 2024) while ensuring zero decryption failure probability--a key security advantage over many existing rank-based schemes.

</details>


### [33] [Joint Source-Channel-Generation Coding: From Distortion-oriented Reconstruction to Semantic-consistent Generation](https://arxiv.org/abs/2601.12808)
*Tong Wu,Zhiyong Chen,Guo Lu,Li Song,Feng Yang,Meixia Tao,Wenjun Zhang*

Main category: cs.IT

TL;DR: 提出JSCGC新范式，从确定性重建转向概率生成，利用接收端生成模型参数化数据分布，在信道约束下最大化互信息，显著提升感知质量和语义保真度。


<details>
  <summary>Details</summary>
Motivation: 传统通信系统依赖香农率失真理论，但通用失真度量无法捕捉复杂的人类视觉感知，导致重建结果模糊或不真实。需要从确定性重建转向更符合人类感知的生成方法。

Method: 提出联合源-信道-生成编码(JSCGC)，在接收端使用生成模型作为生成器而非传统解码器，参数化数据分布，在信道约束下直接最大化互信息，并通过控制随机采样产生位于真实数据流形的高保真输出。

Result: 推导了给定传输互信息下最大语义不一致的理论下界，阐明了控制生成过程的通信基本极限。图像传输实验表明，JSCGC显著优于传统失真导向的JSCC方法。

Conclusion: JSCGC通过将通信目标从确定性重建转向概率生成，利用生成模型参数化数据分布，在信道约束下最大化互信息，实现了感知质量和语义保真度的显著提升，为通信系统设计提供了新范式。

Abstract: Conventional communication systems, including both separation-based coding and AI-driven joint source-channel coding (JSCC), are largely guided by Shannon's rate-distortion theory. However, relying on generic distortion metrics fails to capture complex human visual perception, often resulting in blurred or unrealistic reconstructions. In this paper, we propose Joint Source-Channel-Generation Coding (JSCGC), a novel paradigm that shifts the focus from deterministic reconstruction to probabilistic generation. JSCGC leverages a generative model at the receiver as a generator rather than a conventional decoder to parameterize the data distribution, enabling direct maximization of mutual information under channel constraints while controlling stochastic sampling to produce outputs residing on the authentic data manifold with high fidelity. We further derive a theoretical lower bound on the maximum semantic inconsistency with given transmitted mutual information, elucidating the fundamental limits of communication in controlling the generative process. Extensive experiments on image transmission demonstrate that JSCGC substantially improves perceptual quality and semantic fidelity, significantly outperforming conventional distortion-oriented JSCC methods.

</details>


### [34] [On the Concavity of Tsallis Entropy along the Heat Flow](https://arxiv.org/abs/2601.12944)
*Lukang Sun*

Main category: cs.IT

TL;DR: 证明了Tsallis熵在热流下的凹性，将一维结果推广到任意维度


<details>
  <summary>Details</summary>
Motivation: 之前Wu等人(2025)和Hung(2022)的研究只在一维情况下证明了Tsallis熵沿热流的凹性，需要将这一结果推广到任意维度

Method: 通过新颖的二阶时间导数项估计和严格验证分部积分来证明

Result: 成功证明了任意维度下Tsallis熵沿热流的凹性，并建立了一个新的函数不等式

Conclusion: 将Tsallis熵凹性结果从一维推广到任意维度，为数学分析其他领域提供了有用的不等式工具

Abstract: We demonstrate the concavity of the Tsallis entropy along the heat flow for general dimensions, expanding upon the findings of Wu et al 2025 and Hung 2022, which were previously limited to the one-dimensional case. The core of the proof is a novel estimate of the terms in the second-order time derivative, and a rigorous validation of integration by parts. The resulting bound establishes a new functional inequality, which may be of interest for other areas of mathematical analysis.

</details>


### [35] [Codes Correcting Few Restricted Errors](https://arxiv.org/abs/2601.12959)
*Jens Zumbrägel*

Main category: cs.IT

TL;DR: 该论文提出了在限制错误值条件下的线性码新构造方法，特别针对高斯整数和艾森斯坦整数，能够纠正2-3个错误。


<details>
  <summary>Details</summary>
Motivation: 在代码基密码学背景下，限制错误值的线性码受到关注。这类码能够捕获Lee距离码以及高斯整数或艾森斯坦整数上的码，具有重要的密码学应用价值。

Method: 采用两种构造方法：几何方法和代数方法。借鉴了Roth和Siegel在Lee度量码工作中的技术，针对高斯整数和艾森斯坦整数设计了能够纠正2-3个错误的码。

Result: 提出了新的高斯整数和艾森斯坦整数上的码构造，能够有效纠正2个或3个错误。

Conclusion: 该工作为限制错误值条件下的线性码提供了新的构造技术，特别是在高斯整数和艾森斯坦整数域上，扩展了代码基密码学中的可用工具集。

Abstract: We consider linear codes over a field in which the error values are restricted to a subgroup of its unit group. This scenario captures Lee distance codes as well as codes over the Gaussian or Eisenstein integers. Codes correcting restricted errors gained increased attention recently in the context of code-based cryptography.
  In this work we provide new constructions of codes over the Gaussian or Eisenstein integers correcting two or three errors. We adapt some techniques from Roth and Siegel's work on codes for the Lee metric. We propose two construction methods, which may be seen of geometric and algebraic flavor, respectively.

</details>


### [36] [Weighted-Hamming Metric: Bounds and Codes](https://arxiv.org/abs/2601.12998)
*Sebastian Bitzer,Alberto Ravagnani,Violetta Weger*

Main category: cs.IT

TL;DR: 本文研究了加权汉明度量下的纠错能力，建立了比最小距离论证更紧的界，并提出了基于广义级联的灵活编码构造及其高效解码算法。


<details>
  <summary>Details</summary>
Motivation: 加权汉明度量通过为坐标块分配不同权重，适用于独立并行信道等应用场景。从编码理论角度看，这种度量下码的实际纠错能力可能超过其最小距离的一半，需要更精确的分析。

Method: 建立了加权汉明度量下纠错能力的直接界（比最小距离论证更紧），提出了基于广义级联的灵活编码构造，并设计了能纠错到该能力下界的高效解码算法。

Result: 获得了比传统最小距离论证更紧的纠错能力界，构造的广义级联码能够高效解码到该下界，为加权汉明度量下的编码设计提供了理论保证。

Conclusion: 加权汉明度量下的纠错能力分析需要超越传统最小距离方法，本文建立的直接界和构造的广义级联码为这类应用提供了有效的编码方案和解码算法。

Abstract: The weighted-Hamming metric generalizes the Hamming metric by assigning different weights to blocks of coordinates. It is well-suited for applications such as coding over independent parallel channels, each of which has a different level of importance or noise. From a coding-theoretic perspective, the actual error-correction capability of a code under this metric can exceed half its minimum distance. In this work, we establish direct bounds on this capability, tightening those obtained via minimum-distance arguments. We also propose a flexible code construction based on generalized concatenation and show that these codes can be efficiently decoded up to a lower bound on the error-correction capability.

</details>


### [37] [Two-timescale Optimization for Hybrid Mechanically and Electronically Tunable 6DMA Aided Communication](https://arxiv.org/abs/2601.13064)
*Yuyan Zhou,Haocheng Hua,Jie Xu,Rui Zhang*

Main category: cs.IT

TL;DR: 提出一种混合机械电子可调的六维可移动天线基站架构，通过机械移动适应水平用户热点，电子波束重构适应瞬时信道，采用双时间尺度优化最大化用户平均和速率。


<details>
  <summary>Details</summary>
Motivation: 未来无线通信网络需要更灵活的天线系统来适应动态的用户分布和信道条件。传统固定天线系统在覆盖范围和响应速度方面存在局限性，需要结合机械移动和电子调谐的优势来提升系统性能。

Method: 提出6DMA基站架构：天线阵列沿圆形轨道机械移动适应水平用户热点，每个阵列配备模式可重构天线进行电子波束切换。采用双时间尺度优化：长时间尺度根据用户分布统计优化阵列位置，短时间尺度基于瞬时用户位置优化波束选择向量。开发基于蒙特卡洛采样的交替优化算法。

Result: 仿真结果表明，所提出的设计在各种基准方案上实现了显著的性能增益，验证了混合机械电子可调架构的有效性。

Conclusion: 混合机械电子可调的6DMA基站架构通过结合机械调整的广角覆盖和电子调谐的快速响应优势，能够有效适应动态用户分布和信道条件，为未来无线通信网络提供了有前景的解决方案。

Abstract: This letter proposes a hybrid mechanically and electronically tunable six-dimensional movable antenna (6DMA) base station (BS) architecture for future wireless communication networks. Such BS consists of multiple antenna arrays that are mechanically movable along a circular rail to adapt to the horizontal user hotspots, and each array is equipped with pattern reconfigurable antennas (PRAs) that are capable of electronically switching among a set of specified beam patterns to cater to the instantaneous user channels. The mechanical adjustment provides wide-angle coverage but suffers from slow response, while the electronic tuning enables rapid beam reconfiguration but with limited angular range. To effectively combine their complementary advantages, we propose to jointly design both mechanical and electronic configurations to maximize the average sum-rate of users via a two-timescale optimization approach, in which the array positions are optimized on the long timescale according to large-scale user distribution statistics, and the pattern selection vectors are optimized on the short timescale to enable fast beam alignment based on the instantaneous user locations. An alternating optimization algorithm based on the Monte Carlo sampling method is developed to solve the problem efficiently. Finally, simulation results show that our proposed design achieves significant performance gains over various benchmark schemes.

</details>


### [38] [An AMP-Based Asymptotic Analysis For Nonlinear One-Bit Precoding](https://arxiv.org/abs/2601.13214)
*Zheyu Wu,Junjie Ma,Ya-Feng Liu,Bruno Clerckx*

Main category: cs.IT

TL;DR: 本文对一类非线性1比特预编码方案在瑞利衰落信道下的渐近性能进行了分析，提出了基于近似消息传递（AMP）的新分析框架，推导了符号错误概率的闭式表达式。


<details>
  <summary>Details</summary>
Motivation: 研究1比特预编码方案在瑞利衰落信道下的渐近性能，特别是采用凸松弛后量化（CRQ）方法的MMSE模型，需要建立理论分析框架来量化系统参数对性能的影响。

Method: 提出基于近似消息传递（AMP）的新分析框架，将非线性1比特预编码方案的统计特性渐近地描述为"信号加高斯噪声"模型，并在此基础上推导符号错误概率的闭式表达式。

Result: 建立了1比特预编码方案的渐近性能分析框架，得到了符号错误概率的闭式表达式，定量描述了系统和模型参数对性能的影响，仿真验证了分析的正确性，并显示通过适当调参可以获得比经典SQUID方案更好的性能。

Conclusion: 本文成功分析了非线性1比特预编码方案的渐近性能，提出的AMP分析框架有效，推导的闭式表达式为系统设计提供了理论指导，通过参数优化可以提升性能。

Abstract: This paper focuses on the asymptotic analysis of a class of nonlinear one-bit precoding schemes under Rayleigh fading channels. The considered scheme employs a convex-relaxation-then-quantization (CRQ) approach to the well-known minimum mean square error (MMSE) model, which includes the classical one-bit precoder SQUID as a special case. To analyze its asymptotic behavior, we develop a novel analytical framework based on approximate message passing (AMP). We show that, the statistical properties of the considered scheme can be asymptotically characterized by a scalar ``signal plus Gaussian noise'' model. Based on this, we further derive a closed-form expression for the symbol error probability (SEP) in the large-system limit, which quantitatively characterizes the impact of both system and model parameters on SEP performance. Simulation results validate our analysis and also demonstrate that performance gains over SQUID can be achieved by appropriately tuning the parameters involved in the considered model.

</details>


### [39] [On the Reliability of Estimation Bounds in Low-SNR Bistatic ISAC](https://arxiv.org/abs/2601.13216)
*Ataher Sams,Besma Smida*

Main category: cs.IT

TL;DR: 该论文提出在低信噪比被动感知场景下，采用Ziv-Zakai Bound替代传统Cramér-Rao Bound作为双基地ISAC系统中角度估计的性能评估指标，解决了CRB在低SNR下失效的问题。


<details>
  <summary>Details</summary>
Motivation: 传统ISAC研究主要关注功率分配或波束成形设计，使用CRB作为感知性能评估指标。但在被动感知场景中，由于严重的路径损耗，接收感知信噪比远低于直接通信，CRB在低SNR下变得不可靠，需要更合适的性能评估方法。

Method: 采用双基地ISAC框架，基站发射通信信号同时用于用户通信和目标参数估计。假设感知接收器无法获得瞬时发射信号知识，仅知道接收信号的统计特性。针对到达角估计问题，采用Ziv-Zakai Bound作为估计误差的下界，推导了ZZB和可实现遍历通信速率的解析表达式。

Result: 通过数值仿真分析了通信与感知性能之间的帕累托前沿，证明在低感知SNR的ISAC场景中，ZZB相比传统CRB方法提供了更准确、更有意义的性能评估，能够更好地反映实际估计误差。

Conclusion: 在低信噪比被动感知的ISAC系统中，Ziv-Zakai Bound是比Cramér-Rao Bound更合适的性能评估指标，能够解决CRB在低SNR下失效的问题，为ISAC系统的通信-感知权衡分析提供了更可靠的理论基础。

Abstract: This paper explores a bistatic Integrated Sensing and Communication (ISAC) framework, where a base station transmits communication signal that serve both direct communication with a user and multi-target parameter estimation through reflections captured by a separate sensing receiver. We assume that the instantaneous knowledge of the transmit signal at the sensing receiver is not available, and the sensing receiver only has knowledge of the statistical properties of the received signal. Unlike prior research that focuses on power allocation or optimal beamforming design for ISAC, we emphasize the inadequacy of the Cramér-Rao Bound (and its variant) in low Signal-to-Noise Ratio (SNR) regimes, particularly in passive sensing scenarios. Due to severe path loss and other impairments, the received sensing SNR is often significantly lower than that of direct Line-of-Sight communication, making CRB-based performance evaluation unreliable. To address this, we adopt the Ziv-Zakai Bound (ZZB) for Angle of Arrival estimation, which provides a more meaningful lower bound on estimation error. We derive analytical expressions for the ZZB and the achievable ergodic communication rate as functions of SNR. Through numerical simulations, we analyze the pareto-front between communication and sensing performance, demonstrating why ZZB serves as a better metric in low sensing SNR ISAC where traditional CRB-based approaches fail.

</details>


### [40] [Elias-type Bounds for Codes in the Symmetric Limited-Magnitude Error Channel](https://arxiv.org/abs/2601.13477)
*Zhihao Guan,Hengjia Wei*

Main category: cs.IT

TL;DR: 研究对称有限幅度错误信道中Z^n上的完美纠错码，推导完美码存在的必要条件，针对不同错误幅度建立渐近界


<details>
  <summary>Details</summary>
Motivation: 研究对称有限幅度错误信道中的完美纠错码，这种信道中整数向量最多有e个坐标被幅度不超过s的值改变。几何上对应用对称有限幅度错误球B(n,e,s,s)对Z^n进行平铺。需要理解这种完美码存在的条件

Method: 将汉明度量下Elias界的几何思想适应到对称有限幅度错误信道的距离度量d_s，推导完美码/平铺存在的必要条件。针对不同错误幅度(s=1,2和s≥3)采用不同分析方法，并将方法扩展到非完美码

Result: 对于小错误幅度(s=1,2)，证明如果可纠正错误数不超过n的某个分数，则渐近有界于e=O(√(n log n))。对于较大幅度(s≥3)，建立更尖锐的界e<√(12.36n)，且无需限制e低于n的某个分数。扩展到非完美码时，对于纠正线性或Ω(√n)个错误的码，密度界与错误幅度s成反比

Conclusion: 针对对称有限幅度错误信道中的完美纠错码，根据错误幅度大小建立了不同的存在性条件：小幅度时渐近有界，大幅度时有更尖锐的界。方法可扩展到非完美码，为这类码的设计和分析提供了理论基础

Abstract: We study perfect error-correcting codes in $\mathbb{Z}^n$ for the symmetric limited-magnitude error channel, where at most $e$ coordinates of an integer vector may be altered by a value whose magnitude is at most $s$. Geometrically, such codes correspond to tilings of $\mathbb{Z}^n$ by the symmetric limited-magnitude error ball $\mathcal{B}(n,e,s,s)$. Given $n$ and $s$, we adapt the geometric ideas underlying the Elias bound for the Hamming metric to the distance $d_s$ tailed for this channel, and derive new necessary conditions on $e$ for the existence of perfect codes / tilings, without assuming any lattice structure. Our main results identify two distinct regimes depending on the error magnitude. For small error magnitudes ($s \in \{1, 2\}$), we prove that if the number of correctable errors does not exceed a certain fraction of $n$, then it is asymptotically bounded by $e = \mathcal{O}(\sqrt{n \log n})$. In contrast, for larger magnitudes ($s \geq 3$), we establish a significantly sharper bound of $e < \sqrt{12.36n}$, which holds without any restriction on $e$ being below a given fraction of $n$. Finally, by extending our method to non-perfect codes, we derive an upper bound on packing density, showing that for codes correcting a linear or $Ω(\sqrt{n})$ number of errors, the density is bounded by a factor inversely proportional to the error magnitude $s$.

</details>


### [41] [Group Relative Policy Optimization for Robust Blind Interference Alignment with Fluid Antennas](https://arxiv.org/abs/2601.13506)
*Jianqiu Peng,Tong Zhang,Shuai Wang,Mingjie Shao,Hao Xu,Rui Wang*

Main category: cs.IT

TL;DR: 本文提出首个基于流体天线系统的鲁棒盲干扰对齐框架，用于K用户MISO下行链路，在非完美CSI下通过GRPO算法优化天线位置以最大化和速率。


<details>
  <summary>Details</summary>
Motivation: 流体天线系统能动态重构以解锁空间自由度并重塑无线信道，但在非完美CSI下的鲁棒设计仍具挑战性。现有方法如PPO存在模型复杂、易陷入局部最优等问题。

Method: 提出基于GRPO的鲁棒流体天线驱动盲干扰对齐框架。GRPO是一种新型深度强化学习算法，消除评论家网络，采用基于群体的探索策略，减少模型规模和计算复杂度。

Result: GRPO相比PPO性能提升4.17%，相比预训练的PPO提升30.29%。相比启发式MaximumGain和RandomGain分别提升200.78%和465.38%。模型规模和FLOPs减少近一半。

Conclusion: GRPO为流体天线系统在非完美CSI下提供了高效的鲁棒优化方案，显著提升系统性能同时降低计算复杂度，为无线通信中的智能资源分配提供了新思路。

Abstract: Fluid antenna system (FAS) leverages dynamic reconfigurability to unlock spatial degrees of freedom and reshape wireless channels. This paper proposes, for the first time, a robust fluid antenna-driven blind interference alignment (BIA) framework for a K-user MISO downlink under imperfect channel state information (CSI). We formulate a robust sum-rate maximization problem through optimizing fluid antenna positions. To solve this challenging non-convex problem, we employ group relative policy optimization (GRPO), a novel deep reinforcement learning algorithm that eliminates the critic network. This robust design reduces model size and floating point operations (FLOPs) by nearly half compared to proximal policy optimization (PPO) while significantly enhancing performance through group-based exploration that escapes bad local optima. Simulation results demonstrate that GRPO outperforms PPO by 4.17%, and a 100K-step pre-trained PPO by 30.29%. Due to error distribution learning, GRPO exceeds heuristic MaximumGain and RandomGain by 200.78% and 465.38%, respectively.

</details>


### [42] [An Elementary Approach to Scheduling in Generative Diffusion Models](https://arxiv.org/abs/2601.13602)
*Qiang Sun,H. Vincent Poor,Wenyi Zhang*

Main category: cs.IT

TL;DR: 论文提出了一种分析扩散模型中噪声调度和时间离散化影响的基本方法，通过高斯源分布的简化模型推导KL散度闭式解，并优化噪声调度策略


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型中噪声调度策略和时间离散化对生成质量的影响，为实际应用中选择最优的采样策略提供理论指导

Method: 采用高斯源分布的简化模型，推导反向采样过程中分布的闭式演化轨迹和KL散度，通过欧拉-麦克劳林展开分析时间步数影响，用变分法求解最优噪声调度

Result: 得到最优噪声调度遵循正切定律，其系数由源协方差矩阵特征值决定；实验表明该方法选择的离散化策略在各种数据集和预训练模型上都优于基线方法

Conclusion: 提出的理论框架能有效分析扩散模型中的噪声调度和时间离散化影响，为实际应用中的采样策略选择提供了理论依据和实用指导

Abstract: An elementary approach to characterizing the impact of noise scheduling and time discretization in generative diffusion models is developed. Considering a simplified model where the source distribution is multivariate Gaussian with a given covariance matrix, the explicit closed-form evolution trajectory of the distributions across reverse sampling steps is derived, and consequently, the Kullback-Leibler (KL) divergence between the source distribution and the reverse sampling output is obtained. The effect of the number of time discretization steps on the convergence of this KL divergence is studied via the Euler-Maclaurin expansion. An optimization problem is formulated, and its solution noise schedule is obtained via calculus of variations, shown to follow a tangent law whose coefficient is determined by the eigenvalues of the source covariance matrix. For an alternative scenario, more realistic in practice, where pretrained models have been obtained for some given noise schedules, the KL divergence also provides a measure to compare different time discretization strategies in reverse sampling. Experiments across different datasets and pretrained models demonstrate that the time discretization strategy selected by our approach consistently outperforms baseline and search-based strategies, particularly when the budget on the number of function evaluations is very tight.

</details>


### [43] [Reflections over the Sea: Reconfigurable Intelligent Surface for Maritime Self-Powered Communications](https://arxiv.org/abs/2601.13618)
*Qianqian Zhang,Long Wang,Ben Wu,Jia Mi*

Main category: cs.IT

TL;DR: 提出基于海上风电基础设施RIS的海洋物联网通信框架，通过海洋表面信道建模和波浪能量收集系统，显著提升恶劣海况下的通信性能


<details>
  <summary>Details</summary>
Motivation: 随着海洋经济的快速发展，海洋通信成为6G网络的重要组成部分，但现有技术在信号覆盖、可用性和鲁棒性方面面临严峻挑战，特别是在恶劣海况下

Method: 提出基于海上风电基础设施的可重构智能表面（RIS）框架，开发考虑海浪影响的近海表面信道模型，设计波浪能量收集系统为物联网传感器供电，并通过实时信道状态信息优化RIS反射参数

Result: 仿真结果表明，在恶劣海况下，所提系统将物联网通信性能提升超过20%

Conclusion: 该研究为海洋物联网通信提供了一种创新的解决方案，通过RIS技术和波浪能量收集系统，有效解决了海洋环境中的通信覆盖和可靠性问题

Abstract: Maritime communication is becoming a vital component of 6G networks, driven by the rapid expansion of the maritime economy. However, existing technologies face critical challenges in signal coverage, availability, and robustness, especially under harsh sea conditions. This paper proposes a novel framework for the maritime Internet-of-Things (IoT) communications that leverages the reconfigurable intelligent surface (RIS) mounted on offshore infrastructures, such as wind turbines, to enhance coverage and reliability. To capture dynamic maritime environment, a near-ocean-surface channel model is developed considering the impact of sea waves. In addition, a wave energy harvesting (EH) system is designed to self-power IoT sensors for data acquisition, processing, and transmission. To support real-time adaptation, channel state information is continuously measured to optimize RIS reflection parameters and maximize multi-user communication rates. Simulation results show that the proposed system significantly improves IoT communication performance by over 20%, under harsh sea conditions.

</details>


### [44] [Constrained MARL for Coexisting TN-NTN Resource Allocation: Scalability and Flexibility](https://arxiv.org/abs/2601.13883)
*Cuong Le,Thang X. Vu,Stefano Andrenacci,Symeon Chatzinotas*

Main category: cs.IT

TL;DR: 提出一种基于分解的联合TN-NTN资源分配方案，通过顺序学习解决大规模动态场景下的频谱共享问题


<details>
  <summary>Details</summary>
Motivation: 解决地面基站与非地面基站共存频谱中的大规模资源分配问题，应对大量传输信道、用户和高度动态用户行为带来的挑战

Method: 基于跨段干扰的特殊性质提出分解解决方案，通过顺序学习方式解决子问题，并设计随机训练环境增强策略灵活性

Result: 在20MHz全带宽测试中，相比现有方案显著提升了可扩展性，并在高度动态场景中保持鲁棒性

Conclusion: 提出的分解式顺序学习方法有效解决了大规模TN-NTN联合资源分配问题，在动态环境中表现出优越性能

Abstract: This paper considers the joint TN-NTN constrained resource allocation, where terrestrial base stations and non-terrestrial base stations coexist in the spectrum. We focus on large-scale and practical scenarios characterized by large numbers of transmission channels and users, alongside highly dynamic user behaviors. As common learning solutions fail to address these challenges, we propose a decomposition solution based on the special properties of the cross-segment interference, and then tackle the original problem via solving subproblems in a sequential learning manner. Furthermore, to enhance the flexibility of the learned policies, we design a stochastic training environment that captures the key characteristics of real-world systems. Simulation results tested on the full 20MHz bandwidth with various numerologies show that our solution significantly improves scalability compared to existing solutions and remains robust in highly dynamic scenarios.

</details>


### [45] [Proactive Coded Caching Scheme for D2D Networks](https://arxiv.org/abs/2601.13929)
*Qiaoling Zhang,Changlu Lin,Minquan Cheng*

Main category: cs.IT

TL;DR: 提出一种用于D2D网络的编码缓存方案，同时保证文件隐私和安全传输，在文件较大且缓存充足时达到接近最优性能


<details>
  <summary>Details</summary>
Motivation: 现有编码缓存方案通常假设用户缓存内容不被他人访问，忽视了缓存本身可能遭受攻击导致文件隐私泄露的风险。在D2D通信中，安全传输和文件隐私已成为关键问题。

Method: 提出一种安全的编码缓存方案，专门针对D2D网络设计，同时保证文件隐私和安全传输。方案考虑了缓存本身可能遭受攻击的风险。

Result: 当文件大小足够大且缓存内存充足时，所提出的方案能够达到接近最优的性能表现。

Conclusion: 该安全编码缓存方案有效解决了D2D网络中文件隐私和安全传输的问题，在适当条件下实现了接近最优的性能。

Abstract: Coded caching and device-to-device (D2D) communication are two effective techniques for alleviating network traffic. Secure transmission and file privacy have also become critical concerns in these domains. However, prevailing coded caching schemes typically assume that a user's cached content is inaccessible to others, overlooking the risk of file privacy leakage due to attacks targeting the cache itself. In this paper, we propose a secure coded caching scheme for D2D networks that guarantees both file privacy and secure delivery. We demonstrate that the proposed scheme achieves order-optimal performance when the file size is sufficiently large and the cache memory is ample.

</details>


### [46] [Utilizing the Perceived Age to Maximize Freshness in Query-Based Update Systems](https://arxiv.org/abs/2601.14075)
*Sahan Liyanaarachchi,Sennur Ulukus,Nail Akar*

Main category: cs.IT

TL;DR: 本文研究了在通用延迟分布下监控连续时间马尔可夫链的最优采样策略，发现基于等待的策略能显著提升平均二进制新鲜度。


<details>
  <summary>Details</summary>
Motivation: 现有基于查询的采样方法大多假设查询延迟服从指数分布且反馈瞬时完成，这些假设在实际应用中往往不成立。本文旨在放松这些假设，研究在通用延迟分布下的最优采样策略。

Method: 研究在通用延迟分布下监控连续时间马尔可夫链的最优采样策略，特别关注基于等待的查询采样策略。

Result: 研究发现，采用基于等待的策略能显著提升平均二进制新鲜度（MBF），相比传统方法有显著增益。

Conclusion: 在通用延迟分布下，基于等待的查询采样策略能有效提升监控系统的性能，为实际应用提供了更优的解决方案。

Abstract: Query-based sampling has become an increasingly popular technique for monitoring Markov sources in pull-based update systems. However, most of the contemporary literature on this assumes an exponential distribution for query delay and often relies on the assumption that the feedback or replies to the queries are instantaneous. In this work, we relax both of these assumptions and find optimal sampling policies for monitoring continuous-time Markov chains (CTMC) under generic delay distributions. In particular, we show that one can obtain significant gains in terms of mean binary freshness (MBF) by employing a waiting based strategy for query-based sampling.

</details>


### [47] [Near Optimal Code Construction for the Adversarial Torn Paper Channel With Edit Errors](https://arxiv.org/abs/2601.14088)
*Maria Abu-Sini,Reinhard Heckel*

Main category: cs.IT

TL;DR: 本文研究了对抗性撕裂纸张信道，该信道先对传输字施加最多t_e次编辑错误，然后将其任意位置撕裂成t+1个片段，构建了近乎最优的t-撕裂t_e-编辑错误弹性码，并研究了列表解码。


<details>
  <summary>Details</summary>
Motivation: 受DNA存储系统和3D指纹识别应用启发，研究对抗性撕裂纸张信道，该信道模型结合了编辑错误和物理撕裂，具有实际应用价值。

Method: 构建t-撕裂t_e-编辑错误弹性码，能够从t+1个噪声片段重建传输的码字。同时研究列表解码，推导当撕裂次数t'大于t时，从t-撕裂弹性码的码字切割得到的列表大小界限。

Result: 构造了近乎最优的错误纠正码用于撕裂纸张信道，能够从噪声片段重建码字。推导了列表解码的界限，为撕裂信道提供了理论分析框架。

Conclusion: 本文为对抗性撕裂纸张信道提供了有效的编码方案和理论分析，对DNA存储和3D指纹识别等应用具有重要意义。

Abstract: Motivated by DNA storage systems and 3D fingerprinting, this work studies the adversarial torn paper channel with edit errors. This channel first applies at most $t_e$ edit errors (i.e., insertions, deletions, and substitutions) to the transmitted word and then breaks it into $t+1$ fragments at arbitrary positions. In this paper, we construct a near optimal error correcting code for this channel, which will be referred to as a $t$-breaks $t_e$-edit-errors resilient code. This code enables reconstructing the transmitted codeword from the $t+1$ noisy fragments. Moreover, we study list decoding of the torn paper channel by deriving bounds on the size of the list (of codewords) obtained from cutting a codeword of a $t$-breaks resilient code $t'$ times, where $t' > t$.

</details>


### [48] [Vector Coded Caching Multiplicatively Boosts MU-MIMO Systems Under Practical Considerations](https://arxiv.org/abs/2601.14142)
*Hui Zhao,Petros Elia*

Main category: cs.IT

TL;DR: 本文首次全面分析了矢量编码缓存在多用户MIMO系统中的影响，考虑了多接收天线和可变路径损耗两个关键因素，研究了BD-MRC和ZF两种预编码策略，并提出了低复杂度优化方法。


<details>
  <summary>Details</summary>
Motivation: 多用户MIMO系统固有的单播特性受多接收天线和可变路径损耗两个关键因素影响，需要研究矢量编码缓存在这种场景下的性能表现，特别是在实际考虑如信道衰落、CSI获取开销和公平性功率分配的情况下。

Method: 1) 分析BD-MRC和ZF两种预编码策略下的可达吞吐量；2) 开发基于矩阵结构的低复杂度BD-MRC优化方法，显著降低预编码计算维度；3) 通过高效一维搜索解决相关的最大最小公平性问题；4) 在大规模MIMO场景下推导瑞利衰落信道的渐近吞吐量表达式。

Result: 仿真验证了理论结果，VCC相比优化的无缓存MU-MIMO系统带来显著性能增益。例如，32个发射天线和每个用户2个接收天线时，VCC的吞吐量提升超过300%。在发射端CSI不完美的情况下，VCC将干扰缓解卸载到接收端的能力确保了鲁棒性能。

Conclusion: 矢量编码缓存在多用户MIMO系统中能有效提升性能，特别是在实际CSI获取受限的场景下。提出的低复杂度优化方法具有实际应用价值，VCC的干扰管理能力使其在CSI质量下降和获取成本升高时仍能保持稳健性能。

Abstract: This work presents a first comprehensive analysis of the impact of vector coded caching (VCC) in multi-user multiple-input multiple-output (MU-MIMO) systems with multiple receive antennas and variable pathloss -- two key factors that critically influence systems with inherent MU unicasting behavior. We investigate two widely adopted precoding strategies: (i) blockdiagonalization (BD) at the transmitter combined with maximal ratio combining (MRC) at the receivers, and (ii) zero-forcing (ZF) precoding. Our analysis explicitly accounts for practical considerations such as channel fading, channel state information (CSI) acquisition overhead, and fairness-oriented power allocation.
  Our contributions span both analytical and simulation-based fronts. On the analytical side, we derive analytical expressions for the achievable throughput under BD-MRC and ZF, highlighting the performance benefits of equipping multi-antenna users with cache-aided interference management. Specifically, we develop a low-complexity BD-MRC optimization method that leverages matrix structure to significantly reduce the dimensionality involved in precoding computation, followed by solving the associated maxmin fairness problem through an efficient one-dimensional search. In the massive MIMO regime, an asymptotic expression for the achievable throughput over Rayleigh fading channels is also derived. Simulations validate our theoretical results, confirming that VCC delivers substantial performance gains over optimized cacheless MU-MIMO systems. For example, with 32 transmit antennas and 2 receive antennas per user, VCC yields throughput improvements exceeding 300%. These gains are further amplified under imperfect CSI at the transmitter, where VCC's ability to offload interference mitigation to the receivers ensures robust performance even in the face of degraded CSI quality and elevated acquisition costs.

</details>


### [49] [Storage-Rate Trade-off in A-XPIR](https://arxiv.org/abs/2601.14202)
*Mohamed Nomeir,Sennur Ulukus*

Main category: cs.IT

TL;DR: 研究非对称X安全私有信息检索中的存储问题，分析服务器存储与下载成本之间的权衡关系，针对特定服务器配置推导出可达区域和容量结果。


<details>
  <summary>Details</summary>
Motivation: 研究在非对称安全设置下的私有信息检索问题，其中服务器通信存在特定模式，需要平衡存储成本和下载效率，解决传统对称安全模型无法处理的实际非对称通信场景。

Method: 采用信息论方法分析存储-下载权衡区域，针对N=4服务器、K=2消息和两个非重叠通信集的情况，推导可达区域边界，设计无复制存储和检索方案，并推导非对称安全和合谋情况下的容量表达式。

Result: 对于N=4服务器、K=2消息的情况，证明了与无安全情况相比，三个主要不等式在非对称安全情况下简化为两个不等式；推导了非对称安全和合谋情况下的精确容量C=1/3；提供了达到最优速率的无复制存储方案。

Conclusion: 非对称安全设置显著改变了私有信息检索的存储-下载权衡特性，在特定服务器配置下可以推导出精确容量结果，无复制存储方案能够达到与复制方案相同的最优速率，为实际分布式存储系统设计提供了理论指导。

Abstract: We consider the storage problem in an asymmetric $X$-secure private information retrieval (A-XPIR) setting. The A-XPIR setting considers the $X$-secure PIR problem (XPIR) when a given arbitrary set of servers is communicating. We focus on the trade-off region between the average storage at the servers and the average download cost. In the case of $N=4$ servers and two non-overlapping sets of communicating servers with $K=2$ messages, we characterize the achievable region and show that the three main inequalities compared to the no-security case collapse to two inequalities in the asymmetric security case. In the general case, we derive bounds that need to be satisfied for the general achievable region for an arbitrary number of servers and messages. In addition, we provide the storage and retrieval scheme for the case of $N=4$ servers with $K=2$ messages and two non-overlapping sets of communicating servers, such that the messages are not replicated (in the sense of a coded version of each symbol) and at the same time achieve the optimal achievable rate for the case of replication. Finally, we derive the exact capacity for the case of asymmetric security and asymmetric collusion for $N=4$ servers, with the communication links $\{1,2\}$ and $\{3,4\}$, which splits the servers into two groups, i.e., $g=2$, and with the collusion links $\{1,3\}$, $\{2,4\}$, as $C=\frac{1}{3}$. More generally, we derive a capacity result for a certain family of asymmetric collusion and asymmetric security cases.

</details>


### [50] [Stabilizer-Assisted Inactivation Decoding of Quantum Error-Correcting Codes with Erasures](https://arxiv.org/abs/2601.14236)
*Giulio Pech,Mert Gökduman,Hanwen Yao,Henry D. Pfister*

Main category: cs.IT

TL;DR: 提出一种用于量子低密度奇偶校验码擦除解码的简化复杂度最大似然解码器，结合经典失活解码与新的对偶剥离过程，显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 量子低密度奇偶校验码的擦除解码需要最大似然性能，但传统解码器计算复杂度高。需要开发既能保持最大似然性能又能显著降低复杂度的解码方法。

Method: 结合经典失活解码（剥离与符号猜测）与新的对偶剥离过程。对偶剥离阶段通过对稳定子矩阵进行行操作，高效识别支持集完全位于擦除集合中的稳定子生成元及其线性组合，从而减少需要符号猜测的变量数量。

Result: 解码器在多个QLDPC码族上保持最大似然逻辑失败性能，同时显著降低失活解码复杂度。对于B1提升积码，在高擦除率下符号猜测减少超过20%。对表面码，仅使用剥离和对偶剥离即可实现最大似然擦除解码。

Conclusion: 提出的对偶剥离技术能有效减少量子擦除解码中的符号猜测需求，在保持最大似然性能的同时显著降低计算复杂度，为QLDPC码的实用解码提供了高效解决方案。

Abstract: In this work, we develop a reduced complexity maximum likelihood (ML) decoder for quantum low-density parity-check (QLDPC) codes over erasures. Our decoder combines classical inactivation decoding, which integrates peeling with symbolic guessing, with a new dual peeling procedure. In the dual peeling stage, we perform row operations on the stabilizer matrix to efficiently reveal stabilizer generators and their linear combinations whose support lies entirely on the erased set. Each such stabilizer identified allows us to freely fix a bit in its support without affecting the logical state of the decoded result. This removes one degree of freedom that would otherwise require a symbolic guess, reducing the number of inactivated variables and decreasing the size of the final linear system that must be solved. We further show that dual peeling combined with standard peeling alone, without inactivation, is sufficient to achieve ML for erasure decoding of surface codes. Simulations across several QLDPC code families confirm that our decoder matches ML logical failure performance while significantly reducing the complexity of inactivation decoding, including more than a 20% reduction in symbolic guesses for the B1 lifted product code at high erasure rates.

</details>


### [51] [Identification capacity and rate-query tradeoffs in classification systems](https://arxiv.org/abs/2601.14252)
*Tristan Simas*

Main category: cs.IT

TL;DR: 研究离散分类中的一次性识别问题，分析标签率L、识别成本W和失真D三个资源之间的权衡关系，建立了零误差识别的基本界限和组合结构。


<details>
  <summary>Details</summary>
Motivation: 研究在有限观察条件下从实体属性恢复其类别的识别问题，探索标签存储、查询成本和分类错误之间的基本权衡，为类型系统、数据库和生物分类学等应用提供理论基础。

Method: 采用信息论和组合数学方法，分析离散分类中的识别问题。通过建立零误差可行性阈值，研究标签率、查询成本和失真之间的基本关系，利用拟阵理论和图熵概念构建理论框架。

Result: 建立了零误差识别的基本界限：需要至少log₂k比特的标签才能实现零误差识别；无标签时最坏情况下需要Ω(n)次查询且可能产生误差。发现了最小充分查询族构成拟阵基，区分维度与图熵相关。

Conclusion: 该研究为离散分类识别问题建立了完整的理论框架，揭示了标签存储、查询成本和分类精度之间的基本权衡关系，并通过形式化验证确保了结果的可靠性，对多个应用领域具有指导意义。

Abstract: We study a one-shot identification analogue of rate-distortion for discrete classification under three resources: tag rate L (bits of side information stored per entity), identification cost W (attribute-membership queries per identification, excluding global preprocessing and amortized caching), and distortion D (misclassification probability). The question is to characterize achievable triples (L,W,D) when a decoder must recover an entity's class from limited observations. Zero-error barrier. If two distinct classes induce the same attribute profile, then the observation pi(V) is identical for both and no decoder can identify the class from attribute queries alone. Thus, if the profile map pi is not injective on classes, zero-error identification without tags is impossible (a zero-error feasibility threshold). Achievability and converse at D=0. With k classes, nominal tags of L = ceil(log2 k) bits enable O(1) identification cost with D=0. Conversely, any scheme with D=0 must satisfy L >= log2 k bits (tight). Without tags (L=0), identification requires Omega(n) queries in the worst case and may incur D>0. Combinatorial structure. Minimal sufficient query families form the bases of a matroid; the induced distinguishing dimension is well-defined and links to zero-error source coding via graph entropy. We illustrate implications for type systems, databases, and biological taxonomy. All results are mechanized in Lean4 (6000+ lines, 0 sorry).

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [52] [Knowledge Graph Construction for Stock Markets with LLM-Based Explainable Reasoning](https://arxiv.org/abs/2601.11528)
*Cheonsol Lee,Youngsang Jeong,Jeongyeol Shin,Huiju Kim,Jidong Kim*

Main category: cs.DB

TL;DR: 提出一个结合知识图谱和大型语言模型的股票市场分析框架，用于多跳推理和可解释的投资分析


<details>
  <summary>Details</summary>
Motivation: 传统股票市场研究主要关注时间序列预测和单公司分析，依赖数值数据进行股价预测，但难以捕捉关系模式、竞争动态和可解释的投资推理

Method: 设计专门针对股票市场的知识图谱模式，建模公司、行业、股票指标、财务报表和公司间关系，并与大型语言模型集成，实现多跳推理和关系查询

Result: 通过韩国上市公司的实际案例研究验证了该框架，证明其能够提取传统数据库查询难以获得的洞察

Conclusion: 知识图谱与大型语言模型的结合在高级投资分析和决策支持方面具有巨大潜力

Abstract: The stock market is inherently complex, with interdependent relationships among companies, sectors, and financial indicators. Traditional research has largely focused on time-series forecasting and single-company analysis, relying on numerical data for stock price prediction. While such approaches can provide short-term insights, they are limited in capturing relational patterns, competitive dynamics, and explainable investment reasoning. To address these limitations, we propose a knowledge graph schema specifically designed for the stock market, modeling companies, sectors, stock indicators, financial statements, and inter-company relationships. By integrating this schema with large language models (LLMs), our approach enables multi-hop reasoning and relational queries, producing explainable and in-depth answers to complex financial questions. Figure1 illustrates the system pipeline, detailing the flow from data collection and graph construction to LLM-based query processing and answer generation. We validate the proposed framework through practical case studies on Korean listed companies, demonstrating its capability to extract insights that are difficult or impossible to obtain from traditional database queries alone. The results highlight the potential of combining knowledge graphs with LLMs for advanced investment analysis and decision support.

</details>


### [53] [RelServe: Fast LLM Inference Serving on Relational Data](https://arxiv.org/abs/2601.11546)
*Xin Zhang,Shihong Gao,Yanyan Shen,Haoyang Li,Lei Chen*

Main category: cs.DB

TL;DR: RelServe：针对关系数据查询（relQuery）优化的LLM服务引擎，通过动态优先级更新和自适应批处理解决现有系统中的HoL阻塞问题，显著降低延迟


<details>
  <summary>Details</summary>
Motivation: 随着LLM在关系数据查询（relQuery）中的广泛应用，如AI电子表格等应用对并发查询的快速响应需求日益增长。现有LLM引擎面临严重的HoL阻塞问题，静态优先级调度仅解决等待阶段的阻塞，无法处理核心执行和尾部执行阶段的阻塞问题

Method: 提出RelServe优化引擎，包含两个核心创新：1）动态优先级更新器，通过统计近似持续调整优先级同时最小化开销；2）自适应批处理器，定量评估候选的预填充和解码批次以最小化预计平均延迟

Result: 在四个真实数据集上，使用13B到70B参数的LLM进行实验，RelServe相比vLLM将平均服务延迟降低了最多3.1倍

Conclusion: RelServe通过解决现有LLM引擎在relQuery服务中的HoL阻塞问题，显著提高了并发查询的响应性能，为AI驱动的数据查询应用提供了有效的低延迟解决方案

Abstract: The use of Large Language Models (LLMs) for querying relational data has given rise to relQuery, a workload pattern that applies templated LLM calls to structured tables. As relQuery services become more widely adopted in applications such as AI-powered spreadsheets, fast response times under concurrent query loads are increasingly important. Unfortunately, current LLM engines face severe latency bottlenecks from Head-of-Line (HoL) blocking across three comparable inference phases: waiting, core running, and tail running. Existing static priority scheduling methods only address HoL blocking during the waiting phase, leaving two critical problems unsolved. First, the absence of a priority update mechanism causes inaccurate prioritization and continued HoL blocking during core execution. Second, suboptimal prefill-decode batching exacerbates HoL blocking in tail execution and worsens latency trade-offs between running and waiting relQueries. To address these problems, we propose RelServe, an optimized LLM engine for low-latency relQuery serving. RelServe features two core innovations: a Dynamic Priority Updater that continuously adjusts priorities while minimizing overhead via statistical approximations, and an Adaptive Batch Arranger that quantitatively evaluates candidate prefill and decode batches to minimize projected average latency. Extensive experiments on four real-world datasets using LLMs ranging from 13B to 70B parameters show that RelServe reduces average serving latency by up to 3.1x compared to vLLM.

</details>


### [54] [Uniqueness ratio as a predictor of a privacy leakage](https://arxiv.org/abs/2601.11550)
*Danah A. AlSalem AlKhashti*

Main category: cs.DB

TL;DR: 研究提出使用候选连接属性的唯一性比率作为连接前重识别风险的早期预测指标，实验表明高唯一性比率与连接后身份泄露风险强相关。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注连接后的检测或复杂的隐私模型，缺乏简单、可解释的连接前指标来警告数据工程师和数据库管理员在数据集成前评估风险。

Method: 使用合成的多表数据集，计算每个数据库中属性组合的唯一性比率，并分析这些比率与连接后身份暴露之间的相关性。

Result: 实验结果显示，连接前的高唯一性比率与连接后增加的泄露风险有强相关关系，表现为更多记录变得唯一可识别或落入非常小的组中。

Conclusion: 唯一性比率提供了一个可解释且实用的信号来评估连接引发的隐私风险，为开发更全面的连接前风险评估模型奠定了基础。

Abstract: Identity leakage can emerge when independent databases are joined, even when each dataset is anonymized individually. While previous work focuses on post-join detection or complex privacy models, little attention has been given to simple, interpretable pre-join indicators that can warn data engineers and database administrators before integration occurs. This study investigates the uniqueness ratio of candidate join attributes as an early predictor of re-identification risk. Using synthetic multi-table datasets, we compute the uniqueness ratio of attribute combinations within each database and examine how these ratios correlate with identity exposure after the join. Experimental results show a strong relationship between high pre-join uniqueness and increased post-join leakage, measured by the proportion of records that become uniquely identifiable or fall into very small groups. Our findings demonstrate that uniqueness ratio offers an explainable and practical signal for assessing join induced privacy risk, providing a foundation for developing more comprehensive pre-join risk estimation models.

</details>


### [55] [From HNSW to Information-Theoretic Binarization: Rethinking the Architecture of Scalable Vector Search](https://arxiv.org/abs/2601.11557)
*Seyed Moein Abtahi,Majid Fekri,Tara Khani,Akramul Azim*

Main category: cs.DB

TL;DR: 论文提出基于最大信息二值化(MIB)和比特距离度量的信息论架构，替代传统HNSW+float32+余弦相似度方案，实现高效语义搜索


<details>
  <summary>Details</summary>
Motivation: 当前语义搜索和RAG系统依赖内存中的高精度浮点向量近似最近邻索引，导致运营成本上升，并在延迟、吞吐量和检索准确性之间存在固有权衡

Method: 采用最大信息二值化(MIB)、高效比特距离度量和信息论评分(ITS)机制，实现对紧凑二进制表示的穷举搜索

Result: 在MAIR基准测试中，与Elasticsearch、Pinecone等系统相比，检索质量与全精度系统相当，延迟显著降低，高并发下吞吐量保持稳定

Conclusion: 该架构支持真正的无服务器按查询付费部署模式，挑战了高质量语义搜索必须依赖大型内存ANN索引的传统观念

Abstract: Modern semantic search and retrieval-augmented generation (RAG) systems rely predominantly on in-memory approximate nearest neighbor (ANN) indexes over high-precision floating-point vectors, resulting in escalating operational cost and inherent trade-offs between latency, throughput, and retrieval accuracy. This paper analyzes the architectural limitations of the dominant "HNSW + float32 + cosine similarity" stack and evaluates existing cost-reduction strategies, including storage disaggregation and lossy vector quantization, which inevitably sacrifice either performance or accuracy. We introduce and empirically evaluate an alternative information-theoretic architecture based on maximally informative binarization (MIB), efficient bitwise distance metrics, and an information-theoretic scoring (ITS) mechanism. Unlike conventional ANN systems, this approach enables exhaustive search over compact binary representations, allowing deterministic retrieval and eliminating accuracy degradation under high query concurrency. Using the MAIR benchmark across 14 datasets and 10,038 queries, we compare this architecture against Elasticsearch, Pinecone, PGVector, and Qdrant. Results demonstrate retrieval quality comparable to full-precision systems, while achieving substantially lower latency and maintaining constant throughput at high request rates. We show that this architectural shift enables a truly serverless, cost-per-query deployment model, challenging the necessity of large in-memory ANN indexes for high-quality semantic search.

</details>


### [56] [Bridging Radiology and Pathology: A DICOM-Based Framework for Multimodal Mapping and Integrated Visualization](https://arxiv.org/abs/2601.11558)
*Nilesh P. Rijhwani,Titus J. Brinker,Peter Neher,Marco Nolden,Klaus Maier-Hein,Maximilian Fischer,Christoph Wies*

Main category: cs.DB

TL;DR: 开发了一个连接放射学和病理学的跨学科工具箱，通过自动化图像配准和对齐，促进多模态医学数据分析，加速跨学科研究。


<details>
  <summary>Details</summary>
Motivation: 医学专科间使用不同的数据系统和专有格式，阻碍了联合分析和互补诊断信息的整合。放射学和病理学等多模态集成虽然显示出识别新生物标志物的潜力，但仍依赖耗时的手动数据配对。

Method: 开发了一个可在Kaapana框架内运行或作为独立工具使用的跨学科工具箱，通过连接特定模态的查看器并扩展自动化图像配准和对齐功能，实现高效、可扩展的多模态分析。

Result: 该平台创建了一个集成环境，支持可重复的工作流程，促进跨学科研究，并有助于更深入地了解疾病机制和改善患者护理。

Conclusion: 该工具箱通过自动化图像配准和对齐，有效桥接了放射学和病理学，解决了多模态医学数据集成中的关键障碍，为跨学科协作和疾病研究提供了实用解决方案。

Abstract: Accurate disease diagnosis depends on effective collaboration between medical specialties, yet departments often use distinct data systems and proprietary formats. This heterogeneity hinders joint analysis and integration of complementary diagnostic information. The use of separate viewers for each modality further restricts cross-specialty collaboration. Although multimodal integration, particularly between radiology and pathology, has demonstrated potential for identifying novel biomarkers, it still relies heavily on manual, time-consuming data pairing. This project introduces an interdisciplinary toolbox that can operate within the Kaapana framework or as a standalone tool to bridge radiology and pathology. By linking modalityspecific viewers and extending them with automated image registration and alignment, the platform enables efficient, scalable multimodal analysis. The integrated environment promotes reproducible workflows, accelerates crossdisciplinary research, and facilitates deeper insights into disease mechanisms and patient care.

</details>


### [57] [GPU-Resident Inverted File Index for Streaming Vector Databases](https://arxiv.org/abs/2601.11808)
*Dongfang Zhao*

Main category: cs.DB

TL;DR: SIVF（流式倒排文件）是一种新的GPU原生架构，用于向量数据库，支持高速数据插入和删除，解决了传统静态IVF索引在流式场景中的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 传统GPU加速的IVF索引虽然内存效率高，但采用静态内存布局，缺乏原地修改能力，导致流式场景中需要昂贵的CPU-GPU往返操作，系统延迟从毫秒级激增至秒级，无法满足实时知识更新的需求。

Method: SIVF采用基于slab的分配系统和有效性位图，支持VRAM中的无锁原地修改；引入GPU驻留的地址转换表（ATT），实现O(1)访问物理存储槽位。

Result: 在SIFT1M和GIST1M数据集上，SIVF将删除延迟降低高达13,300倍（从11.8秒降至0.89毫秒），插入吞吐量提升36-105倍；在端到端滑动窗口场景中，消除系统冻结，实现161-266倍加速，保持毫秒级延迟，内存开销小于0.8%。

Conclusion: SIVF为向量数据库提供了高效的流式处理能力，显著提升了实时更新场景下的性能，同时保持了极低的内存开销，是AI基础设施中向量搜索的重要改进。

Abstract: Vector search has emerged as the computational backbone of modern AI infrastructure, powering critical systems ranging from Vector Databases to Retrieval-Augmented Generation (RAG). While the GPU-accelerated Inverted File (IVF) index acts as one of the most widely used techniques for these large-scale workloads due to its memory efficiency, its traditional architecture remains fundamentally static. Existing designs rely on rigid and contiguous memory layouts that lack native support for in-place mutation, creating a severe bottleneck for streaming scenarios. In applications requiring real-time knowledge updates, such as live recommendation engines or dynamic RAG systems, maintaining index freshness necessitates expensive CPU-GPU roundtrips that cause system latency to spike from milliseconds to seconds. In this paper, we propose SIVF (Streaming Inverted File), a new GPU-native architecture designed to empower vector databases with high-velocity data ingestion and deletion capabilities. SIVF replaces the static memory layout with a slab-based allocation system and a validity bitmap, enabling lock-free and in-place mutation directly in VRAM. We further introduce a GPU-resident address translation table (ATT) to resolve the overhead of locating vectors, providing $O(1)$ access to physical storage slots. We evaluate SIVF against the industry-standard GPU IVF implementation on the SIFT1M and GIST1M datasets. Microbenchmarks demonstrate that SIVF reduces deletion latency by up to $13,300\times$ (from 11.8 seconds to 0.89 ms on GIST1M) and improves ingestion throughput by $36\times$ to $105\times$. In end-to-end sliding window scenarios, SIVF eliminates system freezes and achieves a $161\times$ to $266\times$ speedup with single-digit millisecond latency. Notably, this performance incurs negligible storage penalty, maintaining less than 0.8\% memory overhead compared to static indices.

</details>


### [58] [Is Quantum Computing Ready for Real-Time Database Optimization?](https://arxiv.org/abs/2601.12123)
*Hanwen Liu,Ibrahim Sabek*

Main category: cs.DB

TL;DR: Q2O：首个量子增强查询优化器，通过非线性模型编码连接顺序问题，利用低延迟量子求解器实现实时查询优化


<details>
  <summary>Details</summary>
Motivation: 数据库优化问题（如连接顺序、索引调优）随着数据量增长和工作负载复杂化变得指数级困难。量子计算，特别是量子退火，能够通过量子隧穿高效探索大规模搜索空间，但之前的研究因量子计算服务的高延迟（如D-Wave CQM-Solver至少5秒）而难以实际集成到数据库系统中。

Method: 提出Q2O系统：1）将连接顺序问题编码为非线性模型（NL-Solver可解格式），使用实际数据库统计信息；2）利用低延迟量子求解器（如NL-Solver）求解；3）将解转换为计划提示，指导PostgreSQL优化器生成完整执行计划。

Result: Q2O能够实时处理实际查询，首次实现了量子增强查询优化器的端到端工作流程，在效率和解决方案质量之间取得了平衡。

Conclusion: 随着量子退火提供商提供低延迟解决方案，量子计算在数据库系统中的实际应用成为可能。Q2O证明了在效率和解决方案质量之间取得平衡是可行的，为量子增强数据库系统开辟了新道路。

Abstract: Database systems encompass several performance-critical optimization tasks, such as join ordering and index tuning. As data volumes grow and workloads become more complex, these problems have become exponentially harder to solve efficiently. Quantum computing, especially quantum annealing, is a promising paradigm that can efficiently explore very large search spaces through quantum tunneling. It can escape local optima by tunneling through energy barriers rather than climbing over them. Earlier works mainly focused on providing an abstract representation (e.g., Quadratic Unconstrained Binary Optimization (QUBO)) for the database optimization problems (e.g., join order) and overlooked the real integration within database systems due to the high overhead of quantum computing services (e.g., a minimum 5s runtime for D-Wave's CQM-Solver). Recently, quantum annealing providers have offered more low-latency solutions, e.g., NL-Solver, which paves the road to actually realizing quantum solutions within DBMSs. However, this raises new systems research challenges in balancing efficiency and solution quality.
  In this talk, we show that this balance is possible to achieve. As a proof of concept, we present Q2O, the first real Quantum-augmented Query Optimizer. We show the end-to-end workflow: we encode the join order problem as a nonlinear model, a format solvable by the NL-Solver, using actual database statistics; the solution is translated into a plan hint that guides PostgreSQL's optimizer to produce a complete plan. Q2O is capable of handling actual queries in real time.

</details>


### [59] [RLMiner: Finding the Most Frequent k-sized Subgraph via Reinforcement Learning](https://arxiv.org/abs/2601.12416)
*Wei Huang,Hanchen Wang,Dong Wen,Xin Cao,Ying Zhang,Wenjie Zhang*

Main category: cs.DB

TL;DR: RLMiner使用多任务强化学习框架和任务状态感知图神经网络，以线性时间复杂度寻找大小为k的最频繁诱导子图。


<details>
  <summary>Details</summary>
Motivation: 寻找图中最频繁的诱导子图是图挖掘中的基本问题，对Web数据挖掘和社交网络分析有重要意义。但由于子图计数是NP难问题，传统精确枚举算法时间复杂度高，现有方法通常使用向下闭包属性来减少搜索空间，但会施加额外约束。

Method: 将任务建模为马尔可夫决策过程，采用多任务强化学习框架，提出RLMiner框架，结合强化学习和任务状态感知图神经网络，时间复杂度与k呈线性关系。

Result: 在真实数据集上的实验表明，RLMiner能够有效识别频率接近真实最频繁诱导子图的子图，同时相比传统方法运行时间显著缩短且更稳定。

Conclusion: RLMiner通过强化学习框架成功解决了寻找最频繁诱导子图的计算效率问题，为大规模图挖掘提供了有效的解决方案。

Abstract: Identifying the most frequent induced subgraph of size $k$ in a target graph is a fundamental graph mining problem with direct implications for Web-related data mining and social network analysis. Despite its importance, finding the most frequent induced subgraph remains computationally expensive due to the NP-hard nature of the subgraph counting task. Traditional exact enumeration algorithms often suffer from high time complexity, especially for a large graph size $k$. To mitigate this, existing approaches often utilize frequency measurement with the Downward Closure Property to reduce the search space, imposing additional constraints on the task. In this paper, we first formulate this task as a Markov Decision Process and approach it using a multi-task reinforcement learning framework. Specifically, we introduce RLMiner, a novel framework that integrates reinforcement learning with our proposed task-state-aware Graph Neural Network to find the most frequent induced subgraph of size $k$ with a time complexity linear to $k$. Extensive experiments on real-world datasets demonstrate that our proposed RLMiner effectively identifies subgraphs with frequencies closely matching the ground-truth most frequent induced subgraphs, while achieving significantly shorter and more stable running times compared to traditional methods.

</details>


### [60] [Bringing Data Transformations Near-Memory for Low-Latency Analytics in HTAP Environments](https://arxiv.org/abs/2601.12456)
*Arthur Bernhardt,David Volz,Sajjad Tamimi,Andreas Koch,Ilia Petrov*

Main category: cs.DB

TL;DR: 提出在智能存储系统上近存储或存内执行数据转换的方法，避免传统提取-转换方式带来的性能下降和大量数据移动


<details>
  <summary>Details</summary>
Motivation: 当前主流的数据提取后转换方法存在性能下降和大量数据移动的问题，需要在智能存储系统上实现更高效的数据转换

Method: 在智能存储系统上近存储或存内执行数据转换，减少数据移动和资源争用

Result: 实现了前台工作负载的稳健性能，降低了资源争用

Conclusion: 该方法为多引擎、多系统架构以及重用提供了新的架构机会

Abstract: In this paper we propose an approach for executing data transformations near- or in-storage on intelligent storage systems. The currently prevailing approach of extracting the data and then transforming it to a target format suffers degraded performance during transformation and causes heavy data movement. Our results show robust performance of foreground workloads and lower resource contention. Our vision draws architectural opportunities in multi-engine and multi-system settings, as well as for reuse.

</details>


### [61] [xBound: Join Size Lower Bounds](https://arxiv.org/abs/2601.13117)
*Mihail Stoian,Tiemo Bang,Hangdong Zhao,Jesús Camacho-Rodríguez,Yuanyuan Tian,Andreas Kipf*

Main category: cs.DB

TL;DR: xBound是首个推导可证明连接大小下界的框架，旨在解决查询优化中更危险的基数低估问题，在多个数据库系统中显著减少了低估错误。


<details>
  <summary>Details</summary>
Motivation: 基数估计是查询优化的关键，但工业级数据库普遍存在严重的基数估计错误，且低估比高估更常见。基数低估会导致优化器选择为小数据设计的计划，造成CPU和内存资源不足分配。现有悲观基数估计方法只能修正高估问题，无法解决更严重的高估问题。

Method: 提出xBound框架，这是首个能够推导可证明连接大小下界的方法。通过建立数学理论保证，为连接操作提供可靠的下界估计，从而修正优化器中的基数低估问题。

Result: 在JOBlight基准测试中，xBound修正了DuckDB中17.5%的子表达式低估和PostgreSQL中8.7%的低估；在微软企业工作负载中，修正了Fabric Data Warehouse中36.1%的低估，显著改善了基数估计的准确性。

Conclusion: xBound框架为解决长期存在的基数低估问题迈出了重要一步，通过提供可证明的连接大小下界，有效减少了实际系统中的基数低估错误，提升了查询优化器的性能。

Abstract: Cloud database vendors invest substantial resources into their query optimizers, and for good reason. Cardinality estimation, a cornerstone of the optimizer, is critical for the selection of efficient query plans, as well as downstream tasks such as resource allocation and query scheduling. Yet, as many practitioners and researchers have noted, it is also the optimizer's Achilles heel. Prior studies on a number of industrial-strength databases show substantial cardinality estimation errors on all tested systems, with a far greater tendency to underestimate than to overestimate. Unfortunately, cardinality underestimation is more problematic than overestimation, as it misleads the optimizer to choose plans designed for small data, leading to underprovisioned CPU and memory.
  While previous work on pessimistic cardinality estimation has proposed provable join size upper bounds, such methods can only correct overestimation, leaving the more harmful problem of underestimation unaddressed. To fill this critical gap, we introduce xBound, the very first framework for deriving provable join size lower bounds. xBound successfully reduces underestimation in real systems: On the JOBlight benchmark, it corrects 17.5% of subexpression underestimates in DuckDB and 8.7% in PostgreSQL, while on a Microsoft enterprise workload, it fixes 36.1% of Fabric Data Warehouse's underestimates, demonstrating a significant step towards solving this long-standing problem.

</details>


### [62] [A Distributed Spatial Data Warehouse for AIS Data (DIPAAL)](https://arxiv.org/abs/2601.13795)
*Alex S. Klitgaard,Lau E. Josefsen,Mikael V. Mikkelsen,Kristian Torp*

Main category: cs.DB

TL;DR: 提出一个用于处理船舶AIS数据的系统，包含高效的ETL流程和分布式空间数据仓库，采用栅格化查询方法提高大规模船舶数据分析效率


<details>
  <summary>Details</summary>
Motivation: 船舶AIS数据虽然对分析单船运动和区域船舶监控很有价值，但原始数据需要经过清洗、处理和存储才能使用，特别是面对海量数据时（当前存储超过312百万公里轨迹和80亿行数据），需要高效的处理和分析系统

Method: 设计了一个包含高效模块化ETL流程的系统，用于加载AIS数据；建立了分布式空间数据仓库存储船舶轨迹；提出了基于栅格化的AIS数据查询方法；设计了空间分区的数据仓库，采用细粒度单元表示和热力图展示

Result: 系统目前存储约312百万公里船舶轨迹，最大表超过80亿行；单元表示搜索比轨迹表示搜索更快；空间分区分片使单元和热力图分析在大区域中具有良好扩展性，在5倍工作节点增加时扩展性在354%到1164%之间

Conclusion: 提出的系统能有效处理大规模AIS数据，栅格化查询方法和空间分区数据仓库设计显著提高了查询效率和系统扩展性，为大规模船舶数据分析提供了可行解决方案

Abstract: AIS data from ships is excellent for analyzing single-ship movements and monitoring all ships within a specific area. However, the AIS data needs to be cleaned, processed, and stored before being usable. This paper presents a system consisting of an efficient and modular ETL process for loading AIS data, as well as a distributed spatial data warehouse storing the trajectories of ships. To efficiently analyze a large set of ships, a raster approach to querying the AIS data is proposed. A spatially partitioned data warehouse with a granularized cell representation and heatmap presentation is designed, developed, and evaluated. Currently the data warehouse stores ~312 million kilometers of ship trajectories and more than +8 billion rows in the largest table. It is found that searching the cell representation is faster than searching the trajectory representation. Further, we show that the spatially divided shards enable a consistently good scale-up for both cell and heatmap analytics in large areas, ranging between 354% to 1164% with a 5x increase in workers

</details>


### [63] [TLSQL: Table Learning Structured Query Language](https://arxiv.org/abs/2601.14109)
*Feiyang Chen,Ken Zhong,Aoqian Zhang,Zheng Wang,Li Pan,Jianhua Li*

Main category: cs.DB

TL;DR: TLSQL是一个通过SQL-like声明式语言直接在关系数据库上进行表格学习的系统，降低了数据库从业者集成机器学习的门槛。


<details>
  <summary>Details</summary>
Motivation: 现有表格学习框架通常需要显式数据导出和大量特征工程，为数据库从业者设置了高门槛，阻碍了机器学习在数据库工作流中的集成。

Method: TLSQL实现为轻量级Python库，将声明式规范转换为标准SQL查询和结构化学习任务描述。SQL查询由数据库引擎本地执行，任务描述由下游表格学习框架使用。

Result: 在真实数据集上的实验表明，TLSQL有效降低了将机器学习集成到数据库中心工作流的门槛。

Conclusion: TLSQL通过SQL-like声明式规范直接在关系数据库上实现表格学习，让用户专注于建模和分析而非底层数据准备和管道编排。

Abstract: Table learning, which lies at the intersection of machine learning and modern database systems, has recently attracted growing attention. However, existing frameworks typically require explicit data export and extensive feature engineering, creating a high barrier for database practitioners. We present TLSQL (Table Learning Structured Query Language), a system that enables table learning directly over relational databases via SQL-like declarative specifications. TLSQL is implemented as a lightweight Python library that translates these specifications into standard SQL queries and structured learning task descriptions. The generated SQL queries are executed natively by the database engine, while the task descriptions are consumed by downstream table learning frameworks. This design allows users to focus on modeling and analysis rather than low-level data preparation and pipeline orchestration. Experiments on real-world datasets demonstrate that TLSQL effectively lowers the barrier to integrating machine learning into databasecentric workflows. Our code is available at https://github.com/rllmproject/tlsql/.

</details>


### [64] [ReSearch: A Multi-Stage Machine Learning Framework for Earth Science Data Discovery](https://arxiv.org/abs/2601.14176)
*Youran Sun,Yixin Wen,Haizhao Yang*

Main category: cs.DB

TL;DR: ReSearch是一个多阶段推理增强搜索框架，通过意图解释、高召回检索和上下文感知排序来解决地球科学数据发现瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 地球科学数据（卫星观测、再分析产品、数值模拟）的快速增长导致科学发现面临关键瓶颈：难以根据研究目标识别相关数据集。现有发现系统主要是检索中心化的，难以在高层科学意图和异构元数据之间建立规模化桥梁。

Method: ReSearch是一个多阶段、推理增强的搜索框架，将地球科学数据发现制定为迭代过程：意图解释、高召回检索和上下文感知排序。该框架整合了词汇搜索、语义嵌入、缩写扩展和大语言模型重排序，在统一架构中明确分离召回和精度目标。

Result: 实验表明，ReSearch在基于文献构建的基准测试中，相比基线方法持续提高了召回率和排序性能，特别是在表达抽象科学目标的任务型查询中表现突出。

Conclusion: 这些结果强调了意图感知、多阶段搜索作为可重复和可扩展地球科学研究基础能力的重要性。ReSearch框架为解决地球科学数据发现瓶颈提供了有效解决方案。

Abstract: The rapid expansion of Earth Science data from satellite observations, reanalysis products, and numerical simulations has created a critical bottleneck in scientific discovery, namely identifying relevant datasets for a given research objective.
  Existing discovery systems are primarily retrieval-centric and struggle to bridge the gap between high-level scientific intent and heterogeneous metadata at scale.
  We introduce \textbf{ReSearch}, a multi-stage, reasoning-enhanced search framework that formulates Earth Science data discovery as an iterative process of intent interpretation, high-recall retrieval, and context-aware ranking.
  ReSearch integrates lexical search, semantic embeddings, abbreviation expansion, and large language model reranking within a unified architecture that explicitly separates recall and precision objectives.
  To enable realistic evaluation, we construct a literature-grounded benchmark by aligning natural language intent with datasets cited in peer-reviewed Earth Science studies.
  Experiments demonstrate that ReSearch consistently improves recall and ranking performance over baseline methods, particularly for task-based queries expressing abstract scientific goals.
  These results underscore the importance of intent-aware, multi-stage search as a foundational capability for reproducible and scalable Earth Science research.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [65] [Exact Computation of the Catalan Number $C(2,050,572,903)$](https://arxiv.org/abs/2601.11621)
*Mahesh Ramani*

Main category: cs.DS

TL;DR: 提出一种两阶段算法，首次在空前规模上精确计算卡特兰数，成功计算了n=2,050,572,903时的C(n)，结果包含1,234,567,890位十进制数字。


<details>
  <summary>Details</summary>
Motivation: 传统计算大卡特兰数的方法面临内存限制问题，特别是计算大阶乘时。需要开发一种能够处理极大n值的新算法，以突破现有计算规模的限制。

Method: 采用两阶段算法：第一阶段使用并行分段筛法枚举素数至2n，应用Legendre公式确定C(n)的精确素数分解；第二阶段使用内存高效的平衡乘积树和分块技术重建最终整数。算法完全在素数指数域中操作，避免直接计算大阶乘。

Result: 成功计算了n=2,050,572,903时的卡特兰数，结果包含1,234,567,890位十进制数字，这是迄今为止计算的最大精确卡特兰数。算法时间复杂度为Θ(n(log n)²)比特操作，空间复杂度为Θ(n log n)比特。

Conclusion: 该算法通过避免直接计算大阶乘，在素数指数域中操作，成功突破了计算大卡特兰数的内存限制。提供了源代码和分解数据确保可重复性，并通过模数检查和SHA-256哈希验证结果正确性。

Abstract: This paper presents a two-phase algorithm for computing exact Catalan numbers at an unprecedented scale. The method is demonstrated by computing $C(n)$ for $n = 2,050,572,903$ yielding a result with a targeted $1,234,567,890$ decimal digits. To circumvent the memory limitations associated with evaluating large factorials, the algorithm operates exclusively in the prime-exponent domain. Phase 1 employs a parallel segmented sieve to enumerate primes up to $2n$ and applies Legendre's formula to determine the precise prime factorization of $C(n)$. The primes are grouped by exponent and serialized to disk. Phase 2 reconstructs the final integer using a memory-efficient balanced product tree with chunking. The algorithm runs on a time complexity of $Θ(n(\log n)^2)$ bit-operations and a space complexity of $Θ(n \log n)$ bits. This result represents the largest exact Catalan number computed to date. Performance statistics for a single-machine execution are reported, and verification strategies -- including modular checks and SHA-256 hash validation -- are discussed. The source code and factorization data are provided to ensure reproducibility.

</details>


### [66] [Bicriteria Algorithms for Submodular Cover with Partition and Fairness Constraints](https://arxiv.org/abs/2601.11755)
*Wenjing Chen,Yixin Chen,Victoria G. Crawford*

Main category: cs.DS

TL;DR: 提出了带分区约束的子模覆盖问题(SCP)及其变体，开发了可扩展的双准则近似算法，在单调和非单调目标下均有效，显著降低了查询复杂度。


<details>
  <summary>Details</summary>
Motivation: 在许多子模优化应用中，数据集自然被划分为不相交的子集，这产生了基于分区的约束问题，需要解决方案在分区之间保持平衡、公平或资源受限。现有子模覆盖研究大多忽略了这种结构。

Method: 对带分区约束的子模覆盖问题(SCP)及其关键变体进行全面研究，为这些NP难优化问题开发可扩展的双准则近似算法，适用于单调和非单调目标函数。

Result: 单调情况下的算法实现了最优近似保证，同时相比现有方法显著降低了查询复杂度。在真实世界和合成数据集上的实证评估进一步验证了算法的效率和有效性。

Conclusion: 该研究填补了子模优化中分区约束问题的空白，提出的算法在理论和实践上都表现出色，为处理自然分区的子模覆盖问题提供了有效的解决方案。

Abstract: In many submodular optimization applications, datasets are naturally partitioned into disjoint subsets. These scenarios give rise to submodular optimization problems with partition-based constraints, where the desired solution set should be in some sense balanced, fair, or resource-constrained across these partitions. While existing work on submodular cover largely overlooks this structure, we initiate a comprehensive study of the problem of Submodular Cover with Partition Constraints (SCP) and its key variants. Our main contributions are the development and analysis of scalable bicriteria approximation algorithms for these NP-hard optimization problems for both monotone and nonmonotone objectives. Notably, the algorithms proposed for the monotone case achieve optimal approximation guarantees while significantly reducing query complexity compared to existing methods. Finally, empirical evaluations on real-world and synthetic datasets further validate the efficiency and effectiveness of the proposed algorithms.

</details>


### [67] [Sum Estimation via Vector Similarity Search](https://arxiv.org/abs/2601.11765)
*Stephen Mussmann,Mehul Smriti Raje,Kavya Tumkur,Oumayma Messoussi,Cyprien Hachem,Seby Jacob*

Main category: cs.DS

TL;DR: 提出一种新颖的算法，仅需O(log(n))个最相似向量即可估计集合中所有对象的和（如核密度估计），相比现有方法的O(√n)有显著改进。


<details>
  <summary>Details</summary>
Motivation: 语义嵌入在机器学习中广泛应用，但现有方法在估计集合中所有对象的和（如核密度估计、softmax归一化常数）时需要获取O(√n)个最相似向量，计算成本高。

Method: 采用随机分层策略：以指数衰减概率将对象分配到不同层级，为每层构建向量相似性搜索数据结构，通过各层top-k对象构建无偏估计量。

Result: 在OpenImages和Amazon Reviews数据集上的实验表明，该方法比现有方法使用更少计算时间获得更低误差，在密度估计、softmax分母计算和向量计数等应用中表现优异。

Conclusion: 提出的分层随机算法将和估计任务所需的相似向量数量从O(√n)降至O(log(n))，显著提高了计算效率，为大规模语义嵌入应用提供了实用解决方案。

Abstract: Semantic embeddings to represent objects such as image, text and audio are widely used in machine learning and have spurred the development of vector similarity search methods for retrieving semantically related objects. In this work, we study the sibling task of estimating a sum over all objects in a set, such as the kernel density estimate (KDE) and the normalizing constant for softmax distributions. While existing solutions provably reduce the sum estimation task to acquiring $\mathcal{O}(\sqrt{n})$ most similar vectors, where $n$ is the number of objects, we introduce a novel algorithm that only requires $\mathcal{O}(\log(n))$ most similar vectors. Our approach randomly assigns objects to levels with exponentially-decaying probabilities and constructs a vector similarity search data structure for each level. With the top-$k$ objects from each level, we propose an unbiased estimate of the sum and prove a high-probability relative error bound. We run experiments on OpenImages and Amazon Reviews with a vector similar search implementation to show that our method can achieve lower error using less computational time than existing reductions. We show results on applications in estimating densities, computing softmax denominators, and counting the number of vectors within a ball.

</details>


### [68] [Analysis of a Random Local Search Algorithm for Dominating Set](https://arxiv.org/abs/2601.11841)
*Hendrik Higl*

Main category: cs.DS

TL;DR: 本文分析了在环图上寻找最小支配集的随机局部搜索算法，证明了其期望运行时间的上界为O(n⁴log²n)，并引入了新的建模方法。


<details>
  <summary>Details</summary>
Motivation: 支配集问题在计算生物学和移动通信中有重要应用，但由于其NP难特性，通常使用启发式算法。现有启发式算法虽然表现良好，但缺乏理论保证。本文旨在为随机局部搜索算法建立严格的理论基础，分析其在环图上的性能。

Method: 1. 分析随机局部搜索算法在环图上寻找最小支配集的过程；2. 引入多种模型来表示环图上的支配集，帮助理解算法如何探索搜索空间；3. 使用随机算法分析中流行的技术；4. 应用特殊方法分析可逆马尔可夫链。

Result: 证明了在n个顶点的环图上，随机局部搜索算法找到最小支配集的期望运行时间上界为O(n⁴log²n)。

Conclusion: 本文为支配集问题的随机局部搜索算法提供了严格的理论分析，证明了其在环图上的多项式时间性能保证，并引入了新的建模和分析技术，为理解启发式算法的性能提供了理论基础。

Abstract: Dominating Set is a well-known combinatorial optimization problem which finds application in computational biology or mobile communication. Because of its $\mathrm{NP}$-hardness, one often turns to heuristics for good solutions. Many such heuristics have been empirically tested and perform rather well. However, it is not well understood why their results are so good or even what guarantees they can offer regarding their runtime or the quality of their results. For this, a strong theoretical foundation has to be established. We contribute to this by rigorously analyzing a Random Local Search (RLS) algorithm that aims to find a minimum dominating set on a graph. We consider its performance on cycle graphs with $n$ vertices. We prove an upper bound for the expected runtime until an optimum is found of $\mathcal{O}\left(n^4\log^2(n)\right)$. In doing so, we introduce several models to represent dominating sets on cycles that help us understand how RLS explores the search space to find an optimum. For our proof we use techniques which are already quite popular for the analysis of randomized algorithms. We further apply a special method to analyze a reversible Markov Chain, which arises as a result of our modeling. This method has not yet found wide application in this kind of runtime analysis.

</details>


### [69] [Parameterized Complexity of Scheduling Problems in Robotic Process Automation](https://arxiv.org/abs/2601.11984)
*Michal Dvořák,Antonín Novák,Přemysl Šůcha,Dušan Knop,Claire Hanen*

Main category: cs.DS

TL;DR: 研究RPA调度问题1|prec,r_j,d_j|*的参数化复杂度，发现对链数参数是W[2]-难的，但在特定条件下有多项式时间算法和FPT算法


<details>
  <summary>Details</summary>
Motivation: 受机器人流程自动化（RPA）中出现的调度问题启发，研究单机调度问题1|prec,r_j,d_j|*的参数化复杂度，重点关注与RPA系统自然相关的参数

Method: 采用参数化复杂度分析方法，研究链状优先关系、不同处理时间数量、时间窗口结构等参数。通过理论证明建立问题的计算复杂度边界

Result: 发现问题对链数参数是W[2]-难的，即使只有两个处理时间和两个时间窗口长度。在特定条件下获得多项式时间算法（所有作业共享单一时间窗口长度）和FPT算法（处理时间、释放时间和截止时间链一致）。问题对优先关系宽度参数属于XP类

Conclusion: RPA调度问题在一般参数设置下计算困难，但在特定结构约束下存在高效算法，为RPA系统调度提供了理论指导和算法设计基础

Abstract: This paper studies the growing domain of Robotic Process Automation (RPA) problems. Motivated by scheduling problems arising in RPA, we study the parameterized complexity of the single-machine problem $1|\text{prec},r_j,d_j|*$. We focus on parameters naturally linked to RPA systems, including chain-like precedences, the number of distinct processing times, and the structure of the time windows. We show that the problem is W[2]-hard parameterized by the number of chains, even with only two prescribed processing times and two distinct time-window lengths. This hardness remains even for distinct processing times and time windows under prec-consistent time windows. On the positive side, we obtain polynomial-time algorithm when all jobs share a single time-window length and FPT when the processing times, release times and deadlines are chain-uniform. We also show that the problem lies in XP when parameterized by the width of the precedence relation.

</details>


### [70] [Computing Maximal Repeating Subsequences in a String](https://arxiv.org/abs/2601.12200)
*Mingyang Gong,Adiesha Liyanage,Braeden Sopp,Binhai Zhu*

Main category: cs.DS

TL;DR: 本文研究在单个字符串中计算最大（非最长）重复子序列的问题，提出了比最长重复子序列更高效的算法。


<details>
  <summary>Details</summary>
Motivation: 以往关于最大重复模式的研究主要集中在两个或多个输入字符串上（如Hirota和Sakai从2019年开始的工作），而本文首次在单个输入字符串中研究计算最大（非最长）重复子序列的问题。

Method: 针对长度为n的输入字符串S，提出了计算最大平方子序列的O(n log n)算法，以及计算最大k-重复子序列的O(f(k)n log n)算法，其中f(k)是可计算函数且f(k) < k·4^k。这些算法也适用于约束情况（解必须包含S的某个子序列X）。

Result: 最大平方子序列算法将时间复杂度从O(n²)大幅提升到O(n log n)；最大k-重复子序列算法从O(n^{2k-1})提升到O(f(k)n log n)，其中f(k) < k·4^k。约束情况下的运行时间略高。

Conclusion: 本文首次在单个字符串中研究最大重复子序列问题，提出了比计算最长重复子序列更高效的算法，显著改进了时间复杂度，为相关领域提供了新的研究方向。

Abstract: In this paper we initiate the study of computing a maximal (not necessarily maximum) repeating pattern in a single input string, where the corresponding problems have been studied (e.g., a maximal common subsequence) only in two or more input strings by Hirota and Sakai starting 2019. Given an input string $S$ of length $n$, we can compute a maximal square subsequence of $S$ in $O(n\log n)$ time, greatly improving the $O(n^2)$ bound for computing the longest square subsequence of $S$. For a maximal $k$-repeating subsequence, our bound is $O(f(k)n\log n)$, where \(f(k)\) is a computable function such that $f(k) < k\cdot 4^k$. This greatly improves the $O(n^{2k-1})$ bound for computing a longest $k$-repeating subsequence of $S$, for $k\geq 3$. Both results hold for the constrained case, i.e., when the solution must contain a subsequence $X$ of $S$, though with higher running times.

</details>


### [71] [Analyzing Collection Strategies: A Computational Perspective on the Coupon Collector Problem](https://arxiv.org/abs/2601.12351)
*Hadas Abraham,Ido Feldman,Eitan Yaakobi*

Main category: cs.DS

TL;DR: 提出了三种算法解决最通用的优惠券收集问题，其中优惠券按任意概率抽取，目标是从n种优惠券中收集k种特定优惠券各t份，算法基于马尔可夫模型和动态规划精确计算收集过程的期望和方差。


<details>
  <summary>Details</summary>
Motivation: 优惠券收集问题在工程领域有广泛应用，但实际应用中由于计算复杂性，难以推导数值结果（矩和分布），阻碍了实际应用。

Method: 提出了三种算法：1) 基础模型计算收集过程的期望、方差和二阶矩；2) 在均匀抽取分布下，利用基础模型构造在n的多项式时间内计算相同值；3) 扩展到任意一般抽取分布。所有算法都基于专门设计的马尔可夫模型解决计算挑战，采用动态规划方法实现。

Result: 算法能够精确计算收集过程的期望和方差，第二个算法在均匀分布下具有关于n的多项式时间复杂度，第三个算法适用于任意分布。

Conclusion: 提出的三种算法解决了最通用优惠券收集问题的计算挑战，通过马尔可夫模型和动态规划实现了期望和方差的精确计算，为实际工程应用提供了可行的解决方案。

Abstract: The Coupon Collector Problem (CCP) is a well-known combinatorial problem that seeks to estimate the number of random draws required to complete a collection of $n$ distinct coupon types. Various generalizations of this problem have been applied in numerous engineering domains. However, practical applications are often hindered by the computational challenges associated with deriving numerical results for moments and distributions. In this work, we present three algorithms for solving the most general form of the CCP, where coupons are collected under any arbitrary drawing probability, with the objective of obtaining $t$ copies of a subset of $k$ coupons from a total of $n$. The First algorithm provides the base model to compute the expectation, variance, and the second moment of the collection process. The second algorithm utilizes the construction of the base model and computes the same values in polynomial time with respect to $n$ under the uniform drawing distribution, and the third algorithm extends to any general drawing distribution. All algorithms leverage Markov models specifically designed to address computational challenges, ensuring exact computation of the expectation and variance of the collection process. Their implementation uses a dynamic programming approach that follows from the Markov models framework, and their time complexity is analyzed accordingly.

</details>


### [72] [Approximation Schemes for Sequential Hiring Problems](https://arxiv.org/abs/2601.12750)
*Danny Segev,Uri Stein*

Main category: cs.DS

TL;DR: 本文针对序列招聘问题提出了首个多项式时间近似方案，通过引入块响应式策略实现了接近最优的期望收益。


<details>
  <summary>Details</summary>
Motivation: 序列招聘问题是一个动态优化模型，公司需要从已知价值和接受概率的申请人池中自适应地填补有限职位。虽然已有研究建立了LP基础的近似算法，但能否获得更好的近似保证一直是一个核心开放问题。

Method: 1. 引入块响应式策略：选择有序的申请人块而非单个申请人，同时保持内部响应性
2. 证明这些策略能够接近一般自适应策略的性能，同时使用多项式规模的决策树
3. 开发基于递归枚举的高效构造框架，解决"少职位"机制的根本障碍

Result: 提出了首个序列招聘问题的多项式时间近似方案，能够在O(n^{O(1)}·T^{2^{Õ(1/ε²)}})时间内构造半自适应策略，其期望收益在最优解的(1-ε)因子内。

Conclusion: 通过引入块响应式策略和递归枚举框架，本文克服了先前文献中的常数因子最优性损失，突破了自适应策略的内在表示障碍，为序列招聘问题提供了理论上更优的近似算法。

Abstract: The main contribution of this paper resides in providing novel algorithmic advances and analytical insights for the sequential hiring problem, a recently introduced dynamic optimization model where a firm adaptively fills a limited number of positions from a pool of applicants with known values and acceptance probabilities. While earlier research established a strong foundation -- notably an LP-based $(1 - \frac{e^{-k}k^k}{k!})$-approximation by Epstein and Ma (Operations Research, 2024) -- the attainability of superior approximation guarantees has remained a central open question.
  Our work addresses this challenge by establishing the first polynomial-time approximation scheme for sequential hiring, proposing an $O(n^{O(1)} \cdot T^{2^{\tilde{O}(1/ε^{2})}})$-time construction of semi-adaptive policies whose expected reward is within factor $1 - ε$ of optimal. To overcome the constant-factor optimality loss inherent to earlier literature, and to circumvent intrinsic representational barriers of adaptive policies, our approach is driven by the following innovations:
  -- The block-responsive paradigm: We introduce block-responsive policies, a new class of decision-making strategies, selecting ordered sets (blocks) of applicants rather than single individuals, while still allowing for internal reactivity.
  -- Adaptivity and efficiency: We prove that these policies can nearly match the performance of general adaptive policies while utilizing polynomially-sized decision trees.
  -- Efficient construction: By developing a recursive enumeration-based framework, we resolve the problematic ``few-positions'' regime, bypassing a fundamental hurdle that hindered previous approaches.

</details>


### [73] [Kd-tree Based Wasserstein Distance Approximation for High-Dimensional Data](https://arxiv.org/abs/2601.12975)
*Kanata Teshigawara,Keisho Oh,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.DS

TL;DR: 提出kd-Flowtree方法，使用kd树近似计算Wasserstein距离，解决高维空间中现有树方法构建时间长、深度不足导致精度差的问题，在检索任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Wasserstein距离在检索任务中计算复杂度高（立方时间复杂度），现有树近似方法（如Flowtree使用四叉树）在高维空间中构建时间长且深度不足，导致近似精度差，需要更高效、适应高维的近似方法。

Method: 提出kd-Flowtree方法，基于kd树进行数据嵌入和Wasserstein距离近似计算。kd树能在高维情况下深度生长且自适应，构建速度快于四叉树，减少包括预处理在内的最近邻搜索计算时间。

Result: 提供了kd-Flowtree最近邻搜索精度的概率上界，该上界与数据集大小无关。在真实数据检索任务的数值实验中，kd-Flowtree优于现有的Wasserstein距离近似方法。

Conclusion: kd-Flowtree通过使用kd树解决了高维空间中Wasserstein距离近似计算的问题，在保持良好近似精度的同时减少了计算时间，在检索任务中表现优于现有方法。

Abstract: The Wasserstein distance is a discrepancy measure between probability distributions, defined by an optimal transport problem. It has been used for various tasks such as retrieving similar items in high-dimensional images or text data. In retrieval applications, however, the Wasserstein distance is calculated repeatedly, and its cubic time complexity with respect to input size renders it unsuitable for large-scale datasets. Recently, tree-based approximation methods have been proposed to address this bottleneck. For example, the Flowtree algorithm computes transport on a quadtree and evaluates cost using the ground metric, and clustering-tree approaches have been reported to achieve high accuracy. However, these existing trees often incur significant construction time for preprocessing, and crucially, standard quadtrees cannot grow deep enough in high-dimensional spaces, resulting in poor approximation accuracy. In this paper, we propose kd-Flowtree, a kd-tree-based Wasserstein distance approximation method that uses a kd-tree for data embedding. Since kd-trees can grow sufficiently deep and adaptively even in high-dimensional cases, kd-Flowtree is capable of maintaining good approximation accuracy for such cases. In addition, kd-trees can be constructed quickly than quadtrees, which contributes to reducing the computation time required for nearest neighbor search, including preprocessing. We provide a probabilistic upper bound on the nearest-neighbor search accuracy of kd-Flowtree, and show that this bound is independent of the dataset size. In the numerical experiments, we demonstrated that kd-Flowtree outperformed the existing Wasserstein distance approximation methods for retrieval tasks with real-world data.

</details>


### [74] [The Energy-Throughput Trade-off in Lossless-Compressed Source Code Storage](https://arxiv.org/abs/2601.13220)
*Paolo Ferragina,Francesco Tosoni*

Main category: cs.DS

TL;DR: 该论文研究用于大规模源代码数据集索引的压缩键值存储设计，评估其在压缩空间占用、时间和能源效率三个主要计算资源之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 从大规模源代码档案中检索数据对于AI训练、基于神经网络的软件分析和信息检索至关重要。需要设计高效的存储后端来支持这些应用，同时考虑可持续性和能源效率。

Method: 设计压缩键值存储用于源代码数据集索引，在不同压缩配置下进行实验评估，研究数据并行性对性能和能源效率的影响，并在国家高性能计算基础设施上进行广泛实验。

Result: 不同压缩配置产生不同的权衡关系，高压缩比带来检索吞吐量和能源效率的数量级提升。数据并行性显著提高速度，但能源效率扩展更困难，反映了现代硬件的非能源比例特性。

Conclusion: 该工作简化了能源感知配置调优和标准化绿色基准测试的自动化，为系统架构师提供了帕累托最优的能源-压缩-吞吐量权衡谱系和可操作指南，用于构建可持续、高效的大规模开源代码档案存储后端。

Abstract: Retrieving data from large-scale source code archives is vital for AI training, neural-based software analysis, and information retrieval, to cite a few. This paper studies and experiments with the design of a compressed key-value store for the indexing of large-scale source code datasets, evaluating its trade-off among three primary computational resources: (compressed) space occupancy, time, and energy efficiency. Extensive experiments on a national high-performance computing infrastructure demonstrate that different compression configurations yield distinct trade-offs, with high compression ratios and order-of-magnitude gains in retrieval throughput and energy efficiency. We also study data parallelism and show that, while it significantly improves speed, scaling energy efficiency is more difficult, reflecting the known non-energy-proportionality of modern hardware and challenging the assumption of a direct time-energy correlation. This work streamlines automation in energy-aware configuration tuning and standardized green benchmarking deployable in CI/CD pipelines, thus empowering system architects with a spectrum of Pareto-optimal energy-compression-throughput trade-offs and actionable guidelines for building sustainable, efficient storage backends for massive open-source code archival.

</details>


### [75] [Learning-Augmented Online TRP on a Line](https://arxiv.org/abs/2601.13494)
*Swapnil Guragain,Gokarna Sharma*

Main category: cs.DS

TL;DR: 该论文研究了带学习增强的在线旅行修理工问题，在预测可能出错的情况下，设计了竞争比为3.732的确定性算法，当预测误差为δ时，竞争比变为min{3.732+4δ,4}。


<details>
  <summary>Details</summary>
Motivation: 在线旅行修理工问题在传统模型中存在较高的竞争比下界（2.414）和上界（4）。作者希望通过机器学习提供的预测来改进算法性能，探索学习增强框架在该问题中的应用。

Method: 采用学习增强框架，利用可能出错的预测位置信息（已知预测误差δ）。首先建立了3竞争比下界，然后设计了确定性算法，在完美预测时达到3.732竞争比，在预测误差为δ时达到min{3.732+4δ,4}竞争比。

Result: 1) 建立了3竞争比下界；2) 设计了完美预测下3.732竞争比的确定性算法；3) 在预测误差δ下，算法竞争比为min{3.732+4δ,4}；4) 这是该问题在学习增强框架下的首个结果。

Conclusion: 学习增强框架能有效改进在线旅行修理工问题的性能，即使在预测存在误差的情况下，算法也能保持有界的竞争比，为在线优化问题提供了新的解决方案。

Abstract: We study the online traveling repairperson problem on a line within the recently proposed learning-augmented framework, which provides predictions on the requests to be served via machine learning. In the original model (with no predictions), there is a stream of requests released over time along the line. The goal is to minimize the sum (or average) of the completion times of the requests. In the original model, the state-of-the-art competitive ratio lower bound is $1+\sqrt{2} > 2.414$ for any deterministic algorithm and the state-of-the-art competitive ratio upper bound is 4 for a deterministic algorithm. Our prediction model involves predicted positions, possibly error-prone, of each request in the stream known a priori but the arrival times of requests are not known until their arrival. We first establish a 3-competitive lower bound which extends to the original model. We then design a deterministic algorithm that is $(2+\sqrt{3})\approx 3.732$-competitive when predictions are perfect. With imperfect predictions (maximum error $δ> 0$), we show that our deterministic algorithm becomes $\min\{3.732+4δ,4\}$-competitive, knowing $δ$. To the best of our knowledge, these are the first results for online traveling repairperson problem in the learning-augmented framework.

</details>


### [76] [Nemesis, an Escape Game in Graphs](https://arxiv.org/abs/2601.13841)
*Pierre Bergé,Antoine Dailly,Yan Gerard*

Main category: cs.DS

TL;DR: 论文定义了一种名为Nemesis的图论逃逸游戏，逃犯需到达出口顶点，Nemesis通过删除边来困住逃犯。在树和最大度为3的图中可线性时间求解，但在一般图中是PSPACE完全的。


<details>
  <summary>Details</summary>
Motivation: 研究图论中的追逃游戏，探索逃犯和追捕者之间的策略复杂性，特别是在动态删除边的约束条件下。这类游戏在网络安全、机器人路径规划等领域有应用价值。

Method: 定义Nemesis游戏规则：逃犯在图上移动，Nemesis每轮删除一条边。分析不同图类（树、最大度3的图、一般图、平面多重图）的算法复杂性和可解性，并扩展到相关游戏如Blizzard和Cat Herding。

Result: 1. 在树和最大度3的图中，Nemesis可在线性时间内求解；2. 变体游戏Blizzard也有线性时间解；3. 一般图中Nemesis是PSPACE完全的；4. 平面多重图中是NP难的；5. Cat Herding也是PSPACE完全的；6. 基于全二叉逃逸树的策略是NP完全的。

Conclusion: Nemesis游戏在图论追逃问题中展现出丰富的复杂性层次：在某些受限图类中可高效求解，但在一般图中具有很高的计算复杂性（PSPACE完全）。这些结果为图论游戏理论提供了新的复杂性分类。

Abstract: We define a new escape game in graphs that we call Nemesis. The game is played on a graph having a subset of vertices labeled as exits and the goal of one of the two players, called the fugitive, is to reach one of these exit vertices. The second player, i.e. the fugitive adversary, is called the Nemesis. Her goal is to trap the fugitive in a connected component which does not contain any exit. At each round of the game, the fugitive moves from one vertex to an adjacent vertex. Then the Nemesis deletes one edge anywhere in the graph. The game ends when either the fugitive reached an exit or when he is in a connected component that does not contain any exit. In trees and graphs of maximum degree bounded by 3, Nemesis can be solved in linear time. We also show that a variant of the game called Blizzard where only edges adjacent to the position of the fugitive can be deleted also admits a linear time solution. For arbitrary graphs, we show that Nemesis is PSPACE-complete, and that it is NP-hard on planar multigraphs. We extend our results to the related Cat Herding problem, proving its PSPACE-completeness. We also prove that finding a strategy based on a full binary escape tree whose leaves are exists is NP-complete.

</details>


### [77] [Zero-free regions and concentration inequalities for hypergraph colorings in the local lemma regime](https://arxiv.org/abs/2601.13796)
*Jingcheng Liu,Yixiao Yu*

Main category: cs.DS

TL;DR: 该论文证明了对于k-均匀超图的q-染色问题，当k≥50且q≥700Δ^{5/(k-10)}时，配分函数在区间[0,1]周围存在"Lee-Yang"零自由带，从而得到Berry-Esseen型不等式和渐近正态性结果。


<details>
  <summary>Details</summary>
Motivation: 研究超图染色问题的配分函数零自由区域，以建立统计性质（如渐近正态性）并设计确定性近似算法。将最近关于约束满足问题（CSP）的工作扩展到更一般的设置。

Method: 基于投影-提升方案，将实轴上的信息渗流类型分析提升到复平面。引入外部场到变量，扩展Liu等人（STOC 2025）的CSP框架。对于原子CSP，在采样LLL条件下展示Chebyshev型不等式。

Result: 证明了在k≥50且q≥700Δ^{5/(k-10)}条件下，超图q-染色配分函数存在Lee-Yang零自由带，得到Berry-Esseen型不等式，显示均匀随机染色中任何颜色类大小的渐近正态性。框架还扩展到Fisher零点的研究。

Conclusion: 该工作为超图染色问题建立了零自由区域和统计性质，提供了确定性近似算法。投影-提升方案是关键技术，将实分析扩展到复平面。结果对理解CSP的相变和设计高效算法有重要意义。

Abstract: We show that for $q$-colorings in $k$-uniform hypergraphs with maximum degree $Δ$, if $k\ge 50$ and $q\ge 700Δ^{\frac{5}{k-10}}$, there is a "Lee-Yang" zero-free strip around the interval $[0,1]$ of the partition function, which includes the special case of uniform enumeration of hypergraph colorings. As an immediate consequence, we obtain Berry-Esseen type inequalities for hypergraph $q$-colorings under such conditions, demonstrating the asymptotic normality for the size of any color class in a uniformly random coloring. Our framework also extends to the study of "Fisher zeros", leading to deterministic algorithms for approximating the partition function in the zero-free region.
  Our approach is based on extending the recent work of [Liu, Wang, Yin, Yu, STOC 2025] to general constraint satisfaction problems (CSP). We focus on partition functions defined for CSPs by introducing external fields to the variables. A key component in our approach is a projection-lifting scheme, which enables us to essentially lift information percolation type analysis for Markov chains from the real line to the complex plane. Last but not least, we also show a Chebyshev-type inequality under the sampling LLL condition for atomic CSPs.

</details>


### [78] [Efficient Parallel $(Δ+1)$-Edge-Coloring](https://arxiv.org/abs/2601.13822)
*Michael Elkin,Ariel Khuzman*

Main category: cs.DS

TL;DR: 本文提出了一种更快的并行算法来解决(Δ+1)-边着色问题，改进了先前的结果，并提供了多种时间与处理器数量的权衡方案。


<details>
  <summary>Details</summary>
Motivation: Vizing定理表明每个简单图都可以用(Δ+1)种颜色进行边着色。虽然已有并行算法，但现有算法的时间复杂度较高，需要更高效的并行算法来解决这一基础问题。

Method: 设计了新的并行算法，使用O(Δ⁴·log⁴n)时间和O(m·Δ)处理器。还提出了其他变体，包括O(Δ⁴⁺ᵒ⁽¹⁾·log²n)时间配合O(m·Δ·logn·logᵟΔ)处理器的方案，以及针对小树宽图的改进算法。同时开发了快速的(Δ+1)-边着色更新算法。

Result: 新算法显著优于先前结果：Karloff-Shmoys算法为O(Δ⁵·logn·(log³n+Δ²))时间，Liang等人的修正结果为O(Δ⁴·⁵·log³Δ·logn + Δ⁴·log⁴n)时间。新算法达到O(Δ⁴·log⁴n)时间，且边着色更新算法比之前的最先进算法更快更简单。

Conclusion: 本文为(Δ+1)-边着色问题提供了更高效的并行算法，改进了时间复杂度和处理器使用效率，并指出了先前工作中的分析错误。新算法在并行计算模型中具有重要理论价值。

Abstract: We study the $(Δ+1)$-edge-coloring problem in the parallel $\left(\mathrm{PRAM}\right)$ model of computation. The celebrated Vizing's theorem [Viz64] states that every simple graph $G = (V,E)$ can be properly $(Δ+1)$-edge-colored. In a seminal paper, Karloff and Shmoys [KS87] devised a parallel algorithm with time $O\left(Δ^5\cdot\log n\cdot\left(\log^3 n+Δ^2\right)\right)$ and $O(m\cdotΔ)$ processors. This result was improved by Liang et al. [LSH96] to time $O\left(Δ^{4.5}\cdot \log^3Δ\cdot \log n + Δ^4 \cdot\log^4 n\right)$ and $O\left(n\cdotΔ^{3} +n^2\right)$ processors. [LSH96] claimed $O\left(Δ^{3.5} \cdot\log^3Δ\cdot \log n + Δ^3\cdot \log^4 n\right)$ time, but we point out a flaw in their analysis, which once corrected, results in the above bound. We devise a faster parallel algorithm for this fundamental problem. Specifically, our algorithm uses $O\left(Δ^4\cdot \log^4 n\right)$ time and $O(m\cdot Δ)$ processors. Another variant of our algorithm requires $O\left(Δ^{4+o(1)}\cdot\log^2 n\right)$ time, and $O\left(m\cdotΔ\cdot\log n\cdot\log^δΔ\right)$ processors, for an arbitrarily small $δ>0$. We also devise a few other tradeoffs between the time and the number of processors, and devise an improved algorithm for graphs with small arboricity. On the way to these results, we also provide a very fast parallel algorithm for updating $(Δ+1)$-edge-coloring. Our algorithm for this problem is dramatically faster and simpler than the previous state-of-the-art algorithm (due to [LSH96]) for this problem.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [79] [MuseAgent-1: Interactive Grounded Multimodal Understanding of Music Scores and Performance Audio](https://arxiv.org/abs/2601.11968)
*Qihao Zhao,Yunqi Cao,Yangyu Huang,Hui Yi Leong,Fan Zhang,Kim-Hui Yap,Wei Hu*

Main category: cs.MM

TL;DR: MuseAgent是一个音乐中心的多模态代理，通过整合乐谱识别和音乐转录模块，增强语言模型对音乐的理解能力，在音乐理解任务上显著优于现有MLLMs。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在音乐理解方面存在局限，无法有效处理乐谱符号表示和表演音频，需要更专门化的音乐感知基础。

Method: 通过整合光学音乐识别和自动音乐转录模块，将乐谱图像和表演音频转换为结构化符号表示，增强语言模型进行多步推理和交互的能力。

Result: 实验显示现有MLLMs在音乐理解任务上表现不佳，而MuseAgent取得显著改进，证明了结构化多模态基础对交互式音乐理解的重要性。

Conclusion: 结构化多模态基础对于音乐理解至关重要，MuseAgent通过专门的音乐感知模块有效解决了现有MLLMs在音乐领域的局限性。

Abstract: Despite recent advances in multimodal large language models (MLLMs), their ability to understand and interact with music remains limited. Music understanding requires grounded reasoning over symbolic scores and expressive performance audio, which general-purpose MLLMs often fail to handle due to insufficient perceptual grounding. We introduce MuseAgent, a music-centric multimodal agent that augments language models with structured symbolic representations derived from sheet music images and performance audio. By integrating optical music recognition and automatic music transcription modules, MuseAgent enables multi-step reasoning and interaction over fine-grained musical content. To systematically evaluate music understanding capabilities, we further propose MuseBench, a benchmark covering music theory reasoning, score interpretation, and performance-level analysis across text, image, and audio modalities. Experiments show that existing MLLMs perform poorly on these tasks, while MuseAgent achieves substantial improvements, highlighting the importance of structured multimodal grounding for interactive music understanding.

</details>


### [80] [Learning Audio-Visual Embeddings with Inferred Latent Interaction Graphs](https://arxiv.org/abs/2601.11995)
*Donghuo Zeng,Hao Niu,Yanan Wang,Masato Taya*

Main category: cs.MM

TL;DR: 提出AV-SAL、ILI和LIR框架，利用软标签预测和推断的潜在交互来解决音频-视觉嵌入学习中虚假负样本和跨模态依赖缺失问题，提升语义对齐鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习和三元组损失方法在处理音频-视觉信号时存在两个主要问题：1）将未标注的共现事件（如视频标注为"火车"但包含摩托车音频）错误地视为负样本，造成虚假负样本；2）忽略跨模态间的语义依赖关系，导致嵌入学习不完整。

Method: 提出三阶段框架：1）AV-SAL训练教师网络生成跨模态对齐的软标签分布，为非标注事件分配非零概率；2）ILI应用GRaSP算法从教师软标签推断稀疏有向的类别依赖图；3）LIR训练学生网络，结合度量损失和ILI图指导的正则化项，将依赖相关但未标注的嵌入拉近。

Result: 在AVE和VEGAS基准测试上，该方法在平均精度均值（mAP）上取得一致提升，证明通过推断潜在交互能增强嵌入学习的鲁棒性和语义一致性。

Conclusion: 通过软标签预测和推断潜在交互的框架能有效解决音频-视觉嵌入学习中的虚假负样本问题，提升跨模态语义对齐的鲁棒性，为多模态表示学习提供了新思路。

Abstract: Learning robust audio-visual embeddings requires bringing genuinely related audio and visual signals together while filtering out incidental co-occurrences - background noise, unrelated elements, or unannotated events. Most contrastive and triplet-loss methods use sparse annotated labels per clip and treat any co-occurrence as semantic similarity. For example, a video labeled "train" might also contain motorcycle audio and visual, because "motorcycle" is not the chosen annotation; standard methods treat these co-occurrences as negatives to true motorcycle anchors elsewhere, creating false negatives and missing true cross-modal dependencies. We propose a framework that leverages soft-label predictions and inferred latent interactions to address these issues: (1) Audio-Visual Semantic Alignment Loss (AV-SAL) trains a teacher network to produce aligned soft-label distributions across modalities, assigning nonzero probability to co-occurring but unannotated events and enriching the supervision signal. (2) Inferred Latent Interaction Graph (ILI) applies the GRaSP algorithm to teacher soft labels to infer a sparse, directed dependency graph among classes. This graph highlights directional dependencies (e.g., "Train (visual)" -> "Motorcycle (audio)") that expose likely semantic or conditional relationships between classes; these are interpreted as estimated dependency patterns. (3) Latent Interaction Regularizer (LIR): A student network is trained with both metric loss and a regularizer guided by the ILI graph, pulling together embeddings of dependency-linked but unlabeled pairs in proportion to their soft-label probabilities. Experiments on AVE and VEGAS benchmarks show consistent improvements in mean average precision (mAP), demonstrating that integrating inferred latent interactions into embedding learning enhances robustness and semantic coherence.

</details>


### [81] [Chain-of-Thought Compression Should Not Be Blind: V-Skip for Efficient Multimodal Reasoning via Dual-Path Anchoring](https://arxiv.org/abs/2601.13879)
*Dongxu Zhang,Yiding Sun,Cheng Tan,Wenbiao Yan,Ning Yang,Jihua Zhu,Hiajun Zhang*

Main category: cs.MM

TL;DR: V-Skip：一种针对多模态大语言模型的视觉锚定信息瓶颈优化方法，通过双路径门控机制权衡token重要性，解决视觉遗忘问题，实现2.9倍加速且精度损失可忽略


<details>
  <summary>Details</summary>
Motivation: 链式思维推理显著提升多模态大语言模型性能，但其自回归特性带来过高延迟。现有基于文本中心指标的token压缩方法在多模态场景中盲目应用，导致视觉遗忘问题——语言冗余token被错误剪枝，引发幻觉

Method: 提出V-Skip方法，将token剪枝重新表述为视觉锚定信息瓶颈优化问题。采用双路径门控机制，通过语言惊奇度和跨模态注意力流共同权衡token重要性，有效保留视觉显著锚点

Result: 在Qwen2-VL和Llama-3.2系列模型上的实验表明，V-Skip实现2.9倍加速且精度损失可忽略。特别在保留细粒度视觉细节方面表现优异，在DocVQA上超越其他基线方法30%以上

Conclusion: V-Skip通过视觉锚定信息瓶颈优化和双路径门控机制，有效解决多模态场景中的视觉遗忘问题，在显著加速推理的同时保持模型精度，为多模态大语言模型的高效推理提供了新思路

Abstract: While Chain-of-Thought (CoT) reasoning significantly enhances the performance of Multimodal Large Language Models (MLLMs), its autoregressive nature incurs prohibitive latency constraints. Current efforts to mitigate this via token compression often fail by blindly applying text-centric metrics to multimodal contexts. We identify a critical failure mode termed Visual Amnesia, where linguistically redundant tokens are erroneously pruned, leading to hallucinations. To address this, we introduce V-Skip that reformulates token pruning as a Visual-Anchored Information Bottleneck (VA-IB) optimization problem. V-Skip employs a dual-path gating mechanism that weighs token importance through both linguistic surprisal and cross-modal attention flow, effectively rescuing visually salient anchors. Extensive experiments on Qwen2-VL and Llama-3.2 families demonstrate that V-Skip achieves a $2.9\times$ speedup with negligible accuracy loss. Specifically, it preserves fine-grained visual details, outperforming other baselines over 30\% on the DocVQA.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [82] [DeepEvidence: Empowering Biomedical Discovery with Deep Knowledge Graph Research](https://arxiv.org/abs/2601.11560)
*Zifeng Wang,Zheng Chen,Ziwei Yang,Xuan Wang,Qiao Jin,Yifan Peng,Zhiyong Lu,Jimeng Sun*

Main category: cs.IR

TL;DR: DeepEvidence是一个AI代理框架，专门用于在异构生物医学知识图谱上进行深度研究，通过协调广度优先和深度优先搜索策略，系统性地整合多源知识图谱信息，加速生物医学发现。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱包含大量异构信息（文献、基因、通路、药物、疾病、临床试验等），但由于结构差异、持续演化和有限的跨资源对齐，需要大量人工整合，限制了知识探索的深度和规模。

Method: 开发了DeepEvidence框架，包含：1）协调器指导两个互补代理：广度优先搜索（BFRS）用于多图谱实体搜索，深度优先搜索（DFRS）用于多跳证据推理；2）内部增量构建的证据图谱记录检索的实体、关系和证据；3）统一接口查询多样生物医学API；4）执行沙箱支持程序化数据检索、提取和分析。

Result: 在深度推理基准测试和生物医学发现生命周期的四个关键阶段（药物发现、临床前实验、临床试验开发、循证医学）中，DeepEvidence在系统性探索和证据合成方面表现出显著优势。

Conclusion: 知识图谱驱动的深度研究具有加速生物医学发现的潜力，DeepEvidence框架通过协调异构知识图谱的探索策略，实现了更深入、更系统的知识整合和推理。

Abstract: Biomedical knowledge graphs (KGs) encode vast, heterogeneous information spanning literature, genes, pathways, drugs, diseases, and clinical trials, but leveraging them collectively for scientific discovery remains difficult. Their structural differences, continual evolution, and limited cross-resource alignment require substantial manual integration, limiting the depth and scale of knowledge exploration. We introduce DeepEvidence, an AI-agent framework designed to perform Deep Research across various heterogeneous biomedical KGs. Unlike generic Deep Research systems that rely primarily on internet-scale text, DeepEvidence incorporates specialized knowledge-graph tooling and coordinated exploration strategies to systematically bridge heterogeneous resources. At its core is an orchestrator that directs two complementary agents: Breadth-First ReSearch (BFRS) for broad, multi-graph entity search, and Depth-First ReSearch (DFRS) for multi-hop, evidence-focused reasoning. An internal, incrementally built evidence graph provides a structured record of retrieved entities, relations, and supporting evidence. To operate at scale, DeepEvidence includes unified interfaces for querying diverse biomedical APIs and an execution sandbox that enables programmatic data retrieval, extraction, and analysis. Across established deep-reasoning benchmarks and four key stages of the biomedical discovery lifecycle: drug discovery, pre-clinical experimentation, clinical trial development, and evidence-based medicine, DeepEvidence demonstrates substantial gains in systematic exploration and evidence synthesis. These results highlight the potential of knowledge-graph-driven Deep Research to accelerate biomedical discovery.

</details>


### [83] [Utilizing Metadata for Better Retrieval-Augmented Generation](https://arxiv.org/abs/2601.11863)
*Raquib Bin Yousuf,Shengzhe Xu,Mandar Sharma,Andrew Neeser,Chris Latimer,Naren Ramakrishnan*

Main category: cs.IR

TL;DR: 系统研究结构化文档中元数据感知检索策略，发现元数据集成能显著提升RAG系统检索效果


<details>
  <summary>Details</summary>
Motivation: 在结构化重复语料库（如监管文件）中，仅依赖文档块相似性难以区分语言重叠的文档，而实践中将元数据扁平化为文本的启发式方法影响和权衡尚不明确

Method: 比较五种元数据感知检索策略：纯文本基线、元数据作为文本（前缀和后缀）、统一嵌入融合元数据和内容、双编码器后期融合检索、元数据感知查询重写

Result: 前缀方法和统一嵌入方法持续优于纯文本基线，统一嵌入有时超过前缀方法且更易维护；元数据集成通过增加文档内凝聚力、减少文档间混淆、扩大相关与不相关块间的分离来提升效果

Conclusion: 元数据集成能显著提升结构化文档检索效果，结构线索提供强消歧信号，研究提供了代码、评估框架和RAGMATE-10K数据集

Abstract: Retrieval-Augmented Generation systems depend on retrieving semantically relevant document chunks to support accurate, grounded outputs from large language models. In structured and repetitive corpora such as regulatory filings, chunk similarity alone often fails to distinguish between documents with overlapping language. Practitioners often flatten metadata into input text as a heuristic, but the impact and trade-offs of this practice remain poorly understood. We present a systematic study of metadata-aware retrieval strategies, comparing plain-text baselines with approaches that embed metadata directly. Our evaluation spans metadata-as-text (prefix and suffix), a dual-encoder unified embedding that fuses metadata and content in a single index, dual-encoder late-fusion retrieval, and metadata-aware query reformulation. Across multiple retrieval metrics and question types, we find that prefixing and unified embeddings consistently outperform plain-text baselines, with the unified at times exceeding prefixing while being easier to maintain. Beyond empirical comparisons, we analyze embedding space, showing that metadata integration improves effectiveness by increasing intra-document cohesion, reducing inter-document confusion, and widening the separation between relevant and irrelevant chunks. Field-level ablations show that structural cues provide strong disambiguating signals. Our code, evaluation framework, and the RAGMATE-10K dataset are publicly hosted.

</details>


### [84] [Cultural Analytics for Good: Building Inclusive Evaluation Frameworks for Historical IR](https://arxiv.org/abs/2601.11874)
*Suchana Datta,Dwaipayan Roy,Derek Greene,Gerardine Meaney,Karen Wade,Philipp Mayr*

Main category: cs.IR

TL;DR: 该研究构建了一个结合信息检索与文化分析的跨学科框架，使用英国图书馆BL19数字馆藏（1700-1899年间的3.5万+作品）创建基准，研究19世纪小说与非小说中的语言变化和检索问题，重点探索从小说到非小说的知识迁移。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是支持历史知识的公平获取，通过结合信息检索和文化分析，构建一个能够促进数字档案文化包容性、可解释性和透明度的框架，最终朝着更解放性的知识基础设施发展。

Method: 方法结合了专家驱动的查询设计、段落级相关性标注和大型语言模型辅助，创建了一个基于人类专业知识且可扩展的评估框架。研究重点探索从小说到非小说的知识迁移，研究小说中的叙事理解和语义丰富性如何改进学术和事实材料的检索。

Result: 该研究不仅提高了检索准确性，还促进了数字档案的可解释性、透明度和文化包容性。提供了实用的评估资源和方法论范式，用于开发支持更丰富、更具历史意识的数字档案参与度的检索系统。

Conclusion: 该研究构建了一个跨学科框架，将信息检索与文化分析相结合，为开发支持更丰富历史参与度的检索系统提供了方法论范式，朝着更解放性的知识基础设施迈进。

Abstract: This work bridges the fields of information retrieval and cultural analytics to support equitable access to historical knowledge. Using the British Library BL19 digital collection (more than 35,000 works from 1700-1899), we construct a benchmark for studying changes in language, terminology and retrieval in the 19th-century fiction and non-fiction. Our approach combines expert-driven query design, paragraph-level relevance annotation, and Large Language Model (LLM) assistance to create a scalable evaluation framework grounded in human expertise. We focus on knowledge transfer from fiction to non-fiction, investigating how narrative understanding and semantic richness in fiction can improve retrieval for scholarly and factual materials. This interdisciplinary framework not only improves retrieval accuracy but also fosters interpretability, transparency, and cultural inclusivity in digital archives. Our work provides both practical evaluation resources and a methodological paradigm for developing retrieval systems that support richer, historically aware engagement with digital archives, ultimately working towards more emancipatory knowledge infrastructures.

</details>


### [85] [Agentic-R: Learning to Retrieve for Agentic Search](https://arxiv.org/abs/2601.11888)
*Wenhan Liu,Xinyu Ma,Yutao Zhu,Yuchen Li,Daiting Shi,Dawei Yin,Zhicheng Dou*

Main category: cs.IR

TL;DR: 提出专为智能体搜索设计的检索器训练框架，通过结合局部查询-段落相关性和全局答案正确性来衡量段落效用，采用迭代训练策略双向优化搜索智能体和检索器。


<details>
  <summary>Details</summary>
Motivation: 现有搜索智能体通常依赖基于相似性的检索器，但相似段落并不总是对最终答案生成有用。当前缺乏专门为多轮智能体搜索设计的检索器训练方法。

Method: 提出新颖的检索器训练框架：1) 使用局部查询-段落相关性和全局答案正确性双重标准衡量段落效用；2) 采用迭代训练策略，让搜索智能体和检索器双向迭代优化；3) 检索器使用智能体生成的高质量演化查询持续改进。

Result: 在7个单跳和多跳QA基准测试中，提出的Agentic-R检索器在不同搜索智能体上均一致优于强基线方法。

Conclusion: 该研究为智能体搜索设计了专门的检索器训练框架，通过结合局部和全局效用度量以及迭代优化策略，显著提升了检索器在复杂问答任务中的性能。

Abstract: Agentic search has recently emerged as a powerful paradigm, where an agent interleaves multi-step reasoning with on-demand retrieval to solve complex questions. Despite its success, how to design a retriever for agentic search remains largely underexplored. Existing search agents typically rely on similarity-based retrievers, while similar passages are not always useful for final answer generation. In this paper, we propose a novel retriever training framework tailored for agentic search. Unlike retrievers designed for single-turn retrieval-augmented generation (RAG) that only rely on local passage utility, we propose to use both local query-passage relevance and global answer correctness to measure passage utility in a multi-turn agentic search. We further introduce an iterative training strategy, where the search agent and the retriever are optimized bidirectionally and iteratively. Different from RAG retrievers that are only trained once with fixed questions, our retriever is continuously improved using evolving and higher-quality queries from the agent. Extensive experiments on seven single-hop and multi-hop QA benchmarks demonstrate that our retriever, termed \ours{}, consistently outperforms strong baselines across different search agents. Our codes are available at: https://github.com/8421BCD/Agentic-R.

</details>


### [86] [Facet-Aware Multi-Head Mixture-of-Experts Model with Text-Enhanced Pre-training for Sequential Recommendation](https://arxiv.org/abs/2601.12301)
*Mingrui Liu,Sixiao Zhang,Cheng Long*

Main category: cs.IR

TL;DR: 提出FAME模型，通过多头注意力捕捉物品多面性，结合MoE网络解耦用户偏好，并使用文本增强预训练提升语义表示


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐系统通常为每个物品分配单一嵌入向量，无法充分捕捉物品的多面性特征（如电影类型、演员等），也难以表示用户在多个方面的复杂偏好

Method: 提出FAME架构：1）利用多头注意力最后一层的子嵌入分别预测下一物品，捕捉不同物品面；2）门控机制动态整合预测结果；3）每个注意力头内引入MoE网络解耦用户偏好；4）设计文本增强预训练模块，使用预训练文本编码器和对比学习目标从文本元数据中解耦面特征

Result: 论文应会展示FAME模型在序列推荐任务上的优越性能，能够更好地捕捉物品多面性和用户复杂偏好

Conclusion: FAME模型通过解耦物品多面性和用户偏好，结合文本增强预训练，显著提升了序列推荐的性能，为处理复杂用户偏好提供了有效解决方案

Abstract: Sequential recommendation (SR) systems excel at capturing users' dynamic preferences by leveraging their interaction histories. Most existing SR systems assign a single embedding vector to each item to represent its features, adopting various models to combine these embeddings into a sequence representation that captures user intent. However, we argue that this representation alone is insufficient to capture an item's multi-faceted nature (e.g., movie genres, starring actors). Furthermore, users often exhibit complex and varied preferences within these facets (e.g., liking both action and musical films within the genre facet), which are challenging to fully represent with static identifiers. To address these issues, we propose a novel architecture titled Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential Recommendation (FAME). We leverage sub-embeddings from each head in the final multi-head attention layer to predict the next item separately, effectively capturing distinct item facets. A gating mechanism then integrates these predictions by dynamically determining their importance. Additionally, we introduce a Mixture-of-Experts (MoE) network within each attention head to disentangle varied user preferences within each facet, utilizing a learnable router network to aggregate expert outputs based on context. Complementing this architecture, we design a Text-Enhanced Facet-Aware Pre-training module to overcome the limitations of randomly initialized embeddings. By utilizing a pre-trained text encoder and employing an alternating supervised contrastive learning objective, we explicitly disentangle facet-specific features from textual metadata (e.g., descriptions) before sequential training begins. This ensures that the item embeddings are semantically robust and aligned with the downstream multi-facet framework.

</details>


### [87] [Information Farming: From Berry Picking to Berry Growing](https://arxiv.org/abs/2601.12544)
*Leif Azzopardi,Adam Roegiest*

Main category: cs.IR

TL;DR: 论文提出"信息耕作"新框架，类比新石器时代革命，认为生成式AI正推动人们从信息"采集"转向信息"耕作"的根本转变。


<details>
  <summary>Details</summary>
Motivation: 传统的信息采集理论和信息觅食理论将用户视为机会主义的采集者，但生成式AI的出现正在从根本上改变人们生产、组织和重用信息的方式，这些传统范式已无法完全捕捉这种转变。

Method: 采用历史类比和实证证据，引入"信息耕作"作为概念框架，分析其益处、机会、设计影响、评估方法以及伴随的风险。

Result: 提出信息耕作代表人类与信息互动方式的自然演进，随着生成式AI技术普及，信息耕作将逐渐取代临时性的、基于"补丁"的信息觅食，成为主导的互动模式。

Conclusion: 信息耕作框架标志着人类-信息互动及其研究的更广泛转变，需要重新思考信息系统的设计和评估方法，同时认识到这种转变带来的机遇和风险。

Abstract: The classic paradigms of Berry Picking and Information Foraging Theory have framed users as gatherers, opportunistically searching across distributed sources to satisfy evolving information needs. However, the rise of GenAI is driving a fundamental transformation in how people produce, structure, and reuse information - one that these paradigms no longer fully capture. This transformation is analogous to the Neolithic Revolution, when societies shifted from hunting and gathering to cultivation. Generative technologies empower users to "farm" information by planting seeds in the form of prompts, cultivating workflows over time, and harvesting richly structured, relevant yields within their own plots, rather than foraging across others people's patches. In this perspectives paper, we introduce the notion of Information Farming as a conceptual framework and argue that it represents a natural evolution in how people engage with information. Drawing on historical analogy and empirical evidence, we examine the benefits and opportunities of information farming, its implications for design and evaluation, and the accompanying risks posed by this transition. We hypothesize that as GenAI technologies proliferate, cultivating information will increasingly supplant transient, patch-based foraging as a dominant mode of engagement, marking a broader shift in human-information interaction and its study.

</details>


### [88] [HyFormer: Revisiting the Roles of Sequence Modeling and Feature Interaction in CTR Prediction](https://arxiv.org/abs/2601.12681)
*Yunwen Huang,Shiyong Hong,Xijun Xiao,Jinqiu Jin,Xuanyuan Luo,Zhe Wang,Zheng Chai,Shikang Wu,Yuchao Zheng,Jingjian Lin*

Main category: cs.IR

TL;DR: HyFormer是一个统一的混合Transformer架构，将长序列建模和特征交互紧密集成到单一骨干网络中，通过交替优化过程提升工业推荐模型的性能。


<details>
  <summary>Details</summary>
Motivation: 工业大规模推荐模型面临联合建模长范围用户行为序列和异构非序列特征的挑战，现有架构采用解耦管道（先压缩长序列再融合特征）限制了表示能力和交互灵活性。

Method: 提出HyFormer统一架构，重新设计查询令牌，将LRM建模任务框架化为交替优化过程，包含两个核心组件：查询解码（将非序列特征扩展为全局令牌并进行长序列解码）和查询增强（通过高效令牌混合增强跨查询和跨序列的异构交互）。

Result: 在十亿级工业数据集上的实验显示，在可比参数和FLOPs预算下，HyFormer始终优于LONGER和RankMixer基线，并展现出随着参数和FLOPs增加而提升的优越扩展性。大规模在线A/B测试进一步验证了其有效性。

Conclusion: HyFormer作为工业LRMs的统一建模框架具有实用性和可扩展性，能够显著提升推荐系统性能。

Abstract: Industrial large-scale recommendation models (LRMs) face the challenge of jointly modeling long-range user behavior sequences and heterogeneous non-sequential features under strict efficiency constraints. However, most existing architectures employ a decoupled pipeline: long sequences are first compressed with a query-token based sequence compressor like LONGER, followed by fusion with dense features through token-mixing modules like RankMixer, which thereby limits both the representation capacity and the interaction flexibility. This paper presents HyFormer, a unified hybrid transformer architecture that tightly integrates long-sequence modeling and feature interaction into a single backbone. From the perspective of sequence modeling, we revisit and redesign query tokens in LRMs, and frame the LRM modeling task as an alternating optimization process that integrates two core components: Query Decoding which expands non-sequential features into Global Tokens and performs long sequence decoding over layer-wise key-value representations of long behavioral sequences; and Query Boosting which enhances cross-query and cross-sequence heterogeneous interactions via efficient token mixing. The two complementary mechanisms are performed iteratively to refine semantic representations across layers. Extensive experiments on billion-scale industrial datasets demonstrate that HyFormer consistently outperforms strong LONGER and RankMixer baselines under comparable parameter and FLOPs budgets, while exhibiting superior scaling behavior with increasing parameters and FLOPs. Large-scale online A/B tests in high-traffic production systems further validate its effectiveness, showing significant gains over deployed state-of-the-art models. These results highlight the practicality and scalability of HyFormer as a unified modeling framework for industrial LRMs.

</details>


### [89] [The Unfairness of Multifactorial Bias in Recommendation](https://arxiv.org/abs/2601.12828)
*Masoud Mansoury,Jin Huang,Mykola Pechenizkiy,Herke van Hoof,Maarten de Rijke*

Main category: cs.IR

TL;DR: 该论文研究了推荐系统中的多重因子偏差（流行度偏差和积极性偏差的叠加效应），提出了一种基于百分位数的评分转换预处理方法来改善物品侧公平性，实验证明该方法能有效提升曝光公平性且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中存在流行度偏差和积极性偏差，这两种偏差都源于输入数据并通过推荐模型传播，导致不公平或次优结果。虽然每种偏差已被独立研究，但它们的组合效应（多重因子偏差）尚未得到充分探索。作者发现积极性偏差不成比例地集中在流行物品上，进一步放大了它们的过度曝光。

Method: 采用基于百分位数的评分转换作为预处理策略来缓解多重因子偏差。该方法通过重新调整评分分布来平衡流行度偏差和积极性偏差的叠加效应。此外，还将这种预处理步骤整合到后处理公平性流程中，以提高其有效性和效率。

Result: 在四个公共数据集上使用六种推荐算法的实验表明，该方法能改善曝光公平性，且准确率损失可忽略不计。将预处理整合到后处理公平性流程中，能以更低的计算成本实现相当或更好的公平性。

Conclusion: 研究强调了解决多重因子偏差的重要性，并证明了简单、数据驱动的预处理方法在改善推荐系统公平性方面的实用价值。这种方法能有效平衡公平性和准确性，为实际部署提供了可行的解决方案。

Abstract: Popularity bias and positivity bias are two prominent sources of bias in recommender systems. Both arise from input data, propagate through recommendation models, and lead to unfair or suboptimal outcomes. Popularity bias occurs when a small subset of items receives most interactions, while positivity bias stems from the over-representation of high rating values. Although each bias has been studied independently, their combined effect, to which we refer to as multifactorial bias, remains underexplored. In this work, we examine how multifactorial bias influences item-side fairness, focusing on exposure bias, which reflects the unequal visibility of items in recommendation outputs. Through simulation studies, we find that positivity bias is disproportionately concentrated on popular items, further amplifying their over-exposure. Motivated by this insight, we adapt a percentile-based rating transformation as a pre-processing strategy to mitigate multifactorial bias. Experiments using six recommendation algorithms across four public datasets show that this approach improves exposure fairness with negligible accuracy loss. We also demonstrate that integrating this pre-processing step into post-processing fairness pipelines enhances their effectiveness and efficiency, enabling comparable or better fairness with reduced computational cost. These findings highlight the importance of addressing multifactorial bias and demonstrate the practical value of simple, data-driven pre-processing methods for improving fairness in recommender systems.

</details>


### [90] [Rules, Resources, and Restrictions: A Taxonomy of Task-Based Information Request Intents](https://arxiv.org/abs/2601.12985)
*Melanie A. Kilian,David Elsweiler*

Main category: cs.IR

TL;DR: 提出基于任务视角的查询意图分类法，弥补传统查询方法与AI驱动任务导向搜索之间的差距


<details>
  <summary>Details</summary>
Motivation: 现有意图分类法主要基于系统日志数据，关注孤立的信息需求，而忽略了更广泛的任务上下文。随着用户对LLMs的期望从简单问答扩展到全面任务支持，这种局限性日益凸显

Method: 采用扎根理论方法的访谈研究，采访机场信息台工作人员，开发基于任务的信息请求意图分类法

Result: 提出了一个连接传统查询方法和AI驱动任务导向搜索需求的基于任务的信息请求意图分类法

Conclusion: 需要更强的基于任务的查询意图视角来满足现代AI驱动搜索的需求，提出的分类法有助于弥合传统方法与新兴任务导向搜索之间的差距

Abstract: Understanding and classifying query intents can improve retrieval effectiveness by helping align search results with the motivations behind user queries. However, existing intent taxonomies are typically derived from system log data and capture mostly isolated information needs, while the broader task context often remains unaddressed. This limitation becomes increasingly relevant as interactions with Large Language Models (LLMs) expand user expectations from simple query answering toward comprehensive task support, for example, with purchasing decisions or in travel planning. At the same time, current LLMs still struggle to fully interpret complex and multifaceted tasks. To address this gap, we argue for a stronger task-based perspective on query intent. Drawing on a grounded-theory-based interview study with airport information clerks, we present a taxonomy of task-based information request intents that bridges the gap between traditional query-focused approaches and the emerging demands of AI-driven task-oriented search.

</details>


### [91] [Incorporating Q&A Nuggets into Retrieval-Augmented Generation](https://arxiv.org/abs/2601.13222)
*Laura Dietz,Bryan Li,Gabrielle Liu,Jia-Huei Ju,Eugene Yang,Dawn Lawrie,William Walden,James Mayfield*

Main category: cs.IR

TL;DR: RAGE系统将自动评估融入检索增强生成，提出Crucible系统通过构建问答块库来保持明确引用溯源，在TREC NeuCLIR 2024上显著优于现有系统


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在引用溯源和避免信息重复方面存在不足，需要更透明可解释的语义表示来改进检索增强生成的质量

Method: 提出Crucible系统：1) 从检索文档构建问答块库；2) 使用问答块指导提取、选择和报告生成；3) 通过问答语义避免信息重复，保持完整引用溯源

Result: 在TREC NeuCLIR 2024数据集上，Crucible系统在块召回率、密度和引用基础方面显著优于最近的基于块的RAG系统Ginger

Conclusion: RAGE框架通过问答块表示实现了更好的引用溯源和可解释性，为检索增强生成提供了更透明可靠的解决方案

Abstract: RAGE systems integrate ideas from automatic evaluation (E) into Retrieval-augmented Generation (RAG). As one such example, we present Crucible, a Nugget-Augmented Generation System that preserves explicit citation provenance by constructing a bank of Q&A nuggets from retrieved documents and uses them to guide extraction, selection, and report generation. Reasoning on nuggets avoids repeated information through clear and interpretable Q&A semantics - instead of opaque cluster abstractions - while maintaining citation provenance throughout the entire generation process. Evaluated on the TREC NeuCLIR 2024 collection, our Crucible system substantially outperforms Ginger, a recent nugget-based RAG system, in nugget recall, density, and citation grounding.

</details>


### [92] [Insider Knowledge: How Much Can RAG Systems Gain from Evaluation Secrets?](https://arxiv.org/abs/2601.13227)
*Laura Dietz,Bryan Li,Eugene Yang,Dawn Lawrie,William Walden,James Mayfield*

Main category: cs.IR

TL;DR: 论文警告RAG系统中基于nugget的方法与LLM评估之间存在循环风险，可能导致虚假的性能提升而非真正的系统进步


<details>
  <summary>Details</summary>
Motivation: 随着RAG系统越来越多地使用LLM评估进行优化，基于nugget的方法不仅用于评估框架，还集成到系统架构中。这种集成虽然能带来改进，但也存在循环性导致测量错误的风险。

Method: 通过对比实验，研究基于nugget的RAG系统（如Ginger和Crucible）与强基线（如GPT-Researcher）。故意修改Crucible以生成针对LLM评估优化的输出，测试当评估元素（如提示模板或黄金nuggets）泄露或可预测时的表现。

Result: 当评估元素泄露或可预测时，可以轻松获得接近完美的评估分数。这表明系统可能只是在过度拟合评估指标，而非实现真正的进步。

Conclusion: 需要采用盲评估设置和方法多样性，以防止将指标过度拟合误认为是真正的系统进展，确保评估的可靠性。

Abstract: RAG systems are increasingly evaluated and optimized using LLM judges, an approach that is rapidly becoming the dominant paradigm for system assessment. Nugget-based approaches in particular are now embedded not only in evaluation frameworks but also in the architectures of RAG systems themselves. While this integration can lead to genuine improvements, it also creates a risk of faulty measurements due to circularity. In this paper, we investigate this risk through comparative experiments with nugget-based RAG systems, including Ginger and Crucible, against strong baselines such as GPT-Researcher. By deliberately modifying Crucible to generate outputs optimized for an LLM judge, we show that near-perfect evaluation scores can be achieved when elements of the evaluation - such as prompt templates or gold nuggets - are leaked or can be predicted. Our results highlight the importance of blind evaluation settings and methodological diversity to guard against mistaking metric overfitting for genuine system progress.

</details>


### [93] [Guidelines for the Creation of an Annotated Corpus](https://arxiv.org/abs/2601.13353)
*Bahdja Boudoua,Nadia Guiffant,Mathieu Roche,Maguelonne Teisseire,Annelise Tran*

Main category: cs.IR

TL;DR: 提供创建文本标注指南和标注语料库的通用方法论框架


<details>
  <summary>Details</summary>
Motivation: 为研究人员提供系统化的方法论支持，解决文本标注工作中缺乏标准化指导的问题，促进标注语料库的创建、存储、共享和利用

Method: 基于UMR TETIS成员反馈和科学文献，提出包含方法论、存储、共享和利用的完整框架，通过定义和示例清晰说明每个步骤

Result: 提供了一个全面的方法论文档，涵盖标注指南创建、语料库构建、数据管理全流程，为不同研究场景提供可操作的指导

Conclusion: 该文档为文本标注工作提供了系统化的方法论支持，有助于提高标注质量、促进数据共享和再利用，为各类研究提供可靠的数据基础

Abstract: This document, based on feedback from UMR TETIS members and the scientific literature, provides a generic methodology for creating annotation guidelines and annotated textual datasets (corpora). It covers methodological aspects, as well as storage, sharing, and valorization of the data. It includes definitions and examples to clearly illustrate each step of the process, thus providing a comprehensive framework to support the creation and use of corpora in various research contexts.

</details>


### [94] [Integrating Vision-Centric Text Understanding for Conversational Recommender Systems](https://arxiv.org/abs/2601.13505)
*Wei Yuan,Shutong Qiao,Tong Chen,Quoc Viet Hung Nguyen,Zi Huang,Hongzhi Yin*

Main category: cs.IR

TL;DR: STARCRS是一个结合屏幕阅读和LLM文本理解的双通路对话推荐系统，通过视觉化编码和知识锚定融合提升推荐准确性和响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有对话推荐系统通过扩展对话上下文（如引入多样实体信息或检索相关对话）来更准确推断用户偏好，但这导致输入更长更异构，带来输入长度限制、文本风格不一致、无关文本噪声等问题，需要更强的语言理解能力。

Method: 提出STARCRS系统，包含两个互补的文本理解模式：1) 屏幕阅读通路，将辅助文本信息编码为视觉标记，模拟屏幕浏览；2) 基于LLM的文本通路，专注于有限的关键内容进行细粒度推理。设计了知识锚定融合框架，结合对比对齐、交叉注意力交互和自适应门控来整合两种模式。

Result: 在两个广泛使用的基准测试上进行大量实验，证明STARCRS在推荐准确性和生成响应质量方面均取得一致提升。

Conclusion: STARCRS通过整合屏幕阅读和LLM文本理解两种模式，有效解决了对话推荐系统中长异构输入带来的挑战，提升了偏好建模和响应生成能力。

Abstract: Conversational Recommender Systems (CRSs) have attracted growing attention for their ability to deliver personalized recommendations through natural language interactions. To more accurately infer user preferences from multi-turn conversations, recent works increasingly expand conversational context (e.g., by incorporating diverse entity information or retrieving related dialogues). While such context enrichment can assist preference modeling, it also introduces longer and more heterogeneous inputs, leading to practical issues such as input length constraints, text style inconsistency, and irrelevant textual noise, thereby raising the demand for stronger language understanding ability. In this paper, we propose STARCRS, a Screen-Text-AwaRe Conversational Recommender System that integrates two complementary text understanding modes: (1) a screen-reading pathway that encodes auxiliary textual information as visual tokens, mimicking skim reading on a screen, and (2) an LLM-based textual pathway that focuses on a limited set of critical content for fine-grained reasoning. We design a knowledge-anchored fusion framework that combines contrastive alignment, cross-attention interaction, and adaptive gating to integrate the two modes for improved preference modeling and response generation. Extensive experiments on two widely used benchmarks demonstrate that STARCRS consistently improves both recommendation accuracy and generated response quality.

</details>


### [95] [More Than Efficiency: Embedding Compression Improves Domain Adaptation in Dense Retrieval](https://arxiv.org/abs/2601.13525)
*Chunsheng Zuo,Daniel Khashabi*

Main category: cs.IR

TL;DR: PCA压缩查询嵌入可提升专业领域检索性能，无需标注数据或重新训练


<details>
  <summary>Details</summary>
Motivation: 预训练嵌入的密集检索器在专业领域表现不佳，因为训练与目标领域分布不匹配。传统领域适应方法需要昂贵的标注数据和重新训练。

Method: 对领域嵌入应用PCA降维，保留领域相关特征，丢弃非判别性成分。仅对查询嵌入进行PCA压缩，无需修改文档嵌入或重新训练模型。

Result: 在9个检索器和14个MTEB数据集上评估，PCA应用于查询嵌入在75.4%的模型-数据集对上提升了NDCG@10性能。

Conclusion: PCA嵌入压缩是一种简单轻量的领域适应方法，能有效提升检索性能，无需额外标注或重新训练。

Abstract: Dense retrievers powered by pretrained embeddings are widely used for document retrieval but struggle in specialized domains due to the mismatches between the training and target domain distributions. Domain adaptation typically requires costly annotation and retraining of query-document pairs. In this work, we revisit an overlooked alternative: applying PCA to domain embeddings to derive lower-dimensional representations that preserve domain-relevant features while discarding non-discriminative components. Though traditionally used for efficiency, we demonstrate that this simple embedding compression can effectively improve retrieval performance. Evaluated across 9 retrievers and 14 MTEB datasets, PCA applied solely to query embeddings improves NDCG@10 in 75.4% of model-dataset pairs, offering a simple and lightweight method for domain adaptation.

</details>


### [96] [Balancing Fairness and High Match Rates in Reciprocal Recommender Systems: A Nash Social Welfare Approach](https://arxiv.org/abs/2601.13609)
*Yoji Tomita,Tomohiko Yokoyama*

Main category: cs.IR

TL;DR: 该论文研究匹配平台中互惠推荐系统的公平性，提出基于社会福利和纳什社会福利的方法来平衡匹配率与公平性，并开发了高效的近似算法。


<details>
  <summary>Details</summary>
Motivation: 匹配平台（如在线约会服务和职位推荐）日益普及，但现有互惠推荐系统往往只关注最大化匹配数量，忽略了用户间的公平性问题。需要设计既能增加匹配数量又能避免用户不公平的推荐系统。

Method: 1. 从公平分配角度定义用户的推荐机会，建立基于嫉妒自由度的公平概念；2. 提出社会福利(SW)方法近似最大化匹配数量；3. 提出纳什社会福利(NSW)方法通过交替优化两个NSW函数实现近似嫉妒自由的推荐；4. 将SW和NSW方法推广到α-SW方法，平衡公平性与高匹配率；5. 基于Sinkhorn算法开发计算高效的SW/NSW/α-SW近似算法。

Result: 实验表明：1. SW方法虽然能近似最大化匹配数量，但会导致推荐机会的显著不公平；2. NSW方法能实现近似嫉妒自由的推荐；3. α-SW方法能有效平衡公平性与匹配率之间的权衡；4. 提出的近似算法在合成数据集和两个真实数据集上都表现出实际有效性。

Conclusion: 该研究为匹配平台中的互惠推荐系统提供了系统的公平性分析框架，提出的NSW和α-SW方法能有效解决公平性与匹配率之间的权衡问题，开发的高效算法具有实际应用价值。

Abstract: Matching platforms, such as online dating services and job recommendations, have become increasingly prevalent. For the success of these platforms, it is crucial to design reciprocal recommender systems (RRSs) that not only increase the total number of matches but also avoid creating unfairness among users. In this paper, we investigate the fairness of RRSs on matching platforms. From the perspective of fair division, we define the users' opportunities to be recommended and establish the fairness concept of envy-freeness in the allocation of these opportunities. We first introduce the Social Welfare (SW) method, which approximately maximizes the number of matches, and show that it leads to significant unfairness in recommendation opportunities, illustrating the trade-off between fairness and match rates. To address this challenge, we propose the Nash Social Welfare (NSW) method, which alternately optimizes two NSW functions and achieves nearly envy-free recommendations. We further generalize the SW and NSW method to the $α$-SW method, which balances the trade-off between fairness and high match rates. Additionally, we develop a computationally efficient approximation algorithm for the SW/NSW/$α$-SW methods based on the Sinkhorn algorithm. Through extensive experiments on both synthetic datasets and two real-world datasets, we demonstrate the practical effectiveness of our approach.

</details>


### [97] [Question-Focused Filtering for Knowledge-based VQA](https://arxiv.org/abs/2601.13856)
*Wei Ye,Yixin Su,Yueguo Chen,Longxiang Gao,Jianjun Li,Ruixuan Li,Rui Zhang*

Main category: cs.IR

TL;DR: 提出一种基于问题聚焦的知识过滤方法，通过可训练的QFF模块和分块动态多文章选择模块，在保持计算成本与典型方法相当的同时，有效提升KB-VQA性能。


<details>
  <summary>Details</summary>
Motivation: KB-VQA中有效的知识过滤对提升准确性至关重要。现有方法存在两个问题：1）基于相似度的典型方法在文章和文章内层面存在信息选择错误；2）基于MLLM的过滤方法虽然语义理解能力强且能跨文章过滤，但计算成本过高，限制了实际应用。

Method: 提出问题聚焦的过滤方法，包含两个核心模块：1）可训练的问题聚焦过滤器（QFF），用于进行问题聚焦的跨文章过滤；2）分块动态多文章选择（CDA）模块，共同缓解文章层面和文章内层面的信息选择错误。

Result: 在E-VQA数据集上比当前SOTA模型提升4.9%，在InfoSeek数据集上提升3.8%，验证了方法的有效性。同时计算成本与典型方法相当，具有实际应用价值。

Conclusion: 提出的问题聚焦过滤方法能够高效获取高质量过滤知识，在保持计算效率的同时显著提升KB-VQA性能，解决了现有方法在信息选择和计算成本方面的局限性。

Abstract: Knowledge-based Visual Question Answering (KB-VQA) aims to answer questions by integrating images with external knowledge. Effective knowledge filtering is crucial for improving accuracy. Typical filtering methods use similarity metrics to locate relevant article sections from one article, leading to information selection errors at the article and intra-article levels. Although recent explorations of Multimodal Large Language Model (MLLM)-based filtering methods demonstrate superior semantic understanding and cross-article filtering capabilities, their high computational cost limits practical application. To address these issues, this paper proposes a question-focused filtering method. This approach can perform question-focused, cross-article filtering, efficiently obtaining high-quality filtered knowledge while keeping computational costs comparable to typical methods. Specifically, we design a trainable Question-Focused Filter (QFF) and a Chunk-based Dynamic Multi-Article Selection (CDA) module, which collectively alleviate information selection errors at both the article and intra-article levels. Experiments show that our method outperforms current state-of-the-art models by 4.9% on E-VQA and 3.8% on InfoSeek, validating its effectiveness. The code is publicly available at: https://github.com/leaffeall/QKVQA.

</details>


### [98] [IF-GEO: Conflict-Aware Instruction Fusion for Multi-Query Generative Engine Optimization](https://arxiv.org/abs/2601.13938)
*Heyang Zhou,JiaJia Chen,Xiaolu Chen,Jie Bao,Zhen Chen,Yong Liao*

Main category: cs.IR

TL;DR: IF-GEO框架通过挖掘潜在查询偏好并融合冲突指令，解决生成引擎优化中的多查询冲突问题，提升跨查询稳定性。


<details>
  <summary>Details</summary>
Motivation: 生成引擎通过检索源合成直接答案，但源可见性成为挑战。针对不同查询优化文档时，异质查询需求常冲突且竞争有限内容预算，需要解决这一约束优化问题。

Method: 提出IF-GEO框架，采用"发散-收敛"两阶段：1) 从潜在代表性查询中挖掘优化偏好；2) 通过冲突感知指令融合协调偏好，合成全局修订蓝图指导编辑。引入风险感知稳定性指标量化跨查询稳定性。

Result: 在多查询基准测试中，IF-GEO实现了显著的性能提升，同时在多样化检索场景中保持鲁棒性。

Conclusion: IF-GEO框架有效解决了生成引擎优化中的多查询冲突问题，通过协调异质查询偏好实现稳定优化，为提升源可见性提供了实用解决方案。

Abstract: As Generative Engines revolutionize information retrieval by synthesizing direct answers from retrieved sources, ensuring source visibility becomes a significant challenge. Improving it through targeted content revisions is a practical strategy termed Generative Engine Optimization (GEO). However, optimizing a document for diverse queries presents a constrained optimization challenge where heterogeneous queries often impose conflicting and competing revision requirements under a limited content budget. To address this challenge, we propose IF-GEO, a "diverge-then-converge" framework comprising two phases: (i) mining distinct optimization preferences from representative latent queries; (ii) synthesizing a Global Revision Blueprint for guided editing by coordinating preferences via conflict-aware instruction fusion. To explicitly quantify IF-GEO's objective of cross-query stability, we introduce risk-aware stability metrics. Experiments on multi-query benchmarks demonstrate that IF-GEO achieves substantial performance gains while maintaining robustness across diverse retrieval scenarios.

</details>


### [99] [Auditory Brain Passage Retrieval: Cross-Sensory EEG Training for Neural Information Retrieval](https://arxiv.org/abs/2601.14001)
*Niall McGuire,Yashar Moshfeghi*

Main category: cs.IR

TL;DR: 首次系统研究听觉脑电波用于脑通道检索，发现听觉EEG优于视觉EEG，跨感官训练显著提升检索性能，甚至超越传统文本检索方法。


<details>
  <summary>Details</summary>
Motivation: 传统查询构建面临认知复杂性和身体障碍的挑战。现有脑通道检索研究仅使用视觉刺激，未探索听觉EEG在语音界面和视障用户中的潜力，也未研究跨感官训练是否能解决数据稀缺问题。

Method: 使用双编码器架构和四种池化策略（CLS、均值、最大值、多向量），在Alice（听觉）和Nieuwland（视觉）数据集上进行对照实验，比较纯听觉、纯视觉和组合训练的效果。

Result: 听觉EEG始终优于视觉EEG；跨感官训练结合CLS池化显著提升性能：MRR提高31%（0.474），Hit@1提高43%（0.314），Hit@10提高28%（0.858）；组合听觉EEG模型甚至超越了BM25文本基线（0.474 vs 0.428）。

Conclusion: 验证了听觉神经界面在信息检索任务中的有效性，证明跨感官训练不仅能解决数据稀缺问题，还能超越单模态方法，为无障碍界面提供了有竞争力的神经查询方案。

Abstract: Query formulation from internal information needs remains fundamentally challenging across all Information Retrieval paradigms due to cognitive complexity and physical impairments. Brain Passage Retrieval (BPR) addresses this by directly mapping EEG signals to passage representations without intermediate text translation. However, existing BPR research exclusively uses visual stimuli, leaving critical questions unanswered: Can auditory EEG enable effective retrieval for voice-based interfaces and visually impaired users? Can training on combined EEG datasets from different sensory modalities improve performance despite severe data scarcity? We present the first systematic investigation of auditory EEG for BPR and evaluate cross-sensory training benefits. Using dual encoder architectures with four pooling strategies (CLS, mean, max, multi-vector), we conduct controlled experiments comparing auditory-only, visual-only, and combined training on the Alice (auditory) and Nieuwland (visual) datasets. Results demonstrate that auditory EEG consistently outperforms visual EEG, and cross-sensory training with CLS pooling achieves substantial improvements over individual training: 31% in MRR (0.474), 43% in Hit@1 (0.314), and 28% in Hit@10 (0.858). Critically, combined auditory EEG models surpass BM25 text baselines (MRR: 0.474 vs 0.428), establishing neural queries as competitive with traditional retrieval whilst enabling accessible interfaces. These findings validate auditory neural interfaces for IR tasks and demonstrate that cross-sensory training addresses data scarcity whilst outperforming single-modality approaches Code: https://github.com/NiallMcguire/Audio_BPR

</details>


### [100] [Rerank Before You Reason: Analyzing Reranking Tradeoffs through Effective Token Cost in Deep Search Agents](https://arxiv.org/abs/2601.14224)
*Sahel Sharifymoghaddam,Jimmy Lin*

Main category: cs.IR

TL;DR: 研究深度搜索代理中推理预算分配问题，发现适度的列表重排比增加搜索时推理更高效，能在显著降低成本的同时获得可比精度。


<details>
  <summary>Details</summary>
Motivation: 深度研究代理依赖迭代检索和推理来回答复杂查询，但扩展测试时计算会带来显著的效率问题。需要研究如何在深度搜索流程中分配推理预算，特别关注列表重排的作用。

Method: 使用BrowseComp-Plus基准，通过新颖的有效令牌成本（ETC）指标，分析模型规模、推理努力、重排深度和总令牌成本之间的权衡关系。

Result: 重排持续改进检索和端到端准确性，适度的重排通常比增加搜索时推理带来更大的收益，能以显著更低的成本实现可比的准确性。

Conclusion: 在深度搜索流程中，适度的列表重排是比增加搜索时推理更高效的预算分配策略，能在保持准确性的同时大幅降低成本。

Abstract: Deep research agents rely on iterative retrieval and reasoning to answer complex queries, but scaling test-time computation raises significant efficiency concerns. We study how to allocate reasoning budget in deep search pipelines, focusing on the role of listwise reranking. Using the BrowseComp-Plus benchmark, we analyze tradeoffs between model scale, reasoning effort, reranking depth, and total token cost via a novel effective token cost (ETC) metric. Our results show that reranking consistently improves retrieval and end-to-end accuracy, and that moderate reranking often yields larger gains than increasing search-time reasoning, achieving comparable accuracy at substantially lower cost. All our code is available at https://github.com/texttron/BrowseComp-Plus.git

</details>


### [101] [XR: Cross-Modal Agents for Composed Image Retrieval](https://arxiv.org/abs/2601.14245)
*Zhongyu Yang,Wei Pang,Yingfang Yuan*

Main category: cs.IR

TL;DR: XR是一个无需训练的多智能体框架，将检索重构为渐进协调的推理过程，通过三种专门智能体的协作实现组合图像检索，在多个基准上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于嵌入的CIR方法视角有限，只能捕捉有限的跨模态线索，缺乏语义推理能力，无法满足智能体AI时代对多模态推理的需求。

Method: XR框架包含三种专门智能体：想象智能体通过跨模态生成合成目标表示，相似性智能体通过混合匹配进行粗过滤，问题智能体通过针对性推理进行细过滤验证事实一致性。通过渐进式多智能体协调，迭代优化检索结果。

Result: 在FashionIQ、CIRR和CIRCO基准上，XR比强大的无需训练和基于训练的方法提升了高达38%，消融实验显示每个智能体都至关重要。

Conclusion: XR框架通过将检索重构为渐进协调的推理过程，成功解决了传统CIR方法的局限性，为多模态检索提供了新的智能体驱动范式。

Abstract: Retrieval is being redefined by agentic AI, demanding multimodal reasoning beyond conventional similarity-based paradigms. Composed Image Retrieval (CIR) exemplifies this shift as each query combines a reference image with textual modifications, requiring compositional understanding across modalities. While embedding-based CIR methods have achieved progress, they remain narrow in perspective, capturing limited cross-modal cues and lacking semantic reasoning. To address these limitations, we introduce XR, a training-free multi-agent framework that reframes retrieval as a progressively coordinated reasoning process. It orchestrates three specialized types of agents: imagination agents synthesize target representations through cross-modal generation, similarity agents perform coarse filtering via hybrid matching, and question agents verify factual consistency through targeted reasoning for fine filtering. Through progressive multi-agent coordination, XR iteratively refines retrieval to meet both semantic and visual query constraints, achieving up to a 38% gain over strong training-free and training-based baselines on FashionIQ, CIRR, and CIRCO, while ablations show each agent is essential. Code is available: https://01yzzyu.github.io/xr.github.io/.

</details>
