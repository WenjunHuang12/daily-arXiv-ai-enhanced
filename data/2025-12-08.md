<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 4]
- [cs.IT](#cs.IT) [Total: 3]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.GT](#cs.GT) [Total: 5]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [RAG-IGBench: Innovative Evaluation for RAG-based Interleaved Generation in Open-domain Question Answering](https://arxiv.org/abs/2512.05119)
*Rongyang Zhang,Yuqing Huang,Chengqiang Lu,Qimeng Wang,Yan Gao,Yi Wu,Yao Hu,Yin Xu,Wei Wang,Hao Wang,Enhong Chen*

Main category: cs.IR

TL;DR: 提出了RAG-IGBench基准，专门评估基于检索增强生成的交错图像-文本生成任务，包含新的评估指标和高质量数据集。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，视觉增强的响应能显著提升理解和记忆，但现有交错图像-文本生成质量不高，且缺乏合适的评估基准和指标来评估这种多模态内容的复杂性。

Method: 构建RAG-IGBench基准，利用社交媒体最新公开内容创建数据集，设计创新评估指标衡量文本质量、图像质量及其一致性。将多模态大语言模型与检索机制结合，生成连贯的多模态内容。

Result: 通过实验分析现有MLLMs的能力和局限，验证评估指标与人工评估高度相关。在训练集上微调的模型在多个基准上表现提升，证实数据集质量和实用价值。

Conclusion: RAG-IGBench为交错图像-文本生成任务提供了全面的评估框架，有助于推动多模态内容生成技术的发展，数据集和代码已开源。

Abstract: In real-world scenarios, providing user queries with visually enhanced responses can considerably benefit understanding and memory, underscoring the great value of interleaved image-text generation. Despite recent progress, like the visual autoregressive model that unifies text and image processing in a single transformer architecture, generating high-quality interleaved content remains challenging. Moreover, evaluations of these interleaved sequences largely remain underexplored, with existing benchmarks often limited by unimodal metrics that inadequately assess the intricacies of combined image-text outputs. To address these issues, we present RAG-IGBench, a thorough benchmark designed specifically to evaluate the task of Interleaved Generation based on Retrieval-Augmented Generation (RAG-IG) in open-domain question answering. RAG-IG integrates multimodal large language models (MLLMs) with retrieval mechanisms, enabling the models to access external image-text information for generating coherent multimodal content. Distinct from previous datasets, RAG-IGBench draws on the latest publicly available content from social platforms and introduces innovative evaluation metrics that measure the quality of text and images, as well as their consistency. Through extensive experiments with state-of-the-art MLLMs (both open-source and proprietary) on RAG-IGBench, we provide an in-depth analysis examining the capabilities and limitations of these models. Additionally, we validate our evaluation metrics by demonstrating their high correlation with human assessments. Models fine-tuned on RAG-IGBench's training set exhibit improved performance across multiple benchmarks, confirming both the quality and practical utility of our dataset. Our benchmark is available at https://github.com/USTC-StarTeam/RAG-IGBench.

</details>


### [2] [The Effect of Document Summarization on LLM-Based Relevance Judgments](https://arxiv.org/abs/2512.05334)
*Samaneh Mohtadi,Kevin Roitero,Stefano Mizzaro,Gianluca Demartini*

Main category: cs.IR

TL;DR: 研究探讨文本摘要如何影响基于LLM的检索相关性判断的可靠性及其对IR系统评估的影响，发现摘要判断在系统排名稳定性上与全文判断相当，但会引入系统性偏差。


<details>
  <summary>Details</summary>
Motivation: 人工相关性标注成本高、耗时长，LLMs作为自动评估器显示出潜力。现有研究大多将文档作为固定单元，直接将全文输入LLM评估器，但未研究文本摘要对LLM判断可靠性的影响。

Method: 使用最先进的LLMs在多个TREC数据集上，比较基于全文的LLM判断与基于不同长度LLM生成摘要的判断，分析它们与人工标签的一致性、对检索效果评估的影响以及对IR系统排名稳定性的影响。

Result: 摘要判断在系统排名稳定性方面与全文判断相当，但会引入标签分布的系统性偏移和偏差，这些偏差因模型和数据集而异。

Conclusion: 文本摘要既是大规模IR评估效率提升的机会，也是影响自动判断可靠性的重要方法学选择，需要谨慎考虑其引入的偏差。

Abstract: Relevance judgments are central to the evaluation of Information Retrieval (IR) systems, but obtaining them from human annotators is costly and time-consuming. Large Language Models (LLMs) have recently been proposed as automated assessors, showing promising alignment with human annotations. Most prior studies have treated documents as fixed units, feeding their full content directly to LLM assessors. We investigate how text summarization affects the reliability of LLM-based judgments and their downstream impact on IR evaluation. Using state-of-the-art LLMs across multiple TREC collections, we compare judgments made from full documents with those based on LLM-generated summaries of different lengths. We examine their agreement with human labels, their effect on retrieval effectiveness evaluation, and their influence on IR systems' ranking stability. Our findings show that summary-based judgments achieve comparable stability in systems' ranking to full-document judgments, while introducing systematic shifts in label distributions and biases that vary by model and dataset. These results highlight summarization as both an opportunity for more efficient large-scale IR evaluation and a methodological choice with important implications for the reliability of automatic judgments.

</details>


### [3] [A Systematic Framework for Enterprise Knowledge Retrieval: Leveraging LLM-Generated Metadata to Enhance RAG Systems](https://arxiv.org/abs/2512.05411)
*Pranav Pushkar Mishra,Kranti Prakash Yeole,Ramyashree Keshavamurthy,Mokshit Bharat Surana,Fatemeh Sarayloo*

Main category: cs.IR

TL;DR: 使用LLM进行元数据增强的系统框架，通过动态生成文档片段的元数据来提升RAG系统的检索性能，实验显示元数据增强方法优于纯内容基线。


<details>
  <summary>Details</summary>
Motivation: 在企业环境中，从大型复杂知识库中高效检索相关信息对运营效率和决策制定至关重要。现有RAG系统在文档检索方面仍有优化空间。

Method: 提出系统化的元数据增强框架，使用LLM动态生成文档片段的元数据。比较三种分块策略（语义、递归、朴素），结合先进的嵌入技术，并采用交叉编码器重排序生成真实标签。

Result: 元数据增强方法始终优于纯内容基线：递归分块+TF-IDF加权嵌入达到82.5%精确率（语义纯内容为73.3%）；朴素分块+前缀融合获得最高Hit Rate@10（0.925）。元数据增强提升了向量聚类质量并减少了检索延迟。

Conclusion: 元数据增强是提升RAG系统有效性的强大方法，为企业环境部署高性能、可扩展的文档检索解决方案提供了实用见解。

Abstract: In enterprise settings, efficiently retrieving relevant information from large and complex knowledge bases is essential for operational productivity and informed decision-making. This research presents a systematic framework for metadata enrichment using large language models (LLMs) to enhance document retrieval in Retrieval-Augmented Generation (RAG) systems. Our approach employs a comprehensive, structured pipeline that dynamically generates meaningful metadata for document segments, substantially improving their semantic representations and retrieval accuracy. Through extensive experiments, we compare three chunking strategies-semantic, recursive, and naive-and evaluate their effectiveness when combined with advanced embedding techniques. The results demonstrate that metadata-enriched approaches consistently outperform content-only baselines, with recursive chunking paired with TF-IDF weighted embeddings yielding an 82.5% precision rate compared to 73.3% for semantic content-only approaches. The naive chunking strategy with prefix-fusion achieved the highest Hit Rate@10 of 0.925. Our evaluation employs cross-encoder reranking for ground truth generation, enabling rigorous assessment via Hit Rate and Metadata Consistency metrics. These findings confirm that metadata enrichment enhances vector clustering quality while reducing retrieval latency, making it a key optimization for RAG systems across knowledge domains. This work offers practical insights for deploying high-performance, scalable document retrieval solutions in enterprise settings, demonstrating that metadata enrichment is a powerful approach for enhancing RAG effectiveness.

</details>


### [4] [Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms](https://arxiv.org/abs/2512.05967)
*Francesco Granata,Francesco Poggi,Misael Mongiovì*

Main category: cs.IR

TL;DR: 提出了一种增强的RAG架构，通过集成实体链接的事实信号来提高意大利语教育问答系统的准确性，在特定领域场景中基于互惠排名融合的混合方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 在LLM时代，基于语义相似度的RAG系统在专业领域经常无法保证事实准确性，因为术语歧义会影响检索相关性，特别是在教育问答系统中需要更高的可靠性。

Method: 提出增强的RAG架构，包含基于Wikidata的实体链接模块，并实现三种重排序策略：混合分数加权模型、互惠排名融合和交叉编码器重排序器，将语义和基于实体的信息相结合。

Result: 在特定领域上下文中，基于互惠排名融合的混合方案显著优于基线和交叉编码器方法；在通用领域数据集上，交叉编码器获得最佳结果，证实了领域不匹配效应。

Conclusion: 研究强调了领域适应和混合排名策略对于提高检索增强生成的事实精确性和可靠性的重要性，展示了实体感知RAG系统在教育环境中促进自适应和可靠AI辅导工具的潜力。

Abstract: In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [5] [Low-Complexity OFDM Deep Neural Receivers](https://arxiv.org/abs/2512.05249)
*Ankit Gupta,Onur Dizdar,Yun Chen,Fehmi Emre Kadan,Ata Sattarzadeh,Stephen Wang*

Main category: cs.IT

TL;DR: 提出基于ResNet的OFDM神经接收机设计，通过小卷积核、均匀通道等优化降低计算量和训练成本，同时提升解码性能


<details>
  <summary>Details</summary>
Motivation: 现有OFDM神经接收机架构在提升性能时忽略了训练收敛所需的epoch数和计算复杂度(FLOPs)的显著增加，需要更高效的架构设计

Method: 设计新的ResNet块：使用小卷积核和膨胀率降低FLOPs，均匀通道大小减少内存访问成本，引入通道分割和混洗块，移除逐元素加法，采用GELU激活函数

Result: 提出的神经接收机在降低计算复杂度的同时改善了训练收敛性，并提高了解码准确率

Conclusion: 通过优化的ResNet块设计，实现了计算效率更高、训练更快、性能更好的OFDM神经接收机

Abstract: Deep neural receivers (NeuralRxs) for Orthogonal Frequency Division Multiplexing (OFDM) signals are proposed for enhanced decoding performance compared to their signal-processing based counterparts. However, the existing architectures ignore the required number of epochs for training convergence and floating-point operations (FLOPs), which increase significantly with improving performance. To tackle these challenges, we propose a new residual network (ResNet) block design for OFDM NeuralRx. Specifically, we leverage small kernel sizes and dilation rates to lower the number of FLOPs (NFLOPs) and uniform channel sizes to reduce the memory access cost (MAC). The ResNet block is designed with novel channel split and shuffle blocks, element-wise additions are removed, with Gaussian error linear unit (GELU) activations. Extensive simulations show that our proposed NeuralRx reduces NFLOPs and improves training convergence while improving the decoding accuracy.

</details>


### [6] [Uncertainty-Aware Data-Efficient AI: An Information-Theoretic Perspective](https://arxiv.org/abs/2512.05267)
*Osvaldo Simeone,Yaniv Romano*

Main category: cs.IT

TL;DR: 这篇综述论文探讨了在数据有限场景下处理认知不确定性的两种互补方法：量化认知不确定性和通过合成数据增强缓解数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 在机器人、电信和医疗等特定应用领域，人工智能系统常面临训练数据有限的问题，这导致认知不确定性（可减少的不确定性），从而限制了预测性能。

Method: 采用信息论视角，首先回顾广义贝叶斯学习和后贝叶斯学习框架来量化不确定性，然后分析信息论泛化界限，接着调查具有有限样本统计保证的方法（如共形预测），最后探讨结合有限标注数据与丰富模型预测或合成数据的方法。

Result: 论文系统性地梳理了数据有限场景下的不确定性量化方法，从理论框架到实际应用，强调了信息度量在量化数据稀缺影响中的作用。

Conclusion: 通过信息论视角，论文为数据有限场景下的认知不确定性处理提供了全面的方法论框架，将不确定性量化与数据增强相结合，为实际应用提供了理论基础和实践指导。

Abstract: In context-specific applications such as robotics, telecommunications, and healthcare, artificial intelligence systems often face the challenge of limited training data. This scarcity introduces epistemic uncertainty, i.e., reducible uncertainty stemming from incomplete knowledge of the underlying data distribution, which fundamentally limits predictive performance. This review paper examines formal methodologies that address data-limited regimes through two complementary approaches: quantifying epistemic uncertainty and mitigating data scarcity via synthetic data augmentation. We begin by reviewing generalized Bayesian learning frameworks that characterize epistemic uncertainty through generalized posteriors in the model parameter space, as well as ``post-Bayes'' learning frameworks. We continue by presenting information-theoretic generalization bounds that formalize the relationship between training data quantity and predictive uncertainty, providing a theoretical justification for generalized Bayesian learning. Moving beyond methods with asymptotic statistical validity, we survey uncertainty quantification methods that provide finite-sample statistical guarantees, including conformal prediction and conformal risk control. Finally, we examine recent advances in data efficiency by combining limited labeled data with abundant model predictions or synthetic data. Throughout, we take an information-theoretic perspective, highlighting the role of information measures in quantifying the impact of data scarcity.

</details>


### [7] [Foundations of information theory for coding theory](https://arxiv.org/abs/2512.05316)
*El Mahdi Mouloua,Essaid Mohamed*

Main category: cs.IT

TL;DR: 这篇讲义介绍了信息论及其与代数编码理论的关系，重点讨论了香农的信息、熵和信道容量概念，以及最大似然解码和噪声信道编码定理。


<details>
  <summary>Details</summary>
Motivation: 建立信息论与代数编码理论之间的联系，为理解信息传输的数学基础提供框架，特别关注概率框架与代数技术之间的桥梁。

Method: 基于香农的信息论公式，发展量化不确定性和信息传输的数学基础，使用二进制对称信道等示例说明熵、条件熵、互信息和噪声信道模型等核心概念。

Result: 系统阐述了信息论的基本概念和原理，包括熵、互信息、信道容量等度量，以及最大似然解码和香农噪声信道编码定理，后者定义了噪声信道可靠通信的理论极限。

Conclusion: 该讲义为寻求信息论概率框架与现代编码理论中使用的结构和代数技术之间联系的学生和研究人员提供了有价值的资源，建立了信息论与代数编码理论之间的重要桥梁。

Abstract: Information theory is introduced in this lecture note with a particular emphasis on its relevance to algebraic coding theory. The document develops the mathematical foundations for quantifying uncertainty and information transmission by building upon Shannon's pioneering formulation of information, entropy, and channel capacity. Examples, including the binary symmetric channel, illustrate key concepts such as entropy, conditional entropy, mutual information, and the noisy channel model. Furthermore, the note describes the principles of maximum likelihood decoding and Shannon's noisy channel coding theorem, which characterizes the theoretical limits of reliable communication over noisy channels. Students and researchers seeking a connection between probabilistic frameworks of information theory and structural and algebraic techniques used in modern coding theory will find this work helpful.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [8] [Integrating Wearable Data into Process Mining: Event, Case and Activity Enrichment](https://arxiv.org/abs/2512.05203)
*Vinicius Stein Dani,Xixi Lu,Iris Beerepoot*

Main category: cs.DB

TL;DR: 探索将可穿戴设备数据融入事件日志的三种方法：作为事件属性、案例属性或衍生新事件，并用真实数据演示，讨论过程挖掘中整合可穿戴数据的技术与概念挑战。


<details>
  <summary>Details</summary>
Motivation: 研究如何将可穿戴设备收集的健康数据整合到过程挖掘中，以增强对个人生产力和福祉的分析能力。当前事件日志通常缺乏生理和行为数据，而可穿戴设备能提供这些补充信息。

Method: 提出三种方法：1) 将可穿戴数据作为事件属性直接链接到单个事件；2) 作为案例属性使用聚合的日级评分；3) 从可穿戴数据衍生新事件（如睡眠时段、体力活动）。使用真实数据演示，匹配智能手表健康数据与数字日历应用事件。

Result: 成功展示了三种整合可穿戴数据到事件日志的方法，并识别了技术实现和概念整合方面的具体挑战，为个人生产力和福祉的过程挖掘提供了实用框架。

Conclusion: 可穿戴数据能丰富过程挖掘分析，但需要解决数据对齐、时间粒度、隐私和技术集成等挑战。三种方法各有适用场景，为个人过程优化和健康管理提供了新视角。

Abstract: In this short paper, we explore the enrichment of event logs with data from wearable devices. We discuss three approaches: (1) treating wearable data as event attributes, linking them directly to individual events, (2) treating wearable data as case attributes, using aggregated day-level scores, and (3) introducing new events derived from wearable data, such as sleep episodes or physical activities. To illustrate these approaches, we use real-world data from one person, matching health data from a smartwatch with events extracted from a digital calendar application. Finally, we discuss the technical and conceptual challenges involved in integrating wearable data into process mining for personal productivity and well-being.

</details>


### [9] [Featurized-Decomposition Join: Low-Cost Semantic Joins with Guarantees](https://arxiv.org/abs/2512.05399)
*Sepanta Zeighami,Shreya Shankar,Aditya Parameswaran*

Main category: cs.DB

TL;DR: 提出Featurized-Decomposition Join (FDJ)方法，通过自动提取特征并组合成合取范式逻辑表达式，大幅降低语义连接成本，相比现有方法成本减少高达10倍。


<details>
  <summary>Details</summary>
Motivation: LLMs在处理大规模文本数据集时，语义连接（基于自然语言谓词连接两个表）需要为每对元组调用LLM，成本高昂。现有方法使用嵌入相似性过滤候选对，但效果有限，因为语义相似性不能可靠预测连接结果。

Method: 提出FDJ方法：1）自动从文本记录中提取关键特征；2）将这些特征组合成合取范式逻辑表达式（称为特征化分解）；3）在提取的特征上执行低成本比较来修剪非匹配对；4）使用LLMs自动提取可靠特征并组合成逻辑表达式，同时提供统计保证。

Result: 在真实世界数据集上的实验显示，相比最先进方法，成本减少高达10倍，同时提供相同的质量保证。

Conclusion: FDJ通过特征化分解方法有效解决了语义连接的高成本问题，在保持质量的同时显著降低了计算开销，为LLMs在大规模数据处理中的应用提供了更高效的解决方案。

Abstract: Large Language Models (LLMs) are being increasingly used within data systems to process large datasets with text fields. A broad class of such tasks involves a semantic join-joining two tables based on a natural language predicate per pair of tuples, evaluated using an LLM. Semantic joins generalize tasks such as entity matching and record categorization, as well as more complex text understanding tasks. A naive implementation is expensive as it requires invoking an LLM for every pair of rows in the cross product. Existing approaches mitigate this cost by first applying embedding-based semantic similarity to filter candidate pairs, deferring to an LLM only when similarity scores are deemed inconclusive. However, these methods yield limited gains in practice, since semantic similarity may not reliably predict the join outcome. We propose Featurized-Decomposition Join (FDJ for short), a novel approach for performing semantic joins that significantly reduces cost while preserving quality. FDJ automatically extracts features and combines them into a logical expression in conjunctive normal form that we call a featurized decomposition to effectively prune out non-matching pairs. A featurized decomposition extracts key information from text records and performs inexpensive comparisons on the extracted features. We show how to use LLMs to automatically extract reliable features and compose them into logical expressions while providing statistical guarantees on the output result-an inherently challenging problem due to dependencies among features. Experiments on real-world datasets show up to 10 times reduction in cost compared with the state-of-the-art while providing the same quality guarantees.

</details>


### [10] [PETGraphDB: A Property Evolution Temporal Graph Data Management System](https://arxiv.org/abs/2512.05417)
*Jinghe Song,Zongyu Zuo,Xuelian Lin,Yang Wang,Shuai Ma*

Main category: cs.DB

TL;DR: PETGraph是一个专门为属性演化时序图设计的数据管理系统，相比现有方案显著减少了存储空间并大幅提升了查询性能和事务吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着物联网系统发展，属性演化时序图（节点/边属性频繁变化而拓扑结构基本不变）快速增长，但现有时序图管理方案不适合此类数据，导致数据建模复杂且查询处理性能低下。

Method: 采用有效时间时序属性图数据模型简化数据建模，支持ACID事务；设计空间高效的时序属性存储和细粒度多级锁定机制来提升查询性能。

Result: PETGraph平均只需当前最佳方案的33%存储空间；在HTAP工作负载中平均实现58.8倍的事务吞吐量提升；查询延迟平均比最佳方案快267倍。

Conclusion: PETGraph专门针对属性演化时序图设计，有效解决了现有方案在数据建模复杂性和查询性能方面的不足，在存储效率和查询性能方面都取得了显著优势。

Abstract: Temporal graphs are graphs whose nodes and edges, together with their associated properties, continuously change over time. With the development of Internet of Things (IoT) systems, a subclass of the temporal graph, i.e., Property Evolution Temporal Graph, in which the value of properties on nodes or edges changes frequently while the graph's topology barely changes, is growing rapidly. However, existing temporal graph management solutions are not oriented to the Property Evolution Temporal Graph data, which leads to highly complex data modeling and low-performance query processing of temporal graph queries. To solve these problems, we developed PETGraph, a data management system for Property Evolution Temporal Graph data. PETGraph adopts a valid-time temporal property graph data model to facilitate data modeling, supporting ACID features with transactions. To improve temporal graph query performance, we designed a space-efficient temporal property storage and a fine-granularity multi-level locking mechanism. Experimental results show that PETGraph requires, on average, only 33% of the storage space needed by the current best data management solution. Additionally, it achieves an average of 58.8 times higher transaction throughput in HTAP workloads compared to the best current solutions and outperforms them by an average of 267 times in query latency.

</details>


### [11] [Parajudica: An RDF-Based Reasoner and Metamodel for Multi-Framework Context-Dependent Data Compliance Assessments](https://arxiv.org/abs/2512.05453)
*Luc Moreau,Alfred Rossi,Sophie Stalla-Bourdillon*

Main category: cs.DB

TL;DR: Parajudica是一个基于RDF/SPARQL的开放、模块化、可扩展规则系统，用于评估多合规框架下的数据合规状态


<details>
  <summary>Details</summary>
Motivation: 解决在多个同时适用的合规框架下实施基于策略的数据访问控制(PBAC)的挑战

Method: 开发了一个基于RDF/SPARQL的开放、模块化、可扩展规则系统，包含配套的元模型

Result: 通过应用于现有法律框架和行业标准，展示了该资源的实用性，并为比较框架分析提供了见解

Conclusion: 该系统可应用于合规策略执行、合规监控、数据发现和风险评估等多个领域

Abstract: Motivated by the challenges of implementing policy-based data access control (PBAC) under multiple simultaneously applicable compliance frameworks, we present Parajudica, an open, modular, and extensible RDF/SPARQL-based rule system for evaluating context-dependent data compliance status. We demonstrate the utility of this resource and accompanying metamodel through application to existing legal frameworks and industry standards, offering insights for comparative framework analysis. Applications include compliance policy enforcement, compliance monitoring, data discovery, and risk assessment.

</details>


### [12] [Poodle: Seamlessly Scaling Down Large Language Models with Just-in-Time Model Replacement](https://arxiv.org/abs/2512.05525)
*Nils Strassenburg,Boris Glavic,Tilmann Rabl*

Main category: cs.DB

TL;DR: 提出JITR（即时模型替换）框架，在识别出LLM重复任务时自动替换为更便宜的小模型，平衡易用性与资源效率


<details>
  <summary>Details</summary>
Motivation: 企业越来越多使用LLM自动化简单重复任务，但LLM资源能耗远高于小模型，而小模型在简单任务上性能相当。需要平衡LLM易用性与资源效率

Method: 提出JITR框架：1) 识别LLM调用中的重复任务 2) 透明替换为更便宜的替代模型 3) 使用模型搜索和迁移学习快速找到并微调适合特定任务的模型

Result: 通过原型系统Poodle验证，在示例任务中实现了显著的成本和能耗节省

Conclusion: JITR框架能保留LLM的易用性和低开发成本优势，同时显著降低资源消耗，模型搜索和迁移学习是实现该愿景的关键技术

Abstract: Businesses increasingly rely on large language models (LLMs) to automate simple repetitive tasks instead of developing custom machine learning models. LLMs require few, if any, training examples and can be utilized by users without expertise in model development. However, this comes at the cost of substantially higher resource and energy consumption compared to smaller models, which often achieve similar predictive performance for simple tasks. In this paper, we present our vision for just-in-time model replacement (JITR), where, upon identifying a recurring task in calls to an LLM, the model is replaced transparently with a cheaper alternative that performs well for this specific task. JITR retains the ease of use and low development effort of LLMs, while saving significant cost and energy. We discuss the main challenges in realizing our vision regarding the identification of recurring tasks and the creation of a custom model. Specifically, we argue that model search and transfer learning will play a crucial role in JITR to efficiently identify and fine-tune models for a recurring task. Using our JITR prototype Poodle, we achieve significant savings for exemplary tasks.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [13] [Incorporating indel channels into average-case analysis of seed-chain-extend](https://arxiv.org/abs/2512.05247)
*Spencer Gibson,Yun William Yu*

Main category: cs.DS

TL;DR: 该论文将种子-链-延伸启发式算法的理论分析扩展到包含插入和删除的突变通道，证明了最优链的期望恢复率至少为1-O(1/√m)，期望运行时间为O(mn^{3.15·θ_T} log n)，其中θ_T < 0.159。


<details>
  <summary>Details</summary>
Motivation: 现有理论分析仅适用于仅替换的突变通道，而实际基因组数据包含插入和删除(indels)。需要弥合理论与实践的差距，为包含indels的突变通道提供理论保证。

Method: 引入新的数学工具处理indel通道带来的两个新障碍：相邻锚点的依赖性和部分正确锚点的存在。扩展了Shaw和Yu的线性间隙成本链分析框架，使其能够处理更复杂的突变模型。

Result: 证明了在总突变率θ_T = θ_i + θ_d + θ_s < 0.159时，最优链的期望恢复率≥1-O(1/√m)，期望运行时间为O(mn^{3.15·θ_T} log n)。

Conclusion: 该研究为包含插入和删除的突变通道提供了种子-链-延伸算法的理论保证，弥合了理论与实践的差距，证明了该启发式方法在实际基因组数据中的有效性。

Abstract: Given a sequence $s_1$ of $n$ letters drawn i.i.d. from an alphabet of size $σ$ and a mutated substring $s_2$ of length $m < n$, we often want to recover the mutation history that generated $s_2$ from $s_1$. Modern sequence aligners are widely used for this task, and many employ the seed-chain-extend heuristic with $k$-mer seeds. Previously, Shaw and Yu showed that optimal linear-gap cost chaining can produce a chain with $1 - O\left(\frac{1}{\sqrt{m}}\right)$ recoverability, the proportion of the mutation history that is recovered, in $O\left(mn^{2.43θ} \log n\right)$ expected time, where $θ< 0.206$ is the mutation rate under a substitution-only channel and $s_1$ is assumed to be uniformly random. However, a gap remains between theory and practice, since real genomic data includes insertions and deletions (indels), and yet seed-chain-extend remains effective. In this paper, we generalize those prior results by introducing mathematical machinery to deal with the two new obstacles introduced by indel channels: the dependence of neighboring anchors and the presence of anchors that are only partially correct. We are thus able
  to prove that the expected recoverability of an optimal chain is $\ge 1 - O\Bigl(\frac{1}{\sqrt{m}}\Bigr)$ and the expected runtime is $O(mn^{3.15 \cdot θ_T}\log n)$, when the total mutation rate given by the sum of the substitution, insertion, and deletion mutation rates ($θ_T = θ_i + θ_d + θ_s$) is less than $0.159$.

</details>


### [14] [Crude Approximation of Directed Minimum Cut and Arborescence Packing in Almost Linear Time](https://arxiv.org/abs/2512.05300)
*Yonggang Jiang,Yaowei Long,Thatchaphol Saranurak,Benyu Wang*

Main category: cs.DS

TL;DR: 该论文提出了在有向图中近似计算根最小割和最大树状图包装的几乎线性时间算法，这两个问题是相互对偶的。


<details>
  <summary>Details</summary>
Motivation: 现有的根最小割精确算法需要超线性时间，而现有的树状图包装算法虽然无拥塞但需要多项式时间。需要更高效的近似算法来解决这些对偶问题。

Method: 提出了两个算法：1）在加权图中计算O(k log⁵ n)大小的根割，时间m¹⁺ᵒ⁽¹⁾；2）包装k个根树状图，拥塞nᵒ⁽¹⁾，时间m¹⁺ᵒ⁽¹⁾。

Result: 实现了几乎线性时间（m¹⁺ᵒ⁽¹⁾）的近似算法，第一个算法输出割大小为O(k log⁵ n)，第二个算法包装k个树状图拥塞为nᵒ⁽¹⁾。

Conclusion: 该工作首次为有向图中的根最小割和最大树状图包装问题提供了几乎线性时间的近似算法，显著改进了先前算法的运行时间。

Abstract: We give almost-linear-time algorithms for approximating rooted minimum cut and maximum arborescence packing in directed graphs, two problems that are dual to each other [Edm73]. More specifically, for an $n$-vertex, $m$-edge directed graph $G$ whose $s$-rooted minimum cut value is $k$, our first algorithm computes an $s$-rooted cut of size at most $O(k\log^{5} n)$ in $m^{1+o(1)}$ time, and our second algorithm packs $k$ $s$-rooted arborescences with $n^{o(1)}$ congestion in $m^{1+o(1)}$ time, certifying that the $s$-rooted minimum cut is at least $k / n^{o(1)}$. Our first algorithm also works for weighted graphs.
  Prior to our work, the fastest algorithms for computing the $s$-rooted minimum cut were exact but had super-linear running time: either $\tilde{O}(mk)$ [Gab91] or $\tilde{O}(m^{1+o(1)}\min\{\sqrt{n},n/m^{1/3}\})$ [CLN+22]. The fastest known algorithms for packing $s$-rooted arborescences had no congestion, but required $\tilde{O}(m \cdot \mathrm{poly}(k))$ time [BHKP08].

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [15] [Strategyproof Tournament Rules for Teams with a Constant Degree of Selfishness](https://arxiv.org/abs/2512.05235)
*David Pennock,Daniel Schoepflin,Kangning Wang*

Main category: cs.GT

TL;DR: 设计了一种新的锦标赛规则，在自私参数λ=11时具有策略抗性，显著改进了之前需要λ=Ω(n)的结果，同时提出了乘性成对非操纵性概念


<details>
  <summary>Details</summary>
Motivation: 研究公平且抗操纵的锦标赛规则设计问题，需要满足单调性、孔多塞一致性和策略抗性。之前的研究要么假设完全自私的团队（只关心自己获胜概率），要么假设完全无私的团队（关心自己和故意输给的团队的总获胜概率），最近的研究考虑混合设置。现有规则需要λ=Ω(n)才能实现策略抗性，寻找更小λ的规则是开放问题

Method: 提出了新的锦标赛规则设计，实现了λ=11的策略抗性。同时引入了乘性成对非操纵性概念，确保两个团队不能通过操纵比赛结果使其总获胜概率增加超过乘性因子δ=3.5

Result: 成功设计了λ=11的策略抗性锦标赛规则，显著改进了之前需要λ=Ω(n)的结果。同时提出的乘性成对非操纵性规则实现了δ=3.5

Conclusion: 在锦标赛规则设计方面取得了重要进展，将策略抗性所需的自私参数从Ω(n)降低到常数11，解决了长期存在的开放问题，并为该领域提供了新的分析框架

Abstract: We revisit the well-studied problem of designing fair and manipulation-resistant tournament rules. In this problem, we seek a mechanism that (probabilistically) identifies the winner of a tournament after observing round-robin play among $n$ teams in a league. Such a mechanism should satisfy the natural properties of monotonicity and Condorcet consistency. Moreover, from the league's perspective, the winner-determination tournament rule should be strategyproof, meaning that no team can do better by losing a game on purpose.
  Past work considered settings in which each team is fully selfish, caring only about its own probability of winning, and settings in which each team is fully selfless, caring only about the total winning probability of itself and the team to which it deliberately loses. More recently, researchers considered a mixture of these two settings with a parameter $λ$. Intermediate selfishness $λ$ means that a team will not lose on purpose unless its pair gains at least $λs$ winning probability, where $s$ is the individual team's sacrifice from its own winning probability. All of the dozens of previously known tournament rules require $λ= Ω(n)$ to be strategyproof, and it has been an open problem to find such a rule with the smallest $λ$.
  In this work, we make significant progress by designing a tournament rule that is strategyproof with $λ= 11$. Along the way, we propose a new notion of multiplicative pairwise non-manipulability that ensures that two teams cannot manipulate the outcome of a game to increase the sum of their winning probabilities by more than a multiplicative factor $δ$ and provide a rule which is multiplicatively pairwise non-manipulable for $δ= 3.5$.

</details>


### [16] [Robust forecast aggregation via additional queries](https://arxiv.org/abs/2512.05271)
*Rafael Frongillo,Mary Monroe,Eric Neyman,Bo Waggoner*

Main category: cs.GT

TL;DR: 该论文提出了一个通过结构化查询从专家获取更丰富信息的鲁棒预测聚合框架，克服了传统方法只能聚合个体预测的限制，实现了最优聚合并建立了准确性与查询复杂度之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 传统预测聚合存在严重限制，即使在自然假设下，聚合专家个体预测也无法超越随机跟随单个专家的性能。需要开发新框架来克服这些不可能性结果，通过获取更丰富的专家信息实现更好的聚合。

Method: 引入结构化查询框架，允许从专家处获取比简单预测更丰富的信息。该框架确保专家真实报告其底层信念，并定义了查询复杂度的概念。在专家信号独立但重叠的通用模型下，研究查询复杂度与聚合准确性的关系。

Result: 证明了在最坏情况下可以实现最优聚合，且每个复杂度度量以代理数量n为上界。建立了准确性与查询复杂度之间的紧致权衡：聚合误差随查询数量线性减少，当"推理阶数"和与查询相关的代理数量为ω(√n)时误差消失。

Conclusion: 适度的专家查询空间扩展显著增强了鲁棒预测聚合的能力。新的查询框架为该领域开辟了富有成果的研究方向，解决了传统方法的局限性。

Abstract: We study the problem of robust forecast aggregation: combining expert forecasts with provable accuracy guarantees compared to the best possible aggregation of the underlying information. Prior work shows strong impossibility results, e.g. that even under natural assumptions, no aggregation of the experts' individual forecasts can outperform simply following a random expert (Neyman and Roughgarden, 2022).
  In this paper, we introduce a more general framework that allows the principal to elicit richer information from experts through structured queries. Our framework ensures that experts will truthfully report their underlying beliefs, and also enables us to define notions of complexity over the difficulty of asking these queries. Under a general model of independent but overlapping expert signals, we show that optimal aggregation is achievable in the worst case with each complexity measure bounded above by the number of agents $n$. We further establish tight tradeoffs between accuracy and query complexity: aggregation error decreases linearly with the number of queries, and vanishes when the "order of reasoning" and number of agents relevant to a query is $ω(\sqrt{n})$. These results demonstrate that modest extensions to the space of expert queries dramatically strengthen the power of robust forecast aggregation. We therefore expect that our new query framework will open up a fruitful line of research in this area.

</details>


### [17] [Correlation of Rankings in Matching Markets](https://arxiv.org/abs/2512.05304)
*Rémi Castera,Patrick Loiseau,Bary S. R. Pradelski*

Main category: cs.GT

TL;DR: 研究匹配市场中相关性对群体不平等的影响，发现不同社会人口群体在决策者间的相关性差异会导致系统性不平等


<details>
  <summary>Details</summary>
Motivation: 研究匹配市场中相关性如何影响群体不平等，特别是在学校选择、大学录取和就业市场中，不同群体在决策者间的相关性差异可能源于选择标准、考试可选政策或算法同质化

Method: 提出一个模型，其中候选人在不同决策者间的优先级得分相关性因社会人口群体而异，分析相关性差异对匹配结果的影响，并将平局打破机制扩展到多个优先级类别和中等相关性水平

Result: 较高相关性通常提高所有群体的效率，但自身相关性高的群体成员更可能无法匹配，因此属于低相关性群体更有优势，相关性差异是匹配市场中群体不平等的系统性来源

Conclusion: 相关性差异是学校、大学和就业录取中群体不平等的先前被忽视的系统性来源，需要政策制定者关注这一机制

Abstract: We study the role of correlation in matching markets, where multiple decision-makers simultaneously face selection problems from the same pool of candidates. We propose a model in which a candidate's priority scores across different decision-makers exhibit varying levels of correlation dependent on the candidate's sociodemographic group. Such differential correlation can arise in school choice due to the varying prevalence of selection criteria, in college admissions due to test-optional policies, or due to algorithmic monoculture, that is, when decision-makers rely on the same algorithms and data sets to evaluate candidates. We show that higher correlation for one of the groups generally improves the outcome for all groups, leading to higher efficiency. However, students from a given group are more likely to remain unmatched as their own correlation level increases. This implies that it is advantageous to belong to a low-correlation group. Finally, we extend the tie-breaking literature to multiple priority classes and intermediate levels of correlation. Overall, our results point to differential correlation as a previously overlooked systemic source of group inequalities in school, university, and job admissions.

</details>


### [18] [On Dynamic Programming Theory for Leader-Follower Stochastic Games](https://arxiv.org/abs/2512.05667)
*Jilles Steeve Dibangoye,Thibaut Le Marre,Ocan Sankur,François Schwarzentruber*

Main category: cs.GT

TL;DR: 本文提出了一种用于领导者-追随者一般和随机博弈的动态规划框架，通过可信集进行Bellman递归来计算强Stackelberg均衡，证明了该问题可归约为MDP但最优确定性策略是NP难的，并开发了具有理论保证的ε最优算法。


<details>
  <summary>Details</summary>
Motivation: 领导者-追随者一般和随机博弈（LF-GSSGs）建模了非对称承诺下的序贯决策问题，其中领导者承诺策略而追随者最佳响应。现有方法在计算强Stackelberg均衡时面临可扩展性和最优性保证的挑战，需要开发更高效的算法框架。

Method: 提出基于可信集的动态规划框架：1）定义可信集作为状态抽象，表示部分领导者承诺下所有理性追随者最佳响应；2）证明LF-GSSG可无损归约为可信集上的马尔可夫决策过程；3）开发ε最优DP算法，具有领导者可利用性的理论保证。

Result: 1）理论证明：最优无记忆确定性领导者策略合成是NP难的；2）算法性能：在安全博弈、资源分配和对抗规划等混合动机基准测试中，新方法在领导者价值和运行时可扩展性上优于现有最先进方法。

Conclusion: 本文提出的可信集动态规划框架为领导者-追随者随机博弈提供了有效的计算解决方案，通过理论分析和实验验证展示了其在处理非对称承诺序贯决策问题上的优越性和实用性。

Abstract: Leader-follower general-sum stochastic games (LF-GSSGs) model sequential decision-making under asymmetric commitment, where a leader commits to a policy and a follower best responds, yielding a strong Stackelberg equilibrium (SSE) with leader-favourable tie-breaking. This paper introduces a dynamic programming (DP) framework that applies Bellman recursion over credible sets-state abstractions formally representing all rational follower best responses under partial leader commitments-to compute SSEs. We first prove that any LF-GSSG admits a lossless reduction to a Markov decision process (MDP) over credible sets. We further establish that synthesising an optimal memoryless deterministic leader policy is NP-hard, motivating the development of ε-optimal DP algorithms with provable guarantees on leader exploitability. Experiments on standard mixed-motive benchmarks-including security games, resource allocation, and adversarial planning-demonstrate empirical gains in leader value and runtime scalability over state-of-the-art methods.

</details>


### [19] [Invariant Price of Anarchy: a Metric for Welfarist Traffic Control](https://arxiv.org/abs/2512.05843)
*Ilia Shilov,Mingjia He,Heinrich H. Nax,Emilio Frazzoli,Gioele Zardini,Saverio Bolognani*

Main category: cs.GT

TL;DR: 该论文提出"不变价格无政府状态"概念，解决传统PoA在成本缩放和偏移变换下的不稳定性问题，通过社会选择理论建立稳健的效率评估框架。


<details>
  <summary>Details</summary>
Motivation: 传统价格无政府状态分析依赖精确数值成本，但实际应用中成本代表代理偏好，可能只定义到任意缩放和偏移的程度，存在信息和建模模糊性。这些变换虽然保持均衡和最优结果，但会改变PoA值，导致效率评估不稳定。

Method: 基于社会选择理论，定义不变价格无政府状态。通过连接可容许变换与代理成本可比性程度，推导出确保效率评估不依赖个体成本任意缩放或偏移的特定社会福利函数。使用玩具示例和苏黎世网络进行案例研究。

Result: 研究表明，相同的收费策略根据假设的可比性程度可能导致显著不同的效率估计。框架证明需要明确的公理基础来定义效率指标，以稳健有效地指导大规模基础设施设计的政策。

Conclusion: 效率评估必须考虑成本可比性假设，不变PoA框架为在存在信息和建模模糊性的情况下进行稳健政策指导提供了理论基础，强调公理化方法对大规模基础设施设计的重要性。

Abstract: The Price of Anarchy (PoA) is a standard metric for quantifying inefficiency in socio-technical systems, widely used to guide policies like traffic tolling. Conventional PoA analysis relies on exact numerical costs. However, in many settings, costs represent agents' preferences and may be defined only up to possibly arbitrary scaling and shifting, representing informational and modeling ambiguities. We observe that while such transformations preserve equilibrium and optimal outcomes, they change the PoA value. To resolve this issue, we rely on results from Social Choice Theory and define the Invariant PoA. By connecting admissible transformations to degrees of comparability of agents' costs, we derive the specific social welfare functions which ensure that efficiency evaluations do not depend on arbitrary rescalings or translations of individual costs. Case studies on a toy example and the Zurich network demonstrate that identical tolling strategies can lead to substantially different efficiency estimates depending on the assumed comparability. Our framework thus demonstrates that explicit axiomatic foundations are necessary in order to define efficiency metrics and to appropriately guide policy in large-scale infrastructure design robustly and effectively.

</details>
