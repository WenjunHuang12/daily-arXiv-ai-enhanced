{"id": "2601.16367", "categories": ["cs.GT", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.16367", "abs": "https://arxiv.org/abs/2601.16367", "authors": ["Bryce L. Ferguson", "Chinmay Maheshwari", "Manxi Wu", "Shankar Sastry"], "title": "Game-to-Real Gap: Quantifying the Effect of Model Misspecification in Network Games", "comment": null, "summary": "Game-theoretic models and solution concepts provide rigorous tools for predicting collective behavior in multi-agent systems. In practice, however, different agents may rely on different game-theoretic models to design their strategies. As a result, when these heterogeneous models interact, the realized outcome can deviate substantially from the outcome each agent expects based on its own local model. In this work, we introduce the game-to-real gap, a new metric that quantifies the impact of such model misspecification in multi-agent environments. The game-to-real gap is defined as the difference between the utility an agent actually obtains in the multi-agent environment (where other agents may have misspecified models) and the utility it expects under its own game model. Focusing on quadratic network games, we show that misspecifications in either (i) the external shock or (ii) the player interaction network can lead to arbitrarily large game-to-real gaps. We further develop novel network centrality measures that allow exact evaluation of this gap in quadratic network games. Our analysis reveals that standard network centrality measures fail to capture the effects of model misspecification, underscoring the need for new structural metrics that account for this limitation. Finally, through illustrative numerical experiments, we show that existing centrality measures in network games may provide a counterintuitive understanding of the impact of model misspecification."}
{"id": "2601.16412", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16412", "abs": "https://arxiv.org/abs/2601.16412", "authors": ["Yaonan Jin"], "title": "Tight Regret Bounds for Bilateral Trade under Semi Feedback", "comment": null, "summary": "The study of \\textit{regret minimization in fixed-price bilateral trade} has received considerable attention in recent research. Previous works [CCC+24a, CCC+24b, AFF24, BCCF24, CJLZ25, LCM25a, GDFS25] have acquired a thorough understanding of the problem, except for determining the tight regret bound for GBB semi-feedback fixed-price mechanisms under adversarial values.\n  In this paper, we resolve this open question by devising an $\\widetilde{O}(T^{2 / 3})$-regret mechanism, matching the $Ω(T^{2 / 3})$ lower bound from [CJLZ25] up to polylogarithmic factors."}
{"id": "2601.16488", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2601.16488", "abs": "https://arxiv.org/abs/2601.16488", "authors": ["Yaonan Jin", "Yingkai Li"], "title": "Anonymous Pricing in Large Markets", "comment": null, "summary": "We study revenue maximization when a seller offers $k$ identical units to ex ante heterogeneous, unit-demand buyers. While anonymous pricing can be $Θ(\\log k)$ worse than optimal in general multi-unit environments, we show that this pessimism disappears in large markets, where no single buyer accounts for a non-negligible share of optimal revenue. Under (quasi-)regularity, anonymous pricing achieves a $2+O(1/\\sqrt{k})$ approximation to the optimal mechanism; the worst-case ratio is maximized at about $2.47$ when $k=1$ and converges to $2$ as $k$ grows. This indicates that the gains from third-degree price discrimination are mild in large markets."}
{"id": "2601.16511", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.16511", "abs": "https://arxiv.org/abs/2601.16511", "authors": ["Piotr Faliszewski", "Łukasz Janeczko", "Dušan Knop", "Jan Pokorný", "Šimon Schierreich", "Mateusz Słuszniak", "Krzysztof Sornat"], "title": "Participatory Budgeting Project Strength via Candidate Control", "comment": "A preliminary version appeared in IJCAI '25", "summary": "We study the complexity of candidate control in participatory budgeting elections. The goal of constructive candidate control is to ensure that a given candidate wins by either adding or deleting candidates from the election (in the destructive setting, the goal is to prevent a given candidate from winning). We show that such control problems are NP-hard to solve for many participatory budgeting voting rules, including Phragmén and Method of Equal Shares, but there are natural cases with polynomial-time algorithms (e.g., for the GreedyAV rule and projects with costs encoded in unary). We also argue that control by deleting candidates is a useful tool for assessing the performance (or, strength) of initially losing projects, and we support this view with experiments."}
{"id": "2601.16409", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.16409", "abs": "https://arxiv.org/abs/2601.16409", "authors": ["Yeasir Rayhan", "Walid G. Aref"], "title": "Gen-DBA: Generative Database Agents (Towards a Move 37 for Databases)", "comment": null, "summary": "Move\\,37 marks one of the major breakthroughs in AI in terms of its ability to surpass human expertise and discover novel strategies beyond the traditional game play in the strategic two-player board game of Go. The domains of Natural Language Processing, Computer Vision, and Robotics have also undergone a similar phenomenon through the advent of large foundational models in the form of Large Language Models (LLMs), Vision Language Models (VLMs) and Vision Language Action models (VLAs), respectively. In this paper, we investigate the current state of Artificial Intelligence for Database Systems research (AI4DB), and assess how far AI4DB systems are from achieving their own Move\\,37 moment. We envision a Generative Database Agent (Gen-DBA, for short) as the pathway to achieving Move\\,37 for database systems that will bring generative reasoning and creativity into the realm of database learning tasks. This vision paper explores this direction by presenting the recipe for building Gen-DBA that encompasses but is not limited to a Transformer backbone, a hardware-grounded tokenization mechanism, a two-stage Goal-Directed Next Token Prediction training paradigm, and a generative inference process."}
{"id": "2601.16485", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.16485", "abs": "https://arxiv.org/abs/2601.16485", "authors": ["Hiroki Shibata", "Mitsuru Funakoshi", "Takuya Mieno", "Masakazu Ishihata", "Yuto Nakashima", "Shunsuke Inenaga", "Hideo Bannai", "Masayuki Takeda"], "title": "Online Computation of Palindromes and Suffix Trees on Tries", "comment": null, "summary": "We consider the problems of computing maximal palindromes and distinct palindromes in a trie. A trie is a natural generalization of a string, which can be seen as a single-path tree. There is a linear-time offline algorithm to compute maximal palindromes and distinct palindromes in a given (static) trie whose edge-labels are drawn from a linearly-sortable alphabet [Mieno et al., ISAAC 2022]. In this paper, we tackle problems of palindrome enumeration on dynamic tries which support leaf additions and leaf deletions. We propose the first sub-quadratic algorithms to enumerate palindromes in a dynamic trie. For maximal palindromes, we propose an algorithm that runs in $O(N \\min(\\log h, σ))$ time and uses $O(N)$ space, where $N$ is the maximum number of edges in the trie, $σ$ is the size of the alphabet, and $h$ is the height of the trie. For distinct palindromes, we develop several online algorithms based on different algorithmic frameworks, including approaches using the EERTREE (a.k.a. palindromic tree) and the suffix tree of a trie. These algorithms support leaf insertions and deletions in the trie and achieve different time and space trade-offs. Furthermore, as a by-product, we present online algorithms to construct the suffix tree and the EERTREE of the input trie, which is of independent interest."}
{"id": "2601.16382", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16382", "abs": "https://arxiv.org/abs/2601.16382", "authors": ["Zhiyuan Li", "Yi Yu", "Hongsen He", "Yuyu Zhu", "Rodrigo C. de Lamare"], "title": "Study of Switched Step-size Based Filtered-x NLMS Algorithm for Active Noise Cancellation", "comment": "14 pages, 22 figures", "summary": "While the filtered-x normalized least mean square (FxNLMS) algorithm is widely applied due to its simple structure and easy implementation for active noise control system, it faces two critical limitations: the fixed step-size causes a trade-off between convergence rate and steady-state residual error, and its performance deteriorates significantly in impulsive noise environments. To address the step-size constraint issue, we propose the switched \\mbox{step-size} FxNLMS (SSS-FxNLMS) algorithm. Specifically, we derive the \\mbox{mean-square} deviation (MSD) trend of the FxNLMS algorithm, and then by comparing the MSD trends corresponding to different \\mbox{step-sizes}, the optimal step-size for each iteration is selected. Furthermore, to enhance the algorithm's robustness in impulsive noise scenarios, we integrate a robust strategy into the SSS-FxNLMS algorithm, resulting in a robust variant of it. The effectiveness and superiority of the proposed algorithms has been confirmed through computer simulations in different noise scenarios."}
{"id": "2601.16492", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.16492", "abs": "https://arxiv.org/abs/2601.16492", "authors": ["Emad Siddiqui", "Venkatesh Terikuti", "Xuan Lu"], "title": "LLM-based Semantic Search for Conversational Queries in E-commerce", "comment": null, "summary": "Conversational user queries are increasingly challenging traditional e-commerce platforms, whose search systems are typically optimized for keyword-based queries. We present an LLM-based semantic search framework that effectively captures user intent from conversational queries by combining domain-specific embeddings with structured filters. To address the challenge of limited labeled data, we generate synthetic data using LLMs to guide the fine-tuning of two models: an embedding model that positions semantically similar products close together in the representation space, and a generative model for converting natural language queries into structured constraints. By combining similarity-based retrieval with constraint-based filtering, our framework achieves strong precision and recall across various settings compared to baseline approaches on a real-world dataset."}
{"id": "2601.16579", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.16579", "abs": "https://arxiv.org/abs/2601.16579", "authors": ["Moshe Babaioff", "Yuval Grofman"], "title": "On Best-of-Both-Worlds Fairness via Sum-of-Variances Minimization", "comment": null, "summary": "We consider the problem of fairly allocating a set of indivisible goods among agents with additive valuations. Ex-ante fairness (proportionality) can trivially be obtained by giving all goods to a random agent. Yet, such an allocation is very unfair ex-post. This has motivated the Best-of-Both-Worlds (BoBW) approach, seeking a randomized allocation that is ex-ante proportional and is supported only on ex-post fair allocations (e.g., on allocations that are envy-free-up-to-one-good (EF1), or give some constant fraction of the maximin share (MMS)). It is commonly pointed out that the distribution that allocates all goods to one agent at random fails to be ex-post fair as it ignores the variances of the values of the agents. We examine the approach of trying to mitigate this problem by minimizing the sum-of-variances of the values of the agents, subject to ex-ante proportionality. We study the ex-post fairness properties of the resulting distributions. In support of this approach, observe that such an optimization will indeed deterministically output a proportional allocation if such exists. We show that when valuations are identical, this approach indeed guarantees fairness ex-post: all allocations in the support are envy-free-up-to-any-good (EFX), and thus guarantee every agent at least 4/7 of her maximin share (but not her full MMS). On the negative side, we show that this approach completely fails when valuations are not identical: even in the simplest setting of only two agents and two goods, when the additive valuations are not identical, there is positive probability of allocating both goods to the same agent. Thus, the supporting ex-post allocation might not even be EF1, and might not give an agent any constant fraction of her MMS. Finally, we present similar negative results for other natural minimization objectives that are based on variances."}
{"id": "2601.16432", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.16432", "abs": "https://arxiv.org/abs/2601.16432", "authors": ["Udesh Kumarasinghe", "Tyler Liu", "Chunwei Liu", "Walid G. Aref"], "title": "iPDB -- Optimizing SQL Queries with ML and LLM Predicates", "comment": null, "summary": "Structured Query Language (SQL) has remained the standard query language for databases. SQL is highly optimized for processing structured data laid out in relations. Meanwhile, in the present application development landscape, it is highly desirable to utilize the power of learned models to perform complex tasks. Large language models (LLMs) have been shown to understand and extract information from unstructured textual data. However, SQL as a query language and accompanying relational database systems are either incompatible or inefficient for workloads that require leveraging learned models. This results in complex engineering and multiple data migration operations that move data between the data sources and the model inference platform. In this paper, we present iPDB, a relational system that supports in-database machine learning (ML) and large language model (LLM) inferencing using extended SQL syntax. In iPDB, LLMs and ML calls can function as semantic projects, as predicates to perform semantic selects and semantic joins, or for semantic grouping in group-by clauses. iPDB has a novel relational predict operator and semantic query optimizations that enable users to write and efficiently execute semantic SQL queries, outperforming the state-of-the-art."}
{"id": "2601.16910", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.16910", "abs": "https://arxiv.org/abs/2601.16910", "authors": ["Michael Kapralov", "Luca Trevisan", "Weronika Wrzos-Kaminska"], "title": "Recovering Communities in Structured Random Graphs", "comment": null, "summary": "The problem of recovering planted community structure in random graphs has received a lot of attention in the literature on the stochastic block model, where the input is a random graph in which edges crossing between different communities appear with smaller probability than edges induced by communities. The communities themselves form a collection of vertex-disjoint sparse cuts in the expected graph, and can be recovered, often exactly, from a sample as long as a separation condition on the intra- and inter-community edge probabilities is satisfied.\n  In this paper, we ask whether the presence of a large number of overlapping sparsest cuts in the expected graph still allows recovery. For example, the $d$-dimensional hypercube graph admits $d$ distinct (balanced) sparsest cuts, one for every coordinate. Can these cuts be identified given a random sample of the edges of the hypercube where each edge is present independently with some probability $p\\in (0, 1)$? We show that this is the case, in a very strong sense: the sparsest balanced cut in a sample of the hypercube at rate $p=C\\log d/d$ for a sufficiently large constant $C$ is $1/\\text{poly}(d)$-close to a coordinate cut with high probability. This is asymptotically optimal and allows approximate recovery of all $d$ cuts simultaneously. Furthermore, for an appropriate sample of hypercube-like graphs recovery can be made exact. The proof is essentially a strong hypercube cut sparsification bound that combines a theorem of Friedgut, Kalai and Naor on boolean functions whose Fourier transform concentrates on the first level of the Fourier spectrum with Karger's cut counting argument."}
{"id": "2601.16438", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16438", "abs": "https://arxiv.org/abs/2601.16438", "authors": ["Ziwei Zhao", "Xiaoni DU", "Xingbin Qiao"], "title": "Two classes of LCD codes derived from $(\\mathcal{L},\\mathcal{P})$-TGRS codes", "comment": null, "summary": "Twisted generalized Reed-Solomon (TGRS) codes, as a flexible extension of classical generalized Reed-Solomon (GRS) codes, have attracted significant attention in recent years. In this paper, we construct two classes of LCD codes from the $(\\mathcal{L},\\mathcal{P})$-TGRS code $\\mathcal{C}_h$ of length $n$ and dimension $k$, where $\\mathcal{L}=\\{0,1,\\ldots,l\\}$ for $l\\leq n-k-1$ and $\\mathcal{P}=\\{h\\}$ for $1\\leq h\\leq k-1$. First, we derive the parity check matrix of $\\mathcal{C}_h$ and provide a necessary and sufficient condition for $\\mathcal{C}_h$ to be an AMDS code. Then, we construct two classes of LCD codes from $\\mathcal{C}_h$ by suitably choosing the evaluation points together with certain restrictions on the coefficient of $x^{h-1}$ in the polynomial associated with the twisting term. From the constructed LCD codes we further obtain two classes of LCD MDS codes. Finally, several examples are presented."}
{"id": "2601.16556", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16556", "abs": "https://arxiv.org/abs/2601.16556", "authors": ["Dengzhao Fang", "Jingtong Gao", "Yu Li", "Xiangyu Zhao", "Yi Chang"], "title": "PRISM: Purified Representation and Integrated Semantic Modeling for Generative Sequential Recommendation", "comment": null, "summary": "Generative Sequential Recommendation (GSR) has emerged as a promising paradigm, reframing recommendation as an autoregressive sequence generation task over discrete Semantic IDs (SIDs), typically derived via codebook-based quantization. Despite its great potential in unifying retrieval and ranking, existing GSR frameworks still face two critical limitations: (1) impure and unstable semantic tokenization, where quantization methods struggle with interaction noise and codebook collapse, resulting in SIDs with ambiguous discrimination; and (2) lossy and weakly structured generation, where reliance solely on coarse-grained discrete tokens inevitably introduces information loss and neglects items' hierarchical logic. To address these issues, we propose a novel generative recommendation framework, PRISM, with Purified Representation and Integrated Semantic Modeling. Specifically, to ensure high-quality tokenization, we design a Purified Semantic Quantizer that constructs a robust codebook via adaptive collaborative denoising and hierarchical semantic anchoring mechanisms. To compensate for information loss during quantization, we further propose an Integrated Semantic Recommender, which incorporates a dynamic semantic integration mechanism to integrate fine-grained semantics and enforces logical validity through a semantic structure alignment objective. PRISM consistently outperforms state-of-the-art baselines across four real-world datasets, demonstrating substantial performance gains, particularly in high-sparsity scenarios."}
{"id": "2601.16723", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2601.16723", "abs": "https://arxiv.org/abs/2601.16723", "authors": ["Qian Guo", "Yidan Hu", "Rui Zhang"], "title": "The Geometry of Coalition Power: Majorization, Lattices, and Displacement in Multiwinner Elections", "comment": "63 pages, 5 figures", "summary": "How much influence can a coordinated coalition exert in a multiwinner Top-$k$ election under a positional scoring rule? We study the maximum displacement problem: with coalition size $m$, how many of the current top-$k$ winners can be forced out? We show coalition power decomposes into two independent prefix-majorization constraints, capturing how much the coalition can (i) boost outsiders and (ii) suppress weak winners. For arbitrary scoring rules these prefix inequalities are tight, efficiently checkable necessary conditions (exact in the continuous relaxation).\n  For common-step arithmetic-progression (AP) score ladders, including Borda, truncated Borda, $k$-approval/$k$-veto, plurality, and multi-level rules such as $3$--$2$--$1$, we prove a Majorization--Lattice Theorem: feasible aggregate score vectors are exactly the integer points satisfying the Block--HLP prefix-sum capacity constraints plus a single global congruence condition modulo the step size $g$. For Borda ($g=1$) the congruence vanishes, yielding a pure prefix-majorization test.\n  This characterization yields an $O(k'\\log k')$ exact feasibility oracle for displacing $k'$ winners, and an $O(k(\\log k)^2\\log(mx))$ algorithm (via dual-envelope binary search) for computing the maximum achievable displacement $k^\\ast$. Experiments on Mallows profiles and PrefLib elections confirm exact cutoffs, diminishing returns, and substantial gains over baseline heuristics; for $g>1$ they also demonstrate the predicted congruence effect, where prefix-only tests produce false positives. The oracle scales to extreme instances, processing $10^9$ candidates in under 28 seconds (memory permitting)."}
{"id": "2601.16490", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.16490", "abs": "https://arxiv.org/abs/2601.16490", "authors": ["Adam A. E. Alflahi", "Mohammed A. Y. Mohammed", "Abdallah Alsammani"], "title": "A Scalable Transaction Management Framework for Consistent Document-Oriented NoSQL Databases", "comment": null, "summary": "NoSQL databases are widely used in modern applications due to their scalability and schema flexibility, yet they often rely on eventual consistency models that limit reliable transaction processing. This study proposes a four-stage transaction management framework for document-oriented NoSQL databases, with MongoDB as the reference platform. The framework combines transaction lifecycle management, operation classification, pre-execution conflict detection, and an adaptive locking strategy with timeout-based deadlock prevention. Formal correctness analysis shows that the proposed approach guarantees conflict serializability under defined conditions. An experimental evaluation using the Yahoo Cloud Serving Benchmark (YCSB) workloads A, B, and F, with concurrency levels ranging from 1 to 100 clients, demonstrates a reduction in transaction abort rates from 8.3% to 4.7%, the elimination of observed deadlocks, and a 34.2% decrease in latency variance. Throughput improvements ranging from 6.3% to 18.4% are observed under high concurrency, particularly for read-modify-write workloads. Distributed experiments on clusters of up to 9 nodes confirm scalability, achieving 15.2% higher throughput and 53% lower abort rates than baseline systems. Comparisons with MongoDB's native transactions, CockroachDB, and TiDB indicate that the proposed framework strikes a good balance between consistency guarantees and performance overhead. Sensitivity analysis identifies optimal parameter settings, including a lock timeout of 100 ms, an initial backoff of 10 ms, and a maximum backoff of 500 ms. These results show that carefully designed consistency mechanisms can significantly improve data integrity in NoSQL systems without undermining scalability."}
{"id": "2601.16923", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2601.16923", "abs": "https://arxiv.org/abs/2601.16923", "authors": ["Nick Fischer", "Marvin Künnemann", "Mirza Redzic"], "title": "Conditionally Tight Algorithms for Maximum k-Coverage and Partial k-Dominating Set via Arity-Reducing Hypercuts", "comment": null, "summary": "We revisit the classic Maximum $k$-Coverage problem: Determine the largest number $t$ of elements that can be covered by choosing $k$ sets from a given family $\\mathcal{F} = \\{S_1,\\dots, S_n\\}$ of a size-$u$ universe. A notable special case is Partial $k$-Dominating Set, where one chooses $k$ vertices in a graph to maximize the number of dominated vertices.\n  Extensive research has established strong hardness results for various aspects of Maximum $k$-Coverage, such as tight inapproximability results, $W[2]$-hardness, and a conditionally tight worst-case running time of $n^{k\\pm o(1)}$. In this paper we ask: (1) Can this time bound be improved for small $t$, at least for Partial $k$-Dominating Set, ideally to time~$t^{k\\pm O(1)}$? (2) More ambitiously, can we even determine the best-possible running time of Maximum $k$-Coverage with respect to the perhaps most natural parameters: the universe size $u$, the maximum set size $s$, and the maximum frequency $f$?\n  We successfully resolve both questions. (1) We give an algorithm that solves Partial $k$-Dominating Set in time $O(nt + t^{\\frac{2ω}{3} k+O(1)})$ if $ω\\ge 2.25$ and time $O(nt+ t^{\\frac{3}{2} k+O(1)})$ if $ω\\le 2.25$, where $ω\\le 2.372$ is the matrix multiplication exponent. From this we derive a time bound that is conditionally optimal, regardless of $ω$, based on the well-established $k$-clique and 3-uniform hyperclique hypotheses from fine-grained complexity. We also obtain matching upper and lower bounds for sparse graphs. To address (2) we design an algorithm for Maximum $k$-Coverage running in time\n  $$\n  \\min \\left\\{ (f\\cdot \\min\\{\\sqrt[3]{u}, \\sqrt{s}\\})^k + \\min\\{n,f\\cdot \\min\\{\\sqrt{u}, s\\}\\}^{kω/3}, n^k\\right\\}\n  \\cdot g(k)n^{\\pm O(1)}, $$ and, surprisingly, further show that this complicated time bound is also conditionally optimal."}
{"id": "2601.16455", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16455", "abs": "https://arxiv.org/abs/2601.16455", "authors": ["Qian Zhang", "Yufei Zhao", "Jiancheng An", "Zheng Dong", "Yong Liang Guan", "Ju Liu", "Chau Yuen"], "title": "Cramér-Rao Bound Minimization for Flexible Intelligent Metasurface-Enabled ISAC Systems", "comment": null, "summary": "Integrated sensing and communication (ISAC) have been widely recognized as a key enabler for future wireless networks, where the Cramér-Rao bound (CRB) plays a central role in quantifying sensing accuracy.In this paper, we present the first study on CRB minimization in flexible intelligent metasurface (FIM)-enabled ISAC systems.Specifically, we first derive an average CRB expression that explicitly depends on FIM surface shape and demonstrate that array reconfigurability can substantially reduce the CRB, thereby significantly enhancing sensing performance.Moreover, to tackle the challenging CRB minimization problem, we adopt average Fisher information maximization as a surrogate objective and use the Gauss-Hermite quadrature method to obtain an explicit approximation of the objective function.The resulting problem is then decoupled into three subproblem, i.e., beamforming optimization and transmit/receive FIM surface shape optimization.For beamforming optimization, we employ the Schur complement and penalty-based semi-definite relaxation (SDR) technique to solve it.Furthermore, we propose a fixed-point equation method and a projected gradient algorithm to optimize the surface shapes of the receive and transmit FIMs, respectively.Simulation results demonstrate that, compared to rigid arrays, surface shaping of both transmit and receive FIMs can significantly reduce the average sensing CRB while maintaining communication quality, and remains effective even in multi-target scenarios."}
{"id": "2601.16775", "categories": ["cs.IR", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.16775", "abs": "https://arxiv.org/abs/2601.16775", "authors": ["Tianang Deng", "Yu Deng", "Tianchen Gao", "Yonghong Hu", "Rui Pan"], "title": "LLM-powered Real-time Patent Citation Recommendation for Financial Technologies", "comment": null, "summary": "Rapid financial innovation has been accompanied by a sharp increase in patenting activity, making timely and comprehensive prior-art discovery more difficult. This problem is especially evident in financial technologies, where innovations develop quickly, patent collections grow continuously, and citation recommendation systems must be updated as new applications arrive. Existing patent retrieval and citation recommendation methods typically rely on static indexes or periodic retraining, which limits their ability to operate effectively in such dynamic settings. In this study, we propose a real-time patent citation recommendation framework designed for large and fast-changing financial patent corpora. Using a dataset of 428,843 financial patents granted by the China National Intellectual Property Administration (CNIPA) between 2000 and 2024, we build a three-stage recommendation pipeline. The pipeline uses large language model (LLM) embeddings to represent the semantic content of patent abstracts, applies efficient approximate nearest-neighbor search to construct a manageable candidate set, and ranks candidates by semantic similarity to produce top-k citation recommendations. In addition to improving recommendation accuracy, the proposed framework directly addresses the dynamic nature of patent systems. By using an incremental indexing strategy based on hierarchical navigable small-world (HNSW) graphs, newly issued patents can be added without rebuilding the entire index. A rolling day-by-day update experiment shows that incremental updating improves recall while substantially reducing computational cost compared with rebuild-based indexing. The proposed method also consistently outperforms traditional text-based baselines and alternative nearest-neighbor retrieval approaches."}
{"id": "2601.16835", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2601.16835", "abs": "https://arxiv.org/abs/2601.16835", "authors": ["Ke Ding", "Bo Li", "Ankang Sun"], "title": "Multi-Agent Non-Discriminatory Contracts", "comment": "22 pages, submitted to IJCAI 2026", "summary": "We study multi-agent contracts, in which a principal delegates a task to multiple agents and incentivizes them to exert effort. Prior research has mostly focused on maximizing the principal's utility, often resulting in highly disparate payments among agents. Such disparities among agents may be undesirable in practice, for example, in standardized public contracting or worker cooperatives where fairness concerns are essential. Motivated by these considerations, our objective is to quantify the tradeoff between maximizing the principal's utility and equalizing payments among agents, which we call the price of non-discrimination. Our first result is an almost tight bound on the price of non-discrimination, which scales logarithmically with the number of agents. This bound can be improved to a constant by allowing some relaxation of the non-discrimination requirement. We then provide a comprehensive characterization of the tradeoff between the level of non-discrimination and the loss in the optimal utility."}
{"id": "2601.16663", "categories": ["cs.DB", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.16663", "abs": "https://arxiv.org/abs/2601.16663", "authors": ["Zoltan Nagy", "Ryan Wisnesky", "Kevin Carlson", "Eswaran Subrahmanian", "Gioele Zardini"], "title": "A Categorical Approach to Semantic Interoperability across Building Lifecycle", "comment": null, "summary": "Buildings generate heterogeneous data across their lifecycle, yet integrating these data remains a critical unsolved challenge. Despite three decades of standardization efforts, over 40 metadata schemas now span the building lifecycle, with fragmentation accelerating rather than resolving. Current approaches rely on point-to-point mappings that scale quadratically with the number of schemas, or universal ontologies that become unwieldy monoliths. The fundamental gap is the absence of mathematical foundations for structure-preserving transformations across heterogeneous building data. Here we show that category theory provides these foundations, enabling systematic data integration with $O(n)$ specification complexity for $n$ ontologies. We formalize building ontologies as first-order theories and demonstrate two proof-of-concept implementations in Categorical Query Language (CQL): 1) generating BRICK models from IFC design data at commissioning, and 2) three-way integration of IFC, BRICK, and RealEstateCore where only two explicit mappings yield the third automatically through categorical composition. Our correct-by-construction approach treats property sets as first-class schema entities and provides automated bidirectional migrations, and enables cross-ontology queries. These results establish feasibility of categorical methods for building data integration and suggest a path toward an app ecosystem for buildings, where mathematical foundations enable reliable component integration analogous to smartphone platforms."}
{"id": "2601.16723", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2601.16723", "abs": "https://arxiv.org/abs/2601.16723", "authors": ["Qian Guo", "Yidan Hu", "Rui Zhang"], "title": "The Geometry of Coalition Power: Majorization, Lattices, and Displacement in Multiwinner Elections", "comment": "63 pages, 5 figures", "summary": "How much influence can a coordinated coalition exert in a multiwinner Top-$k$ election under a positional scoring rule? We study the maximum displacement problem: with coalition size $m$, how many of the current top-$k$ winners can be forced out? We show coalition power decomposes into two independent prefix-majorization constraints, capturing how much the coalition can (i) boost outsiders and (ii) suppress weak winners. For arbitrary scoring rules these prefix inequalities are tight, efficiently checkable necessary conditions (exact in the continuous relaxation).\n  For common-step arithmetic-progression (AP) score ladders, including Borda, truncated Borda, $k$-approval/$k$-veto, plurality, and multi-level rules such as $3$--$2$--$1$, we prove a Majorization--Lattice Theorem: feasible aggregate score vectors are exactly the integer points satisfying the Block--HLP prefix-sum capacity constraints plus a single global congruence condition modulo the step size $g$. For Borda ($g=1$) the congruence vanishes, yielding a pure prefix-majorization test.\n  This characterization yields an $O(k'\\log k')$ exact feasibility oracle for displacing $k'$ winners, and an $O(k(\\log k)^2\\log(mx))$ algorithm (via dual-envelope binary search) for computing the maximum achievable displacement $k^\\ast$. Experiments on Mallows profiles and PrefLib elections confirm exact cutoffs, diminishing returns, and substantial gains over baseline heuristics; for $g>1$ they also demonstrate the predicted congruence effect, where prefix-only tests produce false positives. The oracle scales to extreme instances, processing $10^9$ candidates in under 28 seconds (memory permitting)."}
{"id": "2601.16461", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16461", "abs": "https://arxiv.org/abs/2601.16461", "authors": ["Anuj Kumar Yadav", "Dan Song", "Yanina Shkel", "Ayfer Özgür"], "title": "Log-Likelihood Loss for Semantic Compression", "comment": "18 pages, 4 figures", "summary": "We study lossy source coding under a distortion measure defined by the negative log-likelihood induced by a prescribed conditional distribution $P_{X|U}$. This \\emph{log-likelihood distortion} models compression settings in which the reconstruction is a semantic representation from which the source can be probabilistically generated, rather than a pointwise approximation. We formulate the corresponding rate-distortion problem and characterize fundamental properties of the resulting rate-distortion function, including its connections to lossy compression under log-loss, classical rate-distortion problems with arbitrary distortion measures, and rate-distortion with perfect perception."}
{"id": "2601.16815", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.16815", "abs": "https://arxiv.org/abs/2601.16815", "authors": ["Shaoqing Wang", "Yingcai Ma", "Kairui Fu", "Ziyang Wang", "Dunxian Huang", "Yuliang Yan", "Jian Wu"], "title": "PI2I: A Personalized Item-Based Collaborative Filtering Retrieval Framework", "comment": "Published on WWW'26: In Proceedings of the ACM Web Conference 2026", "summary": "Efficiently selecting relevant content from vast candidate pools is a critical challenge in modern recommender systems. Traditional methods, such as item-to-item collaborative filtering (CF) and two-tower models, often fall short in capturing the complex user-item interactions due to uniform truncation strategies and overdue user-item crossing. To address these limitations, we propose Personalized Item-to-Item (PI2I), a novel two-stage retrieval framework that enhances the personalization capabilities of CF. In the first Indexer Building Stage (IBS), we optimize the retrieval pool by relaxing truncation thresholds to maximize Hit Rate, thereby temporarily retaining more items users might be interested in. In the second Personalized Retrieval Stage (PRS), we introduce an interactive scoring model to overcome the limitations of inner product calculations, allowing for richer modeling of intricate user-item interactions. Additionally, we construct negative samples based on the trigger-target (item-to-item) relationship, ensuring consistency between offline training and online inference. Offline experiments on large-scale real-world datasets demonstrate that PI2I outperforms traditional CF methods and rivals Two-Tower models. Deployed in the \"Guess You Like\" section on Taobao, PI2I achieved a 1.05% increase in online transaction rates. In addition, we have released a large-scale recommendation dataset collected from Taobao, containing 130 million real-world user interactions used in the experiments of this paper. The dataset is publicly available at https://huggingface.co/datasets/PI2I/PI2I, which could serve as a valuable benchmark for the research community."}
{"id": "2601.16495", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.16495", "abs": "https://arxiv.org/abs/2601.16495", "authors": ["Shivani Singh", "Amudheesan Nakkeeran", "Prem Singh", "Ekant Sharma", "Jyotsna Bapat"], "title": "Load Balanced ISAC Systems for URLLC Users", "comment": null, "summary": "This paper presents an energy-efficient downlink cell-free massive multiple-input multiple-output (CF-mMIMO) integrated sensing and communication (ISAC) network that serves ultra-reliable low-latency communication (URLLC) users while simultaneously detecting a target. We propose a load-balancing algorithm that minimizes the total network power consumption; including transmit power, fixed static power, and traffic-dependent fronthaul power at the access points (APs) without degrading system performance. To this end, we formulate a mixed-integer non-convex optimization problem and introduce an iterative joint power allocation and AP load balancing (JPALB) algorithm. The algorithm aims to reduce total power usage while meeting both the communication quality-of-service (QoS) requirements of URLLC users and the sensing QoS needed for target detection. Proposed JPALB algorithm for ISAC systems was simulated with maximum-ratio transmission (MRT) and regularized zero-forcing (RZF) precoders. Simulation results show approximately 33% reduction in power consumption, using JPALB algorithm compared to a baseline with no load balancing, without compromising communication and sensing QoS requirements."}
{"id": "2601.16858", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.16858", "abs": "https://arxiv.org/abs/2601.16858", "authors": ["Mahe Chen", "Xiaoxuan Wang", "Kaiwen Chen", "Nick Koudas"], "title": "Navigating the Shift: A Comparative Analysis of Web Search and Generative AI Response Generation", "comment": null, "summary": "The rise of generative AI as a primary information source presents a paradigm shift from traditional web search. This paper presents a large-scale empirical study quantifying the fundamental differences between the results returned by Google Search and leading generative AI services. We analyze multiple dimensions, demonstrating that AI-generated answers and web search results diverge significantly in their consulted source domains, the typology of these domains (e.g., earned media vs. owned, social), query intent and the freshness of the information provided. We then investigate the role of LLM pre-training as a key factor shaping these differences, analyzing how this intrinsic knowledge base interacts with and influences real-time web search when enabled. Our findings reveal the distinct mechanics of these two information ecosystems, leading to critical observations on the emergent field of Answer Engine Optimization (AEO) and its contrast with traditional Search Engine Optimization (SEO)."}
{"id": "2601.16518", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16518", "abs": "https://arxiv.org/abs/2601.16518", "authors": ["Zimu Li", "Bingyi Liu", "Lei Zhao", "Qian Zhang", "Yang Liu", "Jun Liu", "Ke Ke", "Huating Kong", "Xiaolei Zuo", "Chunhai Fan", "Fei Wang"], "title": "Noise-immune and AI-enhanced DNA storage via adaptive partition mapping of digital data", "comment": null, "summary": "Encoding digital information into DNA sequences offers an attractive potential solution for storing rapidly growing data under the information age and the rise of artificial intelligence. However, practical implementations of DNA storage are constrained by errors introduced during synthesis, preservation, and sequencing processes, and traditional error-correcting codes remain vulnerable to noise levels that exceed predefined thresholds. Here, we developed a Partitioning-mapping with Jump-rotating (PJ) encoding scheme, which exhibits exceptional noise resilience. PJ removes cross-strand information dependencies so that strand loss manifests as localized gaps rather than catastrophic file failure. It prioritizes file decodability under arbitrary noise conditions and leverages AI-based inference to enable controllable recovery of digital information. For the intra-strand encoding, we develop a jump-rotating strategy that relaxes sequence constraints relative to conventional rotating codes and provides tunable information density via an adjustable jump length. Based on this encoding architecture, the original file information can always be decoded and recovered under any strand loss ratio, with fidelity degrading smoothly as damage increases. We demonstrate that original files can be effectively recovered even with 10% strand loss, and machine learning datasets stored under these conditions retain their classification performance. Experiments further confirmed that PJ successfully decodes image files after extreme environmental disturbance using accelerated aging and high-intensity X-ray irradiation. By eliminating reliance on prior error probabilities, PJ establishes a general framework for robust, archival DNA storage capable of withstanding the rigorous conditions of real-world preservation."}
{"id": "2601.16872", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.16872", "abs": "https://arxiv.org/abs/2601.16872", "authors": ["Yuxin Liao", "Le Wu", "Min Hou", "Yu Wang", "Han Wu", "Meng Wang"], "title": "From Atom to Community: Structured and Evolving Agent Memory for User Behavior Modeling", "comment": null, "summary": "User behavior modeling lies at the heart of personalized applications like recommender systems. With LLM-based agents, user preference representation has evolved from latent embeddings to semantic memory. While existing memory mechanisms show promise in textual dialogues, modeling non-textual behaviors remains challenging, as preferences must be inferred from implicit signals like clicks without ground truth supervision. Current approaches rely on a single unstructured summary, updated through simple overwriting. However, this is suboptimal: users exhibit multi-faceted interests that get conflated, preferences evolve yet naive overwriting causes forgetting, and sparse individual interactions necessitate collaborative signals. We present STEAM (\\textit{\\textbf{ST}ructured and \\textbf{E}volving \\textbf{A}gent \\textbf{M}emory}), a novel framework that reimagines how agent memory is organized and updated. STEAM decomposes preferences into atomic memory units, each capturing a distinct interest dimension with explicit links to observed behaviors. To exploit collaborative patterns, STEAM organizes similar memories across users into communities and generates prototype memories for signal propagation. The framework further incorporates adaptive evolution mechanisms, including consolidation for refining memories and formation for capturing emerging interests. Experiments on three real-world datasets demonstrate that STEAM substantially outperforms state-of-the-art baselines in recommendation accuracy, simulation fidelity, and diversity."}
{"id": "2601.16594", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16594", "abs": "https://arxiv.org/abs/2601.16594", "authors": ["Neri Merhav"], "title": "Generalized Forms of the Kraft Inequality for Finite-State Encoders", "comment": "25 pages, 2 figures, to be submitted for publication", "summary": "We derive a few extended versions of the Kraft inequality for information lossless finite-state encoders. The main basic contribution is in defining a notion of a Kraft matrix and in establishing the fact that a necessary condition for information losslessness of a finite-state encoder is that none of the eigenvalues of this matrix have modulus larger than unity, or equivalently, the generalized Kraft inequality asserts that the spectral radius of the Kraft matrix cannot exceed one. For the important special case where the FS encoder is irreducible, we derive several equivalent forms of this inequality, which are based on well known formulas for spectral radius. It also turns out that in the irreducible case, Kraft sums are bounded by a constant, independent of the block length, and thus cannot grow even in any subexponential rate. Finally, two extensions are outlined - one concerns the case of side information available to both encoder and decoder, and the other is for lossy compression."}
{"id": "2601.16882", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16882", "abs": "https://arxiv.org/abs/2601.16882", "authors": ["Maria Stratigi", "Nikos Bikakis"], "title": "Explaining Group Recommendations via Counterfactuals", "comment": null, "summary": "Group recommender systems help users make collective choices but often lack transparency, leaving group members uncertain about why items are suggested. Existing explanation methods focus on individuals, offering limited support for groups where multiple preferences interact. In this paper, we propose a framework for group counterfactual explanations, which reveal how removing specific past interactions would change a group recommendation. We formalize this concept, introduce utility and fairness measures tailored to groups, and design heuristic algorithms, such as Pareto-based filtering and grow-and-prune strategies, for efficient explanation discovery. Experiments on MovieLens and Amazon datasets show clear trade-offs: low-cost methods produce larger, less fair explanations, while other approaches yield concise and balanced results at higher cost. Furthermore, the Pareto-filtering heuristic demonstrates significant efficiency improvements in sparse settings."}
{"id": "2601.16599", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16599", "abs": "https://arxiv.org/abs/2601.16599", "authors": ["Huaning Liu", "Zilong Liu"], "title": "An Explicit Upper Bound of Generalized Quadratic Gauss Sums and Its Applications for Asymptotically Optimal Aperiodic Polyphase Sequence Design", "comment": "This work has been submitted to IEEE Transactions on Information Theory on 22 January 2026", "summary": "This work is motivated by the long-standing open problem of designing asymptotically order-optimal aperiodic polyphase sequence sets with respect to the celebrated Welch bound. Attempts were made by Mow over 30 years ago, but a comprehensive understanding to this problem is lacking. Our first key contribution is an explicit upper bound of generalized quadratic Gauss sums which is obtained by recursively applying Paris' asymptotic expansion and then bounding it by leveraging the fast convergence property of the Fibonacci zeta function. Building upon this major finding, our second key contribution includes four systematic constructions of order-optimal sequence sets with low aperiodic correlation and/or ambiguity properties via carefully selected Chu sequences and Alltop sequences. For the first time in the literature, we reveal that the full Alltop sequence set is asymptotically optimal for its low aperiodic correlation sidelobes. Besides, we introduce a novel subset of Alltop sequences possessing both order-optimal aperiodic correlation and ambiguity properties for the entire time-shift window."}
{"id": "2601.16614", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16614", "abs": "https://arxiv.org/abs/2601.16614", "authors": ["Søren Riis"], "title": "Term Coding: An Entropic Framework for Extremal Combinatorics and the Guessing--Number Sandwich Theorem", "comment": null, "summary": "Term Coding asks: given a finite system of term identities $Γ$ in $v$ variables, how large can its solution set be on an $n$--element alphabet, when we are free to choose the interpretations of the function symbols? This turns familiar existence problems for quasigroups, designs, and related objects into quantitative extremal questions.\n  We prove a guessing-number sandwich theorem that connects term coding to graph guessing numbers (graph entropy). After explicit normalisation and diversification reductions, every instance yields a canonical directed dependency structure with guessing number $α$ such that the maximum code size satisfies $\\log_n \\Sn(Γ)=α+o(1)$ (equivalently, $\\Sn(Γ)=n^{α+o(1)}$), and $α$ can be bounded or computed using entropy and polymatroid methods.\n  We illustrate the framework with examples from extremal combinatorics (Steiner-type identities, self-orthogonal Latin squares) and from information-flow / network-coding style constraints (including a five-cycle instance with fractional exponent and small storage/relay maps)."}
{"id": "2601.16624", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16624", "abs": "https://arxiv.org/abs/2601.16624", "authors": ["Aimin Li", "Yiğit İnce", "Elif Uysal"], "title": "Taming the Heavy Tail: Age-Optimal Preemption", "comment": "11 pages", "summary": "This paper studies a continuous-time joint sampling-and-preemption problem, incorporating sampling and preemption penalties under general service-time distributions. We formulate the system as an impulse-controlled piecewise-deterministic Markov process (PDMP) and derive coupled integral average-cost optimality equations via the dynamic programming principle, thereby avoiding the smoothness assumptions typically required for an average-cost Hamilton-Jacobi-Bellman quasi-variational inequality (HJB-QVI) characterization. A key invariance in the busy phase collapses the dynamics onto a one-dimensional busy-start boundary, reducing preemption control to an optimal stopping problem. Building on this structure, we develop an efficient policy iteration algorithm with heavy-tail acceleration, employing a hybrid (uniform/log-spaced) action grid and a far-field linear closure. Simulations under Pareto and log-normal service times demonstrate substantial improvements over AoI-optimal non-preemptive sampling and zero-wait baselines, achieving up to a 30x reduction in average cost in heavy-tailed regimes. Finally, simulations uncover a counterintuitive insight: under preemption, delay variance, despite typically being a liability, can become a strategic advantage for information freshness."}
{"id": "2601.16628", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16628", "abs": "https://arxiv.org/abs/2601.16628", "authors": ["Andrea Di Giusto", "Alberto Ravagnani", "Emina Soljanin"], "title": "The Oval Strikes Back", "comment": null, "summary": "We investigate the applications of ovals in projective planes to distributed storage, with a focus on the Service Rate Region problem. Leveraging the incidence relations between lines and ovals, we describe a class of non-systematic MDS matrices with a large number of small and disjoint recovery sets. For certain parameter choices, the service-rate region of these matrices contains the region of a systematic generator matrix for the same code, yielding better service performance. We further apply our construction to analyze the PIR properties of the considered MDS matrices and present a one-step majority-logic decoding algorithm with strong error-correcting capability. These results highlight how ovals, a classical object in finite geometry, re-emerge as a useful tool in modern coding theory."}
{"id": "2601.16680", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16680", "abs": "https://arxiv.org/abs/2601.16680", "authors": ["Zhenduo Wen", "Amin Gohari"], "title": "Stable Source Coding", "comment": null, "summary": "A source encoder is stable if a small change in the source sequence (e.g., changing a few symbols) results in a small (or bounded) change in the output codeword. By this definition, the common technique of random binning is unstable; because the mapping is random, two nearly identical source sequences can be assigned to completely unrelated bin indices. We study compression rates of stable lossless source codes. Using combinatorial arguments, we derive information-theoretic limits on the achievable rate as a function of the stability parameters."}
{"id": "2601.16799", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16799", "abs": "https://arxiv.org/abs/2601.16799", "authors": ["Chunsong Sun", "Lin Zhou"], "title": "Adaptive Beam Alignment using Noisy Twenty Questions Estimation with Trained Questioner", "comment": null, "summary": "The 6G communication systems use mmWave and MIMO technologies to achieve wide bandwidth and high throughout, leading to indispensable need for beam alignment to overcome severe signal attenuation. Traditional sector-search-based beam alignment algorithms rely on sequential sampling to identify the best sector, resulting in a significant latency burden on 6G communication systems. Recently proposed adaptive beam alignment algorithms based on the active learning framework address the problem, aiming to identify the optimal sector with the fewest possible samples under an identical sector partition. Nevertheless, these algorithms either lack feasibility (Chiu, Ronquillo and Javidi, JSAC 2019) due to ideal assumptions or lack interpretability (Sohrabi, Chen and Yu, JSAC 2021) due to the use of end-to-end black-box neural networks. To avoid ideal assumptions and maintain interpretability, we address all above problems by proposing an adaptive beam alignment algorithm using the framework of noisy twenty questions estimation with a trained questioner. Specifically, we use two methods for training the questioner to eliminate reliance on ideal assumptions. The first method maps queries of twenty questions estimation to beamforming vectors via weighted summation of steering vectors, as an initial attempt to address the feasibility problem encountered in prior pioneering study by Chiu, Ronquillo and Javidi (JSAC 2019). The second method uses multi-layer fully connected neural networks to achieve improved performance while only employing them to train the questioner, which can effectively mitigate the interpretability issues in prior study by Sohrabi, Chen and Yu (JSAC 2021). Furthermore, we provide numerical simulations to illustrate the effectiveness of our proposed adaptive beam alignment algorithms and demonstrate that our algorithms outperform all benchmark algorithms."}
{"id": "2601.16825", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16825", "abs": "https://arxiv.org/abs/2601.16825", "authors": ["Chunsong Sun", "Lin Zhou"], "title": "Privacy-Resolution Tradeoff for Adaptive Noisy Twenty Questions Estimation", "comment": null, "summary": "We revisit noisy twenty questions estimation and study the privacy-resolution tradeoff for adaptive query procedures. Specifically, in twenty questions estimation, there are two players: an oracle and a questioner. The questioner aims to estimate target variables by posing queries to the oracle that knows the variables and using noisy responses to form reliable estimates. Typically, there are adaptive and non-adaptive query procedures. In adaptive querying, one designs the current query using previous queries and their noisy responses while in non-adaptive querying, all queries are posed simultaneously. Generally speaking, adaptive query procedures yield better performance. However, adaptive querying leads to privacy concerns, which were first studied by Tsitsiklis, Xu and Xu (COLT 2018) and by Xu, Xu and Yang (AISTATS 2021) for the noiseless case, where the oracle always provides correct answers to queries. In this paper, we generalize the above results to the more practical noisy case, by proposing a two-stage private query procedure, analyzing its non-asymptotic and second-order asymptotic achievable performance and discussing the impact of privacy concerns. Furthermore, when specialized to the noiseless case, our private query procedure achieves better performance than above-mentioned query procedures (COLT 2018, AISTATS 2021)."}
{"id": "2601.16845", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16845", "abs": "https://arxiv.org/abs/2601.16845", "authors": ["Theshani Nuradha", "Ian George", "Christoph Hirche"], "title": "Information Contraction under $(\\varepsilon,δ)$-Differentially Private Mechanisms", "comment": "6 pages, 3 figures; classical results from the paper arXiv:2512.16778 [quant-ph] that studies related quantum results", "summary": "The distinguishability quantified by information measures after being processed by a private mechanism has been a useful tool in studying various statistical and operational tasks while ensuring privacy. To this end, standard data-processing inequalities and strong data-processing inequalities (SDPI) are employed. Most of the previously known and even tight characterizations of contraction of information measures, including total variation distance, hockey-stick divergences, and $f$-divergences, are applicable for $(\\varepsilon,0)$-local differential private (LDP) mechanisms. In this work, we derive both linear and non-linear strong data-processing inequalities for hockey-stick divergence and $f$-divergences that are valid for all $(\\varepsilon,δ)$-LDP mechanisms even when $δ\\neq 0$. Our results either generalize or improve the previously known bounds on the contraction of these distinguishability measures."}
{"id": "2601.16857", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2601.16857", "abs": "https://arxiv.org/abs/2601.16857", "authors": ["Fangwei Ye", "Zonghong Liu", "Parimal Parag", "Salim El Rouayheb"], "title": "Perfect Privacy and Strong Stationary Times for Markovian Sources", "comment": "11 pages", "summary": "We consider the problem of sharing correlated data under a perfect information-theoretic privacy constraint. We focus on redaction (erasure) mechanisms, in which data are either withheld or released unchanged, and measure utility by the average cardinality of the released set, equivalently, the expected Hamming distortion. Assuming the data are generated by a finite time-homogeneous Markov chain, we study the protection of the initial state while maximizing the amount of shared data. We establish a connection between perfect privacy and window-based redaction schemes, showing that erasing data up to a strong stationary time preserves privacy under suitable conditions. We further study an optimal sequential redaction mechanism and prove that it admits an equivalent window interpretation. Interestingly, we show that both mechanisms achieve the optimal distortion while redacting only a constant average number of data points, independent of the data length~$N$."}
