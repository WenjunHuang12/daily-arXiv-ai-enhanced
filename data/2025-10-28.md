<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DS](#cs.DS) [Total: 11]
- [cs.IT](#cs.IT) [Total: 13]
- [cs.IR](#cs.IR) [Total: 30]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Conditional Recall](https://arxiv.org/abs/2510.21904)
*Christoph Schlegel,Xinyuan Sun*

Main category: cs.GT

TL;DR: 该论文探讨了允许代理人承诺遗忘信息的技术（如可信执行环境TEEs）在博弈论中的影响，并讨论了多个应用场景。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于一种能够选择性遗忘记忆并保留遗忘提醒的技术（如X药物），这种技术对个人决策和社会互动产生深远影响，特别是在法律、道德和战略承诺方面。

Method: 采用博弈论分析方法，研究代理人在能够承诺遗忘信息的情况下的策略行为和均衡结果，探讨这种技术如何改变信息不对称和承诺机制。

Result: 分析表明，遗忘承诺技术可以作为一种有效的承诺机制，帮助代理人解决时间不一致性问题，在多个应用场景中（如法律、合同、战略互动）产生新的均衡结果。

Conclusion: 能够承诺遗忘信息的技术为博弈论提供了新的分析维度，这种机制可以增强承诺的可信度，并在各种战略互动中产生重要的实际应用价值。

Abstract: In the neon-lit nights of 2026, Johnson \& Johnson unveiled X. A pill, not
larger than a snowflake, that promised a tempest of change. This miraculous
drug didn't just allow people to cherry-pick memories to erase from their
minds, it could also leave a reminder of this erasure in the minds of those who
ingested it.
  Amidst the iconic red-bricked walls of Harvard Law, you, with books in one
hand and dreams in the other, are on a mission. You are not just another
student; you carry the hope of revolutionizing the archaic chambers of the
legal world. Each night, as you pore over the tomes of law, you wonder what
greatness society can achieve.
  On a cold evening, your phone buzzes. It's Dex, your old college friend
turned underground dealer. His message is simple: ``Got X. Special price for
you.'' The temptation swirls around you. Would you trade the lessons of the
past for a clearer, yet incomplete future? The decision rests in your hands.
  We explore the game theoretic implications of a technology (such as TEEs)
that allows agents to commit to forget information and discuss several
applications.

</details>


### [2] [Rational Adversaries and the Maintenance of Fragility: A Game-Theoretic Theory of Rational Stagnation](https://arxiv.org/abs/2510.22232)
*Daisuke Hirota*

Main category: cs.GT

TL;DR: 论文解释了合作系统中持续存在的次优稳定状态（理性停滞），这是由遵循潜在损失原则的理性对手维持的均衡。通过囚徒困境模型和动态分析，识别了三种战略机制：立即破坏、理性停滞和干预放弃。


<details>
  <summary>Details</summary>
Motivation: 解释为什么合作系统经常停留在次优但稳定的状态，揭示这种"理性停滞"现象背后的机制，特别是理性对手如何通过维持脆弱性来故意保持这种状态。

Method: 从囚徒困境模型出发，通过效用变换u_i' = a*u_i + b*u_j和相互认知比率w = b/a分析合作脆弱带；扩展到具有随机合作收益R_t和干预成本(C_c, C_m)的动态模型，进行贝尔曼式分析；在附录中进一步推广到参考依赖的非线性效用形式。

Result: 识别了三个战略机制：立即破坏、理性停滞和干预放弃；证明了在[w_min, w_max]的脆弱合作带中，(C,C)和(D,D)都是均衡；证明了参考依赖非线性效用形式在参考点变化下的稳定性。

Conclusion: 理性停滞是由遵循潜在损失原则的理性对手维持的均衡状态；该框架具有鲁棒性，可应用于社交媒体算法和政治信任等场景，解释对手如何故意保持脆弱性。

Abstract: Cooperative systems often remain in persistently suboptimal yet stable
states. This paper explains such "rational stagnation" as an equilibrium
sustained by a rational adversary whose utility follows the principle of
potential loss, $u_{D} = U_{ideal} - U_{actual}$. Starting from the Prisoner's
Dilemma, we show that the transformation $u_{i}' = a\,u_{i} + b\,u_{j}$ and the
ratio of mutual recognition $w = b/a$ generate a fragile cooperation band
$[w_{\min},\,w_{\max}]$ where both (C,C) and (D,D) are equilibria. Extending to
a dynamic model with stochastic cooperative payoffs $R_{t}$ and intervention
costs $(C_{c},\,C_{m})$, a Bellman-style analysis yields three strategic
regimes: immediate destruction, rational stagnation, and intervention
abandonment. The appendix further generalizes the utility to a
reference-dependent nonlinear form and proves its stability under reference
shifts, ensuring robustness of the framework. Applications to social-media
algorithms and political trust illustrate how adversarial rationality can
deliberately preserve fragility.

</details>


### [3] [Learning Local Stackelberg Equilibria from Repeated Interactions with a Learning Agent](https://arxiv.org/abs/2510.22471)
*Nivasini Ananthakrishnan,Yuval Dagan,Kunhe Yang*

Main category: cs.GT

TL;DR: 本文研究重复博弈中主理人如何最大化效用，针对采用均值学习算法的智能体，提出了在平滑分析框架下计算局部Stackelberg均衡的多项式时间近似方案(PTAS)。


<details>
  <summary>Details</summary>
Motivation: 研究主理人在与学习智能体的重复交互中如何最大化效用，解决全局Stackelberg均衡计算复杂度高的问题。

Method: 提出一种算法，在平滑分析框架下计算epsilon-近似的局部Stackelberg均衡，运行时间在智能体动作空间大小上是多项式的，但在1/epsilon上是指数的。

Result: 算法构成了PTAS，能够在多项式时间内找到局部Stackelberg均衡，且证明了1/epsilon的指数依赖性是不可避免的。

Conclusion: 通过转向局部Stackelberg均衡计算，克服了全局均衡计算复杂度过高的问题，为重复博弈中的策略设计提供了可行的计算方案。

Abstract: Motivated by the question of how a principal can maximize its utility in
repeated interactions with a learning agent, we study repeated games between an
principal and an agent employing a mean-based learning algorithm. Prior work
has shown that computing or even approximating the global Stackelberg value in
similar settings can require an exponential number of rounds in the size of the
agent's action space, making it computationally intractable. In contrast, we
shift focus to the computation of local Stackelberg equilibria and introduce an
algorithm that, within the smoothed analysis framework, constitutes a
Polynomial Time Approximation Scheme (PTAS) for finding an epsilon-approximate
local Stackelberg equilibrium. Notably, the algorithm's runtime is polynomial
in the size of the agent's action space yet exponential in (1/epsilon) - a
dependency we prove to be unavoidable.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [4] [Enabling American Sign Language Communication Under Low Data Rates](https://arxiv.org/abs/2510.23056)
*Panneer Selvam Santhalingam,Swann Thantsin,Ahmad Kamari,Parth Pathak,Kenneth De Haan*

Main category: cs.MM

TL;DR: VC4ASL是一个通过音频通道传输手语通信的系统，可在网络带宽不足时替代视频传输，使美国手语用户能够在视频会议应用中保持有效沟通。


<details>
  <summary>Details</summary>
Motivation: 视频会议应用依赖高速网络连接，当网络不佳时用户只能使用音频通信，这对依赖手势、身体动作和面部表情的美国手语用户造成严重障碍。

Method: 通过音频编码和传输人体姿态信息，利用接收端的错误检测和校正机制，利用人体姿态数据的结构约束来重建手语内容。

Result: 在模拟网络降级环境下的实验表明，VC4ASL能够在视频传输受损的低带宽场景中有效促进可理解的手语通信。

Conclusion: VC4ASL系统能够无缝集成到现有视频会议平台，无需任何修改，为手语用户在低带宽环境下提供了可行的通信解决方案。

Abstract: In recent years, video conferencing applications have become increasingly
prevalent, relying heavily on high-speed internet connectivity. When such
connectivity is lacking, users often default to audio-only communication, a
mode that significantly disadvantages American Sign Language (ASL) users, whose
communication relies on hand gestures, body movement, and facial expressions.
In this work, we introduce VC4ASL, a system designed to enable ASL
communication over the audio channel of existing video conferencing
applications, even in the absence of reliable video. VC4ASL integrates
seamlessly with current platforms without requiring any modifications. Our
approach establishes a communication channel through audio by encoding and
transmitting human pose information, which is then rendered to reconstruct
signed content. We propose novel receive-side error detection and correction
mechanisms that exploit the inherent structural constraints of human pose data.
To evaluate the system, we simulate network-degraded environments, generate
pose-based ASL video sequences, and conduct user studies to assess
comprehension among ASL users. Experimental results demonstrate that VC4ASL
effectively facilitates intelligible ASL communication over audio in
low-bandwidth scenarios where video transmission is impaired.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [5] [Determining Window Sizes using Species Estimation for Accurate Process Mining over Streams](https://arxiv.org/abs/2510.22314)
*Christian Imenkamp,Martin Kabierski,Hendrik Reiter,Matthias Weidlich,Wilhelm Hasselbring,Agnes Koschmider*

Main category: cs.DB

TL;DR: 提出了一种动态调整窗口大小的流式过程挖掘方法，基于生物多样性研究中的物种估计器来评估样本代表性，提高了对概念漂移的鲁棒性和分析准确性。


<details>
  <summary>Details</summary>
Motivation: 传统流式过程挖掘采用固定窗口大小，无法适应过程动态变化和概念漂移，导致分析结果存在偏差。需要动态调整窗口大小来提高样本代表性。

Method: 采用生物多样性研究中开发的物种估计器来动态确定合适的窗口大小，基于样本代表性评估来调整窗口尺寸。

Result: 在真实世界数据集上的评估显示，相比采用静态窗口大小的现有方法，在准确性和对概念漂移的鲁棒性方面都有提升。

Conclusion: 动态窗口大小调整方法能够有效应对流式过程挖掘中的概念漂移问题，提高分析结果的准确性和可靠性。

Abstract: Streaming process mining deals with the real-time analysis of event streams.
A common approach for it is to adopt windowing mechanisms that select event
data from a stream for subsequent analysis. However, the size of these windows
denotes a crucial parameter, as it influences the representativeness of the
window content and, by extension, of the analysis results. Given that process
dynamics are subject to changes and potential concept drift, a static, fixed
window size leads to inaccurate representations that introduce bias in the
analysis. In this work, we present a novel approach for streaming process
mining that addresses these limitations by adjusting window sizes.
Specifically, we dynamically determine suitable window sizes based on
estimators for the representativeness of samples as developed for species
estimation in biodiversity research. Evaluation results on real-world data sets
show improvements over existing approaches that adopt static window sizes in
terms of accuracy and robustness to concept drifts.

</details>


### [6] [Dynamically Detect and Fix Hardness for Efficient Approximate Nearest Neighbor Search](https://arxiv.org/abs/2510.22316)
*Zhiyuan Hua,Qiji Mo,Zebin Yao,Lixiao Cui,Xiaoguang Liu,Gang Wang,Zijing Wei,Xinyu Liu,Tianxiao Tang,Shaozhi Liu,Lin Qu*

Main category: cs.DB

TL;DR: 本文提出了一种改进图结构ANNS算法的方法，通过引入Escape Hardness指标评估图结构质量，并分两个阶段动态修复缺陷区域，显著提升了OOD查询的性能和构建速度。


<details>
  <summary>Details</summary>
Motivation: 现有图结构ANNS方法在OOD场景下准确率显著下降，RoarGraph方法虽然尝试解决但存在理论支持不足、构建时间长、无法适应查询负载变化等问题。

Method: 提出Escape Hardness指标评估图结构质量，分两阶段修复缺陷：RFix增强关键节点的可导航性，NGFix改善查询密集区域的图连通性。

Result: 在真实数据集上优于现有方法，OOD查询速度比RoarGraph快2.25倍，比HNSW快6.88倍，索引构建速度比RoarGraph快2.35-9.02倍。

Conclusion: 该方法有效解决了图结构ANNS在OOD场景下的性能问题，提供了更高效和适应性更强的解决方案。

Abstract: Approximate Nearest Neighbor Search (ANNS) has become a fundamental component
in many real-world applications. Among various ANNS algorithms, graph-based
methods are state-of-the-art. However, ANNS often suffers from a significant
drop in accuracy for certain queries, especially in Out-of-Distribution (OOD)
scenarios. To address this issue, a recent approach named RoarGraph constructs
a bipartite graph between the base data and historical queries to bridge the
gap between two different distributions. However, it suffers from some
limitations: (1) Building a bipartite graph between two distributions lacks
theoretical support, resulting in the query distribution not being effectively
utilized by the graph index. (2) Requires a sufficient number of historical
queries before graph construction and suffers from high construction times. (3)
When the query workload changes, it requires reconstruction to maintain high
search accuracy.
  In this paper, we first propose Escape Hardness, a metric to evaluate the
quality of the graph structure around the query. Then we divide the graph
search into two stages and dynamically identify and fix defective graph regions
in each stage based on Escape Hardness. (1) From the entry point to the
vicinity of the query. We propose Reachability Fixing (RFix), which enhances
the navigability of some key nodes. (2) Searching within the vicinity of the
query. We propose Neighboring Graph Defects Fixing (NGFix) to improve graph
connectivity in regions where queries are densely distributed. The results of
extensive experiments show that our method outperforms other state-of-the-art
methods on real-world datasets, achieving up to 2.25x faster search speed for
OOD queries at 99% recall compared with RoarGraph and 6.88x faster speed
compared with HNSW. It also accelerates index construction by 2.35-9.02x
compared to RoarGraph.

</details>


### [7] [A Survey of Data Agents: Emerging Paradigm or Overstated Hype?](https://arxiv.org/abs/2510.23587)
*Yizhang Zhu,Liangwei Wang,Chenyu Yang,Xiaotian Lin,Boyan Li,Wei Zhou,Xinyu Liu,Zhangyang Peng,Tianqi Luo,Yu Li,Chengliang Chai,Chong Chen,Shimin Di,Ju Fan,Ji Sun,Nan Tang,Fugee Tsung,Jiannan Wang,Chenglin Wu,Yanwei Xu,Shaolei Zhang,Yong Zhang,Xuanhe Zhou,Guoliang Li,Yuyu Luo*

Main category: cs.DB

TL;DR: 该论文提出了首个数据代理的层次化分类系统，借鉴自动驾驶标准，将数据代理分为6个级别（L0-L5），从手动操作到完全自主，旨在解决当前术语混乱问题。


<details>
  <summary>Details</summary>
Motivation: 当前'数据代理'术语存在歧义和混淆，从简单查询响应到复杂自主架构都被称为数据代理，这导致了用户期望不匹配、责任归属问题和行业发展障碍。

Method: 借鉴SAE J3016自动驾驶标准，建立了六层次分类法，按自主性递增排列：从手动操作（L0）到完全自主的生成式数据代理（L5），并以此框架系统回顾现有研究。

Result: 构建了清晰的数据代理能力边界和责任分配框架，识别了关键的技术演进阶段，特别是从L2（程序执行）到L3（自主编排）的转型。

Conclusion: 提出了前瞻性路线图，展望了主动式、生成式数据代理的发展前景，为数据代理领域的标准化和进一步发展提供了理论基础。

Abstract: The rapid advancement of large language models (LLMs) has spurred the
emergence of data agents--autonomous systems designed to orchestrate Data + AI
ecosystems for tackling complex data-related tasks. However, the term "data
agent" currently suffers from terminological ambiguity and inconsistent
adoption, conflating simple query responders with sophisticated autonomous
architectures. This terminological ambiguity fosters mismatched user
expectations, accountability challenges, and barriers to industry growth.
Inspired by the SAE J3016 standard for driving automation, this survey
introduces the first systematic hierarchical taxonomy for data agents,
comprising six levels that delineate and trace progressive shifts in autonomy,
from manual operations (L0) to a vision of generative, fully autonomous data
agents (L5), thereby clarifying capability boundaries and responsibility
allocation. Through this lens, we offer a structured review of existing
research arranged by increasing autonomy, encompassing specialized data agents
for data management, preparation, and analysis, alongside emerging efforts
toward versatile, comprehensive systems with enhanced autonomy. We further
analyze critical evolutionary leaps and technical gaps for advancing data
agents, especially the ongoing L2-to-L3 transition, where data agents evolve
from procedural execution to autonomous orchestration. Finally, we conclude
with a forward-looking roadmap, envisioning the advent of proactive, generative
data agents.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [8] [An Optimal Density Bound for Discretized Point Patrolling](https://arxiv.org/abs/2510.22060)
*Ahan Mishra*

Main category: cs.DS

TL;DR: 本文解决了离散点巡逻问题中的密度界限猜想，证明了密度至少为∑1/(2^i+1)≈1.264的实例是可调度的，这是最优结果。同时为竹子园修剪问题提供了9/7近似算法。


<details>
  <summary>Details</summary>
Motivation: 研究覆盖设置下的调度问题，对应打包设置中的风车问题。最近打包设置的5/6密度界限猜想已被证实，而覆盖设置的相应猜想尚未解决。

Method: 通过数学证明方法解决离散点巡逻问题的密度界限猜想，并为竹子园修剪问题设计近似算法。

Result: 证明了离散点巡逻问题中密度至少为1.264的实例可调度，这是最优密度界限。同时为竹子园修剪问题提供了9/7近似算法，优于现有的4/3近似。

Conclusion: 成功解决了覆盖设置下的密度界限猜想，证明了最优密度界限1.264，并为相关优化问题提供了改进的近似算法。

Abstract: The pinwheel problem is a real-time scheduling problem that asks, given $n$
tasks with periods $a_i \in \mathbb{N}$, whether it is possible to infinitely
schedule the tasks, one per time unit, such that every task $i$ is scheduled in
every interval of $a_i$ units. We study a corresponding version of this packing
problem in the covering setting, stylized as the discretized point patrolling
problem in the literature. Specifically, given $n$ tasks with periods $a_i$,
the problem asks whether it is possible to assign each day to a task such that
every task $i$ is scheduled at \textit{most} once every $a_i$ days. The density
of an instance in either case is defined as the sum of the inverses of task
periods. Recently, the long-standing $5/6$ density bound conjecture in the
packing setting was resolved affirmatively. The resolution means any instance
with density at least $5/6$ is schedulable. A corresponding conjecture was made
in the covering setting and renewed multiple times in more recent work. We
resolve this conjecture affirmatively by proving that every discretized point
patrolling instance with density at least $\sum_{i = 0}^{\infty} 1/(2^i + 1)
\approx 1.264$ is schedulable. This significantly improves upon the current
best-known density bound of 1.546 and is, in fact, optimal. We also study the
bamboo garden trimming problem, an optimization variant of the pinwheel
problem. Specifically, given $n$ growth rates with values $h_i \in \mathbb{N}$,
the objective is to minimize the maximum height of a bamboo garden with the
corresponding growth rates, where we are allowed to trim one bamboo tree to
height zero per time step. We achieve an efficient $9/7$-approximation
algorithm for this problem, improving on the current best known approximation
factor of $4/3$.

</details>


### [9] [(Approximate) Matrix Multiplication via Convolutions](https://arxiv.org/abs/2510.22193)
*Kevin Pratt,Yahel Uffenheimer,Omri Weinstein*

Main category: cs.DS

TL;DR: 本文提出了一个O(n^{2.89})时间的精确矩阵乘法算法，并基于此开发了快速近似矩阵乘法框架，打破了线性速度-精度权衡，在O(rn^2)时间内达到更好的误差界限。


<details>
  <summary>Details</summary>
Motivation: 解决算法设计中长期存在的开放性问题：组合矩阵乘法算法能否实现真正次立方时间复杂度，并开发实用的近似矩阵乘法方法。

Method: 使用FFT在ℤ_m^k中求和卷积（多元多项式乘法），基于CKSU'05工作，避免递归。开发低度近似CKSU多项式的方案，结合黑盒线性草图技术。

Result: 精确算法达到O(n^{2.89})时间，但仅对极大矩阵有效。近似算法在O(rn^2)时间内达到1/r^{1.1}‖A‖_F^2‖B‖_F^2误差，对高斯随机矩阵误差小于最佳秩r SVD。

Conclusion: 理论和实证结果表明，在LLM训练和推理中可能用卷积和替代矩阵乘法，实现了对迭代Krylov子空间方法的显著改进。

Abstract: A longstanding open question in algorithm design is whether "combinatorial"
matrix multiplication algorithms -- avoiding Strassen-like divide-and-conquer
-- can achieve truly subcubic runtime $n^{3-\delta}$. We present an
$O(n^{2.89})$-time exact algorithm, which only sums convolutions in
$\mathbb{Z}_m^k$ (multivariate polynomial multiplications) via FFT, building on
the work of Cohn, Kleinberg, Szegedy and Umans (CKSU'05). While the algorithm
avoids recursion, the asymptotic speedup arises only for impractically large
matrices.
  Motivated by practical applications, we use this baseline to develop a new
framework for fast approximate matrix multiplication (AMM), via low-degree
approximations of the CKSU polynomials. We show that combining the
aforementioned algorithm with black-box linear sketching already breaks the
longstanding linear speed-accuracy tradeoff for AMM (Sarlos'06,
Clarkson-Woodruff'13 ,Pagh'11, Cohn-Lewis'00), achieving
$\frac{1}{r^{1.1}}\|\mathbf{A}\|_F^2\|\mathbf{B}\|_F^2$ error in
$O(rn^2)$-time.
  Our main result is a low-degree approximation scheme for the CKSU
polynomials, based on a Fourier-concentration lemma, yielding substantially
smaller error in the distributional setting where $\mathbf{A},\mathbf{B}$ come
from an i.i.d product-distribution; For random Gaussian matrices, this
practical AMM algorithm attains smaller error than the best rank-$r$ SVD of the
output matrix $\mathbf{A}\mathbf{B}$, in time $O(rn^2)$. This is a substantial
improvement over iterative Krylov subspace methods for low-rank approximation.
Our theoretical and empirical results suggest the possibility of replacing
MatMuls with sums of convolutions in LLM training and inference.

</details>


### [10] [Johnson-Lindenstrauss Lemma Beyond Euclidean Geometry](https://arxiv.org/abs/2510.22401)
*Chengyuan Deng,Jie Gao,Kevin Lu,Feng Luo,Cheng Xin*

Main category: cs.DS

TL;DR: 将Johnson-Lindenstrauss引理扩展到非欧几里得几何，处理一般相异矩阵，提供两种方法：伪欧几里得空间中的JL变换和广义幂距离表示。


<details>
  <summary>Details</summary>
Motivation: JL引理在欧几里得空间维度缩减中很重要，但难以应用于非欧几里得数据，而现实世界中存在大量非欧几里得数据。

Method: 1. 在伪欧几里得空间中应用JL变换；2. 将对称空心相异矩阵表示为广义幂距离矩阵，并应用JL变换。

Result: 理论保证依赖于数据偏离欧几里得几何的程度，在合成和真实数据集上验证了方法的有效性。

Conclusion: 扩展JL引理到非欧几里得设置，使维度缩减技术适用于更广泛的数据类型，性能分析基于数据偏离欧几里得几何的程度。

Abstract: The Johnson-Lindenstrauss (JL) lemma is a cornerstone of dimensionality
reduction in Euclidean space, but its applicability to non-Euclidean data has
remained limited. This paper extends the JL lemma beyond Euclidean geometry to
handle general dissimilarity matrices that are prevalent in real-world
applications. We present two complementary approaches: First, we show the JL
transform can be applied to vectors in pseudo-Euclidean space with signature
$(p,q)$, providing theoretical guarantees that depend on the ratio of the $(p,
q)$ norm and Euclidean norm of two vectors, measuring the deviation from
Euclidean geometry. Second, we prove that any symmetric hollow dissimilarity
matrix can be represented as a matrix of generalized power distances, with an
additional parameter representing the uncertainty level within the data. In
this representation, applying the JL transform yields multiplicative
approximation with a controlled additive error term proportional to the
deviation from Euclidean geometry. Our theoretical results provide fine-grained
performance analysis based on the degree to which the input data deviates from
Euclidean geometry, making practical and meaningful reduction in dimensionality
accessible to a wider class of data. We validate our approaches on both
synthetic and real-world datasets, demonstrating the effectiveness of extending
the JL lemma to non-Euclidean settings.

</details>


### [11] [On Integer Programs That Look Like Paths](https://arxiv.org/abs/2510.22430)
*Marcin Briański,Alexandra Lassota,Kristýna Pekárková,Michał Pilipczuk,Janina Reuter*

Main category: cs.DS

TL;DR: 该论文证明即使约束矩阵系数有界于8，具有路径状结构的整数规划可行性判定仍然是NP困难的


<details>
  <summary>Details</summary>
Motivation: 研究整数规划中路径状约束矩阵结构的计算复杂性，这类结构比星形结构更复杂但比一般情况简单

Method: 通过从3-SAT问题归约，证明具有路径状结构的整数规划可行性判定是NP困难的

Result: 即使约束矩阵所有系数有界于8，路径状结构的整数规划可行性判定仍然是NP困难的

Conclusion: 路径状结构的整数规划比星形结构更难求解，这一结果与之前对类似结构的研究形成对比

Abstract: Solving integer programs of the form $\min \{\mathbf{x} \mid A\mathbf{x} =
\mathbf{b}, \mathbf{l} \leq \mathbf{x} \leq \mathbf{u}, \mathbf{x} \in
\mathbb{Z}^n \}$ is, in general, $\mathsf{NP}$-hard. Hence, great effort has
been put into identifying subclasses of integer programs that are solvable in
polynomial or $\mathsf{FPT}$ time. A common scheme for many of these integer
programs is a star-like structure of the constraint matrix. The arguably
simplest form that is not a star is a path. We study integer programs where the
constraint matrix $A$ has such a path-like structure: every non-zero
coefficient appears in at most two consecutive constraints. We prove that even
if all coefficients of $A$ are bounded by 8, deciding the feasibility of such
integer programs is $\mathsf{NP}$-hard via a reduction from 3-SAT. Given the
existence of efficient algorithms for integer programs with star-like
structures and a closely related pattern where the sum of absolute values is
column-wise bounded by 2 (hence, there are at most two non-zero entries per
column of size at most 2), this hardness result is surprising.

</details>


### [12] [Tree Embedding in High Dimensions: Dynamic and Massively Parallel](https://arxiv.org/abs/2510.22490)
*Gramoz Goranci,Shaofeng H. -C. Jiang,Peter Kiss,Qihao Kong,Yi Qian,Eva Szilagyi*

Main category: cs.DS

TL;DR: 提出了一个新的树嵌入构建框架，可在高维欧几里得空间中高效构建树嵌入，支持动态更新和并行计算，并应用于k-中位数和地球移动距离问题。


<details>
  <summary>Details</summary>
Motivation: 树嵌入是算法设计中的基本方法，但现有方法在高维欧几里得空间中的效率有限。需要开发能够处理动态更新和并行计算的树嵌入构建方法。

Method: 设计了一个新的树嵌入构建框架，该框架基于有界直径的任意度量分解，在失真度和算法步骤的局部性之间提供权衡。该框架适用于一般度量空间。

Result: 获得了动态算法（支持点插入/删除，更新时间为Õ(n^ε + d)，失真度为O_ε(log n)）和大规模并行算法（O(1)轮，总空间Õ(n^{1+ε})，失真度O_ε(log n)）。

Conclusion: 新的树嵌入框架为高维欧几里得空间中的动态和并行算法设计提供了有效工具，可广泛应用于k-中位数和地球移动距离等问题的近似求解。

Abstract: Tree embedding has been a fundamental method in algorithm design with wide
applications. We focus on the efficiency of building tree embedding in various
computational settings under high-dimensional Euclidean $\mathbb{R}^d$. We
devise a new tree embedding construction framework that operates on an
arbitrary metric decomposition with bounded diameter, offering a tradeoff
between distortion and the locality of its algorithmic steps. This framework
works for general metric spaces and may be of independent interest beyond the
Euclidean setting. Using this framework, we obtain a dynamic algorithm that
maintains an $O_\epsilon(\log n)$-distortion tree embedding with update time
$\tilde O(n^\epsilon + d)$ subject to point insertions/deletions, and a
massively parallel algorithm that achieves $O_\epsilon(\log n)$-distortion in
$O(1)$ rounds and total space $\tilde O(n^{1 + \epsilon})$ (for constant
$\epsilon \in (0, 1)$). These new tree embedding results allow for a wide range
of applications. Notably, under a similar performance guarantee as in our tree
embedding algorithms, i.e., $\tilde O(n^\epsilon + d)$ update time and $O(1)$
rounds, we obtain $O_\epsilon(\log n)$-approximate dynamic and MPC algorithms
for $k$-median and earth-mover distance in $\mathbb{R}^d$.

</details>


### [13] [Generating pivot Gray codes for spanning trees of complete graphs in constant amortized time](https://arxiv.org/abs/2510.22662)
*Bowie Liu,Dennis Wong,Chan-Tong Lam,Sio-Kei Im*

Main category: cs.DS

TL;DR: 提出了第一个已知的完全图生成树的枢轴格雷码，解决了Knuth在《计算机程序设计艺术》中提出的开放性问题，生成每个生成树的时间复杂度为常数分摊时间，空间复杂度为O(n²)。


<details>
  <summary>Details</summary>
Motivation: 解决Knuth在《计算机程序设计艺术》第4卷中提出的开放性问题（难度等级46/50），该问题要求为完全图的生成树设计枢轴格雷码，比现有的旋转门或边交换格雷码条件更严格。

Method: 使用递归算法生成枢轴格雷码，其中连续生成树通过围绕顶点枢转单条边来区分。算法可扩展到一般图的边交换格雷码生成。

Result: 成功生成了完全图生成树的枢轴格雷码，每个生成树生成时间为常数分摊时间，空间复杂度O(n²)。还从递归方法推导出了凯莱公式n^{n-2}的新证明。

Conclusion: 该算法不仅解决了Knuth的开放性问题，还提供了凯莱公式的新证明，并能扩展到各种图类的边交换格雷码生成，在特定图类中实现了优化的时间复杂度。

Abstract: We present the first known pivot Gray code for spanning trees of complete
graphs, listing all spanning trees such that consecutive trees differ by
pivoting a single edge around a vertex. This pivot Gray code thus addresses an
open problem posed by Knuth in The Art of Computer Programming, Volume 4
(Exercise 101, Section 7.2.1.6, [Knuth, 2011]), rated at a difficulty level of
46 out of 50, and imposes stricter conditions than existing revolving-door or
edge-exchange Gray codes for spanning trees of complete graphs. Our recursive
algorithm generates each spanning tree in constant amortized time using
$O(n^2)$ space. In addition, we provide a novel proof of Cayley's formula,
$n^{n-2}$, for the number of spanning trees in a complete graph, derived from
our recursive approach. We extend the algorithm to generate edge-exchange Gray
codes for general graphs with $n$ vertices, achieving $O(n^2)$ time per tree
using $O(n^2)$ space. For specific graph classes, the algorithm can be
optimized to generate edge-exchange Gray codes for spanning trees in constant
amortized time per tree for complete bipartite graphs, $O(n)$-amortized time
per tree for fan graphs, and $O(n)$-amortized time per tree for wheel graphs,
all using $O(n^2)$ space.

</details>


### [14] [Faster Negative-Weight Shortest Paths and Directed Low-Diameter Decompositions](https://arxiv.org/abs/2510.22721)
*Jason Li,Connor Mowry,Satish Rao*

Main category: cs.DS

TL;DR: 提出了一种更快的低直径分解算法，匹配了现有算法的损失因子，并将运行时间改进为期望$O((m+n\log\log n)\log n\log\log n)$。应用该分解算法获得了负权重单源最短路径问题的改进算法。


<details>
  <summary>Details</summary>
Motivation: 改进有向图低直径分解算法的运行时间，并应用这一改进来解决负权重单源最短路径问题，超越现有算法的性能。

Method: 开发了更快的低直径分解算法，匹配了Bringmann等人(ICALP 2025)的$O(\log n\log\log n)$损失因子，但显著提高了运行效率。然后将该分解算法应用于负权重单源最短路径问题。

Result: 低直径分解算法运行时间改进为期望$O((m+n\log\log n)\log n\log\log n)$。负权重单源最短路径算法达到$O((m+n\log\log n)\log(nW)\log n\log\log n)$时间，相比Bringmann等人(FOCS 2023)的算法有近对数因子的改进。

Conclusion: 成功开发了更快的低直径分解算法，并将其应用于负权重单源最短路径问题，实现了显著的运行时间改进，为相关图算法提供了更高效的解决方案。

Abstract: We present a faster algorithm for low-diameter decompositions on directed
graphs, matching the $O(\log n\log\log n)$ loss factor from Bringmann, Fischer,
Haeupler, and Latypov (ICALP 2025) and improving the running time to
$O((m+n\log\log n)\log n\log\log n)$ in expectation. We then apply our faster
low-diameter decomposition to obtain an algorithm for negative-weight single
source shortest paths on integer-weighted graphs in $O((m+n\log\log
n)\log(nW)\log n\log\log n)$ time, a nearly log-factor improvement over the
algorithm of Bringmann, Cassis, and Fischer (FOCS 2023).

</details>


### [15] [$L_p$ Sampling in Distributed Data Streams with Applications to Adversarial Robustness](https://arxiv.org/abs/2510.22816)
*Honghao Lin,Zhao Song,David P. Woodruff,Shenghao Xie,Samson Zhou*

Main category: cs.DS

TL;DR: 本文解决了分布式监控模型中的完美L_p采样问题，提出了对所有p≥1的完美L_p采样算法，通信复杂度为k^{p-1}·polylog(n)比特，达到最优。基于此实现了对抗性鲁棒的F_p矩估计协议，并应用于计数、频率估计、重击者检测等核心问题。


<details>
  <summary>Details</summary>
Motivation: 在分布式监控模型中，从全局数据流中高效收集随机样本是一个关键原语，能够支持频率矩估计、重击者检测等下游任务。特别是完美L_p采样任务，在分布式设置中尚未完全解决。

Method: 开发了适用于所有p≥1的完美L_p采样算法，利用该采样器构建对抗性鲁棒的F_p矩估计协议，通信复杂度为k^{p-1}/ε²·polylog(n)比特。

Result: 算法在通信复杂度上达到最优（除polylog因子外），匹配Woodruff和Zhang在非鲁棒设置中的下界。成功应用于计数、频率估计、重击者检测和不同元素估计等核心问题。

Conclusion: 本文完全解决了分布式监控模型中的完美L_p采样问题，为所有p≥1提供了最优通信复杂度的算法，并基于此构建了对抗性鲁棒的分布式监控协议，在多个核心问题上实现了接近最优的性能。

Abstract: In the distributed monitoring model, a data stream over a universe of size
$n$ is distributed over $k$ servers, who must continuously provide certain
statistics of the overall dataset, while minimizing communication with a
central coordinator. In such settings, the ability to efficiently collect a
random sample from the global stream is a powerful primitive, enabling a wide
array of downstream tasks such as estimating frequency moments, detecting heavy
hitters, or performing sparse recovery. Of particular interest is the task of
producing a perfect $L_p$ sample, which given a frequency vector $f \in
\mathbb{R}^n$, outputs an index $i$ with probability
$\frac{f_i^p}{\|f\|_p^p}+\frac{1}{\mathrm{poly}(n)}$. In this paper, we resolve
the problem of perfect $L_p$ sampling for all $p\ge 1$ in the distributed
monitoring model. Specifically, our algorithm runs in $k^{p-1} \cdot
\mathrm{polylog}(n)$ bits of communication, which is optimal up to
polylogarithmic factors.
  Utilizing our perfect $L_p$ sampler, we achieve adversarially-robust
distributed monitoring protocols for the $F_p$ moment estimation problem, where
the goal is to provide a $(1+\varepsilon)$-approximation to
$f_1^p+\ldots+f_n^p$. Our algorithm uses
$\frac{k^{p-1}}{\varepsilon^2}\cdot\mathrm{polylog}(n)$ bits of communication
for all $p\ge 2$ and achieves optimal bounds up to polylogarithmic factors,
matching lower bounds by Woodruff and Zhang (STOC 2012) in the non-robust
setting. Finally, we apply our framework to achieve near-optimal adversarially
robust distributed protocols for central problems such as counting, frequency
estimation, heavy-hitters, and distinct element estimation.

</details>


### [16] [Hierarchical Exponential Search Via K-Spines](https://arxiv.org/abs/2510.22837)
*Bob Dong*

Main category: cs.DS

TL;DR: 提出k-spine树结构概念，通过移除k-spine路径将树分解为路径宽度较小的组件，并基于此设计O(klog dist)指数搜索算法


<details>
  <summary>Details</summary>
Motivation: 为了解决树结构中的高效搜索问题，需要一种能够有效分解树结构并减少搜索复杂度的新方法

Method: 使用k-spine作为中心引导路径，主要沿spine进行搜索以缩小目标范围，然后递归处理较小的组件

Result: 开发了O(klog dist)时间复杂度的指数搜索算法，其中dist是搜索距离

Conclusion: k-spine概念为树结构搜索提供了有效的分解框架，显著降低了搜索复杂度

Abstract: We introduce the concept of a k-spine of a tree. A k-spine is essentially a
path in the tree whose removal leaves only "less-bushy" components of a smaller
pathwidth. Using a k-spine as a central guide, we introduce an O(klog dist)
exponential search algorithm on a tree by searching mainly along the spine to
narrow down the target's vicinity and then recursively handling the smaller
components.

</details>


### [17] [Testing forbidden order-pattern properties on hypergrids](https://arxiv.org/abs/2510.22845)
*Harish Chandramouleeswaran,Ilan Newman,Tomer Pelleg,Nithin Varma*

Main category: cs.DS

TL;DR: 本文系统研究了高维网格上的模式无性测试，针对k=3的所有排列设计了自适应单边测试器，查询复杂度为O(n^{4/5+o(1)})，并证明了k=3的非自适应测试器需要Ω(n)查询，自适应测试器需要Ω(√n)查询。


<details>
  <summary>Details</summary>
Motivation: 虽然k=2的情况对应单调性测试已有较多研究，但对于k>2的模式无性测试了解甚少。本文旨在系统研究高维网格上的模式无性测试问题。

Method: 设计了针对二维网格上k=3所有排列的自适应单边测试器，并开发了新的擦除弹性单调性测试器作为关键工具。对于单调模式(1,2,3)和(3,2,1)，提出了非自适应测试器。

Result: 获得了k=3模式无性测试的第一个超对数下界，实现了单调模式与非单调模式之间的指数级分离，擦除弹性单调性测试器的查询复杂度为O(log^{O(d)}n/(ε(1-δ)))。

Conclusion: 本文为高维模式无性测试建立了系统框架，证明了当前技术无法为长度4的模式设计亚线性查询测试器，即使是在二维超网格上。

Abstract: We study testing $\pi$-freeness of functions $f:[n]^d\to\mathbb{R}$, where
$f$ is $\pi$-free if there there are no $k$ indices $x_1\prec\cdots\prec x_k\in
[n]^d$ such that $f(x_i)<f(x_j)$ and $\pi(i) < \pi(j)$ for all $i,j \in [k]$,
where $\prec$ is the natural partial order over $[n]^d$. Given
$\epsilon\in(0,1)$, $\epsilon$-testing $\pi$-freeness asks to distinguish
$\pi$-free functions from those which are $\epsilon$-far -- meaning at least
$\epsilon n^d$ function values must be modified to make it $\pi$-free. While
$k=2$ coincides with monotonicity testing, far less is known for $k>2$.
  We initiate a systematic study of pattern freeness on higher-dimensional
grids. For $d=2$ and all permutations of size $k=3$, we design an adaptive
one-sided tester with query complexity $O(n^{4/5+o(1)})$. We also prove general
lower bounds for $k=3$: every nonadaptive tester requires $\Omega(n)$ queries,
and every adaptive tester requires $\Omega(\sqrt{n})$ queries, yielding the
first super-logarithmic lower bounds for $\pi$-freeness. For the monotone
patterns $\pi=(1,2,3)$ and $(3,2,1)$, we present a nonadaptive tester with
polylogarithmic query complexity, giving an exponential separation between
monotone and nonmonotone patterns (unlike the one-dimensional case).
  A key ingredient in our $\pi$-freeness testers is new erasure-resilient
($\delta$-ER) $\epsilon$-testers for monotonicity over $[n]^d$ with query
complexity $O(\log^{O(d)}n/(\epsilon(1-\delta)))$, where $0<\delta<1$ is an
upper bound on the fraction of erasures. Prior ER testers worked only for
$\delta=O(\epsilon/d)$. Our nonadaptive monotonicity tester is nearly optimal
via a matching lower bound due to Pallavoor, Raskhodnikova, and Waingarten
(Random Struct. Algorithms, 2022). Finally, we show that current techniques
cannot yield sublinear-query testers for patterns of length $4$ even on
two-dimensional hypergrids.

</details>


### [18] [Multi-Way Co-Ranking: Index-Space Partitioning of Sorted Sequences Without Merge](https://arxiv.org/abs/2510.22882)
*Amit Joshi*

Main category: cs.DS

TL;DR: 提出了一种免合并的多路协同排序算法，通过索引空间二分搜索而非值空间搜索或多路合并，高效计算分割索引来划分多个有序序列。


<details>
  <summary>Details</summary>
Motivation: 解决多路协同排序问题，即计算分割索引来划分多个有序序列，使得所有前缀段包含恰好K个元素，避免传统多路合并或值空间搜索的高开销。

Method: 将双列表协同排序扩展到任意m个序列，通过索引空间二分搜索维护每个序列的边界，这些边界收敛到一致的全局前沿，无需执行多路合并或值空间搜索。

Result: 算法在O(log(∑n_t)log m)时间和O(m)空间内运行，与K无关，证明了通过交换论证的正确性。

Conclusion: 该方法在分布式分数背包、并行合并分区和多流连接等应用中具有实用价值，提供了一种高效的免合并协同排序解决方案。

Abstract: We present a merge-free algorithm for multi-way co-ranking, the problem of
computing cut indices $i_1,\dots,i_m$ that partition each of the $m$ sorted
sequences such that all prefix segments together contain exactly $K$ elements.
Our method extends two-list co-ranking to arbitrary $m$, maintaining
per-sequence bounds that converge to a consistent global frontier without
performing any multi-way merge or value-space search. Rather, we apply binary
search to \emph{index-space}. The algorithm runs in $O(\log(\sum_t n_t)\,\log
m)$ time and $O(m)$ space, independent of $K$. We prove correctness via an
exchange argument and discuss applications to distributed fractional knapsack,
parallel merge partitioning, and multi-stream joins.
  Keywords: Co-ranking \sep partitioning \sep Merge-free algorithms \sep
Index-space optimization \sep Selection and merging \sep Data structures

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [19] [Fundamental Limits of Coded Caching with Fixed Subpacketization](https://arxiv.org/abs/2510.22145)
*Minquan Cheng,Yifei Huang,Youlong Wu,Jinyan Wang*

Main category: cs.IT

TL;DR: 本文研究了编码缓存网络中传输负载与子分组化级别之间的基本权衡关系，提出了针对固定子分组化级别的传输负载下界，并证明了分区方案能够达到最优的速率-子分组化权衡。


<details>
  <summary>Details</summary>
Motivation: 编码缓存技术通过子分组化可以显著降低传输负载，但过高的子分组化级别会导致编码复杂度增加。现有研究缺乏对固定子分组化级别下最小传输负载的深入分析。

Method: 通过重新表述集中式编码缓存方案对应的放置交付阵列的组合结构，提出了针对任何固定子分组化级别的传输负载通用下界。利用组合结构和计算排序集合的并集大小来建立新的最优性结果。

Result: 提出的下界恢复了现有最优性结果（包括二分图方案和分组二分图方案），并证明了分区方案能够达到最优的速率-子分组化权衡。

Conclusion: 该研究为编码缓存网络中的传输负载与子分组化级别之间的基本权衡关系提供了理论分析框架，证明了分区方案在特定条件下的最优性。

Abstract: Coded caching is a promising technique to create coded multicast
opportunities for cache-aided networks. By splitting each file into $F$ equal
packets (i.e., the subpacketization level $F$) and letting each user cache a
set of packets, the transmission load can be significantly reduced via coded
multicasting. It has been shown that a higher subpacketization level could
potentially lead to a lower transmission load, as more packets can be combined
for efficient transmission. On the other hand, a larger $F$ indicates a higher
coding complexity and is problematic from a practical perspective when $F$ is
extremely large. Despite many works attempting to design coded caching schemes
with low subpacketization levels, a fundamental problem remains open: What is
the minimum transmission load given any fixed subpacketization level? In this
paper, we consider the classical cache-aided networks with identically uncoded
placement and one-shot delivery strategy, and investigate the fundamental
trade-off between the transmission load and the subpacketization level. We
propose a \emph{general} lower bound on the transmission load for any fixed
subpacketization by reformulating the centralized coded caching schemes via the
combinatorial structure of the corresponding placement delivery array. The
lower bound also recovers existing optimality results for the bipartite graph
scheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the
conjugate MN scheme) as well as the grouping bipartite graph scheme.
Furthermore, by carefully exploiting the combinatorial structure and computing
the union size of sorted sets, we establish a new optimality result, i.e., the
partition scheme can achieve the optimal rate-subpacketization trade-off.

</details>


### [20] [Robust MIMO Channel Estimation Using Energy-Based Generative Diffusion Models](https://arxiv.org/abs/2510.22230)
*Ziqi Diao,Xingyu Zhou,Le Liang,Shi Jin*

Main category: cs.IT

TL;DR: 提出了一种结合能量基生成扩散模型和Metropolis-Hastings原理的新型信道估计框架，显著提升了大规模MIMO系统的信道估计精度，特别是在导频开销有限的情况下。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统的信道估计面临导频开销过大和估计延迟高的根本性限制，需要克服这些障碍。

Method: 通过将能量基生成扩散模型与Metropolis-Hastings原理集成，重新参数化扩散过程并融入能量函数，明确估计未归一化的对数先验，同时使用MH校正来优化采样轨迹、减少偏差并增强鲁棒性。

Result: 数值结果表明，与传统的参数化扩散模型和其他基线方法相比，所提出的方法显著提高了估计精度。

Conclusion: 该框架能够实现准确的后验采样，从而实现高保真度的信道估计，特别是在导频开销有限的情况下表现优异。

Abstract: Channel estimation for massive multiple-input multiple-output (MIMO) systems
is fundamentally constrained by excessive pilot overhead and high estimation
latency. To overcome these obstacles, recent studies have leveraged deep
generative networks to capture the prior distribution of wireless channels. In
this paper, we propose a novel estimation framework that integrates an
energy-based generative diffusion model (DM) with the Metropolis-Hastings (MH)
principle. By reparameterizing the diffusion process with an incorporated
energy function, the framework explicitly estimates the unnormalized log-prior,
while MH corrections refine the sampling trajectory, mitigate deviations, and
enhance robustness, ultimately enabling accurate posterior sampling for
high-fidelity channel estimation. Numerical results reveal that the proposed
approach significantly improves estimation accuracy compared with conventional
parameterized DMs and other baseline methods, particularly in cases with
limited pilot overhead.

</details>


### [21] [Infinitely many families of distance-optimal binary linear codes with respect to the sphere packing bound](https://arxiv.org/abs/2510.22259)
*Hao Chen,Conghui Xie,Cunsheng Ding*

Main category: cs.IT

TL;DR: 解决了编码理论中75年来的开放问题：证明了存在关于球填充界的距离最优线性码的无限族，其最小距离可以任意大。


<details>
  <summary>Details</summary>
Motivation: 自1950年Hamming提出Hamming码和球填充界以来，75年间只发现了最小距离不超过8的距离最优线性码无限族。是否存在最小距离任意大的距离最优线性码无限族一直是编码理论的开放问题。

Method: 未在摘要中明确说明具体方法，但通过构造证明了存在这样的无限族。

Result: 成功解决了75年来的开放问题，证明了存在最小距离任意大的距离最优线性码无限族。同时构造了多个最小距离较小的距离最优二进制码无限族，以及两个二进制五重量码无限族。

Conclusion: 本文解决了编码理论中长期存在的开放问题，为距离最优码的研究提供了新的理论支撑，并提出了若干后续研究问题。

Abstract: R. W. Hamming published the Hamming codes and the sphere packing bound in
1950. In the past 75 years, infinite families of distance-optimal linear codes
over finite fields with minimum distance at most 8 with respect to the sphere
packing bound have been reported in the literature. However, it is a
75-year-old open problem in coding theory whether there is an infinite family
of distance-optimal linear codes over finite fields with arbitrarily large
minimum distance with respect to the sphere packing bound. This main objective
of this paper is to settle this long-standing open problem in coding theory.
  As by-products, several infinite families of distance-optimal binary codes
with small minimum distances are presented. Two infinite families of binary
five-weight codes are reported. Some open problems are also proposed.

</details>


### [22] [Optimal Sampling and Scheduling for Remote Fusion Estimation of Correlated Wiener Processes](https://arxiv.org/abs/2510.22288)
*Aimin Li,Elif Uysal*

Main category: cs.IT

TL;DR: 该论文研究了在分布式传感器网络中，如何融合异步且相关的观测信息进行远程融合估计。通过建立分离原理，确定了联合最优策略：基于信息年龄(AoI)的加权和融合估计器、最大年龄优先调度器和AoI最优采样器。


<details>
  <summary>Details</summary>
Motivation: 在分布式传感器网络中，传感器观测重叠区域的动态过程，但由于随机延迟，这些相关观测异步到达融合中心，需要解决如何融合异步相关信息以实现准确估计的问题。

Method: 研究采样、调度和估计策略的联合设计，建立分离原理，识别最优策略包括：基于AoI的加权和融合估计器、最大年龄优先调度器，以及AoI最优采样器设计。

Result: 证明了在无限时域平均成本准则下，优化AoI等价于优化均方误差，即使存在强传感器间相关性。这一结构等价性使得MSE最优采样器就是AoI最优采样器。

Conclusion: 信息新鲜度可以作为相关传感环境中最优估计的设计替代指标，为分布式传感器网络中的异步融合估计提供了理论基础和实用指导。

Abstract: In distributed sensor networks, sensors often observe a dynamic process
within overlapping regions. Due to random delays, these correlated observations
arrive at the fusion center asynchronously, raising a central question: How can
one fuse asynchronous yet correlated information for accurate remote fusion
estimation? This paper addresses this challenge by studying the joint design of
sampling, scheduling, and estimation policies for monitoring a correlated
Wiener process. Though this problem is coupled, we establish a separation
principle and identify the joint optimal policy: the optimal fusion estimator
is a weighted-sum fusion estimator conditioned on Age of Information (AoI), the
optimal scheduler is a Maximum Age First (MAF) scheduler that prioritizes the
most stale source, and the optimal sampling can be designed given the optimal
estimator and the MAF scheduler. To design the optimal sampling, we show that,
under the infinite-horizon average-cost criterion, optimizing AoI is equivalent
to optimizing MSE under pull-based communications, despite the presence of
strong inter-sensor correlations. This structural equivalence allows us to
identify the MSE-optimal sampler as one that is AoI-optimal. This result
underscores an insight: information freshness can serve as a design surrogate
for optimal estimation in correlated sensing environments.

</details>


### [23] [Energy-Efficient UAV-Enabled MEC Systems: NOMA, FDMA, or TDMA Offloading?](https://arxiv.org/abs/2510.22306)
*Qingjie Wu,Miao Cui,Guangchi Zhang,Beixiong Zheng,Xiaoli Chu,Qingqing Wu*

Main category: cs.IT

TL;DR: 该论文比较了无人机移动边缘计算系统中NOMA、FDMA和TDMA三种多址接入方案的能耗效率，发现在有限块长度情况下TDMA能耗最低，NOMA不一定优于FDMA，并提出了一种联合优化算法来降低能耗。


<details>
  <summary>Details</summary>
Motivation: 无人机移动边缘计算系统可以使用不同多址方案协调多用户任务卸载，但尚不清楚哪种方案在有限块长度情况下最节能，需要系统比较NOMA、FDMA和TDMA的能耗性能。

Method: 通过理论分析三种方案的最小能耗需求，提出交替优化算法联合优化任务卸载比例、所有用户设备的卸载时间和无人机位置，解决能耗最小化问题。

Result: TDMA在无限和有限块长度情况下能耗均低于FDMA；NOMA在有限块长度下不一定比FDMA节能，特别是当两个用户设备的信道条件和任务数据大小相对对称时；所提算法相比基准方案能有效降低MEC相关能耗。

Conclusion: TDMA是无人机MEC系统中最节能的多址方案，NOMA在有限块长度下的优势有限，提出的联合优化算法能显著降低系统能耗。

Abstract: Unmanned aerial vehicle (UAV)-enabled mobile edge computing (MEC) systems can
use different multiple access schemes to coordinate multi-user task offloading.
However, it is still unknown which scheme is the most energy-efficient,
especially when the offloading blocklength is finite. To answer this question,
this paper minimizes and compares the MEC-related energy consumption of
non-orthogonal multiple access (NOMA), frequency division multiple access
(FDMA), and time division multiple access (TDMA)-based offloading schemes
within UAV-enabled MEC systems, considering both infinite and finite
blocklength scenarios. Through theoretically analysis of the minimum energy
consumption required by these three schemes, two novel findings are presented.
First, TDMA consistently achieves lower energy consumption than FDMA in both
infinite and finite blocklength cases, due to the degrees of freedom afforded
by sequential task offloading. Second, NOMA does not necessarily achieve lower
energy consumption than FDMA when the offloading blocklength is finite,
especially when the channel conditions and the offloaded task data sizes of two
user equipments (UEs) are relatively symmetric. Furthermore, an alternating
optimization algorithm that jointly optimizes the portions of task offloaded,
the offloading times of all UEs, and the UAV location is proposed to solve the
formulated energy consumption minimization problems. Simulation results verify
the correctness of our analytical findings and demonstrate that the proposed
algorithm effectively reduces MEC-related energy consumption compared to
benchmark schemes that do not optimize task offloading portions and/or
offloading times.

</details>


### [24] [Resource Allocation for XR with Edge Offloading: A Reinforcement Learning Approach](https://arxiv.org/abs/2510.22505)
*Alperen Duru,Mohammad Mozaffari,Ticao Zhang,Mehrnaz Afshang*

Main category: cs.IT

TL;DR: 提出基于强化学习的资源分配框架，动态分配XR应用的上行和下行时隙，根据头显能力和网络状况做出卸载决策，通过部分卸载可扩展覆盖范围55%并降低能耗34%。


<details>
  <summary>Details</summary>
Motivation: 未来沉浸式XR应用需要高能效、高数据速率和低延迟的无线通信，需要智能自适应的资源分配与边缘卸载来支持这些需求。

Method: 使用强化学习框架动态分配上行和下行时隙，基于XR头显能力和网络条件做出卸载决策，进行帧丢失率与能效权衡的数值分析。

Result: 部分卸载相比始终卸载或从不卸载可扩展覆盖范围55%，降低能耗达34%，头显本地计算能力在卸载决策中起关键作用。

Conclusion: 头显本地计算能力越高，越能实现更高效的本地处理，减少卸载需求，提升节能效果，部分卸载策略能显著优化XR应用性能。

Abstract: Future immersive XR applications will require energy-efficient, high data
rate, and low-latency wireless communications in uplink and downlink. One of
the key considerations for supporting such XR applications is intelligent and
adaptive resource allocation with edge offloading. To address these demands,
this paper proposes a reinforcement learning-based resource allocation
framework that dynamically allocates uplink and downlink slots while making
offloading decisions based on the XR headset's capabilities and network
conditions. The paper presents a numerical analysis of the tradeoff between
frame loss rate (FLR) and energy efficiency, identifying decision regions for
partial offloading to optimize performance. Results show that for the used set
of system parameters, partial offloading can extend the coverage area by 55%
and reduce energy consumption by up to 34%, compared to always or never
offloading. The results demonstrate that the headset's local computing
capability plays a crucial role in offloading decisions. Higher computing
abilities enable more efficient local processing, reduce the need for
offloading, and enhance energy savings.

</details>


### [25] [End-to-end Learning of Probabilistic and Geometric Constellation Shaping with Iterative Receivers](https://arxiv.org/abs/2510.22608)
*Harindu Jayarathne,Dileepa Marasinghe,Nandana Rajatheva,Matti Latva-aho*

Main category: cs.IT

TL;DR: 提出了一种端到端的星座成形学习方法，通过成形编码器产生零概率更高的成形比特来优化符号概率分布和星座几何形状。


<details>
  <summary>Details</summary>
Motivation: 传统APSK和QAM星座在性能上存在优化空间，需要开发更高效的星座成形方法来提升通信系统性能。

Method: 使用端到端学习方法联合优化概率分布和星座几何，并扩展到包含完整迭代检测和解码循环的深度展开技术。

Result: 学习到的星座相比标准APSK或QAM，在不同接收器架构下分别获得0.3dB和0.15dB的BER增益，在块衰落信道条件下迭代方案获得0.1dB增益。

Conclusion: 端到端学习方法能有效优化星座成形，在多种接收器架构和信道条件下均能获得显著的性能提升。

Abstract: An end-to-end learning method for constellation shaping with a
shaping-encoder assisted transceiver architecture is presented. The shaping
encoder, which produces shaping bits with a higher probability of zeros, is
used to produce an efficient symbol probability distribution. Both the
probability distribution and the constellation geometry are jointly optimized,
using end-to-end learning. Optimized constellations are evaluated using two
iterative receiver architectures. Bit error rate (BER) performance gain is
quantified against standard amplitude phase-shift keying (APSK) and quadrature
amplitude modulation (QAM) constellations. A maximum BER gain of 0.3 dB and
0.15 dB are observed under two receivers for the learned constellations
compared to standard APSK or QAM. The basic approach is extended to incorporate
the full iterative detection and decoding loop, using the deep unfolding
technique. A bit error rate gain of 0.1 dB is observed for the iterative scheme
with learned constellations under block fading channel conditions, when
compared to standard APSK.

</details>


### [26] [Graph-Theoretic Characterization of Noise Capacity of Conditional Disclosure of Secrets](https://arxiv.org/abs/2510.22671)
*Zhou Li,Siyan Qin,Xiang Zhang,Jihao Fan,Haiqiang Chen,Giuseppe Caire*

Main category: cs.IT

TL;DR: 本文研究了条件秘密披露(CDS)问题中的噪声容量，确定了CDS噪声容量达到最大值1的充要条件，并为任意CDS实例推导了线性噪声容量的上界。


<details>
  <summary>Details</summary>
Motivation: 在CDS问题中，Alice和Bob希望仅在输入满足特定函数关系时向Carol披露共享秘密，同时防止在不合格输入组合下泄露秘密。本文旨在确定噪声容量，即能够安全披露给Carol的最大秘密比特数与Alice和Bob共同持有的独立噪声比特总数之比。

Method: 通过图论方法分析CDS实例，使用图的覆盖参数ρ和未合格路径中的未合格边数d来表征噪声容量。

Result: 建立了CDS噪声容量达到最大值1的充要条件，并推导出线性噪声容量的上界为(ρ-1)(d-1)/(ρd-1)。

Conclusion: 本文为CDS问题的噪声容量提供了理论分析框架，明确了最优噪声容量的条件，并为一般情况下的噪声容量建立了上界。

Abstract: In the problem of conditional disclosure of secrets (CDS), two parties, Alice
and Bob, each has an input and shares a common secret. Their goal is to reveal
the secret to a third party, Carol, as efficiently as possible, only if the
inputs of Alice and Bob satisfy a certain functional relation $f $. To prevent
leakage of the secret to Carol when the input combination is unqualified, both
Alice and Bob introduce noise. This work aims to determine the noise capacity,
defined as the maximum number of secret bits that can be securely revealed to
Carol, normalized by the total number of independent noise bits held jointly by
Alice and Bob. Our contributions are twofold. First, we establish the necessary
and sufficient conditions under which the CDS noise capacity attains its
maximum value of $1$. Second, in addition to the above best-case scenarios, we
derive an upper bound on the linear noise capacity for any CDS instance. In
particular, this upper bound is equal to $(\rho-1)(d-1)/(\rho d-1)$, where
$\rho$ is the covering parameter of the graph representation of $f$, and $d$ is
the number of unqualified edges in residing unqualified path.

</details>


### [27] [Edge Collaborative Gaussian Splatting with Integrated Rendering and Communication](https://arxiv.org/abs/2510.22718)
*Yujie Wan,Chenxuan Liu,Shuai Wang,Tong Zhang,James Jianqiao Yu,Kejiang Ye,Dusit Niyato,Chengzhong Xu*

Main category: cs.IT

TL;DR: 提出ECO-GS系统，通过本地小GS模型和远程大GS模型的协作切换，在低成本设备上平衡渲染质量和实时性，并开发了PMM和ILO算法进行联合优化。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅(GS)在低成本设备上渲染质量下降，需要一种能在保证实时性和保证保真度之间灵活切换的解决方案。

Method: 提出边缘协作GS(ECO-GS)系统，集成渲染与通信(IRAC)联合优化协作状态和边缘功率分配，使用惩罚主化最小化(PMM)算法和模仿学习优化(ILO)算法。

Result: PMM算法获得临界点解，ILO算法计算时间比PMM减少100倍以上，实验证明了PMM的优越性和ILO的实时执行能力。

Conclusion: ECO-GS系统通过智能的模型切换和资源分配，有效解决了低成本设备上GS渲染质量下降的问题，ILO算法实现了实时优化。

Abstract: Gaussian splatting (GS) struggles with degraded rendering quality on low-cost
devices. To address this issue, we present edge collaborative GS (ECO-GS),
where each user can switch between a local small GS model to guarantee
timeliness and a remote large GS model to guarantee fidelity. However, deciding
how to engage the large GS model is nontrivial, due to the interdependency
between rendering requirements and resource conditions. To this end, we propose
integrated rendering and communication (IRAC), which jointly optimizes
collaboration status (i.e., deciding whether to engage large GS) and edge power
allocation (i.e., enabling remote rendering) under communication constraints
across different users by minimizing a newly-derived GS switching function.
Despite the nonconvexity of the problem, we propose an efficient penalty
majorization minimization (PMM) algorithm to obtain the critical point
solution. Furthermore, we develop an imitation learning optimization (ILO)
algorithm, which reduces the computational time by over 100x compared to PMM.
Experiments demonstrate the superiority of PMM and the real-time execution
capability of ILO.

</details>


### [28] [On the Arikan Transformations of Binary-Input Discrete Memoryless Channels](https://arxiv.org/abs/2510.22896)
*Yadong Jiao,Xiaoyan Cheng,Yuansheng Tang,Ming Xu*

Main category: cs.IT

TL;DR: 本文提出了一种将极化码构造中的合成信道表征为二进制对称信道随机切换信道的方法，并推导了合成信道输出字母表中具有相同似然比的元素数量的下界。


<details>
  <summary>Details</summary>
Motivation: 由于合成信道输出字母表规模庞大，目前缺乏高效实用的方法来评估其可靠性。本文旨在解决这一问题，通过代数操作来表征合成信道。

Method: 将极化码构造中的合成信道生成转换为代数操作，在底层信道对称时，将合成信道表征为二进制对称信道的随机切换信道。

Result: 开发了一种方法来表征合成信道，并推导了合成信道输出字母表中具有相同似然比的元素数量的下界。

Conclusion: 该方法为评估极化码合成信道的可靠性提供了新的理论工具，有助于更高效地识别不可靠的合成信道。

Abstract: The polar codes introduced by Arikan in 2009 achieve the capacity of
binary-input discrete memoryless channels (BIDMCs) with low complexity encoding
and decoding. Identifying the unreliable synthetic channels, generated by
Arikan transformation during the construction of these polar codes, is crucial.
Currently, because of the large size of the output alphabets of synthetic
channels, there is no efficient and practical approach to evaluate their
reliability in general. To tackle this problem, by converting the generation of
synthetic channels in polar code construction into algebraic operations, in
this paper we develop a method to characterize the synthetic channels as random
switching channels of binary symmetric channels when the underlying channels
are symmetric. Moreover, a lower bound for the average number of elements that
possess the same likelihood ratio within the output alphabet of any synthetic
channel generated in polar codes is also derived.

</details>


### [29] [On the use of information fusion techniques to improve information quality: Taxonomy, opportunities and challenges](https://arxiv.org/abs/2510.23230)
*Raúl Gutiérrez,Víctor Rampérez,Horacio Paggi,Juan A. Lara,Javier Soriano*

Main category: cs.IT

TL;DR: 本文对信息融合技术用于提升信息质量的研究进行了系统性文献综述，分析了不同融合方法对信息质量的影响、信息质量的表征与评估方法，并提出了未来研究挑战和方向。


<details>
  <summary>Details</summary>
Motivation: 信息融合领域近年来受到广泛关注，但现有文献综述多局限于特定问题或系统类型，缺乏对信息融合如何影响信息质量的系统性认识，特别是在信息质量表征、测量评估以及适应动态环境等方面存在研究空白。

Method: 采用文献综述方法，系统分析信息融合技术在不同应用领域中对信息质量提升的研究现状，重点关注信息质量的表征、测量和评估方法。

Result: 识别了信息融合技术对信息质量影响的关键问题，包括不同融合方法的效果、信息质量在不同应用领域的评估标准，以及融合过程适应性的重要性。

Conclusion: 本文为信息融合技术提升信息质量的研究提供了系统性分析框架，明确了未来研究的关键挑战和发展方向，填补了该领域的系统性知识空白。

Abstract: The information fusion field has recently been attracting a lot of interest
within the scientific community, as it provides, through the combination of
different sources of heterogeneous information, a fuller and/or more precise
understanding of the real world than can be gained considering the above
sources separately. One of the fundamental aims of computer systems, and
especially decision support systems, is to assure that the quality of the
information they process is high. There are many different approaches for this
purpose, including information fusion. Information fusion is currently one of
the most promising methods. It is particularly useful under circumstances where
quality might be compromised, for example, either intrinsically due to
imperfect information (vagueness, uncertainty) or because of limited resources
(energy, time). In response to this goal, a wide range of research has been
undertaken over recent years. To date, the literature reviews in this field
have focused on problem-specific issues and have been circumscribed to certain
system types. Therefore, there is no holistic and systematic knowledge of the
state of the art to help establish the steps to be taken in the future. In
particular, aspects like what impact different information fusion methods have
on information quality, how information quality is characterised, measured and
evaluated in different application domains depending on the problem data type
or whether fusion is designed as a flexible process capable of adapting to
changing system circumstances and their intrinsically limited resources have
not been addressed. This paper aims precisely to review the literature on
research into the use of information fusion techniques specifically to improve
information quality, analysing the above issues in order to identify a series
of challenges and research directions, which are presented in this paper.

</details>


### [30] [Pinching-antenna-enabled Federated Learning: Tail Latency, Participation, and Convergence Analysis](https://arxiv.org/abs/2510.23315)
*Yushen Lin,Zihan Chen,Zhiguo Ding*

Main category: cs.IT

TL;DR: 提出了一种捏合天线系统(PASS)，通过动态调整天线辐射器来缩短最差无线链路，从而解决联邦学习中的掉队者延迟问题，在同步和异步联邦学习中都能显著加速训练过程。


<details>
  <summary>Details</summary>
Motivation: 无线网络中联邦学习受到不可预测信道条件导致的掉队者延迟限制，需要从物理层根本解决这一问题。

Method: 采用捏合天线系统动态调整辐射器位置，结合物理层感知采样和误差反馈压缩技术，通过Lyapunov分析优化训练过程。

Result: 仿真显示PASS能显著缩短最差链路距离，提高按时完成概率，减少延迟尾部，在同步和异步联邦学习中都能实现持续的时钟加速。

Conclusion: 通过从物理层根源解决掉队者问题，PASS能够补充高层调度策略，有效加速无线联邦学习在同步和异步场景下的训练速度。

Abstract: Federated learning (FL) in wireless networks is limited by straggler delays
from unpredictable channel conditions. In this paper, we investigate the
pinching-antenna system (PASS), which dynamically 'pinches' the radiator along
a dielectric waveguide to shorten the worst links. In synchronous FL (SFL), we
prove that PASS shortens the worst-link distance, and it increases the on-time
completion probability in asynchronous FL (AFL). Accordingly, SFL exhibits
stochastic dominance on round time, while AFL yields explicit latency and
participation gains. We then pair physical-layer (PHY)-aware sampling with
error-feedback compression and prove that pinching raises the minimum inclusion
probability, thus shrinking both the sampling variability and
compression-induced floors in a Lyapunov analysis. Simulations demonstrate
consistent wall clock speedups and markedly shorter latency tails. By
addressing stragglers at their PHY root, PASS complements higher-layer
scheduling and accelerates wireless FL in both SFL and AFL.

</details>


### [31] [Efficient Repair of (k+2, k) Degraded Read Friendly MDS Array Codes With Sub-packetization 2](https://arxiv.org/abs/2510.23316)
*Jie Li,Xiaohu Tang*

Main category: cs.IT

TL;DR: 提出了两种具有两个奇偶校验节点的降级读取友好MDS阵列码构造，子分组化级别为2，适用于任意码长，在小有限域上实现。第一种构造在相同参数下达到最小修复带宽且渐近最优；第二种构造支持两种修复机制，分别优化修复带宽或重建访问。


<details>
  <summary>Details</summary>
Motivation: 设计高效的存储系统编码方案，在节点故障时最小化修复带宽和重建访问开销，同时保持MDS属性以确保数据可靠性。

Method: 第一种构造：基于小有限域的子分组化级别为2的MDS阵列码设计；第二种构造：提供两种修复机制选择 - 允许或不允许辅助节点内部计算，分别优化不同性能指标。

Result: 第一种构造在相同参数下达到最小修复带宽，且相对于Zhang等人的下界渐近最优；第二种构造在修复带宽和重建访问之间提供灵活权衡。

Conclusion: 所提出的两种DRF MDS阵列码构造在小有限域上实现了高效的节点修复性能，为存储系统提供了优化的编码解决方案。

Abstract: In this paper, we present two constructions of degraded read friendly (DRF)
MDS array codes with two parity nodes and a sub-packetization level of 2 over
small finite fields, applicable for any arbitrary code length. The first
construction achieves the smallest repair bandwidth among all existing
constructions with the same parameters, and is asymptotically optimal with
respect to the lower bound on the average repair bandwidth characterized by
Zhang et al. The second construction supports two repair mechanisms, depending
on whether computation within the helper nodes is permitted or not during the
node repair process, thereby optimizing either the repair bandwidth or the
rebuilding access.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [32] [Improving E-commerce Search with Category-Aligned Retrieval](https://arxiv.org/abs/2510.21711)
*Rauf Aliev*

Main category: cs.IR

TL;DR: 提出类别对齐检索系统(CARS)，通过预测查询类别并提升该类别产品来改善搜索相关性。虽然类别预测准确率显著提升，但简单集成会损害搜索相关性指标，需要自适应集成策略。


<details>
  <summary>Details</summary>
Motivation: 传统电商搜索系统存在用户查询与产品目录之间的语义鸿沟问题，需要更有效的检索方法。

Method: 使用可训练类别原型从查询嵌入中预测产品类别，然后提升该类别产品。评估了两种模型：all-MiniLM-L6-v2和OpenAI text-embedding-ada-002。

Result: OpenAI模型将Top-3类别预测准确率从零样本基线的43.8%提升到训练后的83.2%。但端到端模拟显示，简单应用类别提升会负面影响nDCG@10等搜索相关性指标。

Conclusion: 该方法具有价值，但由于数据集歧义性和检索系统对过度约束过滤的敏感性，需要置信度感知和自适应集成策略。

Abstract: Traditional e-commerce search systems often struggle with the semantic gap
between user queries and product catalogs. In this paper, we propose a
Category-Aligned Retrieval System (CARS) that improves search relevance by
first predicting the product category from a user's query and then boosting
products within that category. We introduce a novel method for creating
"Trainable Category Prototypes" from query embeddings. We evaluate this method
with two models: a lightweight all-MiniLM-L6-v2 and OpenAI's
text-embedding-ada-002. Our offline evaluation shows this method is highly
effective, with the OpenAI model increasing Top-3 category prediction accuracy
from a zero-shot baseline of 43.8% to 83.2% after training. The end-to-end
simulation, however, highlights the limitations of blindly applying category
boosts in a complex retrieval pipeline: while accuracy is high, naive
integration can negatively affect search relevance metrics such as nDCG@10. We
argue that this is partly due to dataset-specific ambiguities (e.g., polysemous
queries in the Amazon ESCI corpus) and partly due to the sensitivity of
retrieval systems to over-constraining filters. Crucially, these results do not
diminish the value of the approach; rather, they emphasize the need for
confidence-aware and adaptive integration strategies.

</details>


### [33] [DecoupleSearch: Decouple Planning and Search via Hierarchical Reward Modeling](https://arxiv.org/abs/2510.21712)
*Hao Sun,Zile Qiao,Bo Wang,Guoxin Chen,Yingyan Hou,Yong Jiang,Pengjun Xie,Fei Huang,Yan Zhang*

Main category: cs.IR

TL;DR: 提出了DecoupleSearch框架，通过双价值模型解耦规划和搜索过程，使用蒙特卡洛树搜索评估步骤质量，在推理时采用分层束搜索优化候选方案


<details>
  <summary>Details</summary>
Motivation: Agentic RAG面临三个主要挑战：步骤成功依赖高质量规划和准确搜索、中间推理步骤缺乏监督、规划和搜索的候选空间指数级增长

Method: 使用双价值模型解耦规划和搜索过程，构建推理树，利用蒙特卡洛树搜索评估步骤质量，推理时采用分层束搜索迭代优化候选方案

Result: 在不同参数规模的政策模型上进行广泛实验，证明了该方法的有效性

Conclusion: DecoupleSearch框架通过解耦规划和搜索过程，有效解决了Agentic RAG面临的挑战，提升了系统的灵活性和性能

Abstract: Retrieval-Augmented Generation (RAG) systems have emerged as a pivotal
methodology for enhancing Large Language Models (LLMs) through the dynamic
integration of external knowledge. To further improve RAG's flexibility,
Agentic RAG introduces autonomous agents into the workflow. However, Agentic
RAG faces several challenges: (1) the success of each step depends on both
high-quality planning and accurate search, (2) the lack of supervision for
intermediate reasoning steps, and (3) the exponentially large candidate space
for planning and searching. To address these challenges, we propose
DecoupleSearch, a novel framework that decouples planning and search processes
using dual value models, enabling independent optimization of plan reasoning
and search grounding. Our approach constructs a reasoning tree, where each node
represents planning and search steps. We leverage Monte Carlo Tree Search to
assess the quality of each step. During inference, Hierarchical Beam Search
iteratively refines planning and search candidates with dual value models.
Extensive experiments across policy models of varying parameter sizes,
demonstrate the effectiveness of our method.

</details>


### [34] [asLLR: LLM based Leads Ranking in Auto Sales](https://arxiv.org/abs/2510.21713)
*Yin Sun,Yiwen Liu,Junjie Song,Chenyu Zhang,Xinyuan Zhang,Lingjie Liu,Siqi Chen,Yuji Cao*

Main category: cs.IR

TL;DR: 本文提出asLLR模型，结合CTR损失和QA损失，在解码器架构的大语言模型中同时建模表格和自然语言特征，用于汽车销售线索排序，显著提升销售效率和业绩。


<details>
  <summary>Details</summary>
Motivation: 传统CTR预测方法难以处理CRM系统中的复杂自然语言交互特征，限制了销售线索排序的效果，需要新的方法来提升商业智能和决策效率。

Method: 提出asLLR模型，在解码器架构的大语言模型中集成CTR损失和问答损失，同时建模表格数据和自然语言特征。

Result: 在30万训练样本和4万测试样本的数据集上，asLLR达到AUC 0.8127，比传统CTR方法提升0.0231；在线A/B测试显示销售业绩提升约9.5%。

Conclusion: asLLR有效建模商业数据中的复杂模式，为商业智能和运营决策提供了有价值的工具，显著提升销售系统效率。

Abstract: In the area of commercial auto sales system, high-quality lead score
sequencing determines the priority of a sale's work and is essential for
optimizing the efficiency of the sales system. Since CRM (Customer Relationship
Management) system contains plenty of textual interaction features between
sales and customers, traditional techniques such as Click Through Rate (CTR)
prediction struggle with processing the complex information inherent in natural
language features, which limits their effectiveness in sales lead ranking.
Bridging this gap is critical for enhancing business intelligence and
decision-making. Recently, the emergence of large language models (LLMs) has
opened new avenues for improving recommendation systems, this study introduces
asLLR (LLM-based Leads Ranking in Auto Sales), which integrates CTR loss and
Question Answering (QA) loss within a decoder-only large language model
architecture. This integration enables the simultaneous modeling of both
tabular and natural language features. To verify the efficacy of asLLR, we
constructed an innovative dataset derived from the customer lead pool of a
prominent new energy vehicle brand, with 300,000 training samples and 40,000
testing samples. Our experimental results demonstrate that asLLR effectively
models intricate patterns in commercial datasets, achieving the AUC of 0.8127,
surpassing traditional CTR estimation methods by 0.0231. Moreover, asLLR
enhances CTR models when used for extracting text features by 0.0058. In
real-world sales scenarios, after rigorous online A/B testing, asLLR increased
the sales volume by about 9.5% compared to the traditional method, providing a
valuable tool for business intelligence and operational decision-making.

</details>


### [35] [Practice on Long Behavior Sequence Modeling in Tencent Advertising](https://arxiv.org/abs/2510.21714)
*Xian Hu,Ming Yue,Zhixiang Feng,Junwei Pan,Junjie Zhai,Ximei Wang,Xinrui Miao,Qian Li,Xun Liu,Shangyu Zhang,Letian Wang,Hua Lu,Zijian Zeng,Chen Cai,Wei Wang,Fei Xiong,Pengfei Xiong,Jintao Zhang,Zhiyuan Wu,Chunhui Zhang,Anan Liu,Jiulong You,Chao Deng,Yuekui Yang,Shudong Huang,Dapeng Liu,Haijie Gu*

Main category: cs.IR

TL;DR: 提出了一种跨域长序列建模方法，通过整合广告域和内容域的用户行为构建统一商业行为轨迹，解决了特征分类差异、字段间干扰和广告目标间干扰等挑战。


<details>
  <summary>Details</summary>
Motivation: 广告域用户行为稀疏，单一广告域数据难以构建长行为序列，需要跨域整合用户行为来捕捉长期偏好。

Method: 采用两阶段框架：搜索阶段使用分层硬搜索和解耦嵌入软搜索；序列建模阶段引入解耦侧信息时序兴趣网络、目标解耦位置编码、目标解耦SASRec和堆叠TIN。

Result: 在腾讯大规模广告平台部署后，微信视频号GMV提升4.22%，微信朋友圈GMV提升1.96%。

Conclusion: 跨域行为整合和提出的解耦方法有效解决了长序列建模中的关键挑战，显著提升了广告效果。

Abstract: Long-sequence modeling has become an indispensable frontier in recommendation
systems for capturing users' long-term preferences. However, user behaviors
within advertising domains are inherently sparse, posing a significant barrier
to constructing long behavioral sequences using data from a single advertising
domain alone. This motivates us to collect users' behaviors not only across
diverse advertising scenarios, but also beyond the boundaries of the
advertising domain into content domains-thereby constructing unified commercial
behavior trajectories. This cross-domain or cross-scenario integration gives
rise to the following challenges: (1) feature taxonomy gaps between distinct
scenarios and domains, (2) inter-field interference arising from irrelevant
feature field pairs, and (3) target-wise interference in temporal and semantic
patterns when optimizing for different advertising targets. To address these
challenges, we propose several practical approaches within the two-stage
framework for long-sequence modeling. In the first (search) stage, we design a
hierarchical hard search method for handling complex feature taxonomy
hierarchies, alongside a decoupled embedding-based soft search to alleviate
conflicts between attention mechanisms and feature representation. In the
second (sequence modeling) stage, we introduce: (a) Decoupled Side Information
Temporal Interest Networks (TIN) to mitigate inter-field conflicts; (b)
Target-Decoupled Positional Encoding and Target-Decoupled SASRec to address
target-wise interference; and (c) Stacked TIN to model high-order behavioral
correlations. Deployed in production on Tencent's large-scale advertising
platforms, our innovations delivered significant performance gains: an overall
4.22% GMV lift in WeChat Channels and an overall 1.96% GMV increase in WeChat
Moments.

</details>


### [36] [Words to Waves: Emotion-Adaptive Music Recommendation System](https://arxiv.org/abs/2510.21724)
*Apoorva Chavali,Reeve Menezes*

Main category: cs.IR

TL;DR: 提出了一种基于Wide and Deep Learning架构的音乐推荐系统，通过实时情感状态推断来推荐匹配用户情绪的音乐。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统往往忽视情感上下文，过度依赖历史收听模式或静态情绪标签，无法捕捉用户的实时情感需求。

Method: 使用基于transformer的嵌入来从用户文本描述中提取情感上下文，通过微调预测情感维度（效价-唤醒度）。架构包含深度组件（泛化未见情感模式）和宽度组件（记忆用户-情感和情感-流派关联）。

Result: 实验结果显示个性化音乐选择对用户情绪产生积极影响，并在情感相关性方面带来显著提升。

Conclusion: 该框架通过整合实时情感上下文，有效提升了音乐推荐的情感相关性和用户体验。

Abstract: Current recommendation systems often tend to overlook emotional context and
rely on historical listening patterns or static mood tags. This paper
introduces a novel music recommendation framework employing a variant of Wide
and Deep Learning architecture that takes in real-time emotional states
inferred directly from natural language as inputs and recommends songs that
closely portray the mood. The system captures emotional contexts from
user-provided textual descriptions by using transformer-based embeddings, which
were finetuned to predict the emotional dimensions of valence-arousal. The deep
component of the architecture utilizes these embeddings to generalize unseen
emotional patterns, while the wide component effectively memorizes user-emotion
and emotion-genre associations through cross-product features. Experimental
results show that personalized music selections positively influence the user's
emotions and lead to a significant improvement in emotional relevance.

</details>


### [37] [From Authors to Reviewers: Leveraging Rankings to Improve Peer Review](https://arxiv.org/abs/2510.21726)
*Weichen Wang,Chengchun Shi*

Main category: cs.IR

TL;DR: 本文提出了一种替代Su等人(2025)的方法，利用审稿人而非作者的排名信息来评估论文质量，结果表明结合审稿人和作者的排名信息能获得最准确的评估结果。


<details>
  <summary>Details</summary>
Motivation: 机器学习会议评审质量因投稿数量激增而备受关注，需要更有效的论文质量评估方法。

Method: 模拟2023年ICML会议投稿数据，利用审稿人的排名信息作为替代方法，并与作者排名信息进行比较和结合。

Result: (i) 使用审稿人排名信息能显著改善论文质量评估，通常优于仅使用作者排名；(ii) 结合审稿人和作者的排名信息在大多数情况下能获得最准确的评估。

Conclusion: 结合审稿人和作者双方的排名信息是评估机器学习会议论文质量的最有效方法。

Abstract: This paper is a discussion of the 2025 JASA discussion paper by Su et al.
(2025). We would like to congratulate the authors on conducting a comprehensive
and insightful empirical investigation of the 2023 ICML ranking data. The
review quality of machine learning (ML) conferences has become a big concern in
recent years, due to the rapidly growing number of submitted manuscripts. In
this discussion, we propose an approach alternative to Su et al. (2025) that
leverages ranking information from reviewers rather than authors. We simulate
review data that closely mimics the 2023 ICML conference submissions. Our
results show that (i) incorporating ranking information from reviewers can
significantly improve the evaluation of each paper's quality, often
outperforming the use of ranking information from authors alone; and (ii)
combining ranking information from both reviewers and authors yields the most
accurate evaluation of submitted papers in most scenarios.

</details>


### [38] [Your Dense Retriever is Secretly an Expeditious Reasoner](https://arxiv.org/abs/2510.21727)
*Yichi Zhang,Jun Bai,Zhixin Cai,Shuhan Qin,Zhuofan Chen,Jinghua Guan,Wenge Rong*

Main category: cs.IR

TL;DR: AdaQR是一种混合查询重写框架，通过Reasoner Router动态选择快速密集推理或深度LLM推理，在保持检索性能的同时降低28%的推理成本。


<details>
  <summary>Details</summary>
Motivation: 密集检索器在处理推理密集型查询时表现不佳，而LLM虽然能重写查询但计算成本高，需要平衡效率与准确性。

Method: 提出自适应查询推理框架，包含Reasoner Router动态路由查询，Dense Reasoner在嵌入空间直接执行LLM风格推理。

Result: 在BRIGHT大规模检索基准测试中，推理成本降低28%，检索性能提升7%。

Conclusion: AdaQR框架成功实现了效率与准确性的可控权衡，在降低计算成本的同时保持或提升检索性能。

Abstract: Dense retrievers enhance retrieval by encoding queries and documents into
continuous vectors, but they often struggle with reasoning-intensive queries.
Although Large Language Models (LLMs) can reformulate queries to capture
complex reasoning, applying them universally incurs significant computational
cost. In this work, we propose Adaptive Query Reasoning (AdaQR), a hybrid query
rewriting framework. Within this framework, a Reasoner Router dynamically
directs each query to either fast dense reasoning or deep LLM reasoning. The
dense reasoning is achieved by the Dense Reasoner, which performs LLM-style
reasoning directly in the embedding space, enabling a controllable trade-off
between efficiency and accuracy. Experiments on large-scale retrieval
benchmarks BRIGHT show that AdaQR reduces reasoning cost by 28% while
preserving-or even improving-retrieval performance by 7%.

</details>


### [39] [Modeling Bias Evolution in Fashion Recommender Systems: A System Dynamics Approach](https://arxiv.org/abs/2510.21728)
*Mahsa Goodarzi,M. Abdullah Canbaz*

Main category: cs.IR

TL;DR: 本研究采用动态建模方法分析时尚推荐系统中的偏见激活和强化机制，发现归纳偏见比用户偏见对系统结果影响更大，现有去偏策略需要进一步改进。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的偏见不仅扭曲用户体验，还会延续和放大现有社会刻板印象，特别是在时尚电商领域，需要深入理解偏见机制以促进公平性。

Method: 采用系统动力学建模和实验模拟，分析偏见的时序演变及其对系统性能的多方面影响。

Result: 研究发现归纳偏见对系统结果的影响比用户偏见更显著，当前的数据再平衡和算法正则化等去偏策略虽然有效但需要进一步改进。

Conclusion: 需要推进去偏策略并扩展系统边界以纳入用户人口统计和物品多样性等更广泛情境因素，在推荐系统设计中采取主动方法以对抗偏见传播并确保公平用户体验。

Abstract: Bias in recommender systems not only distorts user experience but also
perpetuates and amplifies existing societal stereotypes, particularly in
sectors like fashion e-commerce. This study employs a dynamic modeling approach
to scrutinize the mechanisms of bias activation and reinforcement within
Fashion Recommender Systems (FRS). By leveraging system dynamics modeling and
experimental simulations, we dissect the temporal evolution of bias and its
multifaceted impacts on system performance. Our analysis reveals that inductive
biases exert a more substantial influence on system outcomes than user biases,
suggesting critical areas for intervention. We demonstrate that while current
debiasing strategies, including data rebalancing and algorithmic
regularization, are effective to an extent, they require further enhancement to
comprehensively mitigate biases. This research underscores the necessity for
advancing these strategies and extending system boundaries to incorporate
broader contextual factors such as user demographics and item diversity, aiming
to foster inclusivity and fairness in FRS. The findings advocate for a
proactive approach in recommender system design to counteract bias propagation
and ensure equitable user experiences.

</details>


### [40] [CustomIR: Unsupervised Fine-Tuning of Dense Embeddings for Known Document Corpora](https://arxiv.org/abs/2510.21729)
*Nathan Paull*

Main category: cs.IR

TL;DR: CustomIR框架通过使用LLM生成的查询-文档对和硬负例，对预训练语言嵌入模型进行无监督领域自适应，显著提升在专业语料上的检索性能，让小模型能达到大模型的性能水平。


<details>
  <summary>Details</summary>
Motivation: 密集嵌入模型在专业语料上的性能会下降，需要一种无需人工标注的领域自适应方法来提升检索效果。

Method: 利用大语言模型生成基于目标语料的多样化查询，并配合LLM验证的硬负例，进行无监督的微调。

Result: 在企业邮件和消息数据集上，CustomIR将小模型的Recall@10提升了2.3个百分点，使其性能可媲美大模型。

Conclusion: 定向合成微调为提升领域特定性能提供了一种可扩展且成本效益高的策略。

Abstract: Dense embedding models have become critical for modern information retrieval,
particularly in RAG pipelines, but their performance often degrades when
applied to specialized corpora outside their pre-training distribution. To
address thi we introduce \textbf{CustomIR}, a framework for unsupervised
adaptation of pre-trained language embedding models to domain-specific corpora
using synthetically generated query-document pairs. CustomIR leverages large
language models (LLMs) to create diverse queries grounded in a known target
corpus, paired with LLM-verified hard negatives, eliminating the need for
costly human annotation. Experiments on enterprise email and messaging datasets
show that CustomIR consistently improves retrieval effectiveness with small
models gaining up to 2.3 points in Recall@10. This performance increase allows
these small models to rival the performance of much larger alternatives,
allowing for cheaper RAG deployments. These results highlight that targeted
synthetic fine-tuning offers a scalable and cost-efficient strategy for
increasing domain-specific performance.

</details>


### [41] [TriMat: Context-aware Recommendation by Tri-Matrix Factorization](https://arxiv.org/abs/2510.21730)
*Hao Wang*

Main category: cs.IR

TL;DR: 本文使用三矩阵分解技术将上下文信息融入矩阵分解框架，证明该技术能有效提高推荐系统的准确性和公平性指标。


<details>
  <summary>Details</summary>
Motivation: 上下文感知推荐系统（CARS）长期以来主要停留在理论研究层面，在现实应用中的进展有限。本文旨在解决这一实际问题，将上下文信息有效整合到推荐系统中。

Method: 采用三矩阵分解技术，在传统矩阵分解框架中融入上下文信息，构建包含用户、物品和上下文的三维模型。

Result: 实验证明该方法在准确性和公平性指标上均有显著提升，验证了三矩阵分解技术的有效性。

Conclusion: 三矩阵分解技术能够成功地将上下文信息整合到推荐系统中，为上下文感知推荐系统的实际应用提供了可行的解决方案。

Abstract: Search engine is the symbolic technology of Web 2.0, and many people used to
believe recommender systems is the new frontier of Web 3.0. In the past 10
years, with the advent of TikTok and similar apps, recommender systems has
materialized the vision of the machine learning pioneers. However, many
research topics of the field remain unfixed until today. One such topic is CARS
(Context-aware Recommender Systems) , which is largely a theoretical topic
without much advance in real-world applications. In this paper, we utilize
tri-matrix factorization technique to incorporate contextual information into
our matrix factorization framework, and prove that our technique is effective
in improving both the accuracy and fairness metrics in our experiments.

</details>


### [42] [Augmenting Researchy Questions with Sub-question Judgments](https://arxiv.org/abs/2510.21733)
*Jia-Huei Ju,Eugene Yang,Trevor Adriaanse,Andrew Yates*

Main category: cs.IR

TL;DR: 本研究扩展了Researchy Questions数据集，使用Llama3.3 70B模型为每个子问题添加了LLM判断的标签，旨在为训练更好支持复杂信息需求的检索模型提供资源。


<details>
  <summary>Details</summary>
Motivation: Researchy Questions数据集包含约10万个需要检索多个方面信息的复杂问题查询，每个查询都有GPT-4生成的子问题，但缺乏子问题与相关文档的关联标签。

Method: 使用Llama3.3 70B模型为数据集中的每个子问题生成LLM判断的标签。

Result: 成功扩展了Researchy Questions数据集，为所有子问题添加了标注信息。

Conclusion: 这些子问题标签可以作为训练检索模型的资源，帮助模型更好地支持复杂信息需求。

Abstract: The Researchy Questions dataset provides about 100k question queries with
complex information needs that require retrieving information about several
aspects of a topic. Each query in ResearchyQuestions is associated with
sub-questions that were produced by prompting GPT-4. While ResearchyQuestions
contains labels indicating what documents were clicked after issuing the query,
there are no associations in the dataset between sub-questions and relevant
documents. In this work, we augment the Researchy Questions dataset with
LLM-judged labels for each sub-question using a Llama3.3 70B model. We intend
these sub-question labels to serve as a resource for training retrieval models
that better support complex information needs.

</details>


### [43] [From Factoid Questions to Data Product Requests: Benchmarking Data Product Discovery over Tables and Text](https://arxiv.org/abs/2510.21737)
*Liangliang Zhang,Nandana Mihindukulasooriya,Niharika S. D'Souza,Sola Shirai,Sarthak Dash,Yao Ma,Horst Samulowitz*

Main category: cs.IR

TL;DR: DPBench是首个用户请求驱动的数据产品基准测试，通过重新组织现有表格-文本QA数据集，将相关表格和段落聚类为连贯的数据产品，生成专业级分析请求，并使用多LLM评估验证基准质量。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门用于数据产品发现的基准测试，现有数据集主要关注单个表格的事实问答，而非收集多个数据资产构建更广泛、连贯的数据产品。

Method: 系统性地重新利用现有表格-文本QA数据集，将相关表格和段落聚类为连贯的数据产品，生成专业级分析请求，并通过多LLM评估验证基准质量。

Result: 建立了DPBench基准，保留了完整的数据来源，产生了可操作的分析师级数据产品请求。基线实验证明了数据产品请求评估的可行性，揭示了当前方法的局限性。

Conclusion: DPBench为自动数据产品发现研究提供了新的基准和机会，填补了现有基准在数据产品发现方面的空白。

Abstract: Data products are reusable, self-contained assets designed for specific
business use cases. Automating their discovery and generation is of great
industry interest, as it enables discovery in large data lakes and supports
analytical Data Product Requests (DPRs). Currently, there is no benchmark
established specifically for data product discovery. Existing datasets focus on
answering single factoid questions over individual tables rather than
collecting multiple data assets for broader, coherent products. To address this
gap, we introduce DPBench, the first user-request-driven data product benchmark
over hybrid table-text corpora. Our framework systematically repurposes
existing table-text QA datasets by clustering related tables and passages into
coherent data products, generating professional-level analytical requests that
span both data sources, and validating benchmark quality through multi-LLM
evaluation. DPBench preserves full provenance while producing actionable,
analyst-like data product requests. Baseline experiments with hybrid retrieval
methods establish the feasibility of DPR evaluation, reveal current
limitations, and point to new opportunities for automatic data product
discovery research.
  Code and datasets are available at:
https://anonymous.4open.science/r/data-product-benchmark-BBA7/

</details>


### [44] [DiffGRM: Diffusion-based Generative Recommendation Model](https://arxiv.org/abs/2510.21805)
*Zhao Liu,Yichen Zhu,Yiqing Yang,Guoping Tang,Rui Huang,Qiang Luo,Xiao Lv,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: DiffGRM是一种基于扩散的生成推荐模型，用掩码离散扩散模型替代自回归解码器，解决了语义ID中数字间一致性和异质性问题，在多个数据集上显著提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 传统生成推荐使用自回归模型预测语义ID，但存在两个结构性问题：1) 数字间一致性 - 左到右因果性阻碍双向交叉数字证据；2) 数字间异质性 - 不同数字语义粒度和可预测性不同，但统一目标函数对所有数字同等对待。

Method: 提出DiffGRM模型：1) 使用并行语义编码解耦数字并平衡每数字信息；2) 采用策略相干噪声训练，通过相干掩码优先处理不确定数字；3) 置信度引导并行去噪推理，先填充高置信度数字并生成多样Top-K候选。

Result: 在多个数据集上实验显示，相比强生成和判别推荐基线，NDCG@10提升6.9%-15.5%。

Conclusion: 扩散模型能有效解决语义ID中的结构性问题，通过双向上下文和任意顺序并行生成实现更好的推荐性能。

Abstract: Generative recommendation (GR) is an emerging paradigm that represents each
item via a tokenizer as an n-digit semantic ID (SID) and predicts the next item
by autoregressively generating its SID conditioned on the user's history.
However, two structural properties of SIDs make ARMs ill-suited. First,
intra-item consistency: the n digits jointly specify one item, yet the
left-to-right causality trains each digit only under its prefix and blocks
bidirectional cross-digit evidence, collapsing supervision to a single causal
path. Second, inter-digit heterogeneity: digits differ in semantic granularity
and predictability, while the uniform next-token objective assigns equal weight
to all digits, overtraining easy digits and undertraining hard digits. To
address these two issues, we propose DiffGRM, a diffusion-based GR model that
replaces the autoregressive decoder with a masked discrete diffusion model
(MDM), thereby enabling bidirectional context and any-order parallel generation
of SID digits for recommendation. Specifically, we tailor DiffGRM in three
aspects: (1) tokenization with Parallel Semantic Encoding (PSE) to decouple
digits and balance per-digit information; (2) training with On-policy Coherent
Noising (OCN) that prioritizes uncertain digits via coherent masking to
concentrate supervision on high-value signals; and (3) inference with
Confidence-guided Parallel Denoising (CPD) that fills higher-confidence digits
first and generates diverse Top-K candidates. Experiments show consistent gains
over strong generative and discriminative recommendation baselines on multiple
datasets, improving NDCG@10 by 6.9%-15.5%. Code is available at
https://github.com/liuzhao09/DiffGRM.

</details>


### [45] [Unifying Inductive, Cross-Domain, and Multimodal Learning for Robust and Generalizable Recommendation](https://arxiv.org/abs/2510.21812)
*Chanyoung Chung,Kyeongryul Lee,Sunbin Park,Joyce Jiyoung Whang*

Main category: cs.IR

TL;DR: MICRec是一个统一的推荐系统框架，通过融合归纳建模、多模态引导和跨域迁移来在异构和不完整的现实数据中捕捉用户上下文和潜在偏好。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统主要关注单个方面，难以处理日常消费中跨多个领域的复杂推荐场景，需要统一框架来解决异构数据、数据稀疏性和泛化能力问题。

Method: 基于INMO的归纳骨干网络，通过基于模态的聚合来精化表达表示，并利用跨域重叠用户作为锚点来缓解数据稀疏性问题。

Result: 实验表明MICRec在12个基线方法上表现优异，在训练数据有限的领域取得了显著提升。

Conclusion: MICRec通过统一融合归纳建模、多模态引导和跨域迁移，实现了鲁棒且可泛化的推荐系统。

Abstract: Recommender systems have long been built upon the modeling of interactions
between users and items, while recent studies have sought to broaden this
paradigm by generalizing to new users and items, incorporating diverse
information sources, and transferring knowledge across domains. Nevertheless,
these efforts have largely focused on individual aspects, hindering their
ability to tackle the complex recommendation scenarios that arise in daily
consumptions across diverse domains. In this paper, we present MICRec, a
unified framework that fuses inductive modeling, multimodal guidance, and
cross-domain transfer to capture user contexts and latent preferences in
heterogeneous and incomplete real-world data. Moving beyond the inductive
backbone of INMO, our model refines expressive representations through
modality-based aggregation and alleviates data sparsity by leveraging
overlapping users as anchors across domains, thereby enabling robust and
generalizable recommendation. Experiments show that MICRec outperforms 12
baselines, with notable gains in domains with limited training data.

</details>


### [46] [Development of an Automated Web Application for Efficient Web Scraping: Design and Implementation](https://arxiv.org/abs/2510.21831)
*Alok Dutta,Nilanjana Roy,Rhythm Sen,Sougata Dutta,Prabhat Das*

Main category: cs.IR

TL;DR: 本文介绍了一个用户友好的自动化网络爬虫应用，将复杂的网络爬虫过程简化为获取、提取和执行三个阶段，使非技术用户也能轻松进行数据采集。


<details>
  <summary>Details</summary>
Motivation: 解决传统网络爬虫工具对非技术用户过于复杂的问题，让各个技术水平的用户都能方便地采集和管理网络数据。

Method: 使用Flask框架开发，通过requests库获取HTML内容，BeautifulSoup和正则表达式提取数据，MongoDB存储用户信息和爬取历史，最终将数据转换为CSV等格式。

Result: 开发出了一个功能完整、易于使用的网络爬虫工具，用户只需输入网址和定义提取参数即可获取结构化数据，无需编程知识。

Conclusion: 该工具显著提高了网络爬虫的效率和可访问性，使数据采集技术民主化，为非技术用户提供了强大的数据获取能力。

Abstract: This paper presents the design and implementation of a user-friendly,
automated web application that simplifies and optimizes the web scraping
process for non-technical users. The application breaks down the complex task
of web scraping into three main stages: fetching, extraction, and execution. In
the fetching stage, the application accesses target websites using the HTTP
protocol, leveraging the requests library to retrieve HTML content. The
extraction stage utilizes powerful parsing libraries like BeautifulSoup and
regular expressions to extract relevant data from the HTML. Finally, the
execution stage structures the data into accessible formats, such as CSV,
ensuring the scraped content is organized for easy use. To provide personalized
and secure experiences, the application includes user registration and login
functionalities, supported by MongoDB, which stores user data and scraping
history. Deployed using the Flask framework, the tool offers a scalable, robust
environment for web scraping. Users can easily input website URLs, define data
extraction parameters, and download the data in a simplified format, without
needing technical expertise. This automated tool not only enhances the
efficiency of web scraping but also democratizes access to data extraction by
empowering users of all technical levels to gather and manage data tailored to
their needs. The methodology detailed in this paper represents a significant
advancement in making web scraping tools accessible, efficient, and easy to use
for a broader audience.

</details>


### [47] [Temporal Graph Theoretic Analysis of Geopolitical Dynamics in the U.S. Entity List](https://arxiv.org/abs/2510.21962)
*Yunsen Lei,Kexin Bai,Quan Li,H. Howie Huang*

Main category: cs.IR

TL;DR: 本文提出了一个新颖的时间图框架，将美国实体清单从静态注册表转化为动态的地缘政治战略表征，揭示了出口管控的动态模式。


<details>
  <summary>Details</summary>
Motivation: 美国实体清单已成为最显著的经济治国工具，但其动态机制仍未被充分探索。现有研究缺乏对实体清单随时间演变的系统性分析。

Method: 构建了首个基于事件的美国政府外国实体指定数据集，将其建模为时间二分图，并开发了多层次分析方法。

Result: 应用于25年数据，该框架揭示了静态视图无法捕捉的升级、持续性和协调性动态模式。

Conclusion: 时间图分析为出口管控的地缘政治动态提供了系统的计算洞察，展示了该方法在研究经济治国工具演化中的价值。

Abstract: Export controls have become one of America's most prominent tools of economic
statecraft. They aim to block rival countries' access to sensitive
technologies, safeguard U.S. supply chains, protect national security, and
shape geopolitical competition. Among various instruments, the U.S. Entity List
has emerged as the most salient, yet its dynamics remain underexplored. This
paper introduces a novel temporal graph framework that transforms the Entity
List documents from a static registry of foreign entities of concern into a
dynamic representation of geopolitical strategy. We construct the first
event-based dataset of U.S. government foreign entity designations and model
them as a temporal bipartite graph. Building on this representation, we develop
a multi-level analytical approach that reveals shifting roles, enforcement
strategy, and broader sanction ecosystems. Applied to 25 years of data, the
framework uncovers dynamic patterns of escalation, persistence, and
coordination that static views cannot capture. More broadly, our study
demonstrates how temporal graph analysis offers systematic computational
insights into the geopolitical dynamics of export controls.

</details>


### [48] [Multimodal Item Scoring for Natural Language Recommendation via Gaussian Process Regression with LLM Relevance Judgments](https://arxiv.org/abs/2510.22023)
*Yifan Liu,Qianfeng Wen,Jiazhou Liang,Mark Zhao,Justin Cui,Anton Korikov,Armin Torogh,Junyoung Kim,Scott Sanner*

Main category: cs.IR

TL;DR: 提出GPR-LLM方法，使用高斯过程回归结合LLM相关性判断来改进自然语言推荐系统，能更好地建模多模态相关性评分函数。


<details>
  <summary>Details</summary>
Motivation: 现有NLRec方法使用密集检索，将用户请求视为唯一相关性标签，导致单模态评分函数，这通常是查询相关性的弱代理。需要更好地捕捉复杂NLRec数据中可能出现的多模态相关性评分函数分布。

Method: 使用高斯过程回归（GPR）结合LLM对候选文本子集的相关性判断，采用RBF核函数来建模多模态相关性评分函数。

Result: 在四个NLRec数据集和两个LLM骨干网络上的实验表明，GPR-LLM使用RBF核（能建模多模态相关性评分函数）持续优于简单的单模态核（点积、余弦相似度），以及包括DR、交叉编码器和基于LLM的点式相关性评分在内的基线方法，性能提升高达65%。

Conclusion: GPR-LLM在最小LLM标注预算内为NLRec提供了一种高效有效的方法。

Abstract: Natural Language Recommendation (NLRec) generates item suggestions based on
the relevance between user-issued NL requests and NL item description passages.
Existing NLRec approaches often use Dense Retrieval (DR) to compute item
relevance scores from aggregation of inner products between user request
embeddings and relevant passage embeddings. However, DR views the request as
the sole relevance label, thus leading to a unimodal scoring function centered
on the query embedding that is often a weak proxy for query relevance. To
better capture the potential multimodal distribution of the relevance scoring
function that may arise from complex NLRec data, we propose GPR-LLM that uses
Gaussian Process Regression (GPR) with LLM relevance judgments for a subset of
candidate passages. Experiments on four NLRec datasets and two LLM backbones
demonstrate that GPR-LLM with an RBF kernel, capable of modeling multimodal
relevance scoring functions, consistently outperforms simpler unimodal kernels
(dot product, cosine similarity), as well as baseline methods including DR,
cross-encoder, and pointwise LLM-based relevance scoring by up to 65%. Overall,
GPR-LLM provides an efficient and effective approach to NLRec within a minimal
LLM labeling budget.

</details>


### [49] [Massive Memorization with Hundreds of Trillions of Parameters for Sequential Transducer Generative Recommenders](https://arxiv.org/abs/2510.22049)
*Zhimin Chen,Chenyu Zhao,Ka Chun Mo,Yunjiang Jiang,Jane H. Lee,Shouwei Chen,Khushhall Chandra Mahajan,Ning Jiang,Kai Ren,Jinhui Li,Wen-Yun Yang*

Main category: cs.IR

TL;DR: 提出了VISTA框架，将传统目标注意力分解为两个阶段：用户历史摘要和候选物品注意力，解决了工业推荐系统中超长用户历史序列的延迟和成本问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理超长用户历史序列（10k到100k项）时面临延迟、QPS和GPU成本的工业可扩展性问题，无法满足大规模推荐系统的需求。

Method: 采用两阶段建模框架：首先将用户历史摘要为几百个token，然后候选物品对这些token进行注意力计算，摘要token嵌入可缓存并作为序列特征用于下游训练和推理。

Result: VISTA能够扩展到终身用户历史（高达100万项），同时保持下游训练和推理成本固定，在离线和在线指标上取得显著改进，已在服务数十亿用户的工业推荐平台成功部署。

Conclusion: VISTA框架有效解决了工业推荐系统中超长用户历史序列的可扩展性问题，实现了性能提升和成本控制的平衡。

Abstract: Modern large-scale recommendation systems rely heavily on user interaction
history sequences to enhance the model performance. The advent of large
language models and sequential modeling techniques, particularly
transformer-like architectures, has led to significant advancements recently
(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories
(10k to 100k items) generally improves model performance, it also creates
significant challenges on latency, queries per second (QPS) and GPU cost in
industry-scale recommendation systems. Existing models do not adequately
address these industrial scalability issues. In this paper, we propose a novel
two-stage modeling framework, namely VIrtual Sequential Target Attention
(VISTA), which decomposes traditional target attention from a candidate item to
user history items into two distinct stages: (1) user history summarization
into a few hundred tokens; followed by (2) candidate item attention to those
tokens. These summarization token embeddings are then cached in storage system
and then utilized as sequence features for downstream model training and
inference. This novel design for scalability enables VISTA to scale to lifelong
user histories (up to one million items) while keeping downstream training and
inference costs fixed, which is essential in industry. Our approach achieves
significant improvements in offline and online metrics and has been
successfully deployed on an industry leading recommendation platform serving
billions of users.

</details>


### [50] [A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim Decomposition](https://arxiv.org/abs/2510.22055)
*V Venktesh,Deepali Prabhu,Avishek Anand*

Main category: cs.IR

TL;DR: 提出了QuanTemp++数据集，包含自然数值声明、开放领域语料库和相关证据，通过模拟人类事实核查员的声明分解过程收集证据，避免时间泄漏问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动事实核查方法主要不关注自然数值声明，且现有基准使用启发式声明分解方法和弱监督网络搜索收集证据，导致证据相关性低、来源嘈杂且存在时间泄漏问题。

Method: 通过近似模拟人类事实核查员的声明分解过程收集证据，构建包含自然数值声明和相关证据的数据集，并分析不同声明分解范式的检索性能。

Result: 创建了QuanTemp++数据集，包含自然数值声明和相关证据，避免了时间泄漏问题，并分析了不同声明分解范式对检索性能的影响。

Conclusion: QuanTemp++数据集为开发包含人类事实核查员技能的自动化方法提供了更真实的环境，并揭示了声明分解范式对验证流程结果的影响。

Abstract: Fact-checking numerical claims is critical as the presence of numbers provide
mirage of veracity despite being fake potentially causing catastrophic impacts
on society. The prior works in automatic fact verification do not primarily
focus on natural numerical claims. A typical human fact-checker first retrieves
relevant evidence addressing the different numerical aspects of the claim and
then reasons about them to predict the veracity of the claim. Hence, the search
process of a human fact-checker is a crucial skill that forms the foundation of
the verification process. Emulating a real-world setting is essential to aid in
the development of automated methods that encompass such skills. However,
existing benchmarks employ heuristic claim decomposition approaches augmented
with weakly supervised web search to collect evidences for verifying claims.
This sometimes results in less relevant evidences and noisy sources with
temporal leakage rendering a less realistic retrieval setting for claim
verification. Hence, we introduce QuanTemp++: a dataset consisting of natural
numerical claims, an open domain corpus, with the corresponding relevant
evidence for each claim. The evidences are collected through a claim
decomposition process approximately emulating the approach of human
fact-checker and veracity labels ensuring there is no temporal leakage. Given
this dataset, we also characterize the retrieval performance of key claim
decomposition paradigms. Finally, we observe their effect on the outcome of the
verification pipeline and draw insights. The code for data pipeline along with
link to data can be found at https://github.com/VenkteshV/QuanTemp_Plus

</details>


### [51] [Scaling Up Efficient Small Language Models Serving and Deployment for Semantic Job Search](https://arxiv.org/abs/2510.22101)
*Kayhan Behdin,Qingquan Song,Sriram Vasudevan,Jian Sheng,Xiaojing Ma,Z Zhou,Chuanrui Zhu,Guoyao Li,Chanh Nguyen,Sayan Ghosh,Hejian Sang,Ata Fatahi Baarzi,Sundara Raman Ramachandran,Xiaoqing Wang,Qing Lan,Vinay Y S,Qi Guo,Caleb Johnson,Zhipeng Wang,Fedor Borisyuk*

Main category: cs.IR

TL;DR: 本文介绍了LinkedIn开发用于语义搜索的小型语言模型的经验，通过模型压缩和上下文压缩技术，在保持精度的同时显著提升系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在预测任务中表现优异，但部署成本过高，难以满足工业应用对延迟和吞吐量的严格要求。

Method: 采用纯文本解码器的小型语言模型，结合剪枝等模型压缩技术减少模型规模，以及上下文压缩技术减少输入长度，同时优化GPU服务基础设施。

Result: 模型大小减少40%，输入上下文长度减少10倍，系统吞吐量提升10倍，同时满足质量要求。

Conclusion: 通过模型压缩和基础设施优化，可以在保持质量的同时显著提升小型语言模型的部署效率，满足工业级应用需求。

Abstract: Large Language Models (LLMs) have demonstrated impressive quality when
applied to predictive tasks such as relevance ranking and semantic search.
However, deployment of such LLMs remains prohibitively expensive for industry
applications with strict latency and throughput requirements. In this work, we
present lessons and efficiency insights from developing a purely text-based
decoder-only Small Language Model (SLM) for a semantic search application at
LinkedIn. Particularly, we discuss model compression techniques such as pruning
that allow us to reduce the model size by up to $40\%$ while maintaining the
accuracy. Additionally, we present context compression techniques that allow us
to reduce the input context length by up to $10$x with minimal loss of
accuracy. Finally, we present practical lessons from optimizing the serving
infrastructure for deploying such a system on GPUs at scale, serving millions
of requests per second. Taken together, this allows us to increase our system's
throughput by $10$x in a real-world deployment, while meeting our quality bar.

</details>


### [52] [Hybrid-Vector Retrieval for Visually Rich Documents: Combining Single-Vector Efficiency and Multi-Vector Accuracy](https://arxiv.org/abs/2510.22215)
*Juyeon Kim,Geon Lee,Dongwon Choi,Taeuk Kim,Kijung Shin*

Main category: cs.IR

TL;DR: HEAVEN是一个两阶段混合向量检索框架，用于视觉丰富文档检索，在保持高准确率的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决视觉丰富文档检索中单向量检索效率高但精度低、多向量检索精度高但计算成本大的权衡问题。

Method: 第一阶段使用单向量方法在视觉汇总页面上高效检索候选页面，第二阶段使用多向量方法重排序候选结果，并通过语言重要性过滤查询标记以减少冗余计算。

Result: 在四个基准测试中，HEAVEN平均达到多向量模型Recall@1性能的99.87%，同时将每查询计算量减少99.82%。

Conclusion: HEAVEN框架在视觉丰富文档检索中实现了效率和准确性的平衡，并引入了ViMDOC基准来评估真实条件下的检索系统。

Abstract: Retrieval over visually rich documents is essential for tasks such as legal
discovery, scientific search, and enterprise knowledge management. Existing
approaches fall into two paradigms: single-vector retrieval, which is efficient
but coarse, and multi-vector retrieval, which is accurate but computationally
expensive. To address this trade-off, we propose HEAVEN, a two-stage
hybrid-vector framework. In the first stage, HEAVEN efficiently retrieves
candidate pages using a single-vector method over Visually-Summarized Pages
(VS-Pages), which assemble representative visual layouts from multiple pages.
In the second stage, it reranks candidates with a multi-vector method while
filtering query tokens by linguistic importance to reduce redundant
computations. To evaluate retrieval systems under realistic conditions, we also
introduce ViMDOC, the first benchmark for visually rich, multi-document, and
long-document retrieval. Across four benchmarks, HEAVEN attains 99.87% of the
Recall@1 performance of multi-vector models on average while reducing per-query
computation by 99.82%, achieving efficiency and accuracy. Our code and datasets
are available at: https://github.com/juyeonnn/HEAVEN

</details>


### [53] [PaperAsk: A Benchmark for Reliability Evaluation of LLMs in Paper Search and Reading](https://arxiv.org/abs/2510.22242)
*Yutao Wu,Xiao Liu,Yunhao Feng,Jiale Ding,Xingjun Ma*

Main category: cs.IR

TL;DR: PaperAsk是一个系统性评估LLM在学术任务中可靠性的基准，涵盖引用检索、内容提取、论文发现和声明验证四个关键任务。研究发现LLM存在严重的可靠性问题，包括高失败率和虚构答案。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地作为研究助手使用，其在学术任务中的可靠性尚未得到充分评估，需要系统性的基准来测试其实际表现。

Method: 通过PaperAsk基准系统评估GPT-4o、GPT-5和Gemini-2.5-Flash在四个研究任务中的表现，包括引用检索、内容提取、论文发现和声明验证，使用真实使用条件下的web界面进行测试。

Result: 发现LLM存在一致的可靠性失败：多引用查询失败率48-98%，特定章节内容提取失败率72-91%，主题论文发现的F1分数低于0.32，错过60%以上相关文献。不同LLM有不同失败模式：ChatGPT倾向于拒绝回答，而Gemini产生流畅但虚构的答案。

Conclusion: LLM在学术任务中存在严重的可靠性问题，主要归因于检索上下文的不受控扩展和LLM优先考虑语义相关文本而非任务指令的倾向。开发了轻量级可靠性分类器来识别不可靠输出，PaperAsk为LLM学术助手系统的可靠性评估提供了可复现的诊断框架。

Abstract: Large Language Models (LLMs) increasingly serve as research assistants, yet
their reliability in scholarly tasks remains under-evaluated. In this work, we
introduce PaperAsk, a benchmark that systematically evaluates LLMs across four
key research tasks: citation retrieval, content extraction, paper discovery,
and claim verification. We evaluate GPT-4o, GPT-5, and Gemini-2.5-Flash under
realistic usage conditions-via web interfaces where search operations are
opaque to the user. Through controlled experiments, we find consistent
reliability failures: citation retrieval fails in 48-98% of multi-reference
queries, section-specific content extraction fails in 72-91% of cases, and
topical paper discovery yields F1 scores below 0.32, missing over 60% of
relevant literature. Further human analysis attributes these failures to the
uncontrolled expansion of retrieved context and the tendency of LLMs to
prioritize semantically relevant text over task instructions. Across basic
tasks, the LLMs display distinct failure behaviors: ChatGPT often withholds
responses rather than risk errors, whereas Gemini produces fluent but
fabricated answers. To address these issues, we develop lightweight reliability
classifiers trained on PaperAsk data to identify unreliable outputs. PaperAsk
provides a reproducible and diagnostic framework for advancing the reliability
evaluation of LLM-based scholarly assistance systems.

</details>


### [54] [Tools are under-documented: Simple Document Expansion Boosts Tool Retrieval](https://arxiv.org/abs/2510.22670)
*Xuan Lu,Haohang Huang,Rui Meng,Yaohui Jin,Wenjun Zeng,Xiaoyu Shen*

Main category: cs.IR

TL;DR: 提出了Tool-DE基准和框架，通过结构化文档扩展提升工具检索性能，并开发了Tool-Embed和Tool-Rank两个专用模型，在多个基准上达到最先进结果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在工具使用方面表现出色，但工具检索进展受到不完整和异构工具文档的限制，需要解决文档质量问题。

Method: 设计可扩展的文档扩展流程，利用开源和闭源LLM生成、验证和精炼增强的工具配置文件，构建大规模语料库，并开发专用的Tool-Embed密集检索器和Tool-Rank重排序器。

Result: 文档扩展显著提升检索性能，Tool-Embed和Tool-Rank在ToolRet和Tool-DE基准上均达到新的最先进结果。

Conclusion: 研究展示了LLM驱动的文档扩展的前景和局限性，Tool-DE及相关模型为未来工具检索研究奠定了基础。

Abstract: Large Language Models (LLMs) have recently demonstrated strong capabilities
in tool use, yet progress in tool retrieval remains hindered by incomplete and
heterogeneous tool documentation. To address this challenge, we introduce
Tool-DE, a new benchmark and framework that systematically enriches tool
documentation with structured fields to enable more effective tool retrieval,
together with two dedicated models, Tool-Embed and Tool-Rank. We design a
scalable document expansion pipeline that leverages both open- and
closed-source LLMs to generate, validate, and refine enriched tool profiles at
low cost, producing large-scale corpora with 50k instances for embedding-based
retrievers and 200k for rerankers. On top of this data, we develop two models
specifically tailored for tool retrieval: Tool-Embed, a dense retriever, and
Tool-Rank, an LLM-based reranker. Extensive experiments on ToolRet and Tool-DE
demonstrate that document expansion substantially improves retrieval
performance, with Tool-Embed and Tool-Rank achieving new state-of-the-art
results on both benchmarks. We further analyze the contribution of individual
fields to retrieval effectiveness, as well as the broader impact of document
expansion on both training and evaluation. Overall, our findings highlight both
the promise and limitations of LLM-driven document expansion, positioning
Tool-DE, along with the proposed Tool-Embed and Tool-Rank, as a foundation for
future research in tool retrieval.

</details>


### [55] [Diversification as Risk Minimization](https://arxiv.org/abs/2510.22681)
*Rikiya Takehi,Fernando Diaz,Tetsuya Sakai*

Main category: cs.IR

TL;DR: 本文提出VRisk框架，将搜索多样化重新定义为风险最小化问题，通过保护少数意图用户来提升搜索系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多样化方法只关注平均相关性，无法保证对少数意图用户的鲁棒性。用户往往更记得搜索失败的经历，而模糊查询中的少数意图容易被主流结果淹没。

Method: 提出VRisk指标来衡量查询中最不受服务意图的预期风险，并开发VRisker算法——一种具有可证明近似保证的快速贪婪重排序器。

Result: 在NTCIR INTENT-2、TREC Web 2012和MovieLens数据集上的实验显示，现有方法存在脆弱性，VRisker能将最坏情况意图失败减少高达33%，平均性能仅下降2%。

Conclusion: 将多样化作为风险最小化问题处理能显著提升搜索系统的鲁棒性，VRisker算法在保护少数意图用户方面表现出色，且计算效率高。

Abstract: Users tend to remember failures of a search session more than its many
successes. This observation has led to work on search robustness, where systems
are penalized if they perform very poorly on some queries. However, this
principle of robustness has been overlooked within a single query. An ambiguous
or underspecified query (e.g., ``jaguar'') can have several user intents, where
popular intents often dominate the ranking, leaving users with minority intents
unsatisfied. Although the diversification literature has long recognized this
issue, existing metrics only model the average relevance across intents and
provide no robustness guarantees. More surprisingly, we show theoretically and
empirically that many well-known diversification algorithms are no more robust
than a naive, non-diversified algorithm. To address this critical gap, we
propose to frame diversification as a risk-minimization problem. We introduce
VRisk, which measures the expected risk faced by the least-served fraction of
intents in a query. Optimizing VRisk produces a robust ranking, reducing the
likelihood of poor user experiences. We then propose VRisker, a fast greedy
re-ranker with provable approximation guarantees. Finally, experiments on NTCIR
INTENT-2, TREC Web 2012, and MovieLens show the vulnerability of existing
methods. VRisker reduces worst-case intent failures by up to 33% with a minimal
2% drop in average performance.

</details>


### [56] [REVISION:Reflective Intent Mining and Online Reasoning Auxiliary for E-commerce Visual Search System Optimization](https://arxiv.org/abs/2510.22739)
*Yiwen Tang,Qiuyu Zhao,Zenghui Sun,Jinsong Lan,Xiaoyong Zhu,Bo Zheng,Kaifu Zhang*

Main category: cs.IR

TL;DR: 提出REVISION框架解决淘宝视觉搜索中的用户-系统意图差异问题，通过离线推理挖掘和在线决策执行，降低无点击率。


<details>
  <summary>Details</summary>
Motivation: 淘宝视觉搜索中存在大量无点击请求，表明用户存在多样且隐式的搜索意图，但平台策略难以适应这些意图，导致用户表达受限和系统扩展性受阻。

Method: REVISION框架包含离线阶段和在线阶段：离线阶段使用大模型分析历史无点击请求，挖掘隐式意图因素并推理最优建议；在线阶段使用REVISION-R1-3B模型对查询图像和历史商品进行综合分析，生成优化方案并自适应调度搜索流程策略。

Result: 实验结果表明，该方法提高了从大规模搜索日志中挖掘隐式意图的效率，并显著降低了无点击率。

Conclusion: REVISION框架为大模型与传统搜索系统的集成提供了精简范式，实现了信息聚合和用户交互的端到端智能优化。

Abstract: In Taobao e-commerce visual search, user behavior analysis reveals a large
proportion of no-click requests, suggesting diverse and implicit user intents.
These intents are expressed in various forms and are difficult to mine and
discover, thereby leading to the limited adaptability and lag in platform
strategies. This greatly restricts users' ability to express diverse intents
and hinders the scalability of the visual search system. This mismatch between
user implicit intent expression and system response defines the User-SearchSys
Intent Discrepancy. To alleviate the issue, we propose a novel framework
REVISION. This framework integrates offline reasoning mining with online
decision-making and execution, enabling adaptive strategies to solve implicit
user demands. In the offline stage, we construct a periodic pipeline to mine
discrepancies from historical no-click requests. Leveraging large models, we
analyze implicit intent factors and infer optimal suggestions by jointly
reasoning over query and product metadata. These inferred suggestions serve as
actionable insights for refining platform strategies. In the online stage,
REVISION-R1-3B, trained on the curated offline data, performs holistic analysis
over query images and associated historical products to generate optimization
plans and adaptively schedule strategies across the search pipeline. Our
framework offers a streamlined paradigm for integrating large models with
traditional search systems, enabling end-to-end intelligent optimization across
information aggregation and user interaction. Experimental results demonstrate
that our approach improves the efficiency of implicit intent mining from
large-scale search logs and significantly reduces the no-click rate.

</details>


### [57] [Civic Ground Truth in News Recommenders: A Method for Public Value Scoring](https://arxiv.org/abs/2510.22865)
*James Meese,Kyle Herbertson*

Main category: cs.IR

TL;DR: 提出了一种通过大规模结构化受众评估将公民价值观嵌入新闻推荐系统的方法，使用公民基础事实方法生成基于价值观的标签。


<details>
  <summary>Details</summary>
Motivation: 探索如何在新闻推荐系统中整合规范性目标，如编辑目标和公共服务价值，为更具公民意识的推荐系统奠定基础。

Method: 使用全国代表性调查生成可泛化的基于价值观标签，通过自动化元数据丰富实现公民基础事实方法。

Result: 提出了一种将公民价值观嵌入新闻推荐系统的可行方法框架。

Conclusion: 该方法为构建更具公民意识的新闻推荐系统提供了新的技术路径。

Abstract: Research in news recommendation systems (NRS) continues to explore the best
ways to integrate normative goals such as editorial objectives and public
service values into existing systems. Prior efforts have incorporated expert
input or audience feedback to quantify these values, laying the groundwork for
more civic-minded recommender systems. This paper contributes to that
trajectory, introducing a method for embedding civic values into NRS through
large-scale, structured audience evaluations. The proposed civic ground truth
approach aims to generate value-based labels through a nationally
representative survey that are generalisable across a wider news corpus, using
automated metadata enrichment.

</details>


### [58] [MGFRec: Towards Reinforced Reasoning Recommendation with Multiple Groundings and Feedback](https://arxiv.org/abs/2510.22888)
*Shihao Cai,Chongming Gao,Haoyan Liu,Wentao Shi,Jianshan Sun,Ruiming Tang,Fuli Feng*

Main category: cs.IR

TL;DR: 提出在推理过程中进行多轮接地，将大语言模型的推理与实际物品空间对齐，避免过度解释用户兴趣，并通过用户代理提供反馈来更好地识别用户兴趣。


<details>
  <summary>Details</summary>
Motivation: 现有基于推理的推荐方法仅在语言空间中进行推理，未结合实际物品空间，导致过度解释用户兴趣并偏离真实物品。

Method: 在推理过程中执行多轮接地，帮助LLM更好地理解实际物品空间，并引入用户代理在每次接地步骤中提供反馈。

Result: 在三个Amazon评论数据集上的综合实验证明了多轮接地和反馈机制的有效性。

Conclusion: 对于推荐任务，在实际物品空间中进行推理而非局限于语言空间至关重要。

Abstract: The powerful reasoning and generative capabilities of large language models
(LLMs) have inspired researchers to apply them to reasoning-based
recommendation tasks, which require in-depth reasoning about user interests and
the generation of recommended items. However, previous reasoning-based
recommendation methods have typically performed inference within the language
space alone, without incorporating the actual item space. This has led to
over-interpreting user interests and deviating from real items. Towards this
research gap, we propose performing multiple rounds of grounding during
inference to help the LLM better understand the actual item space, which could
ensure that its reasoning remains aligned with real items. Furthermore, we
introduce a user agent that provides feedback during each grounding step,
enabling the LLM to better recognize and adapt to user interests. Comprehensive
experiments conducted on three Amazon review datasets demonstrate the
effectiveness of incorporating multiple groundings and feedback. These findings
underscore the critical importance of reasoning within the actual item space,
rather than being confined to the language space, for recommendation tasks.

</details>


### [59] [Improving Product Search Relevance with EAR-MP: A Solution for the CIKM 2025 AnalytiCup](https://arxiv.org/abs/2510.23018)
*JaeEun Lim,Soomin Kim,Jaeyong Seo,Iori Ono,Qimu Ran,Jae-woong Lee*

Main category: cs.IR

TL;DR: 该论文提出了一种针对多语言电商搜索的解决方案，通过数据翻译、清洗和模型优化，在查询-类别和查询-商品相关性任务上取得了竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 解决多语言电商搜索中因语言多样性和用户生成查询的噪声带来的挑战。

Method: 首先将多语言数据集翻译成英语进行标准化，然后进行数据清洗和归一化。基于DeBERTa-v3-large模型，采用标签平滑、自蒸馏和dropout等技术提升性能，并为不同任务引入层次化token注入和混合评分机制。

Result: 在计算资源受限的情况下，查询-类别任务的F1得分为0.8796，查询-商品任务的F1得分为0.8744。

Conclusion: 系统化的数据预处理和定制化的训练策略对于构建鲁棒且资源高效的多语言相关性系统至关重要。

Abstract: Multilingual e-commerce search is challenging due to linguistic diversity and
the noise inherent in user-generated queries. This paper documents the solution
employed by our team (EAR-MP) for the CIKM 2025 AnalytiCup, which addresses two
core tasks: Query-Category (QC) relevance and Query-Item (QI) relevance. Our
approach first normalizes the multilingual dataset by translating all text into
English, then mitigates noise through extensive data cleaning and
normalization. For model training, we build on DeBERTa-v3-large and improve
performance with label smoothing, self-distillation, and dropout. In addition,
we introduce task-specific upgrades, including hierarchical token injection for
QC and a hybrid scoring mechanism for QI. Under constrained compute, our method
achieves competitive results, attaining an F1 score of 0.8796 on QC and 0.8744
on QI. These findings underscore the importance of systematic data
preprocessing and tailored training strategies for building robust,
resource-efficient multilingual relevance systems.

</details>


### [60] [Multi-Stage Field Extraction of Financial Documents with OCR and Compact Vision-Language Models](https://arxiv.org/abs/2510.23066)
*Yichao Jin,Yushuo Wang,Qishuai Zhong,Kent Chiu Jin-Chun,Kenneth Zhu Ke,Donald MacDonald*

Main category: cs.IR

TL;DR: 提出了一种多阶段管道方法，结合传统图像处理、OCR和紧凑型视觉语言模型，用于从扫描财务文档中提取结构化字段，相比直接使用大型VLM，在准确性和效率上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 中小企业的财务文档通常是扫描图像，存在分辨率低、倾斜、噪声背景等问题，且内容异构（包含叙述、表格、图表和多语言内容），这对自动化信息提取构成挑战，特别是使用端到端大型视觉语言模型时计算成本高、对噪声敏感且处理速度慢。

Method: 采用多阶段管道：1) 图像预处理（分割、方向检测、尺寸归一化）；2) 多语言OCR提取页面级文本；3) 基于文本分析检索相关页面；4) 在缩小范围内使用紧凑型VLM提取结构化财务指标。

Result: 在内部多语言扫描财务文档语料库上的评估显示，相比直接使用大型VLM，该方法在字段级准确率提高了8.8倍，GPU成本仅为0.7%，端到端服务延迟降低了92.6%。

Conclusion: 紧凑型VLM结合多阶段管道方法能够高效地从大规模扫描财务文档中提取结构化信息，在准确性和计算效率方面都显著优于直接使用大型VLM的方法。

Abstract: Financial documents are essential sources of information for regulators,
auditors, and financial institutions, particularly for assessing the wealth and
compliance of Small and Medium-sized Businesses. However, SMB documents are
often difficult to parse. They are rarely born digital and instead are
distributed as scanned images that are none machine readable. The scans
themselves are low in resolution, affected by skew or rotation, and often
contain noisy backgrounds. These documents also tend to be heterogeneous,
mixing narratives, tables, figures, and multilingual content within the same
report. Such characteristics pose major challenges for automated information
extraction, especially when relying on end to end large Vision Language Models,
which are computationally expensive, sensitive to noise, and slow when applied
to files with hundreds of pages.
  We propose a multistage pipeline that leverages traditional image processing
models and OCR extraction, together with compact VLMs for structured field
extraction of large-scale financial documents. Our approach begins with image
pre-processing, including segmentation, orientation detection, and size
normalization. Multilingual OCR is then applied to recover page-level text.
Upon analyzing the text information, pages are retrieved for coherent sections.
Finally, compact VLMs are operated within these narrowed-down scopes to extract
structured financial indicators.
  Our approach is evaluated using an internal corpus of multi-lingual, scanned
financial documents. The results demonstrate that compact VLMs, together with a
multistage pipeline, achieves 8.8 times higher field level accuracy relative to
directly feeding the whole document into large VLMs, only at 0.7 percent of the
GPU cost and 92.6 percent less end-to-end service latency.

</details>


### [61] [Think before Recommendation: Autonomous Reasoning-enhanced Recommender](https://arxiv.org/abs/2510.23077)
*Xiaoyu Kong,Junguang Jiang,Bin Liu,Ziru Xu,Han Zhu,Jian Xu,Bo Zheng,Jiancan Wu,Xiang Wang*

Main category: cs.IR

TL;DR: RecZero：基于强化学习的推荐范式，通过纯RL训练单个LLM实现自主推理能力，解决现有蒸馏方法的局限性


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的推荐系统蒸馏方法存在教师模型推荐能力不足、监督成本高且静态、推理能力迁移肤浅等问题

Method: 提出RecZero：1）"推荐前思考"提示构建，使用结构化推理模板引导模型逐步分析；2）基于规则的奖励建模，采用GRPO计算推理轨迹奖励

Result: 在多个基准数据集上，RecZero和RecOne显著优于现有基线方法

Conclusion: RL范式在实现自主推理增强推荐系统方面具有优越性

Abstract: The core task of recommender systems is to learn user preferences from
historical user-item interactions. With the rapid development of large language
models (LLMs), recent research has explored leveraging the reasoning
capabilities of LLMs to enhance rating prediction tasks. However, existing
distillation-based methods suffer from limitations such as the teacher model's
insufficient recommendation capability, costly and static supervision, and
superficial transfer of reasoning ability. To address these issues, this paper
proposes RecZero, a reinforcement learning (RL)-based recommendation paradigm
that abandons the traditional multi-model and multi-stage distillation
approach. Instead, RecZero trains a single LLM through pure RL to autonomously
develop reasoning capabilities for rating prediction. RecZero consists of two
key components: (1) "Think-before-Recommendation" prompt construction, which
employs a structured reasoning template to guide the model in step-wise
analysis of user interests, item features, and user-item compatibility; and (2)
rule-based reward modeling, which adopts group relative policy optimization
(GRPO) to compute rewards for reasoning trajectories and optimize the LLM.
Additionally, the paper explores a hybrid paradigm, RecOne, which combines
supervised fine-tuning with RL, initializing the model with cold-start
reasoning samples and further optimizing it with RL. Experimental results
demonstrate that RecZero and RecOne significantly outperform existing baseline
methods on multiple benchmark datasets, validating the superiority of the RL
paradigm in achieving autonomous reasoning-enhanced recommender systems.

</details>
