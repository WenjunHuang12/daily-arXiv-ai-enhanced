<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 62]
- [cs.IT](#cs.IT) [Total: 8]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.DS](#cs.DS) [Total: 1]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Audio Prototypical Network For Controllable Music Recommendation](https://arxiv.org/abs/2508.00194)
*Fırat Öncel,Emiliano Penaloza,Haolun Wu,Shubham Gupta,Mirco Ravanelli,Laurent Charlin,Cem Subakan*

Main category: cs.IR

TL;DR: 提出了一种基于音频原型网络的可控音乐推荐系统，解决了传统推荐系统缺乏可解释性和可控性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统使用黑盒编码模型生成密集表示，缺乏可解释性，用户无法理解或控制系统对其偏好的建模。音乐推荐中这一问题尤为突出，因为用户偏好高度个性化且基于细微的音乐品质（如情绪、流派、节奏或乐器）变化。

Method: 提出音频原型网络，通过代表音乐品质语义特征的表达用户偏好。

Result: 模型在推荐性能上与流行基线模型相当，同时提供可解释和可控的用户档案。

Conclusion: 该模型在保持推荐性能的同时，增强了系统的可解释性和用户控制能力。

Abstract: Traditional recommendation systems represent user preferences in dense
representations obtained through black-box encoder models. While these models
often provide strong recommendation performance, they lack interpretability for
users, leaving users unable to understand or control the system's modeling of
their preferences. This limitation is especially challenging in music
recommendation, where user preferences are highly personal and often evolve
based on nuanced qualities like mood, genre, tempo, or instrumentation. In this
paper, we propose an audio prototypical network for controllable music
recommendation. This network expresses user preferences in terms of prototypes
representative of semantically meaningful features pertaining to musical
qualities. We show that the model obtains competitive recommendation
performance compared to popular baseline models while also providing
interpretable and controllable user profiles.

</details>


### [2] [When Relevance Meets Novelty: Dual-Stable Periodic Optimization for Exploratory Recommendation](https://arxiv.org/abs/2508.00450)
*Hongxiang Lin,Hao Guo,Zeshun Li,Erpeng Xue,Yongqian He,Xiangyu Hou,Zhaoyu Hu,Lei Wang,Sheng Chen*

Main category: cs.IR

TL;DR: 论文提出Co-Evolutionary Alignment (CoEA)方法，通过Dual-Stable Interest Exploration (DSIE)模块和Periodic Collaborative Optimization (PCO)机制，解决传统推荐系统反馈循环和LLM增强框架的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统易陷入反馈循环，限制用户探索；现有LLM增强框架存在兴趣建模偏差和静态优化缺陷。

Method: 提出CoEA方法，包括DSIE模块（并行建模长期群体身份和短期个体兴趣）和PCO机制（周期性协同优化）。

Result: 实验验证CoEA在探索性推荐中的有效性。

Conclusion: CoEA方法通过动态闭环优化和双兴趣建模，显著提升推荐系统的探索能力。

Abstract: Traditional recommendation systems tend to trap users in strong feedback
loops by excessively pushing content aligned with their historical preferences,
thereby limiting exploration opportunities and causing content fatigue.
Although large language models (LLMs) demonstrate potential with their diverse
content generation capabilities, existing LLM-enhanced dual-model frameworks
face two major limitations: first, they overlook long-term preferences driven
by group identity, leading to biased interest modeling; second, they suffer
from static optimization flaws, as a one-time alignment process fails to
leverage incremental user data for closed-loop optimization. To address these
challenges, we propose the Co-Evolutionary Alignment (CoEA) method. For
interest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE)
module, jointly modeling long-term group identity and short-term individual
interests through parallel processing of behavioral sequences. For static
optimization limitations, we design a Periodic Collaborative Optimization (PCO)
mechanism. This mechanism regularly conducts preference verification on
incremental data using the Relevance LLM, then guides the Novelty LLM to
perform fine-tuning based on the verification results, and subsequently feeds
back the output of the incrementally fine-tuned Novelty LLM to the Relevance
LLM for re-evaluation, thereby achieving a dynamic closed-loop optimization.
Extensive online and offline experiments verify the effectiveness of the CoEA
model in exploratory recommendation.

</details>


### [3] [M^2VAE: Multi-Modal Multi-View Variational Autoencoder for Cold-start Item Recommendation](https://arxiv.org/abs/2508.00452)
*Chuan He,Yongchao Liu,Qiang Li,Wenliang Zhong,Chuntao Hong,Xinwei Yao*

Main category: cs.IR

TL;DR: 提出M^2VAE模型，解决冷启动推荐中多模态多视图特征建模问题，通过生成类型特定隐变量和对比学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 冷启动物品推荐缺乏历史交互数据，现有方法忽视多模态的多视图结构和特征区分。

Method: 使用M^2VAE生成类型特定隐变量，结合PoE和MoE建模共同与独特视图，引入对比学习。

Result: 在真实数据集上验证了模型的有效性。

Conclusion: M^2VAE能有效建模多模态多视图特征，提升冷启动推荐性能。

Abstract: Cold-start item recommendation is a significant challenge in recommendation
systems, particularly when new items are introduced without any historical
interaction data. While existing methods leverage multi-modal content to
alleviate the cold-start issue, they often neglect the inherent multi-view
structure of modalities, the distinction between shared and modality-specific
features. In this paper, we propose Multi-Modal Multi-View Variational
AutoEncoder (M^2VAE), a generative model that addresses the challenges of
modeling common and unique views in attribute and multi-modal features, as well
as user preferences over single-typed item features. Specifically, we generate
type-specific latent variables for item IDs, categorical attributes, and image
features, and use Product-of-Experts (PoE) to derive a common representation. A
disentangled contrastive loss decouples the common view from unique views while
preserving feature informativeness. To model user inclinations, we employ a
preference-guided Mixture-of-Experts (MoE) to adaptively fuse representations.
We further incorporate co-occurrence signals via contrastive learning,
eliminating the need for pretraining. Extensive experiments on real-world
datasets validate the effectiveness of our approach.

</details>


### [4] [Session-Based Recommendation with Validated and Enriched LLM Intents](https://arxiv.org/abs/2508.00570)
*Gyuseok Lee,Yaokun Liu,Yifan Liu,Susik Yoon,Dong Wang,SeongKu Kang*

Main category: cs.IR

TL;DR: VELI4SBR是一个两阶段框架，通过验证和丰富LLM生成的意图来改进基于会话的推荐（SBR）。


<details>
  <summary>Details</summary>
Motivation: 解决SBR中数据稀疏性和LLM生成意图的质量、多意图整合及LLM失败补偿问题。

Method: 两阶段方法：1) 通过预测-校正循环生成高质量意图；2) 通过多意图预测和融合机制增强SBR模型。

Result: VELI4SBR在实验中优于现有基线，并提高了可解释性。

Conclusion: VELI4SBR有效解决了SBR中的关键挑战，提升了推荐性能。

Abstract: Session-based recommendation (SBR) aims to predict the next item for an
anonymous user in a timely manner. However, SBR suffers from data sparsity due
to the short and anonymous nature of sessions. Recently, an emerging line of
work has explored inferring the underlying user intents of a session using
large language models (LLMs), with the generated intents serving as auxiliary
training signals to enhance SBR models. Despite its promise, this approach
faces three key challenges: validating intent quality, incorporating
session-level multi-intents, and complementing inevitable LLM failure cases. In
this paper, we propose VELI4SBR, a two-stage framework that leverages Validated
and Enriched LLM-generated Intents for SBR. In the first stage, we generate
high-quality intents using a predict-and-correct loop that validates the
informativeness of LLM-generated intents with a global intent pool to constrain
the LLM's output space and reduce hallucination. In the second stage, we
enhance the SBR model using the generated intents through a lightweight
multi-intent prediction and fusion mechanism. Furthermore, we introduce a
training strategy that compensates for LLM failures by inferring intents from
inter-session behavioral similarities. Extensive experiments show that VELI4SBR
outperforms state-of-the-art baselines while improving explainability.

</details>


### [5] [Experimental Evaluation of Dynamic Topic Modeling Algorithms](https://arxiv.org/abs/2508.00710)
*Ngozichukwuka Onah,Nadine Steinmetz,Hani Al-Sayeh,Kai-Uwe Sattler*

Main category: cs.IR

TL;DR: 该研究比较了自驱动主题模型并提出了一种评估指标，用于记录主题随时间的变化。


<details>
  <summary>Details</summary>
Motivation: 社交媒体每天生成的文本量巨大，分析这些文本对许多用途非常有用，但目前缺乏对这些模型的全面定量比较。

Method: 比较了自驱动主题模型，并提出了一个评估指标。

Result: 提出了一个能够记录主题随时间变化的评估指标。

Conclusion: 该研究填补了自驱动主题模型定量比较的空白，并提供了有效的评估工具。

Abstract: The amount of text generated daily on social media is gigantic and analyzing
this text is useful for many purposes. To understand what lies beneath a huge
amount of text, we need dependable and effective computing techniques from
self-powered topic models. Nevertheless, there are currently relatively few
thorough quantitative comparisons between these models. In this study, we
compare these models and propose an assessment metric that documents how the
topics change in time.

</details>


### [6] [Harnessing the Power of Interleaving and Counterfactual Evaluation for Airbnb Search Ranking](https://arxiv.org/abs/2508.00751)
*Qing Zhang,Alex Deng,Michelle Du,Huiji Gao,Liwei He,Sanjeev Katariya*

Main category: cs.IR

TL;DR: 论文提出了一种结合交错和反事实评估的方法，以快速筛选A/B测试候选方案，显著提升实验效率和灵敏度。


<details>
  <summary>Details</summary>
Motivation: 在线平台在评估排名算法时面临统计功效不足和耗时问题，传统离线评估又缺乏准确性。

Method: 开发了交错和反事实评估方法，用于快速在线评估和筛选A/B测试候选方案。

Result: 实验灵敏度提升高达100倍，并优化了实验流程。

Conclusion: 该方法为类似组织提供了实用解决方案，显著提升了评估效率和效果。

Abstract: Evaluation plays a crucial role in the development of ranking algorithms on
search and recommender systems. It enables online platforms to create
user-friendly features that drive commercial success in a steady and effective
manner. The online environment is particularly conducive to applying causal
inference techniques, such as randomized controlled experiments (known as A/B
test), which are often more challenging to implement in fields like medicine
and public policy. However, businesses face unique challenges when it comes to
effective A/B test. Specifically, achieving sufficient statistical power for
conversion-based metrics can be time-consuming, especially for significant
purchases like booking accommodations. While offline evaluations are quicker
and more cost-effective, they often lack accuracy and are inadequate for
selecting candidates for A/B test. To address these challenges, we developed
interleaving and counterfactual evaluation methods to facilitate rapid online
assessments for identifying the most promising candidates for A/B tests. Our
approach not only increased the sensitivity of experiments by a factor of up to
100 (depending on the approach and metrics) compared to traditional A/B testing
but also streamlined the experimental process. The practical insights gained
from usage in production can also benefit organizations with similar interests.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Predicting Large-scale Urban Network Dynamics with Energy-informed Graph Neural Diffusion](https://arxiv.org/abs/2508.00037)
*Tong Nie,Jian Sun,Wei Ma*

Main category: cs.LG

TL;DR: 论文提出了一种基于物理定律启发的可扩展时空Transformer（ScaleSTF），用于预测大规模城市网络的动态，解决了现有模型在效能与效率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 城市系统中的复杂过程通常缺乏明确的规则，而现有预测模型（如图神经网络）在计算效率与预测效果之间存在矛盾，限制了其在大规模网络中的应用。

Method: 通过结合物理定律和Transformer结构，设计了一种低维嵌入诱导的注意力机制，提出了线性复杂度的ScaleSTF模型。

Result: 在交通流量、太阳能发电和智能电表等大规模城市系统中验证了ScaleSTF的先进性能和卓越的可扩展性。

Conclusion: ScaleSTF为大规模城市网络动态预测提供了新的视角，展示了其在效能与效率上的平衡优势。

Abstract: Networked urban systems facilitate the flow of people, resources, and
services, and are essential for economic and social interactions. These systems
often involve complex processes with unknown governing rules, observed by
sensor-based time series. To aid decision-making in industrial and engineering
contexts, data-driven predictive models are used to forecast spatiotemporal
dynamics of urban systems. Current models such as graph neural networks have
shown promise but face a trade-off between efficacy and efficiency due to
computational demands. Hence, their applications in large-scale networks still
require further efforts. This paper addresses this trade-off challenge by
drawing inspiration from physical laws to inform essential model designs that
align with fundamental principles and avoid architectural redundancy. By
understanding both micro- and macro-processes, we present a principled
interpretable neural diffusion scheme based on Transformer-like structures
whose attention layers are induced by low-dimensional embeddings. The proposed
scalable spatiotemporal Transformer (ScaleSTF), with linear complexity, is
validated on large-scale urban systems including traffic flow, solar power, and
smart meters, showing state-of-the-art performance and remarkable scalability.
Our results constitute a fresh perspective on the dynamics prediction in
large-scale urban networks.

</details>


### [8] [Hybrid LSTM-Transformer Models for Profiling Highway-Railway Grade Crossings](https://arxiv.org/abs/2508.00039)
*Kaustav Chatterjee,Joshua Q. Li,Fatemeh Ansari,Masud Rana Munna,Kundan Parajulee,Jared Schwennesen*

Main category: cs.LG

TL;DR: 研究提出了一种结合LSTM和Transformer的混合深度学习框架，用于高效测量HRGC剖面，提升安全性。


<details>
  <summary>Details</summary>
Motivation: 传统HRGC剖面测量方法成本高、耗时长且存在安全隐患，需改进。

Method: 开发了三种混合深度学习模型，利用IMU和GPS传感器数据与地面真实数据对比。

Result: 模型2和3表现最佳，能快速生成2D/3D HRGC剖面，显著提升安全性评估效率。

Conclusion: 混合深度学习模型为HRGC剖面测量提供了高效、准确的解决方案，有望改善公路和铁路安全。

Abstract: Hump crossings, or high-profile Highway Railway Grade Crossings (HRGCs), pose
safety risks to highway vehicles due to potential hang-ups. These crossings
typically result from post-construction railway track maintenance activities or
non-compliance with design guidelines for HRGC vertical alignments.
Conventional methods for measuring HRGC profiles are costly, time-consuming,
traffic-disruptive, and present safety challenges. To address these issues,
this research employed advanced, cost-effective techniques and innovative
modeling approaches for HRGC profile measurement. A novel hybrid deep learning
framework combining Long Short-Term Memory (LSTM) and Transformer architectures
was developed by utilizing instrumentation and ground truth data.
Instrumentation data were gathered using a highway testing vehicle equipped
with Inertial Measurement Unit (IMU) and Global Positioning System (GPS)
sensors, while ground truth data were obtained via an industrial-standard
walking profiler. Field data was collected at the Red Rock Railroad Corridor in
Oklahoma. Three advanced deep learning models Transformer-LSTM sequential
(model 1), LSTM-Transformer sequential (model 2), and LSTM-Transformer parallel
(model 3) were evaluated to identify the most efficient architecture. Models 2
and 3 outperformed the others and were deployed to generate 2D/3D HRGC
profiles. The deep learning models demonstrated significant potential to
enhance highway and railroad safety by enabling rapid and accurate assessment
of HRGC hang-up susceptibility.

</details>


### [9] [Regime-Aware Conditional Neural Processes with Multi-Criteria Decision Support for Operational Electricity Price Forecasting](https://arxiv.org/abs/2508.00040)
*Abhinav Das,Stephan Schlüter*

Main category: cs.LG

TL;DR: 该研究结合贝叶斯机制检测与条件神经过程，用于德国市场24小时电价预测，提出R-NP模型，并在多场景下评估其性能。


<details>
  <summary>Details</summary>
Motivation: 解决电价预测中因市场机制变化导致的预测不准确问题，并探索预测模型在实际运营中的实用性。

Method: 使用DS-HDP-HMM进行机制检测，每个机制由独立的CNP建模，最终预测为机制加权的CNP输出。

Result: R-NP在多场景评估中表现均衡，优于DNN和LEAR模型，尤其在2021-2023年表现最佳。

Conclusion: R-NP模型在电价预测中具有平衡性和实用性，是综合性能最优的解决方案。

Abstract: This work integrates Bayesian regime detection with conditional neural
processes for 24-hour electricity price prediction in the German market. Our
methodology integrates regime detection using a disentangled sticky
hierarchical Dirichlet process hidden Markov model (DS-HDP-HMM) applied to
daily electricity prices. Each identified regime is subsequently modeled by an
independent conditional neural process (CNP), trained to learn localized
mappings from input contexts to 24-dimensional hourly price trajectories, with
final predictions computed as regime-weighted mixtures of these CNP outputs. We
rigorously evaluate R-NP against deep neural networks (DNN) and Lasso estimated
auto-regressive (LEAR) models by integrating their forecasts into diverse
battery storage optimization frameworks, including price arbitrage, risk
management, grid services, and cost minimization. This operational utility
assessment revealed complex performance trade-offs: LEAR often yielded superior
absolute profits or lower costs, while DNN showed exceptional optimality in
specific cost-minimization contexts. Recognizing that raw prediction accuracy
doesn't always translate to optimal operational outcomes, we employed TOPSIS as
a comprehensive multi-criteria evaluation layer. Our TOPSIS analysis identified
LEAR as the top-ranked model for 2021, but crucially, our proposed R-NP model
emerged as the most balanced and preferred solution for 2021, 2022 and 2023.

</details>


### [10] [Learning Like Humans: Resource-Efficient Federated Fine-Tuning through Cognitive Developmental Stages](https://arxiv.org/abs/2508.00041)
*Yebo Wu,Jingguang Li,Zhijiang Guo,Li Li*

Main category: cs.LG

TL;DR: DevFT是一种资源高效的联邦微调方法，通过分阶段构建LLM，显著提升性能并减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 解决联邦微调在边缘设备上资源消耗高的问题，同时保护数据隐私。

Method: 分阶段微调，逐步增加模型参数容量，并通过知识转移优化初始化参数。

Result: DevFT在多个基准测试中表现优异，收敛速度提升4.59倍，通信开销减少10.67倍，性能平均提升9.07%。

Conclusion: DevFT是一种高效且兼容现有方法的联邦微调解决方案。

Abstract: Federated fine-tuning enables Large Language Models (LLMs) to adapt to
downstream tasks while preserving data privacy, but its resource-intensive
nature limits deployment on edge devices. In this paper, we introduce
Developmental Federated Tuning (DevFT), a resource-efficient approach inspired
by cognitive development that progressively builds a powerful LLM from a
compact foundation. DevFT decomposes the fine-tuning process into developmental
stages, each optimizing submodels with increasing parameter capacity. Knowledge
from earlier stages transfers to subsequent submodels, providing optimized
initialization parameters that prevent convergence to local minima and
accelerate training. This paradigm mirrors human learning, gradually
constructing comprehensive knowledge structure while refining existing skills.
To efficiently build stage-specific submodels, DevFT introduces
deconfliction-guided layer grouping and differential-based layer fusion to
distill essential information and construct representative layers. Evaluations
across multiple benchmarks demonstrate that DevFT significantly outperforms
state-of-the-art methods, achieving up to 4.59$\times$ faster convergence,
10.67$\times$ reduction in communication overhead, and 9.07% average
performance improvement, while maintaining compatibility with existing
approaches.

</details>


### [11] [Improved Robustness and Functional Localization in Topographic CNNs Through Weight Similarity](https://arxiv.org/abs/2508.00043)
*Nhut Truong,Uri Hasson*

Main category: cs.LG

TL;DR: 比较了两种地形约束（权重相似性和激活相似性）对卷积神经网络的影响，发现权重相似性在鲁棒性、输入敏感性和功能定位方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究不同地形约束对神经网络学习表示的影响，填补系统性比较的空白。

Method: 训练两种地形约束的卷积神经网络（权重相似性和激活相似性），评估分类准确性、鲁棒性和表示空间组织。

Result: 权重相似性在噪声鲁棒性、输入敏感性和功能定位方面优于激活相似性和标准CNN。

Conclusion: 权重相似性约束能产生更鲁棒的表示，并影响网络的特征学习和功能组织。

Abstract: Topographic neural networks are computational models that can simulate the
spatial and functional organization of the brain. Topographic constraints in
neural networks can be implemented in multiple ways, with potentially different
impacts on the representations learned by the network. The impact of such
different implementations has not been systematically examined. To this end,
here we compare topographic convolutional neural networks trained with two
spatial constraints: Weight Similarity (WS), which pushes neighboring units to
develop similar incoming weights, and Activation Similarity (AS), which
enforces similarity in unit activations. We evaluate the resulting models on
classification accuracy, robustness to weight perturbations and input
degradation, and the spatial organization of learned representations. Compared
to both AS and standard CNNs, WS provided three main advantages: i) improved
robustness to noise, also showing higher accuracy under weight corruption; ii)
greater input sensitivity, reflected in higher activation variance; and iii)
stronger functional localization, with units showing similar activations
positioned at closer distances. In addition, WS produced differences in
orientation tuning, symmetry sensitivity, and eccentricity profiles of units,
indicating an influence of this spatial constraint on the representational
geometry of the network. Our findings suggest that during end-to-end training,
WS constraints produce more robust representations than AS or non-topographic
CNNs. These findings also suggest that weight-based spatial constraints can
shape feature learning and functional organization in biophysical inspired
models.

</details>


### [12] [Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains](https://arxiv.org/abs/2508.00046)
*Ruo Yu Tao,Kaicheng Guo,Cameron Allen,George Konidaris*

Main category: cs.LG

TL;DR: 论文提出了一个用于评估强化学习算法在部分可观测性下性能的基准框架POBAX，强调基准需覆盖多种部分可观测形式且具有记忆改进性。


<details>
  <summary>Details</summary>
Motivation: 当前评估部分可观测性的基准过于简单，无法反映真实场景中的复杂性，因此需要更全面的基准来推动算法研究。

Method: 提出POBAX开源库，包含多种部分可观测环境（如定位、视觉控制、游戏等），并提供超参数和算法实现。

Result: POBAX中的任务均具有记忆改进性，且需要难以学习的记忆功能，为研究提供了明确信号。

Conclusion: POBAX为部分可观测性研究提供了标准化、高效的评估工具，有助于推动算法发展。

Abstract: Mitigating partial observability is a necessary but challenging task for
general reinforcement learning algorithms. To improve an algorithm's ability to
mitigate partial observability, researchers need comprehensive benchmarks to
gauge progress. Most algorithms tackling partial observability are only
evaluated on benchmarks with simple forms of state aliasing, such as feature
masking and Gaussian noise. Such benchmarks do not represent the many forms of
partial observability seen in real domains, like visual occlusion or unknown
opponent intent. We argue that a partially observable benchmark should have two
key properties. The first is coverage in its forms of partial observability, to
ensure an algorithm's generalizability. The second is a large gap between the
performance of a agents with more or less state information, all other factors
roughly equal. This gap implies that an environment is memory improvable: where
performance gains in a domain are from an algorithm's ability to cope with
partial observability as opposed to other factors. We introduce best-practice
guidelines for empirically benchmarking reinforcement learning under partial
observability, as well as the open-source library POBAX: Partially Observable
Benchmarks in JAX. We characterize the types of partial observability present
in various environments and select representative environments for our
benchmark. These environments include localization and mapping, visual control,
games, and more. Additionally, we show that these tasks are all memory
improvable and require hard-to-learn memory functions, providing a concrete
signal for partial observability research. This framework includes recommended
hyperparameters as well as algorithm implementations for fast, out-of-the-box
evaluation, as well as highly performant environments implemented in JAX for
GPU-scalable experimentation.

</details>


### [13] [TriP-LLM: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection](https://arxiv.org/abs/2508.00047)
*Yuan-Cheng Yu,Yen-Chieh Ouyang,Chun-An Lin*

Main category: cs.LG

TL;DR: 提出了一种名为TriP-LLM的无监督时间序列异常检测框架，通过三分支设计和预训练大语言模型（LLM）处理数据，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着物联网和智能制造的普及，时间序列数据的规模和维度急剧增加，传统统计方法难以处理其高异质性和复杂性。

Method: TriP-LLM采用三分支设计（Patching、Selection、Global）将时间序列编码为分块标记，由冻结的预训练LLM处理，并通过轻量级解码器重构输入以计算异常分数。

Result: 在多个公共基准数据集上，TriP-LLM表现优于最新方法，且内存消耗显著低于基于通道独立的LLM方法。

Conclusion: TriP-LLM展示了LLM在时间序列异常检测中的潜力，适合内存受限的环境，代码和模型已开源。

Abstract: Time-series anomaly detection plays a central role across a wide range of
application domains. With the increasing proliferation of the Internet of
Things (IoT) and smart manufacturing, time-series data has dramatically
increased in both scale and dimensionality. This growth has exposed the
limitations of traditional statistical methods in handling the high
heterogeneity and complexity of such data. Inspired by the recent success of
large language models (LLMs) in multimodal tasks across language and vision
domains, we propose a novel unsupervised anomaly detection framework: A
Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly
Detection (TriP-LLM). TriP-LLM integrates local and global temporal features
through a tri-branch design-Patching, Selection, and Global-to encode the input
time series into patch-wise tokens, which are then processed by a frozen,
pretrained LLM. A lightweight patch-wise decoder reconstructs the input, from
which anomaly scores are derived. We evaluate TriP-LLM on several public
benchmark datasets using PATE, a recently proposed threshold-free evaluation
metric, and conduct all comparisons within a unified open-source framework to
ensure fairness. Experimental results show that TriP-LLM consistently
outperforms recent state-of-the-art methods across all datasets, demonstrating
strong detection capabilities. Furthermore, through extensive ablation studies,
we verify the substantial contribution of the LLM to the overall architecture.
Compared to LLM-based approaches using Channel Independence (CI) patch
processing, TriP-LLM achieves significantly lower memory consumption, making it
more suitable for GPU memory-constrained environments. All code and model
checkpoints are publicly available on https://github.com/YYZStart/TriP-LLM.git

</details>


### [14] [Evaluating COVID 19 Feature Contributions to Bitcoin Return Forecasting: Methodology Based on LightGBM and Genetic Optimization](https://arxiv.org/abs/2508.00078)
*Imen Mahmoud,Andrei Velichko*

Main category: cs.LG

TL;DR: 该研究提出了一种结合LightGBM回归模型和遗传算法优化的新方法框架，用于评估COVID-19相关指标对比特币回报预测的贡献。结果表明，疫情指标显著提升了预测准确性，尤其是疫苗接种数据。


<details>
  <summary>Details</summary>
Motivation: 研究的主要目标不仅是预测比特币回报，而是确定是否包含疫情相关健康数据能显著提高预测准确性。

Method: 构建了包含比特币回报和COVID-19指标的全面数据集，使用遗传算法优化模型，并通过统计方法比较性能指标。

Result: COVID-19指标显著提升了模型性能（R2增加40%，RMSE降低2%），疫苗接种数据是主要预测因素。

Conclusion: 该方法扩展了金融分析工具，为投资者和政策制定者在系统性危机中提供了更精细的市场指标。

Abstract: This study proposes a novel methodological framework integrating a LightGBM
regression model and genetic algorithm (GA) optimization to systematically
evaluate the contribution of COVID-19-related indicators to Bitcoin return
prediction. The primary objective was not merely to forecast Bitcoin returns
but rather to determine whether including pandemic-related health data
significantly enhances prediction accuracy. A comprehensive dataset comprising
daily Bitcoin returns and COVID-19 metrics (vaccination rates,
hospitalizations, testing statistics) was constructed. Predictive models,
trained with and without COVID-19 features, were optimized using GA over 31
independent runs, allowing robust statistical assessment. Performance metrics
(R2, RMSE, MAE) were statistically compared through distribution overlaps and
Mann-Whitney U tests. Permutation Feature Importance (PFI) analysis quantified
individual feature contributions. Results indicate that COVID-19 indicators
significantly improved model performance, particularly in capturing extreme
market fluctuations (R2 increased by 40%, RMSE decreased by 2%, both highly
significant statistically). Among COVID-19 features, vaccination metrics,
especially the 75th percentile of fully vaccinated individuals, emerged as
dominant predictors. The proposed methodology extends existing financial
analytics tools by incorporating public health signals, providing investors and
policymakers with refined indicators to navigate market uncertainty during
systemic crises.

</details>


### [15] [Stress-Aware Resilient Neural Training](https://arxiv.org/abs/2508.00098)
*Ashkan Shakarami,Yousef Yeganeh,Azade Farshad,Lorenzo Nicole,Stefano Ghidoni,Nassir Navab*

Main category: cs.LG

TL;DR: Stress-Aware Learning（SAL）是一种弹性神经训练范式，通过动态调整优化行为，提升深度神经网络的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 受材料科学中结构疲劳的启发，提出临时（弹性）和永久（塑性）变形的概念，以解决训练过程中的停滞问题。

Method: 提出Plastic Deformation Optimizer，通过自适应噪声注入模型参数，帮助模型逃离尖锐最小值，收敛到更平坦的损失区域。

Result: 在六种架构、四种优化器和七个视觉基准测试中，表现出更高的鲁棒性和泛化能力，计算开销极小。

Conclusion: SAL通过动态调整优化行为，显著提升了模型的性能，代码和3D可视化将在GitHub上开源。

Abstract: This paper introduces Stress-Aware Learning, a resilient neural training
paradigm in which deep neural networks dynamically adjust their optimization
behavior - whether under stable training regimes or in settings with uncertain
dynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic)
Deformation, inspired by structural fatigue in materials science. To
instantiate this concept, we propose Plastic Deformation Optimizer, a
stress-aware mechanism that injects adaptive noise into model parameters
whenever an internal stress signal - reflecting stagnation in training loss and
accuracy - indicates persistent optimization difficulty. This enables the model
to escape sharp minima and converge toward flatter, more generalizable regions
of the loss landscape. Experiments across six architectures, four optimizers,
and seven vision benchmarks demonstrate improved robustness and generalization
with minimal computational overhead. The code and 3D visuals will be available
on GitHub: https://github.com/Stress-Aware-Learning/SAL.

</details>


### [16] [StackLiverNet: A Novel Stacked Ensemble Model for Accurate and Interpretable Liver Disease Detection](https://arxiv.org/abs/2508.00117)
*Md. Ehsanul Haque,S. M. Jahidul Islam,Shakil Mia,Rumana Sharmin,Ashikuzzaman,Md Samir Morshed,Md. Tahmidul Huque*

Main category: cs.LG

TL;DR: StackLiverNet是一种可解释的堆叠集成模型，用于肝病检测，通过高级数据预处理和特征选择提高模型性能，测试准确率达99.89%。


<details>
  <summary>Details</summary>
Motivation: 现有肝病分类模型存在高误分类率、解释性差、计算成本高等问题，需要改进。

Method: 采用随机欠采样处理类别不平衡，构建超参数优化的基分类器集成，通过LightGBM元模型结合其互补优势，并使用LIME和SHAP增强解释性。

Result: 测试准确率99.89%，Cohen Kappa 0.9974，AUC 0.9993，仅5次误分类，训练和推理速度快。

Conclusion: StackLiverNet在肝病检测中表现出色，兼具高性能和可解释性，适合临床应用。

Abstract: Liver diseases are a serious health concern in the world, which requires
precise and timely diagnosis to enhance the survival chances of patients. The
current literature implemented numerous machine learning and deep learning
models to classify liver diseases, but most of them had some issues like high
misclassification error, poor interpretability, prohibitive computational
expense, and lack of good preprocessing strategies. In order to address these
drawbacks, we introduced StackLiverNet in this study; an interpretable stacked
ensemble model tailored to the liver disease detection task. The framework uses
advanced data preprocessing and feature selection technique to increase model
robustness and predictive ability. Random undersampling is performed to deal
with class imbalance and make the training balanced. StackLiverNet is an
ensemble of several hyperparameter-optimized base classifiers, whose
complementary advantages are used through a LightGBM meta-model. The provided
model demonstrates excellent performance, with the testing accuracy of 99.89%,
Cohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and
efficient training and inference speeds that are amenable to clinical practice
(training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local
Interpretable Model-Agnostic Explanations (LIME) are applied to generate
transparent explanations of individual predictions, revealing high
concentrations of Alkaline Phosphatase and moderate SGOT as important
observations of liver disease. Also, SHAP was used to rank features by their
global contribution to predictions, while the Morris method confirmed the most
influential features through sensitivity analysis.

</details>


### [17] [Structured Transformations for Stable and Interpretable Neural Computation](https://arxiv.org/abs/2508.00127)
*Saleh Nikooroo,Thomas Engel*

Main category: cs.LG

TL;DR: 论文提出了一种新的神经网络层变换方法，通过结构化线性算子和残差修正组件，提升训练的稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当代神经网络缺乏结构保障，导致学习不稳定和行为难以解释。

Method: 将层变换分解为结构化线性算子和残差修正组件，优化信号传播和训练动态。

Result: 实验表明，该方法改善了梯度条件、降低了对扰动的敏感性，并增强了层间鲁棒性。

Conclusion: 该方法为设计更稳定、透明的神经网络架构提供了基础，同时保持了表达能力。

Abstract: Despite their impressive performance, contemporary neural networks often lack
structural safeguards that promote stable learning and interpretable behavior.
In this work, we introduce a reformulation of layer-level transformations that
departs from the standard unconstrained affine paradigm. Each transformation is
decomposed into a structured linear operator and a residual corrective
component, enabling more disciplined signal propagation and improved training
dynamics. Our formulation encourages internal consistency and supports stable
information flow across depth, while remaining fully compatible with standard
learning objectives and backpropagation. Through a series of synthetic and
real-world experiments, we demonstrate that models constructed with these
structured transformations exhibit improved gradient conditioning, reduced
sensitivity to perturbations, and layer-wise robustness. We further show that
these benefits persist across architectural scales and training regimes. This
study serves as a foundation for a more principled class of neural
architectures that prioritize stability and transparency-offering new tools for
reasoning about learning behavior without sacrificing expressive power.

</details>


### [18] [ECG Latent Feature Extraction with Autoencoders for Downstream Prediction Tasks](https://arxiv.org/abs/2508.00131)
*Christopher Harvey,Sumaiya Shomaji,Zijun Yao,Amit Noheria*

Main category: cs.LG

TL;DR: 论文探讨了通过PCA和自动编码器简化ECG信号的方法，并提出了三种新的VAE变体，显著提高了信号重建和预测性能。


<details>
  <summary>Details</summary>
Motivation: ECG信号复杂且个体差异大，传统深度学习方法在小数据集上表现不佳，因此需要更高效的特征提取方法。

Method: 研究提出了三种VAE变体（SAE、A beta-VAE、C beta-VAE），结合PCA和自动编码器，用于ECG信号降维和特征提取。

Result: A beta-VAE在信号重建中表现最佳（MAE为15.7±3.2 μV），SAE编码结合传统特征提高了LVEF预测（AUROC 0.901）。

Conclusion: 提出的VAE变体不仅简化了ECG数据，还为小规模数据集的深度学习应用提供了实用解决方案。

Abstract: The electrocardiogram (ECG) is an inexpensive and widely available tool for
cardiac assessment. Despite its standardized format and small file size, the
high complexity and inter-individual variability of ECG signals (typically a
60,000-size vector with 12 leads at 500 Hz) make it challenging to use in deep
learning models, especially when only small training datasets are available.
This study addresses these challenges by exploring feature generation methods
from representative beat ECGs, focusing on Principal Component Analysis (PCA)
and Autoencoders to reduce data complexity. We introduce three novel
Variational Autoencoder (VAE) variants-Stochastic Autoencoder (SAE), Annealed
beta-VAE (A beta-VAE), and Cyclical beta VAE (C beta-VAE)-and compare their
effectiveness in maintaining signal fidelity and enhancing downstream
prediction tasks using a Light Gradient Boost Machine (LGBM). The A beta-VAE
achieved superior signal reconstruction, reducing the mean absolute error (MAE)
to 15.7+/-3.2 muV, which is at the level of signal noise. Moreover, the SAE
encodings, when combined with traditional ECG summary features, improved the
prediction of reduced Left Ventricular Ejection Fraction (LVEF), achieving an
holdout test set area under the receiver operating characteristic curve (AUROC)
of 0.901 with a LGBM classifier. This performance nearly matches the 0.909
AUROC of state-of-the-art CNN model but requires significantly less
computational resources. Further, the ECG feature extraction-LGBM pipeline
avoids overfitting and retains predictive performance when trained with less
data. Our findings demonstrate that these VAE encodings are not only effective
in simplifying ECG data but also provide a practical solution for applying deep
learning in contexts with limited-scale labeled training data.

</details>


### [19] [INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks](https://arxiv.org/abs/2508.00141)
*Mohit Gupta,Debjit Bhowmick,Rhys Newbury,Meead Saberi,Shirui Pan,Ben Beck*

Main category: cs.LG

TL;DR: INSPIRE-GNN是一种结合强化学习和图神经网络的框架，用于优化稀疏数据环境中的自行车流量传感器布局和估计。


<details>
  <summary>Details</summary>
Motivation: 解决城市自行车流量数据稀疏性问题，提升交通规划的准确性和可靠性。

Method: 结合图卷积网络（GCN）、图注意力网络（GAT）和深度Q网络（DQN）的强化学习代理，优化传感器布局。

Result: 在墨尔本自行车网络中，INSPIRE-GNN显著提升了流量估计性能，优于传统启发式方法和其他机器学习模型。

Conclusion: 该框架为交通规划者提供了优化传感器网络和提升数据可靠性的有效工具。

Abstract: Accurate link-level bicycling volume estimation is essential for sustainable
urban transportation planning. However, many cities face significant challenges
of high data sparsity due to limited bicycling count sensor coverage. To
address this issue, we propose INSPIRE-GNN, a novel Reinforcement Learning
(RL)-boosted hybrid Graph Neural Network (GNN) framework designed to optimize
sensor placement and improve link-level bicycling volume estimation in
data-sparse environments. INSPIRE-GNN integrates Graph Convolutional Networks
(GCN) and Graph Attention Networks (GAT) with a Deep Q-Network (DQN)-based RL
agent, enabling a data-driven strategic selection of sensor locations to
maximize estimation performance. Applied to Melbourne's bicycling network,
comprising 15,933 road segments with sensor coverage on only 141 road segments
(99% sparsity) - INSPIRE-GNN demonstrates significant improvements in volume
estimation by strategically selecting additional sensor locations in
deployments of 50, 100, 200 and 500 sensors. Our framework outperforms
traditional heuristic methods for sensor placement such as betweenness
centrality, closeness centrality, observed bicycling activity and random
placement, across key metrics such as Mean Squared Error (MSE), Root Mean
Squared Error (RMSE) and Mean Absolute Error (MAE). Furthermore, our
experiments benchmark INSPIRE-GNN against standard machine learning and deep
learning models in the bicycle volume estimation performance, underscoring its
effectiveness. Our proposed framework provides transport planners actionable
insights to effectively expand sensor networks, optimize sensor placement and
maximize volume estimation accuracy and reliability of bicycling data for
informed transportation planning decisions.

</details>


### [20] [Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs](https://arxiv.org/abs/2508.00161)
*Ziqian Zhong,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 提出了一种基于权重而非激活的新方法，用于理解和监控微调后的大语言模型（LLMs），无需依赖与训练数据分布相似的数据。


<details>
  <summary>Details</summary>
Motivation: 现有基于激活的解释方法需要分布相似的数据，限制了其在检测新型威胁（如后门）时的应用。

Method: 通过分析微调模型与基础模型权重差异的顶部奇异向量，识别新行为，并通过激活的余弦相似度监控这些行为。

Result: 方法在后门攻击检测中达到100%拦截率（假阳性率低于1.2%），在未学习模型中对已删除主题的检测准确率达95.42%，并能恢复“未学习”信息。

Conclusion: 该方法不仅适用于模型监控，还可用于预部署审计，揭示了商业模型微调的特定关注点。

Abstract: The releases of powerful open-weight large language models (LLMs) are often
not accompanied by access to their full training data. Existing
interpretability methods, particularly those based on activations, often
require or assume distributionally similar data. This is a significant
limitation when detecting and defending against novel potential threats like
backdoors, which are by definition out-of-distribution.
  In this work, we introduce a new method for understanding, monitoring and
controlling fine-tuned LLMs that interprets weights, rather than activations,
thereby side stepping the need for data that is distributionally similar to the
unknown training data. We demonstrate that the top singular vectors of the
weight difference between a fine-tuned model and its base model correspond to
newly acquired behaviors. By monitoring the cosine similarity of activations
along these directions, we can detect salient behaviors introduced during
fine-tuning with high precision.
  For backdoored models that bypasses safety mechanisms when a secret trigger
is present, our method stops up to 100% of attacks with a false positive rate
below 1.2%. For models that have undergone unlearning, we detect inference on
erased topics with accuracy up to 95.42% and can even steer the model to
recover "unlearned" information. Besides monitoring, our method also shows
potential for pre-deployment model auditing: by analyzing commercial
instruction-tuned models (OLMo, Llama, Qwen), we are able to uncover
model-specific fine-tuning focus including marketing strategies and Midjourney
prompt generation.
  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.

</details>


### [21] [DiSC-Med: Diffusion-based Semantic Communications for Robust Medical Image Transmission](https://arxiv.org/abs/2508.00172)
*Fupei Guo,Hao Zheng,Xiang Zhang,Li Chen,Yue Wang,Songyang Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于扩散的语义通信框架DiSC-Med，用于高效且鲁棒的医学图像传输。


<details>
  <summary>Details</summary>
Motivation: 人工智能和无线通信技术的发展推动了远程医疗，但医学数据在有限带宽和噪声信道中的高效传输成为关键挑战。

Method: 开发了医学增强的压缩和去噪模块，用于提高带宽效率和鲁棒性，通过语义信息捕获实现高效重建。

Result: 在真实医学数据集上的实验验证了框架的有效性，显示出在远程医疗中的潜力。

Conclusion: DiSC-Med在噪声信道中实现了超高带宽效率和优异的重建性能，适用于高效且鲁棒的远程医疗应用。

Abstract: The rapid development of artificial intelligence has driven smart health with
next-generation wireless communication technologies, stimulating exciting
applications in remote diagnosis and intervention. To enable a timely and
effective response for remote healthcare, efficient transmission of medical
data through noisy channels with limited bandwidth emerges as a critical
challenge. In this work, we propose a novel diffusion-based semantic
communication framework, namely DiSC-Med, for the medical image transmission,
where medical-enhanced compression and denoising blocks are developed for
bandwidth efficiency and robustness, respectively. Unlike conventional
pixel-wise communication framework, our proposed DiSC-Med is able to capture
the key semantic information and achieve superior reconstruction performance
with ultra-high bandwidth efficiency against noisy channels. Extensive
experiments on real-world medical datasets validate the effectiveness of our
framework, demonstrating its potential for robust and efficient telehealth
applications.

</details>


### [22] [RL as Regressor: A Reinforcement Learning Approach for Function Approximation](https://arxiv.org/abs/2508.00174)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 论文提出将回归问题转化为强化学习问题，通过自定义奖励信号和RL算法解决传统回归方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统回归方法受限于预定义的可微损失函数，无法灵活处理非对称成本或复杂目标。

Method: 将模型预测视为动作，定义基于预测误差的自定义奖励信号，利用Actor-Critic算法及改进技术（如优先经验回放、网络容量扩展和位置编码）进行函数逼近。

Result: RL框架成功解决了回归问题，并在目标定义和学习过程中提供了更高的灵活性。

Conclusion: 强化学习为回归问题提供了更灵活的解决方案，能够适应复杂和非可微的目标。

Abstract: Standard regression techniques, while powerful, are often constrained by
predefined, differentiable loss functions such as mean squared error. These
functions may not fully capture the desired behavior of a system, especially
when dealing with asymmetric costs or complex, non-differentiable objectives.
In this paper, we explore an alternative paradigm: framing regression as a
Reinforcement Learning (RL) problem. We demonstrate this by treating a model's
prediction as an action and defining a custom reward signal based on the
prediction error, and we can leverage powerful RL algorithms to perform
function approximation. Through a progressive case study of learning a noisy
sine wave, we illustrate the development of an Actor-Critic agent, iteratively
enhancing it with Prioritized Experience Replay, increased network capacity,
and positional encoding to enable a capable RL agent for this regression task.
Our results show that the RL framework not only successfully solves the
regression problem but also offers enhanced flexibility in defining objectives
and guiding the learning process.

</details>


### [23] [EMA Without the Lag: Bias-Corrected Iterate Averaging Schemes](https://arxiv.org/abs/2508.00180)
*Adam Block,Cyril Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种改进的指数移动平均方法（BEMA），用于减少语言模型微调中的训练不稳定性，同时避免传统EMA引入的偏差。


<details>
  <summary>Details</summary>
Motivation: 语言模型微调中的随机性（如小批量训练）会导致训练不稳定，传统EMA方法虽能平滑训练但会引入偏差，影响优化速度。

Method: 提出BEMA方法，通过校正偏差保留EMA的方差减少优势，同时消除偏差。

Result: 实验证明BEMA在多种语言模型基准测试中显著提升了收敛速度和最终性能。

Conclusion: BEMA是一种实用且理论支持的方法，能更稳定高效地进行语言模型微调。

Abstract: Stochasticity in language model fine-tuning, often caused by the small batch
sizes typically used in this regime, can destabilize training by introducing
large oscillations in generation quality. A popular approach to mitigating this
instability is to take an Exponential moving average (EMA) of weights
throughout training. While EMA reduces stochasticity, thereby smoothing
training, the introduction of bias from old iterates often creates a lag in
optimization relative to vanilla training. In this work, we propose the
Bias-Corrected Exponential Moving Average (BEMA), a simple and practical
augmentation of EMA that retains variance-reduction benefits while eliminating
bias. BEMA is motivated by a simple theoretical model wherein we demonstrate
provable acceleration of BEMA over both a standard EMA and vanilla training.
Through an extensive suite of experiments on Language Models, we show that BEMA
leads to significantly improved convergence rates and final performance over
both EMA and vanilla training in a variety of standard LM benchmarks, making
BEMA a practical and theoretically motivated intervention for more stable and
efficient fine-tuning.

</details>


### [24] [RecoMind: A Reinforcement Learning Framework for Optimizing In-Session User Satisfaction in Recommendation Systems](https://arxiv.org/abs/2508.00201)
*Mehdi Ben Ayed,Fei Feng,Jay Adams,Vishwakarma Singh,Kritarth Anand,Jiajing Xu*

Main category: cs.LG

TL;DR: RecoMind是一个基于模拟器的强化学习框架，用于优化大规模推荐系统中的会话目标，显著提升用户满意度。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统主要依赖监督学习，难以优化长期目标（如会话参与度），而强化学习在大规模应用中面临工程复杂性挑战。

Method: RecoMind利用现有推荐模型构建模拟环境，并通过自定义探索策略高效探索大规模动作空间，简化RL策略的训练和部署。

Result: 离线模拟和在线A/B测试表明，RecoMind训练的RL策略在会话用户满意度上显著优于传统监督学习方法，视频观看时长和会话深度均有提升。

Conclusion: RecoMind为大规模推荐系统提供了一种可扩展的RL嵌入方法，展示了优化会话用户满意度的巨大潜力。

Abstract: Existing web-scale recommendation systems commonly use supervised learning
methods that prioritize immediate user feedback. Although reinforcement
learning (RL) offers a solution to optimize longer-term goals, such as
in-session engagement, applying it at web scale is challenging due to the
extremely large action space and engineering complexity. In this paper, we
introduce RecoMind, a simulator-based RL framework designed for the effective
optimization of session-based goals at web-scale. RecoMind leverages existing
recommendation models to establish a simulation environment and to bootstrap
the RL policy to optimize immediate user interactions from the outset. This
method integrates well with existing industry pipelines, simplifying the
training and deployment of RL policies. Additionally, RecoMind introduces a
custom exploration strategy to efficiently explore web-scale action spaces with
hundreds of millions of items. We evaluated RecoMind through extensive offline
simulations and online A/B testing on a video streaming platform. Both methods
showed that the RL policy trained using RecoMind significantly outperforms
traditional supervised learning recommendation approaches in in-session user
satisfaction. In online A/B tests, the RL policy increased videos watched for
more than 10 seconds by 15.81\% and improved session depth by 4.71\% for
sessions with at least 10 interactions. As a result, RecoMind presents a
systematic and scalable approach for embedding RL into web-scale recommendation
systems, showing great promise for optimizing session-based user satisfaction.

</details>


### [25] [Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models](https://arxiv.org/abs/2508.00202)
*Ecem Bozkurt,Antonio Ortega*

Main category: cs.LG

TL;DR: 提出了一种两阶段框架，利用几何信息在标签噪声下实现鲁棒分类，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 在标签噪声环境下，现有kNN方法依赖局部几何信息，但性能仍有提升空间。

Method: 采用两阶段方法：可靠性估计和可靠性加权推断，引入非负核（NNK）邻域构建。

Result: 在CIFAR-10和DermaMNIST上，方法优于标准kNN和自适应邻域基线。

Conclusion: 通过几何信息改进的可靠性估计和推断方法，显著提升了标签噪声下的分类鲁棒性。

Abstract: Foundation models (FMs) pretrained on large datasets have become fundamental
for various downstream machine learning tasks, in particular in scenarios where
obtaining perfectly labeled data is prohibitively expensive. In this paper, we
assume an FM has to be fine-tuned with noisy data and present a two-stage
framework to ensure robust classification in the presence of label noise
without model retraining. Recent work has shown that simple k-nearest neighbor
(kNN) approaches using an embedding derived from an FM can achieve good
performance even in the presence of severe label noise. Our work is motivated
by the fact that these methods make use of local geometry. In this paper,
following a similar two-stage procedure, reliability estimation followed by
reliability-weighted inference, we show that improved performance can be
achieved by introducing geometry information. For a given instance, our
proposed inference uses a local neighborhood of training data, obtained using
the non-negative kernel (NNK) neighborhood construction. We propose several
methods for reliability estimation that can rely less on distance and local
neighborhood as the label noise increases. Our evaluation on CIFAR-10 and
DermaMNIST shows that our methods improve robustness across various noise
conditions, surpassing standard K-NN approaches and recent
adaptive-neighborhood baselines.

</details>


### [26] [Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product](https://arxiv.org/abs/2508.00230)
*Paul Albert,Frederic Z. Zhang,Hemanth Saratchandran,Anton van den Hengel,Ehsan Abbasnejad*

Main category: cs.LG

TL;DR: 论文比较了低秩适应（LoRA）和全秩PEFT方法，发现LoRA在近似平坦谱或高频矩阵时表现不佳，提出了一种新方法KRAdapter，利用Khatri-Rao积生成高有效秩的权重更新，并在大规模模型上验证了其性能优势。


<details>
  <summary>Details</summary>
Motivation: 研究LoRA在多模态和大语言模型中的局限性，探索更高效的PEFT方法。

Method: 通过合成矩阵近似基准定量比较全秩和低秩PEFT方法，提出KRAdapter算法。

Result: KRAdapter在视觉语言模型和大型语言模型上表现优于LoRA，尤其在未见常识推理任务中。

Conclusion: KRAdapter是一种高效且实用的PEFT方法，适用于十亿级参数模型的微调。

Abstract: Parameter-efficient fine-tuning (PEFT) has become a standard approach for
adapting large pre-trained models. Amongst PEFT methods, low-rank adaptation
(LoRA) has achieved notable success. However, recent studies have highlighted
its limitations compared against full-rank alternatives, particularly when
applied to multimodal and large language models. In this work, we present a
quantitative comparison amongst full-rank and low-rank PEFT methods using a
synthetic matrix approximation benchmark with controlled spectral properties.
Our results confirm that LoRA struggles to approximate matrices with relatively
flat spectrums or high frequency components -- signs of high effective ranks.
To this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the
Khatri-Rao product to produce weight updates, which, by construction, tends to
produce matrix product with a high effective rank. We demonstrate performance
gains with KRAdapter on vision-language models up to 1B parameters and on large
language models up to 8B parameters, particularly on unseen common-sense
reasoning tasks. In addition, KRAdapter maintains the memory and compute
efficiency of LoRA, making it a practical and robust alternative to fine-tune
billion-scale parameter models.

</details>


### [27] [Calibrated Language Models and How to Find Them with Label Smoothing](https://arxiv.org/abs/2508.00264)
*Jerry Huang,Peng Lu,Qiuhao Zeng*

Main category: cs.LG

TL;DR: 研究发现指令微调会导致大语言模型（LLMs）的置信度校准退化，提出标签平滑作为解决方案，并针对大词汇量LLMs的问题进行了理论和实验验证，同时优化了内存消耗。


<details>
  <summary>Details</summary>
Motivation: 尽管NLP和LLMs的进步提升了交互能力，但指令微调对模型输出置信度校准的影响尚未充分研究。

Method: 通过分析开源LLMs，发现指令微调导致校准退化，提出标签平滑作为解决方案，并针对大词汇量LLMs的问题进行理论和实验验证。

Result: 标签平滑能有效维持校准，但在大词汇量LLMs中效果受限，通过定制内核优化了内存消耗。

Conclusion: 标签平滑是指令微调中维持校准的有效方法，但需针对大词汇量LLMs进一步优化。

Abstract: Recent advances in natural language processing (NLP) have opened up greater
opportunities to enable fine-tuned large language models (LLMs) to behave as
more powerful interactive agents through improved instruction-following
ability. However, understanding how this impacts confidence calibration for
reliable model output has not been researched in full. In this work, we examine
various open-sourced LLMs, identifying significant calibration degradation
after instruction tuning in each. Seeking a practical solution, we look towards
label smoothing, which has been shown as an effective method to regularize for
overconfident predictions but has yet to be widely adopted in the supervised
fine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing
is sufficient to maintain calibration throughout the SFT process. However,
settings remain where the effectiveness of smoothing is severely diminished, in
particular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to
stem from the ability to become over-confident, which has a direct relationship
with the hidden size and vocabulary size, and justify this theoretically and
experimentally. Finally, we address an outstanding issue regarding the memory
footprint of the cross-entropy loss computation in the label smoothed loss
setting, designing a customized kernel to dramatically reduce memory
consumption without sacrificing speed or performance in comparison to existing
solutions for non-smoothed losses.

</details>


### [28] [Learning to Optimize Feedback for One Million Students: Insights from Multi-Armed and Contextual Bandits in Large-Scale Online Tutoring](https://arxiv.org/abs/2508.00270)
*Robin Schmucker,Nimish Pachapurkar,Shanmuga Bala,Miral Shah,Tom Mitchell*

Main category: cs.LG

TL;DR: 论文介绍了一种在线辅导系统，通过多臂老虎机框架和离线策略评估，优化学生反馈以提高学习效果。


<details>
  <summary>Details</summary>
Motivation: 目标是提供有效的反馈以优化学生学习，并探索个性化反馈的潜力。

Method: 使用多臂老虎机（MAB）框架和离线策略评估，分析43,000个辅助动作，并设计算法选择适合的策略目标。进一步研究上下文老虎机（CB）策略的个性化效果。

Result: 在166,000次练习会话中验证，MAB策略显著改善学生表现。CB策略在某些情况下效果有限。

Conclusion: 数据驱动系统在大规模部署中表现良好，但个性化反馈的效果提升有限。

Abstract: We present an online tutoring system that learns to provide effective
feedback to students after they answer questions incorrectly. Using data from
one million students, the system learns which assistance action (e.g., one of
multiple hints) to provide for each question to optimize student learning.
Employing the multi-armed bandit (MAB) framework and offline policy evaluation,
we assess 43,000 assistance actions, and identify trade-offs between assistance
policies optimized for different student outcomes (e.g., response correctness,
session completion). We design an algorithm that for each question decides on a
suitable policy training objective to enhance students' immediate second
attempt success and overall practice session performance. We evaluate the
resulting MAB policies in 166,000 practice sessions, verifying significant
improvements in student outcomes. While MAB policies optimize feedback for the
overall student population, we further investigate whether contextual bandit
(CB) policies can enhance outcomes by personalizing feedback based on
individual student features (e.g., ability estimates, response times). Using
causal inference, we examine (i) how effects of assistance actions vary across
students and (ii) whether CB policies, which leverage such effect
heterogeneity, outperform MAB policies. While our analysis reveals that some
actions for some questions exhibit effect heterogeneity, effect sizes may often
be too small for CB policies to provide significant improvements beyond what
well-optimized MAB policies that deliver the same action to all students
already achieve. We discuss insights gained from deploying data-driven systems
at scale and implications for future refinements. Today, the teaching policies
optimized by our system support thousands of students daily.

</details>


### [29] [Toward using explainable data-driven surrogate models for treating performance-based seismic design as an inverse engineering problem](https://arxiv.org/abs/2508.00286)
*Mohsen Zaker Esteghamati*

Main category: cs.LG

TL;DR: 该研究提出了一种基于可解释机器学习模型的性能抗震设计方法，通过遗传优化算法解决设计参数与性能目标之间的逆向工程问题，并在实际建筑案例中验证了其高准确性和工程一致性。


<details>
  <summary>Details</summary>
Motivation: 解决传统性能抗震设计中的计算效率低下问题，通过逆向工程方法直接映射设计变量与性能指标。

Method: 采用可解释机器学习模型映射设计变量与性能指标，并将其作为评估函数集成到遗传优化算法中。

Result: 在多种建筑类型和地震条件下，模型表现出高准确性（R2>90%），优化算法能识别符合工程原则的构件最优属性。

Conclusion: 该方法为性能抗震设计提供了高效且准确的解决方案，适用于多样化的建筑和地震条件。

Abstract: This study presents a methodology to treat performance-based seismic design
as an inverse engineering problem, where design parameters are directly derived
to achieve specific performance objectives. By implementing explainable machine
learning models, this methodology directly maps design variables and
performance metrics, tackling computational inefficiencies of performance-based
design. The resultant machine learning model is integrated as an evaluation
function into a genetic optimization algorithm to solve the inverse problem.
The developed methodology is then applied to two different inventories of steel
and concrete moment frames in Los Angeles and Charleston to obtain sectional
properties of frame members that minimize expected annualized seismic loss in
terms of repair costs. The results show high accuracy of the surrogate models
(e.g., R2> 90%) across a diverse set of building types, geometries, seismic
design, and site hazard, where the optimization algorithm could identify the
optimum values of members' properties for a fixed set of geometric variables,
consistent with engineering principles.

</details>


### [30] [Invariant Graph Transformer for Out-of-Distribution Generalization](https://arxiv.org/abs/2508.00304)
*Tianyin Liao,Ziwei Zhang,Yufei Sun,Chunyu Hu,Jianxin Li*

Main category: cs.LG

TL;DR: GOODFormer是一种基于图不变学习的Transformer模型，旨在解决图数据分布偏移下的泛化问题，通过分离不变和变体子图、动态编码和不变学习模块实现。


<details>
  <summary>Details</summary>
Motivation: 现有图Transformer在数据分布偏移下泛化能力不足，图不变学习可能提供解决方案，但如何设计相关机制仍具挑战性。

Method: 提出GOODFormer，包含熵引导的不变子图分离器、动态子图编码器和不变学习模块，联合优化以捕捉不变关系。

Result: 在基准数据集上，GOODFormer在分布偏移下表现优于现有方法。

Conclusion: GOODFormer通过不变学习机制有效提升图Transformer在分布偏移下的泛化能力。

Abstract: Graph Transformers (GTs) have demonstrated great effectiveness across various
graph analytical tasks. However, the existing GTs focus on training and testing
graph data originated from the same distribution, but fail to generalize under
distribution shifts. Graph invariant learning, aiming to capture generalizable
graph structural patterns with labels under distribution shifts, is potentially
a promising solution, but how to design attention mechanisms and positional and
structural encodings (PSEs) based on graph invariant learning principles
remains challenging. To solve these challenges, we introduce Graph
Out-Of-Distribution generalized Transformer (GOODFormer), aiming to learn
generalized graph representations by capturing invariant relationships between
predictive graph structures and labels through jointly optimizing three
modules. Specifically, we first develop a GT-based entropy-guided invariant
subgraph disentangler to separate invariant and variant subgraphs while
preserving the sharpness of the attention function. Next, we design an evolving
subgraph positional and structural encoder to effectively and efficiently
capture the encoding information of dynamically changing subgraphs during
training. Finally, we propose an invariant learning module utilizing subgraph
node representations and encodings to derive generalizable graph
representations that can to unseen graphs. We also provide theoretical
justifications for our method. Extensive experiments on benchmark datasets
demonstrate the superiority of our method over state-of-the-art baselines under
distribution shifts.

</details>


### [31] [PnP-DA: Towards Principled Plug-and-Play Integration of Variational Data Assimilation and Generative Models](https://arxiv.org/abs/2508.00325)
*Yongquan Qu,Matthieu Blanke,Sara Shamekh,Pierre Gentine*

Main category: cs.LG

TL;DR: PnP-DA是一种新的数据同化方法，通过结合轻量级梯度更新和预训练生成先验，减少地球系统建模中的预测误差。


<details>
  <summary>Details</summary>
Motivation: 地球系统建模需要高效计算复杂多尺度非线性动态，但传统方法因简化假设导致误差累积。数据同化旨在优化观测与模型预测的融合，但传统变分方法假设高斯误差统计，无法捕捉混沌系统的非高斯行为。

Method: PnP-DA交替执行轻量级梯度分析更新和预训练生成先验的前向传递，利用条件Wasserstein耦合，避免复杂神经网络的反向传播。

Result: 在标准混沌测试中，PnP-DA在不同观测稀疏性和噪声水平下均能减少预测误差，优于传统变分方法。

Conclusion: PnP-DA通过放松统计假设和利用历史数据，提供了一种更有效的数据同化策略。

Abstract: Earth system modeling presents a fundamental challenge in scientific
computing: capturing complex, multiscale nonlinear dynamics in computationally
efficient models while minimizing forecast errors caused by necessary
simplifications. Even the most powerful AI- or physics-based forecast system
suffer from gradual error accumulation. Data assimilation (DA) aims to mitigate
these errors by optimally blending (noisy) observations with prior model
forecasts, but conventional variational methods often assume Gaussian error
statistics that fail to capture the true, non-Gaussian behavior of chaotic
dynamical systems. We propose PnP-DA, a Plug-and-Play algorithm that alternates
(1) a lightweight, gradient-based analysis update (using a Mahalanobis-distance
misfit on new observations) with (2) a single forward pass through a pretrained
generative prior conditioned on the background forecast via a conditional
Wasserstein coupling. This strategy relaxes restrictive statistical assumptions
and leverages rich historical data without requiring an explicit regularization
functional, and it also avoids the need to backpropagate gradients through the
complex neural network that encodes the prior during assimilation cycles.
Experiments on standard chaotic testbeds demonstrate that this strategy
consistently reduces forecast errors across a range of observation sparsities
and noise levels, outperforming classical variational methods.

</details>


### [32] [Embryology of a Language Model](https://arxiv.org/abs/2508.00331)
*George Wang,Garrett Baker,Andrew Gordon,Daniel Murfet*

Main category: cs.LG

TL;DR: 论文提出了一种基于UMAP和敏感性分析的方法，可视化语言模型在训练过程中的结构发展，揭示了新的网络机制。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型的内部计算结构是深度学习科学的核心问题，敏感性分析作为工具尚未充分发挥潜力。

Method: 应用UMAP对敏感性矩阵进行可视化，分析模型在训练中的结构发展。

Result: 可视化结果揭示了模型结构的“身体计划”，包括已知特征（如感应电路）和新发现的结构（如用于计数空格标记的“间距鳍”）。

Conclusion: 敏感性分析不仅能验证模型，还能发现新机制，为研究复杂神经网络的发育原理提供了全面视角。

Abstract: Understanding how language models develop their internal computational
structure is a central problem in the science of deep learning. While
susceptibilities, drawn from statistical physics, offer a promising analytical
tool, their full potential for visualizing network organization remains
untapped. In this work, we introduce an embryological approach, applying UMAP
to the susceptibility matrix to visualize the model's structural development
over training. Our visualizations reveal the emergence of a clear ``body
plan,'' charting the formation of known features like the induction circuit and
discovering previously unknown structures, such as a ``spacing fin'' dedicated
to counting space tokens. This work demonstrates that susceptibility analysis
can move beyond validation to uncover novel mechanisms, providing a powerful,
holistic lens for studying the developmental principles of complex neural
networks.

</details>


### [33] [BOOD: Boundary-based Out-Of-Distribution Data Generation](https://arxiv.org/abs/2508.00350)
*Qilin Liao,Shuo Yang,Bo Zhao,Ping Luo,Hengshuang Zhao*

Main category: cs.LG

TL;DR: BOOD框架利用扩散模型生成高质量OOD特征和图像，显著提升OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决在潜在空间中提取有效OOD特征的挑战，明确ID与OOD数据的边界。

Method: 通过文本条件潜在特征空间学习，扰动ID边界特征生成OOD特征，并用扩散模型解码为图像。

Result: 在CIFAR-100上，FPR95降低29.64%，AUROC提升7.27%。

Conclusion: BOOD提供了一种高效的OOD特征生成方法，显著优于现有技术。

Abstract: Harnessing the power of diffusion models to synthesize auxiliary training
data based on latent space features has proven effective in enhancing
out-of-distribution (OOD) detection performance. However, extracting effective
features outside the in-distribution (ID) boundary in latent space remains
challenging due to the difficulty of identifying decision boundaries between
classes. This paper proposes a novel framework called Boundary-based
Out-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD
features and generates human-compatible outlier images using diffusion models.
BOOD first learns a text-conditioned latent feature space from the ID dataset,
selects ID features closest to the decision boundary, and perturbs them to
cross the decision boundary to form OOD features. These synthetic OOD features
are then decoded into images in pixel space by a diffusion model. Compared to
previous works, BOOD provides a more training efficient strategy for
synthesizing informative OOD features, facilitating clearer distinctions
between ID and OOD data. Extensive experimental results on common benchmarks
demonstrate that BOOD surpasses the state-of-the-art method significantly,
achieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27%
improvement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset.

</details>


### [34] [Sheaf Graph Neural Networks via PAC-Bayes Spectral Optimization](https://arxiv.org/abs/2508.00357)
*Yoonhyuk Choi,Jiho Choi,Chong-Kwon Kim*

Main category: cs.LG

TL;DR: SGPC是一种结合了细胞鞘消息传递、最优传输提升、方差减少扩散和PAC-Bayes谱正则化的新型图神经网络架构，用于解决异质图上的过平滑问题，并在半监督节点分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络在异质图上因过平滑导致的节点特征崩溃问题，同时克服现有鞘神经网络在泛化和扩展性上的局限性。

Method: 提出SGPC架构，结合细胞鞘消息传递、最优传输提升、方差减少扩散和PAC-Bayes谱正则化，通过端到端训练实现线性计算复杂度。

Result: 在九个同质和异质基准测试中，SGPC优于现有谱和鞘基图神经网络，并提供未见节点的认证置信区间。

Conclusion: SGPC通过理论性能界限和实验验证，为异质图上的半监督节点分类提供了高效且稳定的解决方案。

Abstract: Over-smoothing in Graph Neural Networks (GNNs) causes collapse in distinct
node features, particularly on heterophilic graphs where adjacent nodes often
have dissimilar labels. Although sheaf neural networks partially mitigate this
problem, they typically rely on static or heavily parameterized sheaf
structures that hinder generalization and scalability. Existing sheaf-based
models either predefine restriction maps or introduce excessive complexity, yet
fail to provide rigorous stability guarantees. In this paper, we introduce a
novel scheme called SGPC (Sheaf GNNs with PAC-Bayes Calibration), a unified
architecture that combines cellular-sheaf message passing with several
mechanisms, including optimal transport-based lifting, variance-reduced
diffusion, and PAC-Bayes spectral regularization for robust semi-supervised
node classification. We establish performance bounds theoretically and
demonstrate that the resulting bound-aware objective can be achieved via
end-to-end training in linear computational complexity. Experiments on nine
homophilic and heterophilic benchmarks show that SGPC outperforms
state-of-the-art spectral and sheaf-based GNNs while providing certified
confidence intervals on unseen nodes.

</details>


### [35] [OID-PPO: Optimal Interior Design using Proximal Policy Optimization by Transforming Design Guidelines into Reward Functions](https://arxiv.org/abs/2508.00364)
*Chanyoung Yoon,Sangbong Yoo,Soobin Yim,Chansoo Kim,Yun Jang*

Main category: cs.LG

TL;DR: OID-PPO是一种基于强化学习的新框架，用于优化室内设计，通过整合专家定义的设计准则，显著提升了布局质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 住宅室内设计对居住满意度至关重要，但现有方法存在计算成本高、数据稀缺或设计准则不足的问题。

Method: 提出OID-PPO框架，结合近端策略优化和专家定义的功能与视觉准则，采用对角高斯策略实现连续灵活的家具布局。

Result: 实验表明，OID-PPO在多样化的房间形状和家具配置中，显著优于现有方法。

Conclusion: OID-PPO通过整合设计准则和灵活的布局策略，有效解决了室内设计的挑战。

Abstract: Designing residential interiors strongly impacts occupant satisfaction but
remains challenging due to unstructured spatial layouts, high computational
demands, and reliance on expert knowledge. Existing methods based on
optimization or deep learning are either computationally expensive or
constrained by data scarcity. Reinforcement learning (RL) approaches often
limit furniture placement to discrete positions and fail to incorporate design
principles adequately. We propose OID-PPO, a novel RL framework for Optimal
Interior Design using Proximal Policy Optimization, which integrates
expert-defined functional and visual guidelines into a structured reward
function. OID-PPO utilizes a diagonal Gaussian policy for continuous and
flexible furniture placement, effectively exploring latent environmental
dynamics under partial observability. Experiments conducted across diverse room
shapes and furniture configurations demonstrate that OID-PPO significantly
outperforms state-of-the-art methods in terms of layout quality and
computational efficiency. Ablation studies further demonstrate the impact of
structured guideline integration and reveal the distinct contributions of
individual design constraints.

</details>


### [36] [Dual Adaptivity: Universal Algorithms for Minimizing the Adaptive Regret of Convex Functions](https://arxiv.org/abs/2508.00392)
*Lijun Zhang,Wenhao Yang,Guanghui Wang,Wei Jiang,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种具有双重适应性的通用算法，通过元专家框架动态创建和聚合专家，以适应不同凸函数类型和变化环境。


<details>
  <summary>Details</summary>
Motivation: 现有算法缺乏通用性，只能处理单一凸函数类型且需要先验参数知识，限制了实际应用。

Method: 采用元专家框架，动态创建多个专家并通过元算法聚合，结合睡眠专家技术捕捉环境变化。

Result: 理论分析表明，算法能同时最小化多种凸函数的自适应遗憾，并允许函数类型在轮次间切换。

Conclusion: 该框架扩展至在线复合优化，开发了通用算法以最小化复合函数的自适应遗憾。

Abstract: To deal with changing environments, a new performance measure -- adaptive
regret, defined as the maximum static regret over any interval, was proposed in
online learning. Under the setting of online convex optimization, several
algorithms have been successfully developed to minimize the adaptive regret.
However, existing algorithms lack universality in the sense that they can only
handle one type of convex functions and need apriori knowledge of parameters,
which hinders their application in real-world scenarios. To address this
limitation, this paper investigates universal algorithms with dual adaptivity,
which automatically adapt to the property of functions (convex, exponentially
concave, or strongly convex), as well as the nature of environments (stationary
or changing). Specifically, we propose a meta-expert framework for dual
adaptive algorithms, where multiple experts are created dynamically and
aggregated by a meta-algorithm. The meta-algorithm is required to yield a
second-order bound, which can accommodate unknown function types. We further
incorporate the technique of sleeping experts to capture the changing
environments. For the construction of experts, we introduce two strategies
(increasing the number of experts or enhancing the capabilities of experts) to
achieve universality. Theoretical analysis shows that our algorithms are able
to minimize the adaptive regret for multiple types of convex functions
simultaneously, and also allow the type of functions to switch between rounds.
Moreover, we extend our meta-expert framework to online composite optimization,
and develop a universal algorithm for minimizing the adaptive regret of
composite functions.

</details>


### [37] [ExeKGLib: A Platform for Machine Learning Analytics based on Knowledge Graphs](https://arxiv.org/abs/2508.00394)
*Antonis Klironomos,Baifan Zhou,Zhipeng Tan,Zhuoxun Zheng,Mohamed H. Gad-Elrab,Heiko Paulheim,Evgeny Kharlamov*

Main category: cs.LG

TL;DR: ExeKGLib是一个Python库，通过图形界面和知识图谱帮助非ML专家构建ML管道，提升透明度和可重用性。


<details>
  <summary>Details</summary>
Motivation: 解决领域专家缺乏ML专业知识但需要ML分析工具的问题。

Method: 利用知识图谱编码ML知识，提供图形界面简化ML管道构建。

Result: 展示了ExeKGLib的实际用例，证明其可用性和实用性。

Conclusion: ExeKGLib为非ML专家提供了高效、透明的ML管道构建工具。

Abstract: Nowadays machine learning (ML) practitioners have access to numerous ML
libraries available online. Such libraries can be used to create ML pipelines
that consist of a series of steps where each step may invoke up to several ML
libraries that are used for various data-driven analytical tasks. Development
of high-quality ML pipelines is non-trivial; it requires training, ML
expertise, and careful development of each step. At the same time, domain
experts in science and engineering may not possess such ML expertise and
training while they are in pressing need of ML-based analytics. In this paper,
we present our ExeKGLib, a Python library enhanced with a graphical interface
layer that allows users with minimal ML knowledge to build ML pipelines. This
is achieved by relying on knowledge graphs that encode ML knowledge in simple
terms accessible to non-ML experts. ExeKGLib also allows improving the
transparency and reusability of the built ML workflows and ensures that they
are executable. We show the usability and usefulness of ExeKGLib by presenting
real use cases.

</details>


### [38] [Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement](https://arxiv.org/abs/2508.00410)
*Zizhuo Zhang,Jianing Zhu,Xinmu Ge,Zihua Zhao,Zhanke Zhou,Xuan Li,Xiao Feng,Jiangchao Yao,Bo Han*

Main category: cs.LG

TL;DR: 论文提出了一种名为Co-Reward的新型强化学习框架，通过对比语义相似问题的答案一致性作为奖励基础，解决了现有自奖励方法中的崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于人类标注的强化学习方法在复杂任务中难以扩展，而自奖励方法虽能激发大语言模型的推理能力，但存在崩溃问题。

Method: 利用语义相似问题的对比一致性构建奖励，通过简单的投票机制合成代理标签，并交叉引用问题对的标签以增强推理一致性。

Result: Co-Reward在多个推理基准测试和大语言模型系列中表现优异，甚至在某些情况下超过基于真实标注的奖励，如在MATH500上比真实标注奖励提升6.8%。

Conclusion: Co-Reward通过自监督奖励机制有效避免了崩溃问题，并显著提升了推理能力，为强化学习在大语言模型中的应用提供了新思路。

Abstract: Although reinforcement learning with verifiable rewards (RLVR) shows promise
in improving the reasoning ability of large language models (LLMs), the scaling
up dilemma remains due to the reliance on human annotated labels especially for
complex tasks. Recent alternatives that explore various self-reward signals
exhibit the eliciting potential of LLM reasoning, but suffer from the
non-negligible collapse issue. Inspired by the success of self-supervised
learning, we propose \textit{Co-Reward}, a novel RL framework that leverages
contrastive agreement across semantically analogical questions as a reward
basis. Specifically, we construct a similar question for each training sample
(without labels) and synthesize their individual surrogate labels through a
simple rollout voting, and then the reward is constructed by cross-referring
the labels of each question pair to enforce the internal reasoning consistency
across analogical inputs. Intuitively, such a self-supervised reward-shaping
mechanism increases the difficulty of learning collapse into a trivial
solution, and promotes stable reasoning elicitation and improvement through
expanding the input sample variants. Empirically, Co-Reward achieves superior
performance compared to other self-reward baselines on multiple reasoning
benchmarks and LLM series, and reaches or even surpasses ground-truth (GT)
labeled reward, with improvements of up to $+6.8\%$ on MATH500 over GT reward
on Llama-3.2-3B-Instruct. Our code is publicly available at
https://github.com/tmlr-group/Co-Reward.

</details>


### [39] [Transforming Credit Risk Analysis: A Time-Series-Driven ResE-BiLSTM Framework for Post-Loan Default Detection](https://arxiv.org/abs/2508.00415)
*Yue Yang,Yuxiang Lin,Ying Zhang,Zihan Su,Chang Chuan Goh,Tangtangfang Fang,Anthony Graham Bellotti,Boon Giin Lee*

Main category: cs.LG

TL;DR: 该研究提出了一种ResE-BiLSTM模型，用于预测贷款违约，通过滑动窗口技术和多种基线模型比较，证明了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 贷款违约预测对信用风险管理至关重要，机器学习方法可以检测财务异常，提高预测准确性。

Method: 使用ResE-BiLSTM模型和滑动窗口技术，在Freddie Mac抵押贷款数据集上评估，并与LSTM、BiLSTM、GRU、CNN和RNN等基线模型比较。

Result: ResE-BiLSTM在准确性、精确度、召回率、F1和AUC等指标上优于基线模型。

Conclusion: ResE-BiLSTM在贷款违约预测中表现出色，具有实际应用价值。

Abstract: Prediction of post-loan default is an important task in credit risk
management, and can be addressed by detection of financial anomalies using
machine learning. This study introduces a ResE-BiLSTM model, using a sliding
window technique, and is evaluated on 44 independent cohorts from the extensive
Freddie Mac US mortgage dataset, to improve prediction performance. The
ResE-BiLSTM is compared with five baseline models: Long Short-Term Memory
(LSTM), BiLSTM, Gated Recurrent Units (GRU), Convolutional Neural Networks
(CNN), and Recurrent Neural Networks (RNN), across multiple metrics, including
Accuracy, Precision, Recall, F1, and AUC. An ablation study was conducted to
evaluate the contribution of individual components in the ResE-BiLSTM
architecture. Additionally, SHAP analysis was employed to interpret the
underlying features the model relied upon for its predictions. Experimental
results demonstrate that ResE-BiLSTM achieves superior predictive performance
compared to baseline models, underscoring its practical value and applicability
in real-world scenarios.

</details>


### [40] [A Conditional GAN for Tabular Data Generation with Probabilistic Sampling of Latent Subspaces](https://arxiv.org/abs/2508.00472)
*Leonidas Akritidis,Panayiotis Bozanis*

Main category: cs.LG

TL;DR: ctdGAN是一种用于缓解表格数据类别不平衡问题的条件GAN，通过空间分区和新的损失函数生成高质量样本。


<details>
  <summary>Details</summary>
Motivation: 表格数据中的类别不平衡会导致机器学习任务性能下降，现有GAN方法未考虑输入样本的向量子空间，且类别标签处理不够有效。

Method: ctdGAN通过空间分区分配聚类标签，利用新的概率采样策略和损失函数生成样本，并结合聚类缩放技术。

Result: 在14个不平衡数据集上的实验表明，ctdGAN能生成高保真样本并提升分类准确率。

Conclusion: ctdGAN通过改进采样和损失函数，有效解决了表格数据类别不平衡问题。

Abstract: The tabular form constitutes the standard way of representing data in
relational database systems and spreadsheets. But, similarly to other forms,
tabular data suffers from class imbalance, a problem that causes serious
performance degradation in a wide variety of machine learning tasks. One of the
most effective solutions dictates the usage of Generative Adversarial Networks
(GANs) in order to synthesize artificial data instances for the
under-represented classes. Despite their good performance, none of the proposed
GAN models takes into account the vector subspaces of the input samples in the
real data space, leading to data generation in arbitrary locations. Moreover,
the class labels are treated in the same manner as the other categorical
variables during training, so conditional sampling by class is rendered less
effective. To overcome these problems, this study presents ctdGAN, a
conditional GAN for alleviating class imbalance in tabular datasets. Initially,
ctdGAN executes a space partitioning step to assign cluster labels to the input
samples. Subsequently, it utilizes these labels to synthesize samples via a
novel probabilistic sampling strategy and a new loss function that penalizes
both cluster and class mis-predictions. In this way, ctdGAN is trained to
generate samples in subspaces that resemble those of the original data
distribution. We also introduce several other improvements, including a simple,
yet effective cluster-wise scaling technique that captures multiple feature
modes without affecting data dimensionality. The exhaustive evaluation of
ctdGAN with 14 imbalanced datasets demonstrated its superiority in generating
high fidelity samples and improving classification accuracy.

</details>


### [41] [Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration for Text-Attributed Graph Anomaly Detection](https://arxiv.org/abs/2508.00507)
*Yiming Xu,Jiarun Chen,Zhen Peng,Zihan Chen,Qika Lin,Lan Ma,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 论文提出了一种结合大型语言模型（LLMs）和图神经网络（GNNs）的新框架CoLL，用于文本属性图（TAGs）中的异常检测，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有图异常检测方法主要关注图域内的优化目标，忽略了文本模态的互补价值，且文本特征编码方式浅层，可能遗漏异常相关的语义信息。

Method: CoLL框架通过多LLM协作生成证据增强的文本特征，并结合GNN的门控机制自适应融合文本与图结构信息。

Result: 实验表明CoLL在AP指标上平均提升13.37%。

Conclusion: CoLL为将LLMs应用于图异常检测开辟了新途径，展示了其潜力。

Abstract: The natural combination of intricate topological structures and rich textual
information in text-attributed graphs (TAGs) opens up a novel perspective for
graph anomaly detection (GAD). However, existing GAD methods primarily focus on
designing complex optimization objectives within the graph domain, overlooking
the complementary value of the textual modality, whose features are often
encoded by shallow embedding techniques, such as bag-of-words or skip-gram, so
that semantic context related to anomalies may be missed. To unleash the
enormous potential of textual modality, large language models (LLMs) have
emerged as promising alternatives due to their strong semantic understanding
and reasoning capabilities. Nevertheless, their application to TAG anomaly
detection remains nascent, and they struggle to encode high-order structural
information inherent in graphs due to input length constraints. For
high-quality anomaly detection in TAGs, we propose CoLL, a novel framework that
combines LLMs and graph neural networks (GNNs) to leverage their complementary
strengths. CoLL employs multi-LLM collaboration for evidence-augmented
generation to capture anomaly-relevant contexts while delivering human-readable
rationales for detected anomalies. Moreover, CoLL integrates a GNN equipped
with a gating mechanism to adaptively fuse textual features with evidence while
preserving high-order topological information. Extensive experiments
demonstrate the superiority of CoLL, achieving an average improvement of 13.37%
in AP. This study opens a new avenue for incorporating LLMs in advancing GAD.

</details>


### [42] [Text-Attributed Graph Anomaly Detection via Multi-Scale Cross- and Uni-Modal Contrastive Learning](https://arxiv.org/abs/2508.00513)
*Yiming Xu,Xu Hua,Zhen Peng,Bin Shi,Jiarun Chen,Xingbo Fu,Song Wang,Bo Dong*

Main category: cs.LG

TL;DR: 论文提出了一种名为CMUCL的端到端方法，用于文本属性图的异常检测，通过联合训练文本和图编码器，利用跨模态和多尺度一致性提高检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法将文本编码与异常检测分离，导致提取的文本特征可能不聚焦于异常检测相关信息，限制了检测能力。

Method: 提出CMUCL方法，联合建模文本和图结构数据，利用跨模态和单模态多尺度一致性训练编码器，并设计基于不一致性挖掘的异常评分器。

Result: CMUCL在文本属性图异常检测中表现优异，平均准确率（AP）比次优方法提升11.13%。

Conclusion: CMUCL通过整合文本和图拓扑信息，显著提升了异常检测性能，并发布了8个数据集以促进未来研究。

Abstract: The widespread application of graph data in various high-risk scenarios has
increased attention to graph anomaly detection (GAD). Faced with real-world
graphs that often carry node descriptions in the form of raw text sequences,
termed text-attributed graphs (TAGs), existing graph anomaly detection
pipelines typically involve shallow embedding techniques to encode such textual
information into features, and then rely on complex self-supervised tasks
within the graph domain to detect anomalies. However, this text encoding
process is separated from the anomaly detection training objective in the graph
domain, making it difficult to ensure that the extracted textual features focus
on GAD-relevant information, seriously constraining the detection capability.
How to seamlessly integrate raw text and graph topology to unleash the vast
potential of cross-modal data in TAGs for anomaly detection poses a challenging
issue. This paper presents a novel end-to-end paradigm for text-attributed
graph anomaly detection, named CMUCL. We simultaneously model data from both
text and graph structures, and jointly train text and graph encoders by
leveraging cross-modal and uni-modal multi-scale consistency to uncover
potential anomaly-related information. Accordingly, we design an anomaly score
estimator based on inconsistency mining to derive node-specific anomaly scores.
Considering the lack of benchmark datasets tailored for anomaly detection on
TAGs, we release 8 datasets to facilitate future research. Extensive
evaluations show that CMUCL significantly advances in text-attributed graph
anomaly detection, delivering an 11.13% increase in average accuracy (AP) over
the suboptimal.

</details>


### [43] [Online Nonsubmodular Optimization with Delayed Feedback in the Bandit Setting](https://arxiv.org/abs/2508.00523)
*Sifan Yang,Yuanyu Wan,Lijun Zhang*

Main category: cs.LG

TL;DR: 论文研究了在线非子模优化问题，提出了两种算法DBGD-NF和改进版，分别解决了延迟反馈和带反馈的联合效应问题，实验验证了方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法对延迟敏感且未解耦延迟与带反馈的联合效应，限制了性能。

Method: 提出DBGD-NF算法和改进版，分别利用单点梯度估计和块更新机制。

Result: DBGD-NF的遗憾界与平均延迟相关，改进版进一步解耦效应，性能更优。

Conclusion: 新算法在延迟和带反馈场景下表现优越，实验验证了其有效性。

Abstract: We investigate the online nonsubmodular optimization with delayed feedback in
the bandit setting, where the loss function is $\alpha$-weakly DR-submodular
and $\beta$-weakly DR-supermodular. Previous work has established an
$(\alpha,\beta)$-regret bound of $\mathcal{O}(nd^{1/3}T^{2/3})$, where $n$ is
the dimensionality and $d$ is the maximum delay. However, its regret bound
relies on the maximum delay and is thus sensitive to irregular delays.
Additionally, it couples the effects of delays and bandit feedback as its bound
is the product of the delay term and the $\mathcal{O}(nT^{2/3})$ regret bound
in the bandit setting without delayed feedback. In this paper, we develop two
algorithms to address these limitations, respectively. Firstly, we propose a
novel method, namely DBGD-NF, which employs the one-point gradient estimator
and utilizes all the available estimated gradients in each round to update the
decision. It achieves a better $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ regret
bound, which is relevant to the average delay $\bar{d} =
\frac{1}{T}\sum_{t=1}^T d_t\leq d$. Secondly, we extend DBGD-NF by employing a
blocking update mechanism to decouple the joint effect of the delays and bandit
feedback, which enjoys an $\mathcal{O}(n(T^{2/3} + \sqrt{dT}))$ regret bound.
When $d = \mathcal{O}(T^{1/3})$, our regret bound matches the
$\mathcal{O}(nT^{2/3})$ bound in the bandit setting without delayed feedback.
Compared to our first $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ bound, it is more
advantageous when the maximum delay $d = o(\bar{d}^{2/3}T^{1/3})$. Finally, we
conduct experiments on structured sparse learning to demonstrate the
superiority of our methods.

</details>


### [44] [Phase-Locked SNR Band Selection for Weak Mineral Signal Detection in Hyperspectral Imagery](https://arxiv.org/abs/2508.00539)
*Judy X Yang*

Main category: cs.LG

TL;DR: 提出了一种两阶段框架，用于增强Cuprite矿区矿物检测，通过SNR阈值和Savitzky-Golay滤波降噪，再结合KMeans和NNLS提高解混精度。


<details>
  <summary>Details</summary>
Motivation: 解决高光谱成像中弱矿物信号被噪声和冗余波段掩盖的问题。

Method: 两阶段方法：1) SNR阈值和Savitzky-Golay滤波降噪；2) KMeans聚类和NNLS解混。

Result: 实验表明，该方法提高了弱矿物区域的检测精度和解混效果。

Conclusion: 该策略为地质高光谱应用提供了一种实用且可重复的解决方案。

Abstract: Hyperspectral imaging offers detailed spectral information for mineral
mapping; however, weak mineral signatures are often masked by noisy and
redundant bands, limiting detection performance. To address this, we propose a
two-stage integrated framework for enhanced mineral detection in the Cuprite
mining district. In the first stage, we compute the signal-to-noise ratio (SNR)
for each spectral band and apply a phase-locked thresholding technique to
discard low-SNR bands, effectively removing redundancy and suppressing
background noise. Savitzky-Golay filtering is then employed for spectral
smoothing, serving a dual role first to stabilize trends during band selection,
and second to preserve fine-grained spectral features during preprocessing. In
the second stage, the refined HSI data is reintroduced into the model, where
KMeans clustering is used to extract 12 endmember spectra (W1 custom), followed
by non negative least squares (NNLS) for abundance unmixing. The resulting
endmembers are quantitatively compared with laboratory spectra (W1 raw) using
cosine similarity and RMSE metrics. Experimental results confirm that our
proposed pipeline improves unmixing accuracy and enhances the detection of weak
mineral zones. This two-pass strategy demonstrates a practical and reproducible
solution for spectral dimensionality reduction and unmixing in geological HSI
applications.

</details>


### [45] [Foundations of Interpretable Models](https://arxiv.org/abs/2508.00545)
*Pietro Barbiero,Mateo Espinosa Zarlenga,Alberto Termine,Mateja Jamnik,Giuseppe Marra*

Main category: cs.LG

TL;DR: 论文提出了一种新的可解释性定义，解决了现有定义不具可操作性的问题，并提供了一个设计可解释模型的蓝图和开源库。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性定义缺乏操作性，无法指导用户设计通用、可靠且稳健的可解释模型，导致研究问题不明确。

Method: 提出了一种通用、简单且涵盖现有非正式概念的可解释性定义，并基于此设计了一个可解释模型的蓝图和开源库。

Result: 新定义具有可操作性，揭示了设计可解释模型所需的基本属性、假设、原则、数据结构和架构特征。

Conclusion: 论文通过新定义和工具推动了可解释AI研究的发展，使其更具实践指导意义。

Abstract: We argue that existing definitions of interpretability are not actionable in
that they fail to inform users about general, sound, and robust interpretable
model design. This makes current interpretability research fundamentally
ill-posed. To address this issue, we propose a definition of interpretability
that is general, simple, and subsumes existing informal notions within the
interpretable AI community. We show that our definition is actionable, as it
directly reveals the foundational properties, underlying assumptions,
principles, data structures, and architectural features necessary for designing
interpretable models. Building on this, we propose a general blueprint for
designing interpretable models and introduce the first open-sourced library
with native support for interpretable data structures and processes.

</details>


### [46] [Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data](https://arxiv.org/abs/2508.00615)
*Mukesh Kumar Sahu,Pinki Roy*

Main category: cs.LG

TL;DR: 提出了一种基于相似性自构建图模型（SBSCGM）和混合图神经网络（HybridGraphMedGNN）的方法，用于预测ICU患者的死亡风险和关键性评分，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以利用电子健康记录（EHR）中的关系结构，且孤立处理患者数据，无法动态捕捉患者间的相似性。

Method: SBSCGM动态构建患者相似性图，HybridGraphMedGNN结合GCN、GraphSAGE和GAT层学习患者表示。

Result: 在MIMIC-III数据集上，模型AUC-ROC达到0.94，优于基线模型，并提供可解释性。

Conclusion: 该框架为ICU风险预测提供了可扩展且可解释的解决方案，有望支持临床决策。

Abstract: Accurately predicting the criticalness of ICU patients (such as in-ICU
mortality risk) is vital for early intervention in critical care. However,
conventional models often treat each patient in isolation and struggle to
exploit the relational structure in Electronic Health Records (EHR). We propose
a Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds
a patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN
architecture that operates on this graph to predict patient mortality and a
continuous criticalness score. SBSCGM uses a hybrid similarity measure
(combining feature-based and structural similarities) to connect patients with
analogous clinical profiles in real-time. The HybridGraphMedGNN integrates
Graph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)
layers to learn robust patient representations, leveraging both local and
global graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III
dataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)
outperforming baseline classifiers and single-type GNN models. We also
demonstrate improved precision/recall and show that the attention mechanism
provides interpretable insights into model predictions. Our framework offers a
scalable and interpretable solution for critical care risk prediction, with
potential to support clinicians in real-world ICU deployment.

</details>


### [47] [Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides](https://arxiv.org/abs/2508.00578)
*Marlen Neubert,Patrick Reiser,Frauke Gräter,Pascal Friederich*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习的方法，用于模拟氢原子转移（HAT）反应的势能面，并通过系统生成数据和评估不同图神经网络架构，发现MACE在预测能量、力和反应势垒方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 氢原子转移（HAT）反应在生物过程中至关重要，但其机理尚不完全清楚。传统模拟方法（如经典力场或DFT分子动力学）无法满足量子化学精度的需求，因此需要开发新的方法。

Method: 通过半经验方法和DFT生成大量HAT构型数据集，并评估三种图神经网络架构（SchNet、Allegro和MACE）在学习和预测HAT势能面及反应势垒方面的性能。

Result: MACE在能量、力和势垒预测方面表现最优，其平均绝对误差为1.13 kcal/mol。该方法成功应用于胶原蛋白模拟，预测反应速率并揭示HAT机理。

Conclusion: 该方法可推广至其他生物分子系统，实现复杂环境中化学反应的高精度模拟，并结合过渡态搜索算法和主动学习进一步提升性能。

Abstract: Hydrogen atom transfer (HAT) reactions are essential in many biological
processes, such as radical migration in damaged proteins, but their mechanistic
pathways remain incompletely understood. Simulating HAT is challenging due to
the need for quantum chemical accuracy at biologically relevant scales; thus,
neither classical force fields nor DFT-based molecular dynamics are applicable.
Machine-learned potentials offer an alternative, able to learn potential energy
surfaces (PESs) with near-quantum accuracy. However, training these models to
generalize across diverse HAT configurations, especially at radical positions
in proteins, requires tailored data generation and careful model selection.
Here, we systematically generate HAT configurations in peptides to build large
datasets using semiempirical methods and DFT. We benchmark three graph neural
network architectures (SchNet, Allegro, and MACE) on their ability to learn HAT
PESs and indirectly predict reaction barriers from energy predictions. MACE
consistently outperforms the others in energy, force, and barrier prediction,
achieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT
barrier predictions. This accuracy enables integration of ML potentials into
large-scale collagen simulations to compute reaction rates from predicted
barriers, advancing mechanistic understanding of HAT and radical migration in
peptides. We analyze scaling laws, model transferability, and cost-performance
trade-offs, and outline strategies for improvement by combining ML potentials
with transition state search algorithms and active learning. Our approach is
generalizable to other biomolecular systems, enabling quantum-accurate
simulations of chemical reactivity in complex environments.

</details>


### [48] [Efficient Solution and Learning of Robust Factored MDPs](https://arxiv.org/abs/2508.00707)
*Yannik Schnitzer,Alessandro Abate,David Parker*

Main category: cs.LG

TL;DR: 本文提出了一种基于分解状态空间表示的新方法，用于解决和学习鲁棒马尔可夫决策过程（r-MDPs），通过利用系统组件间的不确定性独立性，提高了样本效率。


<details>
  <summary>Details</summary>
Motivation: r-MDPs通过显式建模转移动态的认知不确定性扩展了MDPs，但学习r-MDPs需要大量样本交互。本文旨在通过分解状态空间表示减少样本需求。

Method: 提出基于分解状态空间表示的方法，将非凸优化问题转化为可处理的线性规划，并直接学习分解模型表示。

Result: 实验结果表明，利用分解结构可显著提高样本效率，生成比现有方法更有效的鲁棒策略和更严格的性能保证。

Conclusion: 分解状态空间表示是解决和学习r-MDPs的有效方法，能够显著提升样本效率和策略性能。

Abstract: Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling
epistemic uncertainty about transition dynamics. Learning r-MDPs from
interactions with an unknown environment enables the synthesis of robust
policies with provable (PAC) guarantees on performance, but this can require a
large number of sample interactions. We propose novel methods for solving and
learning r-MDPs based on factored state-space representations that leverage the
independence between model uncertainty across system components. Although
policy synthesis for factored r-MDPs leads to hard, non-convex optimisation
problems, we show how to reformulate these into tractable linear programs.
Building on these, we also propose methods to learn factored model
representations directly. Our experimental results show that exploiting
factored structure can yield dimensional gains in sample efficiency, producing
more effective robust policies with tighter performance guarantees than
state-of-the-art methods.

</details>


### [49] [The Role of Active Learning in Modern Machine Learning](https://arxiv.org/abs/2508.00586)
*Thorben Werner,Lars Schmidt-Thieme,Vijaya Krishna Yalavarthi*

Main category: cs.LG

TL;DR: 研究发现，主动学习（AL）在低数据场景下效率最低，提升仅1-4%，而数据增强（DA）和半监督学习（SSL）提升可达60%。但AL与DA和SSL结合后仍能提供额外提升。


<details>
  <summary>Details</summary>
Motivation: 探讨AL在实际应用中较少使用的原因，并研究其在低数据场景下的表现。

Method: 比较AL、DA和SSL在低数据场景下的效果，并测试AL与DA和SSL结合的效果。

Result: AL单独效果较差，但与DA和SSL结合后仍能提供额外性能提升。

Conclusion: AL应作为DA和SSL后的补充方法，而非解决标签缺失的主要手段。

Abstract: Even though Active Learning (AL) is widely studied, it is rarely applied in
contexts outside its own scientific literature. We posit that the reason for
this is AL's high computational cost coupled with the comparatively small lifts
it is typically able to generate in scenarios with few labeled points. In this
work we study the impact of different methods to combat this low data scenario,
namely data augmentation (DA), semi-supervised learning (SSL) and AL. We find
that AL is by far the least efficient method of solving the low data problem,
generating a lift of only 1-4\% over random sampling, while DA and SSL methods
can generate up to 60\% lift in combination with random sampling. However, when
AL is combined with strong DA and SSL techniques, it surprisingly is still able
to provide improvements. Based on these results, we frame AL not as a method to
combat missing labels, but as the final building block to squeeze the last bits
of performance out of data after appropriate DA and SSL methods as been
applied.

</details>


### [50] [JSON-Bag: A generic game trajectory representation](https://arxiv.org/abs/2508.00712)
*Dien Nguyen,Diego Perez-Liebana,Simon Lucas*

Main category: cs.LG

TL;DR: JSON-Bag模型通过JSON描述的游戏轨迹进行标记化，并使用Jensen-Shannon距离（JSD）作为度量标准，在六款桌游的分类任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 提出一种通用的游戏轨迹表示方法，以解决传统手工特征提取的局限性。

Method: 使用JSON-Bag模型标记化游戏轨迹的JSON描述，并应用JSD作为距离度量，结合原型最近邻搜索（P-NNS）进行评估。

Result: 在大多数任务中优于基线方法，且样本效率高；通过自动特征提取进一步提升了分类准确性。

Conclusion: JSON-Bag模型是一种有效的游戏轨迹表示方法，JSD与策略距离高度相关。

Abstract: We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically
represent game trajectories by tokenizing their JSON descriptions and apply
Jensen-Shannon distance (JSD) as distance metric for them. Using a
prototype-based nearest-neighbor search (P-NNS), we evaluate the validity of
JSON-Bag with JSD on six tabletop games -- \textit{7 Wonders},
\textit{Dominion}, \textit{Sea Salt and Paper}, \textit{Can't Stop},
\textit{Connect4}, \textit{Dots and boxes} -- each over three game trajectory
classification tasks: classifying the playing agents, game parameters, or game
seeds that were used to generate the trajectories.
  Our approach outperforms a baseline using hand-crafted features in the
majority of tasks. Evaluating on N-shot classification suggests using JSON-Bag
prototype to represent game trajectory classes is also sample efficient.
Additionally, we demonstrate JSON-Bag ability for automatic feature extraction
by treating tokens as individual features to be used in Random Forest to solve
the tasks above, which significantly improves accuracy on underperforming
tasks. Finally, we show that, across all six games, the JSD between JSON-Bag
prototypes of agent classes highly correlates with the distances between
agents' policies.

</details>


### [51] [Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning](https://arxiv.org/abs/2508.00716)
*Yingxu Wang,Mengzhu Wang,Zhichao Huang,Suyu Liu*

Main category: cs.LG

TL;DR: NeGPR是一种针对带噪声标签的图级域适应框架，通过双分支预训练和嵌套伪标签细化机制提升跨域学习鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图域适应方法假设源标签干净，但实际中标签噪声普遍存在，严重影响性能。

Method: NeGPR通过双分支预训练（语义和拓扑）和嵌套伪标签细化机制，结合噪声感知正则化策略。

Result: 在基准数据集上，NeGPR在严重标签噪声下表现优于现有方法，准确率提升高达12.7%。

Conclusion: NeGPR有效解决了标签噪声问题，提升了图域适应的鲁棒性和性能。

Abstract: Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled
source graphs to unlabeled target graphs by learning domain-invariant
representations, which is essential in applications such as molecular property
prediction and social network analysis. However, most existing GDA methods rely
on the assumption of clean source labels, which rarely holds in real-world
scenarios where annotation noise is pervasive. This label noise severely
impairs feature alignment and degrades adaptation performance under domain
shifts. To address this challenge, we propose Nested Graph Pseudo-Label
Refinement (NeGPR), a novel framework tailored for graph-level domain
adaptation with noisy labels. NeGPR first pretrains dual branches, i.e.,
semantic and topology branches, by enforcing neighborhood consistency in the
feature space, thereby reducing the influence of noisy supervision. To bridge
domain gaps, NeGPR employs a nested refinement mechanism in which one branch
selects high-confidence target samples to guide the adaptation of the other,
enabling progressive cross-domain learning. Furthermore, since pseudo-labels
may still contain noise and the pre-trained branches are already overfitted to
the noisy labels in the source domain, NeGPR incorporates a noise-aware
regularization strategy. This regularization is theoretically proven to
mitigate the adverse effects of pseudo-label noise, even under the presence of
source overfitting, thus enhancing the robustness of the adaptation process.
Extensive experiments on benchmark datasets demonstrate that NeGPR consistently
outperforms state-of-the-art methods under severe label noise, achieving gains
of up to 12.7% in accuracy.

</details>


### [52] [IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources](https://arxiv.org/abs/2508.00627)
*Paul Tresson,Pierre Le Coz,Hadrien Tulet,Anthony Malkassian,Maxime Réjou Méchain*

Main category: cs.LG

TL;DR: IAMAP是一个用户友好的QGIS插件，通过自监督学习策略简化了遥感图像分析，使非AI专家也能高效使用深度学习。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在遥感应用中需要大量数据、计算资源和编码技能的问题。

Method: 基于自监督学习的通用模型，提供特征提取、降维、聚类等功能，无需GPU或大量数据。

Result: IAMAP为非专家提供了高效、节能的深度学习工具，推动了深度学习的普及。

Conclusion: IAMAP通过简化流程和降低门槛，促进了遥感领域深度学习的广泛应用。

Abstract: Remote sensing has entered a new era with the rapid development of artificial
intelligence approaches. However, the implementation of deep learning has
largely remained restricted to specialists and has been impractical because it
often requires (i) large reference datasets for model training and validation;
(ii) substantial computing resources; and (iii) strong coding skills. Here, we
introduce IAMAP, a user-friendly QGIS plugin that addresses these three
challenges in an easy yet flexible way. IAMAP builds on recent advancements in
self-supervised learning strategies, which now provide robust feature
extractors, often referred to as foundation models. These generalist models can
often be reliably used in few-shot or zero-shot scenarios (i.e., with little to
no fine-tuning). IAMAP's interface allows users to streamline several key steps
in remote sensing image analysis: (i) extracting image features using a wide
range of deep learning architectures; (ii) reducing dimensionality with
built-in algorithms; (iii) performing clustering on features or their reduced
representations; (iv) generating feature similarity maps; and (v) calibrating
and validating supervised machine learning models for prediction. By enabling
non-AI specialists to leverage the high-quality features provided by recent
deep learning approaches without requiring GPU capacity or extensive reference
datasets, IAMAP contributes to the democratization of computationally efficient
and energy-conscious deep learning methods.

</details>


### [53] [Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems](https://arxiv.org/abs/2508.00734)
*Liuyun Xu,Seymour M. J. Spence*

Main category: cs.LG

TL;DR: 提出了一种基于多保真度分层采样和自适应机器学习元模型的方法，用于高效估计小失效概率，显著节省计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂非线性有限元模型中计算小失效概率时仍需要大量模型评估，计算成本高。

Method: 结合分层采样生成高保真数据集训练深度学习元模型，作为低保真模型，并通过自适应训练平衡精度与计算需求。

Result: 应用于高层钢结构建筑，能准确估计非线性响应的超越概率曲线，计算效率显著优于单保真度方法。

Conclusion: 该方法在保证精度的同时大幅降低了计算成本，适用于复杂系统的罕见事件分析。

Abstract: Existing variance reduction techniques used in stochastic simulations for
rare event analysis still require a substantial number of model evaluations to
estimate small failure probabilities. In the context of complex, nonlinear
finite element modeling environments, this can become computationally
challenging-particularly for systems subjected to stochastic excitation. To
address this challenge, a multi-fidelity stratified sampling scheme with
adaptive machine learning metamodels is introduced for efficiently propagating
uncertainties and estimating small failure probabilities. In this approach, a
high-fidelity dataset generated through stratified sampling is used to train a
deep learning-based metamodel, which then serves as a cost-effective and highly
correlated low-fidelity model. An adaptive training scheme is proposed to
balance the trade-off between approximation quality and computational demand
associated with the development of the low-fidelity model. By integrating the
low-fidelity outputs with additional high-fidelity results, an unbiased
estimate of the strata-wise failure probabilities is obtained using a
multi-fidelity Monte Carlo framework. The overall probability of failure is
then computed using the total probability theorem. Application to a full-scale
high-rise steel building subjected to stochastic wind excitation demonstrates
that the proposed scheme can accurately estimate exceedance probability curves
for nonlinear responses of interest, while achieving significant computational
savings compared to single-fidelity variance reduction approaches.

</details>


### [54] [Separated-Variable Spectral Neural Networks: A Physics-Informed Learning Approach for High-Frequency PDEs](https://arxiv.org/abs/2508.00628)
*Xiong Xiong,Zhuo Zhang,Rongchun Hu,Chen Gao,Zichen Deng*

Main category: cs.LG

TL;DR: SV-SNN是一种新型神经网络框架，通过分离变量和自适应谱方法解决了传统PINNs在高频振荡PDE求解中的谱偏差问题，显著提升了精度和效率。


<details>
  <summary>Details</summary>
Motivation: 高频振荡PDE求解在科学计算中至关重要，但传统PINNs因谱偏差难以捕捉高频解分量。

Method: SV-SNN将多元函数分解为单变量函数乘积，结合自适应傅里叶谱特征和可学习频率参数，并通过SVD理论量化谱偏差。

Result: 在多个基准问题中，SV-SNN精度提升1-3个数量级，参数减少90%以上，训练时间缩短60%。

Conclusion: SV-SNN有效解决了神经PDE求解中的谱偏差问题，具有显著优势。

Abstract: Solving high-frequency oscillatory partial differential equations (PDEs) is a
critical challenge in scientific computing, with applications in fluid
mechanics, quantum mechanics, and electromagnetic wave propagation. Traditional
physics-informed neural networks (PINNs) suffer from spectral bias, limiting
their ability to capture high-frequency solution components. We introduce
Separated-Variable Spectral Neural Networks (SV-SNN), a novel framework that
addresses these limitations by integrating separation of variables with
adaptive spectral methods. Our approach features three key innovations: (1)
decomposition of multivariate functions into univariate function products,
enabling independent spatial and temporal networks; (2) adaptive Fourier
spectral features with learnable frequency parameters for high-frequency
capture; and (3) theoretical framework based on singular value decomposition to
quantify spectral bias. Comprehensive evaluation on benchmark problems
including Heat equation, Helmholtz equation, Poisson equations and
Navier-Stokes equations demonstrates that SV-SNN achieves 1-3 orders of
magnitude improvement in accuracy while reducing parameter count by over 90\%
and training time by 60\%. These results establish SV-SNN as an effective
solution to the spectral bias problem in neural PDE solving. The implementation
will be made publicly available upon acceptance at
https://github.com/xgxgnpu/SV-SNN.

</details>


### [55] [KFS: KAN based adaptive Frequency Selection learning architecture for long term time series forecasting](https://arxiv.org/abs/2508.00635)
*Changning Wu,Gao Wu,Rongyao Cai,Yong Liu,Kexin Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于KAN的自适应频率选择学习架构（KFS），用于解决时间序列预测中多尺度噪声干扰和频率信息分布不均的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列存在多尺度噪声干扰和频率信息分布不均，导致多尺度表示不理想。

Method: 结合Kolmogorov-Arnold Networks（KAN）和Parseval定理，设计KFS架构，包括FreK模块（基于能量分布选择主导频率）、时间戳嵌入对齐和特征混合模块。

Result: 在多个真实时间序列数据集上的实验表明，KFS实现了最先进的性能。

Conclusion: KFS是一种简单而有效的架构，能够有效解决多尺度噪声干扰和复杂模式建模问题。

Abstract: Multi-scale decomposition architectures have emerged as predominant
methodologies in time series forecasting. However, real-world time series
exhibit noise interference across different scales, while heterogeneous
information distribution among frequency components at varying scales leads to
suboptimal multi-scale representation. Inspired by Kolmogorov-Arnold Networks
(KAN) and Parseval's theorem, we propose a KAN based adaptive Frequency
Selection learning architecture (KFS) to address these challenges. This
framework tackles prediction challenges stemming from cross-scale noise
interference and complex pattern modeling through its FreK module, which
performs energy-distribution-based dominant frequency selection in the spectral
domain. Simultaneously, KAN enables sophisticated pattern representation while
timestamp embedding alignment synchronizes temporal representations across
scales. The feature mixing module then fuses scale-specific patterns with
aligned temporal features. Extensive experiments across multiple real-world
time series datasets demonstrate that KT achieves state-of-the-art performance
as a simple yet effective architecture.

</details>


### [56] [A Simple and Effective Method for Uncertainty Quantification and OOD Detection](https://arxiv.org/abs/2508.00754)
*Yaxin Ma,Benjamin Colburn,Jose C. Principe*

Main category: cs.LG

TL;DR: 提出了一种基于特征空间密度的单确定性模型方法，用于量化分布偏移和OOD检测，解决了贝叶斯神经网络和深度集成方法的高计算和存储需求问题。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯神经网络和深度集成方法虽然能进行不确定性量化，但计算和存储成本高，因此需要一种更高效的方法。

Method: 利用核密度估计得到的信息势场近似训练集的特征空间密度，通过与测试样本的特征空间表示比较，检测分布偏移和OOD样本。

Result: 在2D合成数据集（Two Moons和Three Spirals）和OOD检测任务（CIFAR-10 vs. SVHN）上，该方法优于基线模型。

Conclusion: 该方法在高效性和性能上均优于传统方法，适用于分布偏移和OOD检测任务。

Abstract: Bayesian neural networks and deep ensemble methods have been proposed for
uncertainty quantification; however, they are computationally intensive and
require large storage. By utilizing a single deterministic model, we can solve
the above issue. We propose an effective method based on feature space density
to quantify uncertainty for distributional shifts and out-of-distribution (OOD)
detection. Specifically, we leverage the information potential field derived
from kernel density estimation to approximate the feature space density of the
training set. By comparing this density with the feature space representation
of test samples, we can effectively determine whether a distributional shift
has occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons
and Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The
results demonstrate that our method outperforms baseline models.

</details>


### [57] [Reinforcement Learning for Decision-Level Interception Prioritization in Drone Swarm Defense](https://arxiv.org/abs/2508.00641)
*Alessandro Palmas*

Main category: cs.LG

TL;DR: 论文探讨了强化学习在防御低成本自杀式无人机群威胁中的应用，通过高保真模拟环境和决策级强化学习代理，优化拦截优先级，显著降低平均损害并提高防御效率。


<details>
  <summary>Details</summary>
Motivation: 低成本自杀式无人机群的威胁日益增长，现代防御系统需要快速、战略性的决策来优先拦截多个效应器和高价值目标区域。

Method: 引入高保真模拟环境，利用强化学习代理在离散动作空间中协调多个效应器，基于状态特征（如位置、类别和效应器状态）选择拦截目标。

Result: 强化学习策略在数百次模拟攻击场景中表现优于手工规则基线，平均损害更低，防御效率更高。

Conclusion: 强化学习可作为防御架构的战略层，提升韧性而不取代现有控制系统，代码和模拟资源已公开以确保可复现性。

Abstract: The growing threat of low-cost kamikaze drone swarms poses a critical
challenge to modern defense systems demanding rapid and strategic
decision-making to prioritize interceptions across multiple effectors and
high-value target zones. In this work, we present a case study demonstrating
the practical advantages of reinforcement learning in addressing this
challenge. We introduce a high-fidelity simulation environment that captures
realistic operational constraints, within which a decision-level reinforcement
learning agent learns to coordinate multiple effectors for optimal interception
prioritization. Operating in a discrete action space, the agent selects which
drone to engage per effector based on observed state features such as
positions, classes, and effector status. We evaluate the learned policy against
a handcrafted rule-based baseline across hundreds of simulated attack
scenarios. The reinforcement learning based policy consistently achieves lower
average damage and higher defensive efficiency in protecting critical zones.
This case study highlights the potential of reinforcement learning as a
strategic layer within defense architectures, enhancing resilience without
displacing existing control systems. All code and simulation assets are
publicly released for full reproducibility, and a video demonstration
illustrates the policy's qualitative behavior.

</details>


### [58] [Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators](https://arxiv.org/abs/2508.00643)
*Albert Matveev,Sanmitra Ghosh,Aamal Hussain,James-Michael Leahy,Michalis Michaelides*

Main category: cs.LG

TL;DR: DINOZAUR是一种基于扩散的神经算子参数化方法，具有不确定性量化功能，解决了FNO的过参数化和缺乏原生不确定性量化的问题。


<details>
  <summary>Details</summary>
Motivation: FNO存在过参数化和缺乏原生不确定性量化的问题，限制了其在科学和工程应用中的可靠性。

Method: DINOZAUR通过引入扩散乘子替代FNO中的密集张量乘子，减少参数数量和内存占用，并通过贝叶斯方法实现不确定性量化。

Result: DINOZAUR在多个PDE基准测试中表现优异，同时提供了高效的不确定性量化。

Conclusion: DINOZAUR是一种高效且可靠的神经算子参数化方法，适用于科学和工程应用。

Abstract: Operator learning is a powerful paradigm for solving partial differential
equations, with Fourier Neural Operators serving as a widely adopted
foundation. However, FNOs face significant scalability challenges due to
overparameterization and offer no native uncertainty quantification -- a key
requirement for reliable scientific and engineering applications. Instead,
neural operators rely on post hoc UQ methods that ignore geometric inductive
biases. In this work, we introduce DINOZAUR: a diffusion-based neural operator
parametrization with uncertainty quantification. Inspired by the structure of
the heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a
dimensionality-independent diffusion multiplier that has a single learnable
time parameter per channel, drastically reducing parameter count and memory
footprint without compromising predictive performance. By defining priors over
those time parameters, we cast DINOZAUR as a Bayesian neural operator to yield
spatially correlated outputs and calibrated uncertainty estimates. Our method
achieves competitive or superior performance across several PDE benchmarks
while providing efficient uncertainty quantification.

</details>


### [59] [TrajSurv: Learning Continuous Latent Trajectories from Electronic Health Records for Trustworthy Survival Prediction](https://arxiv.org/abs/2508.00657)
*Sihang Zeng,Lucas Jing Liu,Jun Wen,Meliha Yetisgen,Ruth Etzioni,Gang Luo*

Main category: cs.LG

TL;DR: TrajSurv是一个基于纵向电子健康记录（EHR）的生存预测模型，通过神经控制微分方程（NCDE）学习连续潜在轨迹，并结合对比学习和解释方法提高透明度和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决纵向EHR数据中不规则采样的临床特征建模困难，以及如何透明地将临床进展与生存结果关联的问题。

Method: 使用NCDE提取连续时间潜在状态，通过对比学习对齐潜在状态与患者状态空间，并采用两步解释方法（向量场和聚类）分析临床进展与生存结果的关系。

Result: 在MIMIC-III和eICU数据集上，TrajSurv表现出优于现有深度学习方法的准确性和透明度。

Conclusion: TrajSurv为临床决策提供了可信赖的生存预测，同时通过透明的方法解释了临床进展与生存结果的关系。

Abstract: Trustworthy survival prediction is essential for clinical decision making.
Longitudinal electronic health records (EHRs) provide a uniquely powerful
opportunity for the prediction. However, it is challenging to accurately model
the continuous clinical progression of patients underlying the irregularly
sampled clinical features and to transparently link the progression to survival
outcomes. To address these challenges, we develop TrajSurv, a model that learns
continuous latent trajectories from longitudinal EHR data for trustworthy
survival prediction. TrajSurv employs a neural controlled differential equation
(NCDE) to extract continuous-time latent states from the irregularly sampled
data, forming continuous latent trajectories. To ensure the latent trajectories
reflect the clinical progression, TrajSurv aligns the latent state space with
patient state space through a time-aware contrastive learning approach. To
transparently link clinical progression to the survival outcome, TrajSurv uses
latent trajectories in a two-step divide-and-conquer interpretation process.
First, it explains how the changes in clinical features translate into the
latent trajectory's evolution using a learned vector field. Second, it clusters
these latent trajectories to identify key clinical progression patterns
associated with different survival outcomes. Evaluations on two real-world
medical datasets, MIMIC-III and eICU, show TrajSurv's competitive accuracy and
superior transparency over existing deep learning methods.

</details>


### [60] [DP-DGAD: A Generalist Dynamic Graph Anomaly Detector with Dynamic Prototypes](https://arxiv.org/abs/2508.00664)
*Jialun Zheng,Jie Liu,Jiannong Cao,Xiao Wang,Hanchen Yang,Yankai Chen,Philip S. Yu*

Main category: cs.LG

TL;DR: 论文提出了一种动态原型（DP）的动态图异常检测（DGAD）模型，用于捕捉跨领域和领域内的动态异常模式，并在多个真实数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 动态图异常检测在多个领域（如金融、交通、社交网络）中至关重要，但现有通用模型难以捕捉动态图中的异常模式，且新领域缺乏标注数据。

Method: 提出DP-DGAD模型，通过动态原型提取和存储演化模式，选择性更新内存缓冲区，并结合异常评分器和置信度伪标签进行自监督适应。

Result: 在十个不同领域的真实数据集上实现了最先进的性能。

Conclusion: DP-DGAD模型能有效捕捉动态图中的跨领域和领域内异常模式，适用于缺乏标注数据的新领域。

Abstract: Dynamic graph anomaly detection (DGAD) is essential for identifying anomalies
in evolving graphs across domains such as finance, traffic, and social
networks. Recently, generalist graph anomaly detection (GAD) models have shown
promising results. They are pretrained on multiple source datasets and
generalize across domains. While effective on static graphs, they struggle to
capture evolving anomalies in dynamic graphs. Moreover, the continuous
emergence of new domains and the lack of labeled data further challenge
generalist DGAD. Effective cross-domain DGAD requires both domain-specific and
domain-agnostic anomalous patterns. Importantly, these patterns evolve
temporally within and across domains. Building on these insights, we propose a
DGAD model with Dynamic Prototypes (DP) to capture evolving domain-specific and
domain-agnostic patterns. Firstly, DP-DGAD extracts dynamic prototypes, i.e.,
evolving representations of normal and anomalous patterns, from temporal
ego-graphs and stores them in a memory buffer. The buffer is selectively
updated to retain general, domain-agnostic patterns while incorporating new
domain-specific ones. Then, an anomaly scorer compares incoming data with
dynamic prototypes to flag both general and domain-specific anomalies. Finally,
DP-DGAD employs confidence-based pseudo-labeling for effective self-supervised
adaptation in target domains. Extensive experiments demonstrate
state-of-the-art performance across ten real-world datasets from different
domains.

</details>


### [61] [Wind Power Scenario Generation based on the Generalized Dynamic Factor Model and Generative Adversarial Network](https://arxiv.org/abs/2508.00692)
*Young-ho Cho,Hao Zhu,Duehee Lee,Ross Baldick*

Main category: cs.LG

TL;DR: 论文提出了一种结合广义动态因子模型（GDFM）和生成对抗网络（GAN）的方法，用于合成分布式风电场的长期风电功率场景，以改进资源充足性研究。


<details>
  <summary>Details</summary>
Motivation: 现有的GDFM方法虽能提取空间相关性，但无法准确模拟波形；而GAN虽能生成时间相关性强的样本，但缺乏动态因子。结合两者优势以提升风电场景合成的性能。

Method: 使用GAN提取动态因子并作为滤波器，再将其应用于GDFM中，以同时捕捉空间和频率相关性。

Result: 在澳大利亚风电数据上的测试表明，该方法在合成风电场景时优于其他替代方案，能更真实地反映实际风电的统计特性。

Conclusion: 结合GDFM和GAN的方法显著提升了风电场景合成的性能，为资源充足性研究提供了更可靠的输入。

Abstract: For conducting resource adequacy studies, we synthesize multiple long-term
wind power scenarios of distributed wind farms simultaneously by using the
spatio-temporal features: spatial and temporal correlation, waveforms, marginal
and ramp rates distributions of waveform, power spectral densities, and
statistical characteristics. Generating the spatial correlation in scenarios
requires the design of common factors for neighboring wind farms and
antithetical factors for distant wind farms. The generalized dynamic factor
model (GDFM) can extract the common factors through cross spectral density
analysis, but it cannot closely imitate waveforms. The GAN can synthesize
plausible samples representing the temporal correlation by verifying samples
through a fake sample discriminator. To combine the advantages of GDFM and GAN,
we use the GAN to provide a filter that extracts dynamic factors with temporal
information from the observation data, and we then apply this filter in the
GDFM to represent both spatial and frequency correlations of plausible
waveforms. Numerical tests on the combination of GDFM and GAN have demonstrated
performance improvements over competing alternatives in synthesizing wind power
scenarios from Australia, better realizing plausible statistical
characteristics of actual wind power compared to alternatives such as the GDFM
with a filter synthesized from distributions of actual dynamic filters and the
GAN with direct synthesis without dynamic factors.

</details>


### [62] [Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach](https://arxiv.org/abs/2508.00695)
*Sergio Rubio-Martín,María Teresa García-Ordás,Antonio Serrano-García,Clara Margarita Franch-Pato,Arturo Crespo-Álvaro,José Alberto Benítez-Andrades*

Main category: cs.LG

TL;DR: 比较多种AI模型（传统机器学习和深度学习）对临床笔记分类的性能，发现超参数调优显著提升模型准确性，而过采样技术影响有限。


<details>
  <summary>Details</summary>
Motivation: 研究AI模型在心理健康诊断中的分类性能，为AI辅助诊断工具提供参考。

Method: 使用多种机器学习和深度学习模型，结合不同过采样策略和超参数调优，评估分类准确性。

Result: 决策树和XGBoost在机器学习中表现最佳（96%准确率），DistilBERT和SciBERT在深度学习中同样达到96%。超参数调优显著提升性能。

Conclusion: 超参数调优对模型性能至关重要，过采样技术影响有限，研究为AI辅助心理健康诊断提供了实用见解。

Abstract: The classification of clinical notes into specific diagnostic categories is
critical in healthcare, especially for mental health conditions like Anxiety
and Adjustment Disorder. In this study, we compare the performance of various
Artificial Intelligence models, including both traditional Machine Learning
approaches (Random Forest, Support Vector Machine, K-nearest neighbors,
Decision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT
and SciBERT), to classify clinical notes into these two diagnoses.
Additionally, we implemented three oversampling strategies: No Oversampling,
Random Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to
assess their impact on model performance. Hyperparameter tuning was also
applied to optimize model accuracy. Our results indicate that oversampling
techniques had minimal impact on model performance overall. The only exception
was SMOTE, which showed a positive effect specifically with BERT-based models.
However, hyperparameter optimization significantly improved accuracy across the
models, enhancing their ability to generalize and perform on the dataset. The
Decision Tree and eXtreme Gradient Boost models achieved the highest accuracy
among machine learning approaches, both reaching 96%, while the DistilBERT and
SciBERT models also attained 96% accuracy in the deep learning category. These
findings underscore the importance of hyperparameter tuning in maximizing model
performance. This study contributes to the ongoing research on AI-assisted
diagnostic tools in mental health by providing insights into the efficacy of
different model architectures and data balancing methods.

</details>


### [63] [Learning Network Dismantling without Handcrafted Inputs](https://arxiv.org/abs/2508.00706)
*Haozhe Tian,Pietro Ferraro,Robert Shorten,Mahdi Jalili,Homayoun Hamedmoghadam*

Main category: cs.LG

TL;DR: 论文提出了一种无需手工特征的消息传递图神经网络框架MIND，用于解决NP难的网络拆解问题，并在大规模真实网络中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖手工特征，增加了计算成本并引入偏差，因此需要一种更纯粹的数据驱动方法。

Method: 引入注意力机制和消息迭代剖面，结合多样化的合成网络训练集，构建了MIND框架。

Result: MIND在未见过的百万节点真实网络中表现优于现有方法，具有高效性和泛化能力。

Conclusion: MIND不仅适用于网络拆解，还可推广到其他复杂网络问题。

Abstract: The application of message-passing Graph Neural Networks has been a
breakthrough for important network science problems. However, the competitive
performance often relies on using handcrafted structural features as inputs,
which increases computational cost and introduces bias into the otherwise
purely data-driven network representations. Here, we eliminate the need for
handcrafted features by introducing an attention mechanism and utilizing
message-iteration profiles, in addition to an effective algorithmic approach to
generate a structurally diverse training set of small synthetic networks.
Thereby, we build an expressive message-passing framework and use it to
efficiently solve the NP-hard problem of Network Dismantling, virtually
equivalent to vital node identification, with significant real-world
applications. Trained solely on diversified synthetic networks, our proposed
model -- MIND: Message Iteration Network Dismantler -- generalizes to large,
unseen real networks with millions of nodes, outperforming state-of-the-art
network dismantling methods. Increased efficiency and generalizability of the
proposed model can be leveraged beyond dismantling in a range of complex
network problems.

</details>


### [64] [Democratizing Tabular Data Access with an Open$\unicode{x2013}$Source Synthetic$\unicode{x2013}$Data SDK](https://arxiv.org/abs/2508.00718)
*Ivona Krchova,Mariana Vargas Vieyra,Mario Scriminaci,Andrey Sidorenko*

Main category: cs.LG

TL;DR: MOSTLY AI SDK是一个开源工具包，用于生成高质量的合成表格数据，解决数据隐私和访问问题。


<details>
  <summary>Details</summary>
Motivation: 由于隐私、专有利益和伦理问题，高质量数据的获取受限，合成数据成为一种解决方案。

Method: 基于TabularARGN自回归框架，集成差分隐私、公平性生成和自动化质量保证。

Result: SDK在速度和可用性上表现优异，支持多种数据类型和复杂数据集。

Conclusion: SDK的快速采用表明其能有效解决数据瓶颈，推动数据民主化。

Abstract: Machine learning development critically depends on access to high-quality
data. However, increasing restrictions due to privacy, proprietary interests,
and ethical concerns have created significant barriers to data accessibility.
Synthetic data offers a viable solution by enabling safe, broad data usage
without compromising sensitive information. This paper presents the MOSTLY AI
Synthetic Data Software Development Kit (SDK), an open-source toolkit designed
specifically for synthesizing high-quality tabular data. The SDK integrates
robust features such as differential privacy guarantees, fairness-aware data
generation, and automated quality assurance into a flexible and accessible
Python interface. Leveraging the TabularARGN autoregressive framework, the SDK
supports diverse data types and complex multi-table and sequential datasets,
delivering competitive performance with notable improvements in speed and
usability. Currently deployed both as a cloud service and locally installable
software, the SDK has seen rapid adoption, highlighting its practicality in
addressing real-world data bottlenecks and promoting widespread data
democratization.

</details>


### [65] [Diffusion-Scheduled Denoising Autoencoders for Anomaly Detection in Tabular Data](https://arxiv.org/abs/2508.00758)
*Timur Sattarov,Marco Schreyer,Damian Borth*

Main category: cs.LG

TL;DR: 提出了一种结合扩散模型噪声调度和对比学习的去噪自编码器（DDAE），用于表格数据异常检测，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 表格数据异常检测因复杂特征交互和异常样本稀缺而困难，现有方法（如去噪自编码器和扩散模型）各有局限性。

Method: 提出DDAE框架，整合扩散模型的噪声调度和对比学习，优化编码过程。

Result: 在ADBench的57个数据集上，DDAE在半监督和无监督设置中表现优异，PR-AUC和ROC-AUC显著提升。

Conclusion: 噪声策略对表格异常检测至关重要，DDAE通过优化噪声调度和对比学习取得了显著效果。

Abstract: Anomaly detection in tabular data remains challenging due to complex feature
interactions and the scarcity of anomalous examples. Denoising autoencoders
rely on fixed-magnitude noise, limiting adaptability to diverse data
distributions. Diffusion models introduce scheduled noise and iterative
denoising, but lack explicit reconstruction mappings. We propose the
Diffusion-Scheduled Denoising Autoencoder (DDAE), a framework that integrates
diffusion-based noise scheduling and contrastive learning into the encoding
process to improve anomaly detection. We evaluated DDAE on 57 datasets from
ADBench. Our method outperforms in semi-supervised settings and achieves
competitive results in unsupervised settings, improving PR-AUC by up to 65%
(9%) and ROC-AUC by 16% (6%) over state-of-the-art autoencoder (diffusion)
model baselines. We observed that higher noise levels benefit unsupervised
training, while lower noise with linear scheduling is optimal in
semi-supervised settings. These findings underscore the importance of
principled noise strategies in tabular anomaly detection.

</details>


### [66] [Evaluating Angle and Amplitude Encoding Strategies for Variational Quantum Machine Learning: their impact on model's accuracy](https://arxiv.org/abs/2508.00768)
*Antonio Tudisco,Andrea Marchesin,Maurizio Zamboni,Mariagrazia Graziano,Giovanna Turvani*

Main category: cs.LG

TL;DR: 论文分析了量子机器学习中变分量子电路（VQC）的性能差异，比较了振幅和角度编码模型在不同旋转门下的分类表现。


<details>
  <summary>Details</summary>
Motivation: 量子计算和机器学习的结合推动了量子机器学习（QML）的发展，其中变分量子电路（VQC）是一种常用模型。研究旨在探索不同编码方式和旋转门对模型性能的影响。

Method: 通过振幅和角度编码模型，结合不同旋转门，在Wine和Diabetes数据集上训练并评估VQC的分类性能。

Result: 在相同拓扑结构下，最佳和最差模型的准确率差异为10%至30%，最高达41%。旋转门的选择对分类性能有显著影响。

Conclusion: 编码方式（嵌入）是VQC模型的一个重要超参数，旋转门的选择显著影响模型性能。

Abstract: Recent advancements in Quantum Computing and Machine Learning have increased
attention to Quantum Machine Learning (QML), which aims to develop machine
learning models by exploiting the quantum computing paradigm. One of the widely
used models in this area is the Variational Quantum Circuit (VQC), a hybrid
model where the quantum circuit handles data inference while classical
optimization adjusts the parameters of the circuit. The quantum circuit
consists of an encoding layer, which loads data into the circuit, and a
template circuit, known as the ansatz, responsible for processing the data.
This work involves performing an analysis by considering both Amplitude- and
Angle-encoding models, and examining how the type of rotational gate applied
affects the classification performance of the model. This comparison is carried
out by training the different models on two datasets, Wine and Diabetes, and
evaluating their performance. The study demonstrates that, under identical
model topologies, the difference in accuracy between the best and worst models
ranges from 10% to 30%, with differences reaching up to 41%. Moreover, the
results highlight how the choice of rotational gates used in encoding can
significantly impact the model's classification performance. The findings
confirm that the embedding represents a hyperparameter for VQC models.

</details>


### [67] [Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors](https://arxiv.org/abs/2508.00785)
*Bushra Akter,Md Biplob Hosen,Sabbir Ahmed,Mehrin Anannya,Md. Farhad Hossain*

Main category: cs.LG

TL;DR: 研究探讨了影响学生CGPA的多变量因素，通过文献综述和调查数据构建因果图，利用回归和分类模型分析，开发了预测学术表现的Web应用。


<details>
  <summary>Details</summary>
Motivation: 探究影响学术表现的多变量因素，优化学生CGPA。

Method: 文献综述、在线调查、数据预处理、因果分析、回归与分类模型（如Ridge回归和随机森林）、可解释AI技术（SHAP、LIME、Interpret）。

Result: Ridge回归预测CGPA的MAE为0.12，MSE为0.023；随机森林分类准确率达98.68%。关键因素包括学习时间、奖学金、父母教育背景和先前学术表现。

Conclusion: 研究成功识别关键影响因素并开发了Web应用，为学生提供个性化建议以优化学术表现。

Abstract: Academic performance depends on a multivariable nexus of socio-academic and
financial factors. This study investigates these influences to develop
effective strategies for optimizing students' CGPA. To achieve this, we
reviewed various literature to identify key influencing factors and constructed
an initial hypothetical causal graph based on the findings. Additionally, an
online survey was conducted, where 1,050 students participated, providing
comprehensive data for analysis. Rigorous data preprocessing techniques,
including cleaning and visualization, ensured data quality before analysis.
Causal analysis validated the relationships among variables, offering deeper
insights into their direct and indirect effects on CGPA. Regression models were
implemented for CGPA prediction, while classification models categorized
students based on performance levels. Ridge Regression demonstrated strong
predictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared
Error of 0.023. Random Forest outperformed in classification, attaining an
F1-score near perfection and an accuracy of 98.68%. Explainable AI techniques
such as SHAP, LIME, and Interpret enhanced model interpretability, highlighting
critical factors such as study hours, scholarships, parental education, and
prior academic performance. The study culminated in the development of a
web-based application that provides students with personalized insights,
allowing them to predict academic performance, identify areas for improvement,
and make informed decisions to enhance their outcomes.

</details>


### [68] [Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management](https://arxiv.org/abs/2508.00806)
*Ping Chen,Zhuohong Deng,Ping Li,Shuibing He,Hongzi Zhu,Yi Zheng,Zhefeng Wang,Baoxing Huai,Minyi Guo*

Main category: cs.LG

TL;DR: Adacc是一个结合自适应压缩和激活检查点的内存管理框架，旨在减少GPU内存占用并加速大型语言模型训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练中的重计算会引入高达30%的开销，Adacc旨在通过优化内存管理减少这一开销。

Method: Adacc包含三个模块：层特定压缩算法、基于MILP的最优调度策略和自适应策略演化机制。

Result: 实验表明，Adacc比现有框架加速1.01x至1.37x，同时保持与基线相当的模型精度。

Conclusion: Adacc通过自适应压缩和策略优化，有效提升了训练效率，同时保证了模型精度。

Abstract: Training large language models often employs recomputation to alleviate
memory pressure, which can introduce up to 30% overhead in real-world
scenarios. In this paper, we propose Adacc, a novel memory management framework
that combines adaptive compression and activation checkpointing to reduce the
GPU memory footprint. It comprises three modules: (1) We design layer-specific
compression algorithms that account for outliers in LLM tensors, instead of
directly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We
propose an optimal scheduling policy that employs MILP to determine the best
memory optimization for each tensor. (3) To accommodate changes in training
tensors, we introduce an adaptive policy evolution mechanism that adjusts the
policy during training to enhance throughput. Experimental results show that
Adacc can accelerate the LLM training by 1.01x to 1.37x compared to
state-of-the-art frameworks, while maintaining comparable model accuracy to the
Baseline.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [69] [Improved Bounds on Access-Redundancy Tradeoffs in Quantized Linear Computations](https://arxiv.org/abs/2508.00183)
*Ching-Fang Li,Mary Wootters*

Main category: cs.IT

TL;DR: 论文研究了如何用少量查询计算量化线性函数的问题，改进了先前的不可能性结果，并首次探讨了近似恢复的构造方法。


<details>
  <summary>Details</summary>
Motivation: 解决在有限查询次数下精确或近似计算量化线性函数的挑战，优化现有构造的局限性。

Method: 通过改进先前的不可能性结果，并引入近似恢复的构造方法，特别是针对ε=0.1的情况。

Result: 证明了先前块构造的最优性，并展示了近似构造在某些情况下优于精确构造。

Conclusion: 论文为量化线性函数的高效计算提供了新的理论和构造方法，特别是在近似恢复方面具有潜力。

Abstract: Consider the problem of computing quantized linear functions with only a few
queries. Formally, given $\mathbf{x}\in \mathbb{R}^k$, our goal is to encode
$\mathbf{x}$ as $\mathbf{c} \in \mathbb{R}^n$, for $n > k$, so that for any
$\mathbf{w} \in A^k$, $\mathbf{w}^T \mathbf{x}$ can be computed using at most
$\ell$ queries to $\mathbf{c}$. Here, $A$ is some finite set; in this paper we
focus on the case where $|A| = 2$.
  Prior work \emph{(Ramkumar, Raviv, and Tamo, Trans. IT, 2024)} has given
constructions and established impossibility results for this problem. We give
improved impossibility results, both for the general problem, and for the
specific class of construction (block construction) presented in that work. The
latter establishes that the block constructions of prior work are optimal
within that class.
  We also initiate the study of \emph{approximate} recovery for this problem,
where the goal is not to recover $\mathbf{w}^T \mathbf{x}$ exactly but rather
to approximate it up to a parameter $\varepsilon > 0$. We give several
constructions, and give constructions for $\varepsilon = 0.1$ that outperform
our impossibility result for exact schemes.

</details>


### [70] [Channel Estimation for Flexible Intelligent Metasurfaces: From Model-Based Approaches to Neural Operators](https://arxiv.org/abs/2508.00268)
*Jian Xiao,Ji Wang,Qimei Cui,Yucang Yang,Xingwang Li,Dusit Niyato,Chau Yuen*

Main category: cs.IT

TL;DR: 本文研究了柔性智能超表面（FIM）辅助毫米波通信系统中的信道估计问题，提出了基于模型和深度学习的框架，其中层次化傅里叶神经算子（H-FNO）显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: FIM通过动态变形实现多径信号的相长干涉，但其性能依赖于高维变形空间中的精确信道状态信息获取。

Method: 开发了基于模型的框架（插值、核方法和稀疏信号恢复）和深度学习框架（FNO和H-FNO），用于信道估计。

Result: H-FNO在估计精度和导频效率上显著优于基于模型的基准方法，并能准确重建非线性信道响应。

Conclusion: H-FNO通过学习适应FIM物理几何的各向异性空间滤波器，为FIM系统提供了高效的信道估计解决方案。

Abstract: Flexible intelligent metasurfaces (FIMs) offer a new solution for wireless
communications by introducing morphological degrees of freedom, dynamically
morphing their three-dimensional shape to ensure multipath signals interfere
constructively. However, realizing the desired performance gains in FIM systems
is critically dependent on acquiring accurate channel state information across
a continuous and high-dimensional deformation space. Therefore, this paper
investigates this fundamental channel estimation problem for FIM assisted
millimeter-wave communication systems. First, we develop model-based frameworks
that structure the problem as either function approximation using interpolation
and kernel methods or as a sparse signal recovery problem that leverages the
inherent angular sparsity of millimeter-wave channels. To further advance the
estimation capability beyond explicit assumptions in model-based channel
estimation frameworks, we propose a deep learning-based framework using a
Fourier neural operator (FNO). By parameterizing a global convolution operator
in the Fourier domain, we design an efficient FNO architecture to learn the
continuous operator that maps FIM shapes to channel responses with
mesh-independent properties. Furthermore, we exploit a hierarchical FNO (H-FNO)
architecture to efficiently capture the multi-scale features across a hierarchy
of spatial resolutions. Numerical results demonstrate that the proposed H-FNO
significantly outperforms the model-based benchmarks in estimation accuracy and
pilot efficiency. In particular, the interpretability analysis show that the
proposed H-FNO learns an anisotropic spatial filter adapted to the physical
geometry of FIM and is capable of accurately reconstructing the non-linear
channel response across the continuous deformation space.

</details>


### [71] [Active IRS-Enabled Integrated Sensing and Communications with Extended Targets](https://arxiv.org/abs/2508.00379)
*Yuan Fang,Xianxin Song,Huazhou Hou,Ziguo Zhong,Xianghao Yu,Jie Xu,Yongming Huang*

Main category: cs.IT

TL;DR: 本文研究了主动智能反射面（IRS）支持的集成传感与通信（ISAC），通过优化波束成形最小化传感CRB，同时满足通信用户的SINR需求。


<details>
  <summary>Details</summary>
Motivation: 解决非视距（NLoS）区域中通信和传感的路径损耗问题，提升系统性能。

Method: 联合优化基站（BS）的发射波束成形和主动IRS的反射波束成形，提出交替优化（AO）算法。

Result: 主动IRS在实用系统设置下总是使用最大放大增益，验证了AO算法的有效性。

Conclusion: 主动IRS支持的ISAC优于被动IRS，显著提升了系统性能。

Abstract: This paper studies the active intelligent reflecting surface (IRS)-enabled
integrated sensing and communications (ISAC), in which an active IRS is
deployed to assist the base station (BS) in serving multiple communication
users (CUs) and simultaneously sensing an \emph{extended} target at the
non-line-of-sight (NLoS) area of the BS. The active IRS has the capability of
amplifying the reflected signals so as to overcome significant reflection path
loss in NLoS communication and sensing. In particular, we derive the sensing
Cram\'{e}r-Rao bound (CRB) for estimating the target response matrix.
Accordingly, we jointly optimize the transmit beamforming at the BS and the
reflective beamforming at the active IRS to minimize the sensing CRB, subject
to the signal-to-interference-plus-noise ratio (SINR) requirements at the CUs,
the transmit power budgets at the BS and active IRS, as well as the power
amplification gain constraints at the active IRS. The CRB minimization problem
is highly non-convex and thus difficult to solve in general. To address this
challenge, we first focus on two specified conditions by considering the
sensing-only scenario via ignoring the SINR constraints for communications, for
which the closed-form optimal transmit beamforming is derived. Then, we propose
two efficient alternating optimization (AO)-based algorithms to obtain
high-quality solutions for the general ISAC scenarios. Next, we analyze the
inherent relationship between the power scaling at the BS and the amplification
scaling at the active IRS. It is shown that the active IRS always amplifies the
signal using the maximum amplification gain under practical system settings.
Finally, numerical results are provided to verify the effectiveness of the
proposed AO-based algorithms and the benefits of active IRS-enabled ISAC
compared to its passive IRSs counterparts.

</details>


### [72] [LO-Aware Adaptive Modulation for Rydberg Atomic Receivers](https://arxiv.org/abs/2508.00458)
*Jiuyu Liu,Yi Ma,Rahim Tafazolli*

Main category: cs.IT

TL;DR: 本文提出了一种针对Rydberg原子接收器的LO感知自适应调制方案（LOAM），通过动态适应复杂衰落信道系数，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: Rydberg原子接收器在无线通信中具有革命性潜力，但现有调制方案无法有效支持其量子检测机制，导致性能受限。

Method: LOAM方案通过自适应共线星座架构，根据参考信号和信道系数的相位动态调整星座点分布，最大化最小振幅差。

Result: 仿真结果显示，LOAM性能优于传统调制方案（如QAM、PSK、PAM），增益超过45 dB。

Conclusion: LOAM为Rydberg原子接收器提供了一种高效的调制方案，显著提升了通信性能。

Abstract: Rydberg atomic (RA) receivers represent a revolutionary quantum technology
for wireless communications, offering unprecedented sensitivity beyond
conventional radio frequency (RF) antennas. However, these receivers detect
only signal amplitude, losing critical phase information. While reference
signals generated by a local oscillator (LO) can assist in phase recovery,
existing modulation schemes designed for conventional systems perform poorly
with this quantum detection mechanism. This paper introduces a breakthrough
LO-aware adaptive modulation (LOAM) scheme specifically developed for RA
receivers that dynamically adapts to complex fading channel coefficients. LOAM
maximizes the minimum amplitude difference between constellation points,
ensuring optimal detection performance. The innovation employs an adaptive
co-linear constellation architecture aligned with the combined phase of
reference signal and channel coefficient. For strong reference signals, LOAM
generates symmetric constellation points centered at origin; for weak signals,
it adopts non-symmetric distributions. The paper mathematically derives the
threshold governing these operational regimes. Simulation results reveal the
transformative impact of LOAM, demonstrating performance gains exceeding 45 dB
over conventional modulation schemes, including quadrature amplitude modulation
(QAM), phase-shift keying (PSK), and pulse-amplitude modulation (PAM).

</details>


### [73] [Towards a Measure Theory of Semantic Information](https://arxiv.org/abs/2508.00525)
*George M. Coghill*

Main category: cs.IT

TL;DR: 本文批判了Floridi的强语义信息理论，并提出了一种基于单位圆的新方法，解决了Bar-Hillel-Carnap悖论。


<details>
  <summary>Details</summary>
Motivation: Floridi的理论未能完全解决Bar-Hillel-Carnap悖论，作者希望通过新方法彻底消除这一矛盾。

Method: 采用单位圆作为基础，类比von Neumann的量子概率，构建了一个满足Floridi要求的度量空间。

Result: 新方法不仅解决了悖论，还发现相互矛盾的信息具有相同的信息量。

Conclusion: 基于单位圆的方法有效解决了Floridi理论的不足，并提供了实际应用的示例。

Abstract: A classic account of the quantification of semantic information is that of
Bar-Hiller and Carnap. Their account proposes an inverse relation between the
informativeness of a statement and its probability. However, their approach
assigns the maximum informativeness to a contradiction: which Floridi refers to
as the Bar-Hillel-Carnap paradox. He developed a novel theory founded on a
distance metric and parabolic relation, designed to remove this paradox.
Unfortunately is approach does not succeed in that aim.
  In this paper I critique Floridi's theory of strongly semantic information on
its own terms and show where it succeeds and fails. I then present a new
approach based on the unit circle (a relation that has been the basis of
theories from basic trigonometry to quantum theory). This is used, by analogy
with von Neumann's quantum probability to construct a measure space for
informativeness that meets all the requirements stipulated by Floridi and
removes the paradox. In addition, while contradictions and tautologies have
zero informativeness, it is found that messages which are contradictory to each
other are equally informative. The utility of this is explained by means of an
example.

</details>


### [74] [Appendices for "Closed-Form BER Analysis for Uplink NOMA with Dynamic SIC Decoding"](https://arxiv.org/abs/2508.00540)
*Hequn Zhang,Qu Luo,Pei Xiao,Yue Zhang,Huiyu Zhou*

Main category: cs.IT

TL;DR: 本文补充材料提供了上行NOMA动态SIC解码的闭式BER分析详细数学推导，包括信道增益排序、误差概率和调制方案的PDF推导。


<details>
  <summary>Details</summary>
Motivation: 为上行NOMA系统在瑞利衰落信道下的动态SIC解码提供数学基础，支持闭式BER分析。

Method: 详细数学推导，包括信道增益排序、PDF推导和误差概率分析。

Result: 提供了多种调制方案和系统配置下的闭式BER表达式。

Conclusion: 补充材料为上行NOMA动态SIC解码的闭式BER分析提供了坚实的数学基础。

Abstract: This document provides the supplementary materials for the paper Closed-Form
BER Analysis for Uplink NOMA with Dynamic SIC Decoding. The appendices present
detailed mathematical derivations and proofs that support the analytical
framework of the main paper. Specifically, we include: (i) cumulative
distribution functions for ordered channel gains; (ii) probability density
functions of normalized signal-plus-interference variances in NOMA dynamic SIC
decoding; (iii) closed-form expressions for pairwise error probability (PEP)
with two users; (iv) probability derivations for channel gain ordering in the
two-UE case, specifically when UE 1 and UE 2 have the strongest or second
strongest channel gains; (v) BER analysis for M-QAM modulation schemes
including BPSK, 4QAM, 16QAM and 64QAM; (vi) PDF derivations for channel gains
under various ordering conditions; and (vii) challenges of PDF derivations for
real part of channel gain under various ordering condition. These mathematical
foundations enable the closed-form BER analysis of uplink NOMA systems with
dynamic SIC decoding under Rayleigh fading channels, supporting analytical
expressions for various modulation schemes and system configurations.

</details>


### [75] [Information-Theoretic Decentralized Secure Aggregation with Collusion Resilience](https://arxiv.org/abs/2508.00596)
*Xiang Zhang,Zhou Li,Shuangyang Li,Kai Wan,Derrick Wing Kwan Ng,Giuseppe Caire*

Main category: cs.IT

TL;DR: 该论文研究了去中心化联邦学习中的安全聚合问题，从信息论的角度分析了通信和密钥使用的最优界限。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注协议设计和计算保证，缺乏对系统基本信息论限制的理解，尤其是在去中心化环境中。

Method: 通过分析一个由K个全连接用户组成的网络，每个用户持有私有输入，目标是安全计算所有输入的总和。

Result: 确定了最优速率区域，表明每个用户必须传输至少一个符号、持有至少一个密钥符号，且所有用户共同持有不少于K-1个独立密钥符号。

Conclusion: 研究结果为分布式学习系统中设计可证明安全和通信高效的协议提供了理论基础。

Abstract: In decentralized federated learning (FL), multiple clients collaboratively
learn a shared machine learning (ML) model by leveraging their privately held
datasets distributed across the network, through interactive exchange of the
intermediate model updates. To ensure data security, cryptographic techniques
are commonly employed to protect model updates during aggregation. Despite
growing interest in secure aggregation, existing works predominantly focus on
protocol design and computational guarantees, with limited understanding of the
fundamental information-theoretic limits of such systems. Moreover, optimal
bounds on communication and key usage remain unknown in decentralized settings,
where no central aggregator is available. Motivated by these gaps, we study the
problem of decentralized secure aggregation (DSA) from an information-theoretic
perspective. Specifically, we consider a network of $K$ fully-connected users,
each holding a private input -- an abstraction of local training data -- who
aim to securely compute the sum of all inputs. The security constraint requires
that no user learns anything beyond the input sum, even when colluding with up
to $T$ other users. We characterize the optimal rate region, which specifies
the minimum achievable communication and secret key rates for DSA. In
particular, we show that to securely compute one symbol of the desired input
sum, each user must (i) transmit at least one symbol to others, (ii) hold at
least one symbol of secret key, and (iii) all users must collectively hold no
fewer than $K - 1$ independent key symbols. Our results establish the
fundamental performance limits of DSA, providing insights for the design of
provably secure and communication-efficient protocols in distributed learning
systems.

</details>


### [76] [Deep Learning-Based Rate-Adaptive CSI Feedback for Wideband XL-MIMO Systems in the Near-Field Domain](https://arxiv.org/abs/2508.00626)
*Zhenyu Liu,Yi Ma,Rahim Tafazolli*

Main category: cs.IT

TL;DR: 论文提出了一种名为WideNLNet-CA的深度学习框架，用于在宽带近场XL-MIMO系统中实现高效的CSI反馈。


<details>
  <summary>Details</summary>
Motivation: 在6G网络中，XL-MIMO系统的频谱效率增益依赖于准确且高效的CSI反馈，但近场球面波传播和宽带场景中的频率依赖性波束分裂效应带来了挑战。

Method: WideNLNet-CA采用轻量级编码器-解码器架构，结合多阶段下采样和上采样，以及计算高效的残差块，并引入压缩比自适应模块动态调整特征选择。

Result: 实验表明，WideNLNet-CA在各种压缩比和带宽下均优于现有方法，同时保持了快速推理和低存储需求。

Conclusion: WideNLNet-CA为宽带近场XL-MIMO系统中的CSI反馈提供了一种高效且灵活的解决方案。

Abstract: Accurate and efficient channel state information (CSI) feedback is crucial
for unlocking the substantial spectral efficiency gains of extremely
large-scale MIMO (XL-MIMO) systems in future 6G networks. However, the
combination of near-field spherical wave propagation and frequency-dependent
beam split effects in wideband scenarios poses significant challenges for CSI
representation and compression. This paper proposes WideNLNet-CA, a
rate-adaptive deep learning framework designed to enable efficient CSI feedback
in wideband near-field XL-MIMO systems. WideNLNet-CA introduces a lightweight
encoder-decoder architecture with multi-stage downsampling and upsampling,
incorporating computationally efficient residual blocks to capture complex
multi-scale channel features with reduced overhead. A novel compression ratio
adaptive module with feature importance estimation is introduced to dynamically
modulate feature selection based on target compression ratios, enabling
flexible adaptation across a wide range of feedback rates using a single model.
Evaluation results demonstrate that WideNLNet-CA consistently outperforms
existing compressive sensing and deep learning-based works across various
compression ratios and bandwidths, while maintaining fast inference and low
model storage requirements.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [77] [Computation of Approximately Stable Committees in Approval-based Elections](https://arxiv.org/abs/2508.00130)
*Drew Gao,Yihang Sun,Jan Vondrák*

Main category: cs.GT

TL;DR: 论文研究了基于批准的委员会选择模型中的近似稳定性问题，证明了3.65-近似稳定委员会的存在性，并提出了一种基于Lindahl均衡和强瑞利分布的算法。


<details>
  <summary>Details</summary>
Motivation: 研究基于批准的委员会选择模型中的稳定性问题，旨在找到能够代表选民偏好的委员会。

Method: 通过寻找Lindahl均衡，并利用强瑞利分布进行采样，设计了一种算法。

Result: 证明了3.65-近似稳定委员会的存在性，并提供了计算该委员会的算法。

Conclusion: 该研究为委员会选择问题提供了新的理论保证和计算方法。

Abstract: Approval-based committee selection is a model of significant interest in
social choice theory. In this model, we have a set of voters $\mathcal{V}$, a
set of candidates $\mathcal{C}$, and each voter has a set $A_v \subset
\mathcal{C}$ of approved candidates. For any committee size $K$, the goal is to
choose $K$ candidates to represent the voters' preferences. We study a
criterion known as \emph{approximate stability}, where a committee is
$\lambda$-approximately-stable if there is no other committee $T$ preferred by
at least $\frac{\lambda|T|}{k} |\mathcal{V}| $ voters. We prove that a
$3.65$-approximately stable committee always exists and can be computed
algorithmically in this setting. Our approach is based on finding a Lindahl
equilibrium and sampling from a strongly Rayleigh distribution associated with
it.

</details>


### [78] [On the Equivalence of the Graph-Structural and Optimization-Based Characterizations of Popular Matchings](https://arxiv.org/abs/2508.00349)
*Yuga Kanaya,Kenjiro Takazawa*

Main category: cs.GT

TL;DR: 本文研究了三种流行匹配问题，通过连接图结构特征和优化特征，提供了对两者等价性的全面理解。


<details>
  <summary>Details</summary>
Motivation: 研究流行匹配问题，解决如何高效确定匹配的流行性，避免直接应用定义所需的指数时间。

Method: 通过图结构特征和基于优化的特征（最大权重匹配）对三种流行匹配问题进行分析，并证明两者之间的直接联系。

Result: 证明了两种特征可以相互推导，提供了对流行匹配结构的深入理解，并揭示了图结构特征与最大权重匹配对偶解的新解释。

Conclusion: 本文通过连接两种特征，为流行匹配问题提供了更全面的理解，并提出了新的解释视角。

Abstract: Popular matchings provide a model of matching under preferences in which a
solution corresponds to a Condorcet winner in voting systems. In a bipartite
graph in which the vertices have preferences over their neighbours, a matching
is defined to be popular if it does not lose in a majority vote against any
matching. In this paper, we study the following three primary problems: only
the vertices on one side have preferences; a generalization of this problem
allowing ties in the preferences; and the vertices on both sides have
preferences. A principal issue in the algorithmic aspects of popular matchings
is how to determine the popularity of a matching, because it requires
exponential time if the definition is simply applied. In the literature, we
have the following two types of characterizations: a graph-structural
characterization; and an optimization-based characterization described by
maximum-weight matchings. The graph-structural characterizations are
specifically designed for each problem and provide a combinatorial structure of
the popular matchings. The optimization-based characterizations work in the
same manner for all problems, while they do not reveal the structure of the
popular matchings. A main contribution of this paper is to provide a direct
connection of the above two types of characterizations for all of the three
problems. Specifically, we prove that each characterization can be derived from
the other, without relying on the fact that they characterize popular
matchings. Our proofs offer a comprehensive understanding of the equivalence of
the two types of characterizations, and suggest a new interpretation of the
graph-structural characterization in terms of the dual optimal solution for the
maximum-weight matching problem.

</details>


### [79] [Justified Representation: From Hare to Droop](https://arxiv.org/abs/2508.00811)
*Matthew M. Casey,Edith Elkind*

Main category: cs.GT

TL;DR: 本文系统研究了基于Droop配额的多赢家投票中的比例性公理，提出了满足Droop版本JR公理的投票规则，并通过实验验证了其严格性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注Hare配额下的比例性公理，而对更严格的Droop配额分析不足。本文旨在填补这一空白。

Method: 通过修改已知投票规则或设计新规则，验证其满足Droop版本的JR公理（如JR、PJR、EJR等）。

Result: 成功为每种Droop版本JR公理找到满足的投票规则，并通过实验证明Droop配额比Hare配额更严格。

Conclusion: 本文扩展了可满足比例性公理的范围，为多赢家投票提供了更严格的理论基础。

Abstract: The study of proportionality in multiwinner voting with approval ballots has
received much attention in recent years. Typically, proportionality is captured
by variants of the Justified Representation axiom, which say that cohesive
groups of at least $\ell\cdot\frac{n}{k}$ voters (where $n$ is the total number
of voters and $k$ is the desired number of winners) deserve $\ell$
representatives. The quantity $\frac{n}{k}$ is known as the Hare quota in the
social choice literature. Another -- more demanding -- choice of quota is the
Droop quota, defined as $\lfloor\frac{n}{k+1}\rfloor+1$. This quota is often
used in multiwinner voting with ranked ballots: in algorithms such as Single
Transferable Voting, and in proportionality axioms, such as Droop's
Proportionality Criterion. A few authors have considered it in the context of
approval ballots, but the existing analysis is far from comprehensive. The
contribution of our work is a systematic study of JR-style axioms (and voting
rules that satisfy them) defined using the Droop quota instead of the Hare
quota. For each of the standard JR axioms (namely, JR, PJR, EJR, FPJR, FJR,
PJR+ and EJR+), we identify a voting rule that satisfies the Droop version of
this axiom. In some cases, it suffices to consider known rules (modifying the
corresponding Hare proof, sometimes quite substantially), and in other cases it
is necessary to modify the rules from prior work. Each axiom is more difficult
to satisfy when defined using the Droop quota, so our results expand the
frontier of satisfiable proportionality axioms. We complement our theoretical
results with an experimental study, showing that for many probabilistic models
of voter approvals, Droop JR/EJR+ are considerably more demanding than standard
(Hare) JR/EJR+.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [80] [MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for Document Question-Answering with Hierarchical Index and Multi-Granularity Retrieval](https://arxiv.org/abs/2508.00579)
*Ziyu Gong,Yihua Huang,Chengcheng Mai*

Main category: cs.MM

TL;DR: 提出了一种名为MMRAG-DocQA的多模态RAG模型，通过结合文本和视觉信息解决多模态长文档问答任务中的幻觉和模态断开问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（LVLM和RAG）在多模态长文档问答中存在幻觉和模态断开问题，需要一种更有效的方法。

Method: 设计了分层索引方法，结合扁平化的页面内块和拓扑结构的跨页面块，提出多粒度语义检索方法，包括页面级和文档级检索。

Result: 在MMLongBench-Doc和LongDocURL数据集上实验，证明了MMRAG-DocQA在理解和回答多模态多页文档方面的优越性。

Conclusion: MMRAG-DocQA通过多模态关联和长距离依赖的联合建模，显著提升了多模态长文档问答的性能。

Abstract: The multi-modal long-context document question-answering task aims to locate
and integrate multi-modal evidences (such as texts, tables, charts, images, and
layouts) distributed across multiple pages, for question understanding and
answer generation. The existing methods can be categorized into Large
Vision-Language Model (LVLM)-based and Retrieval-Augmented Generation
(RAG)-based methods. However, the former were susceptible to hallucinations,
while the latter struggled for inter-modal disconnection and cross-page
fragmentation. To address these challenges, a novel multi-modal RAG model,
named MMRAG-DocQA, was proposed, leveraging both textual and visual information
across long-range pages to facilitate accurate question answering. A
hierarchical indexing method with the integration of flattened in-page chunks
and topological cross-page chunks was designed to jointly establish in-page
multi-modal associations and long-distance cross-page dependencies. By means of
joint similarity evaluation and large language model (LLM)-based re-ranking, a
multi-granularity semantic retrieval method, including the page-level parent
page retrieval and document-level summary retrieval, was proposed to foster
multi-modal evidence connection and long-distance evidence integration and
reasoning. Experimental results performed on public datasets, MMLongBench-Doc
and LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in
understanding and answering modality-rich and multi-page documents.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [81] [Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench](https://arxiv.org/abs/2508.00081)
*Fred Mutisya,Shikoh Gitau,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha*

Main category: cs.AI

TL;DR: HealthBench是一个评估AI医疗能力的基准，但依赖专家意见可能引入偏见。作者提出基于临床实践指南（CPGs）的改进方案，以增强全球适用性和公平性。


<details>
  <summary>Details</summary>
Motivation: HealthBench的局限性在于依赖专家意见，可能引入区域偏见和临床个体差异，尤其在低收入地区问题更突出。需要更全球化和公平的基准。

Method: 提出基于版本控制CPGs的奖励函数，结合系统评价和GRADE证据评级，通过强化学习实现证据稳健的评估。

Result: 改进方案旨在提升医疗语言模型的临床可信度、伦理性和全球适用性，同时保留透明度和医生参与。

Conclusion: 通过基于严格审查的CPGs重新设计奖励机制，可以构建更可靠、公平且全球适用的医疗AI评估基准。

Abstract: HealthBench, a benchmark designed to measure the capabilities of AI systems
for health better (Arora et al., 2025), has advanced medical language model
evaluation through physician-crafted dialogues and transparent rubrics.
However, its reliance on expert opinion, rather than high-tier clinical
evidence, risks codifying regional biases and individual clinician
idiosyncrasies, further compounded by potential biases in automated grading
systems. These limitations are particularly magnified in low- and middle-income
settings, where issues like sparse neglected tropical disease coverage and
region-specific guideline mismatches are prevalent.
  The unique challenges of the African context, including data scarcity,
inadequate infrastructure, and nascent regulatory frameworks, underscore the
urgent need for more globally relevant and equitable benchmarks. To address
these shortcomings, we propose anchoring reward functions in version-controlled
Clinical Practice Guidelines (CPGs) that incorporate systematic reviews and
GRADE evidence ratings.
  Our roadmap outlines "evidence-robust" reinforcement learning via
rubric-to-guideline linkage, evidence-weighted scoring, and contextual override
logic, complemented by a focus on ethical considerations and the integration of
delayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,
while preserving HealthBench's transparency and physician engagement, we aim to
foster medical language models that are not only linguistically polished but
also clinically trustworthy, ethically sound, and globally relevant.

</details>


### [82] [Hyperproperty-Constrained Secure Reinforcement Learning](https://arxiv.org/abs/2508.00106)
*Ernest Bonnah,Luan Viet Nguyen,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: 本文提出了一种基于HyperTWTL的安全强化学习方法（SecRL），解决了现有研究中安全感知强化学习的空白，并通过案例研究验证了其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在安全强化学习中未充分探索基于超属性的安全约束，尤其是在机器人应用中。本文旨在填补这一空白。

Method: 提出了一种动态Boltzmann softmax强化学习方法，结合HyperTWTL约束，用于学习安全感知的最优策略。

Result: 通过机器人任务案例研究验证了方法的有效性，并优于两种基线强化学习算法。

Conclusion: HyperTWTL约束的安全强化学习方法在机器人应用中表现出色，具有实际应用潜力。

Abstract: Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a
domain-specific formal specification language known for its effectiveness in
compactly representing security, opacity, and concurrency properties for
robotics applications. This paper focuses on HyperTWTL-constrained secure
reinforcement learning (SecRL). Although temporal logic-constrained safe
reinforcement learning (SRL) is an evolving research problem with several
existing literature, there is a significant research gap in exploring
security-aware reinforcement learning (RL) using hyperproperties. Given the
dynamics of an agent as a Markov Decision Process (MDP) and opacity/security
constraints formalized as HyperTWTL, we propose an approach for learning
security-aware optimal policies using dynamic Boltzmann softmax RL while
satisfying the HyperTWTL constraints. The effectiveness and scalability of our
proposed approach are demonstrated using a pick-up and delivery robotic mission
case study. We also compare our results with two other baseline RL algorithms,
showing that our proposed method outperforms them.

</details>


### [83] [No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence](https://arxiv.org/abs/2508.00116)
*Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: AI在工业应用中面临挑战，需结合对象中心过程挖掘（OCPM）实现过程智能（PI），以提升端到端操作流程。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在工业环境中成功应用的障碍，提出结合OCPM的必要性。

Method: 分析生成式、预测式和规定式AI，引入OCPM作为数据与过程的桥梁。

Result: OCPM是实现过程智能（PI）的关键，能有效支持多种AI形式。

Conclusion: AI需结合PI和OCPM，以优化操作流程并推动工业应用成功。

Abstract: The uptake of Artificial Intelligence (AI) impacts the way we work, interact,
do business, and conduct research. However, organizations struggle to apply AI
successfully in industrial settings where the focus is on end-to-end
operational processes. Here, we consider generative, predictive, and
prescriptive AI and elaborate on the challenges of diagnosing and improving
such processes. We show that AI needs to be grounded using Object-Centric
Process Mining (OCPM). Process-related data are structured and
organization-specific and, unlike text, processes are often highly dynamic.
OCPM is the missing link connecting data and processes and enables different
forms of AI. We use the term Process Intelligence (PI) to refer to the
amalgamation of process-centric data-driven techniques able to deal with a
variety of object and event types, enabling AI in an organizational context.
This paper explains why AI requires PI to improve operational processes and
highlights opportunities for successfully combining OCPM and generative,
predictive, and prescriptive AI.

</details>


### [84] [Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis](https://arxiv.org/abs/2508.00129)
*Agustín Borda,Juan Bautista Cabral,Gonzalo Giarda,Diego Nicolás Gimenez Irusta,Paula Pacheco,Alvaro Roy Schachner*

Main category: cs.AI

TL;DR: 论文提出了三种检测多准则决策分析中排名反转问题的测试方法，并讨论了其在Scikit-Criteria库中的实现及其对方法评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 排名反转是多准则决策分析中的严重问题，影响决策方法的有效性，因此需要一种机制来衡量方法的性能。

Method: 提出了三种测试方法，检测排名反转，并在Scikit-Criteria库中实现，同时讨论了通用场景下的实现挑战。

Result: 成功实现了三种测试方法，并解决了通用场景中的设计问题。

Conclusion: 这些测试方法在多准则决策方法的评估中可能发挥重要作用。

Abstract: In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem
that can greatly affect the results of a Multi-Criteria Decision Method against
a particular set of alternatives. It is therefore useful to have a mechanism
that allows one to measure the performance of a method on a set of
alternatives. This idea could be taken further to build a global ranking of the
effectiveness of different methods to solve a problem. In this paper, we
present three tests that detect the presence of Rank Reversals, along with
their implementation in the Scikit-Criteria library. We also address the
complications that arise when implementing these tests for general scenarios
and the design considerations we made to handle them. We close with a
discussion about how these additions could play a major role in the judgment of
multi-criteria decision methods for problem solving.

</details>


### [85] [Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings](https://arxiv.org/abs/2508.00632)
*Alexia Jolicoeur-Martineau*

Main category: cs.AI

TL;DR: 论文提出了AVR-Eval和AVR-Agent，分别用于评估和生成交互式多媒体内容，解决了当前AI在复杂内容生成中的不足。


<details>
  <summary>Details</summary>
Motivation: 当前AI在生成交互式多媒体内容（如视频游戏）时面临挑战，缺乏自动化评估指标且难以处理复杂内容。

Method: 提出AVR-Eval作为多媒体内容质量的相对评估指标，并开发AVR-Agent多代理系统生成JavaScript代码。

Result: AVR-Agent生成的内容在实验中表现优于单次生成的内容，但未能有效利用自定义资源和反馈。

Conclusion: 研究揭示了人类与机器在内容创作上的根本差异，强调了高质量资源和反馈的重要性。

Abstract: While AI excels at generating text, audio, images, and videos, creating
interactive audio-visual content such as video games remains challenging.
Current LLMs can generate JavaScript games and animations, but lack automated
evaluation metrics and struggle with complex content that normally requires
teams of humans working for many months (multi-shot, multi-agents) using assets
made by artists. To tackle these issues, we built a new metric and a
multi-agent system.
  We propose AVR-Eval, a relative metric for multimedia content quality using
Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video,
and audio) compares the AVRs of two contents, with a text model reviewing
evaluations to determine superiority. We show that AVR-Eval properly identifies
good from broken or mismatched content.
  We built AVR-Agent, a multi-agent system generating JavaScript code from a
bank of multimedia assets (audio, images, 3D models). The coding agent selects
relevant assets, generates multiple initial codes, uses AVR-Eval to identify
the best version, and iteratively improves it through omni-modal agent feedback
from the AVR.
  We run experiments on games and animations with AVR-Eval (win rate of content
A against B). We find that content generated by AVR-Agent has a significantly
higher win rate against content made through one-shot generation. However,
models struggle to leverage custom assets and AVR feedback effectively, showing
no higher win rate. This reveals a critical gap: while humans benefit from
high-quality assets and audio-visual feedback, current coding models do not
seem to utilize these resources as effectively, highlighting fundamental
differences between human and machine content creation approaches.

</details>


### [86] [SHACL Validation under Graph Updates (Extended Paper)](https://arxiv.org/abs/2508.00137)
*Shqiponja Ahmetaj,George Konstantinidis,Magdalena Ortiz,Paolo Pareti,Mantas Simkus*

Main category: cs.AI

TL;DR: 研究了SHACL在RDF图更新下的验证问题，提出了一种基于SHACL的更新语言，并通过回归技术将静态验证问题转化为SHACL约束的（不）可满足性问题。


<details>
  <summary>Details</summary>
Motivation: 研究RDF图在更新后是否仍满足SHACL规范，为动态RDF图提供验证基础。

Method: 提出SHACL更新语言，使用回归技术将更新动作嵌入SHACL约束，分析计算复杂性。

Result: 证明静态验证问题可转化为SHACL约束的（不）可满足性问题，并实现原型进行验证。

Conclusion: 为动态RDF图的SHACL验证提供了理论基础和实用工具。

Abstract: SHACL (SHApe Constraint Language) is a W3C standardized constraint language
for RDF graphs. In this paper, we study SHACL validation in RDF graphs under
updates. We present a SHACL-based update language that can capture intuitive
and realistic modifications on RDF graphs and study the problem of static
validation under such updates. This problem asks to verify whether every graph
that validates a SHACL specification will still do so after applying a given
update sequence. More importantly, it provides a basis for further services for
reasoning about evolving RDF graphs. Using a regression technique that embeds
the update actions into SHACL constraints, we show that static validation under
updates can be reduced to (un)satisfiability of constraints in (a minor
extension of) SHACL. We analyze the computational complexity of the static
validation problem for SHACL and some key fragments. Finally, we present a
prototype implementation that performs static validation and other static
analysis tasks on SHACL constraints and demonstrate its behavior through
preliminary experiments.

</details>


### [87] [Co-Producing AI: Toward an Augmented, Participatory Lifecycle](https://arxiv.org/abs/2508.00138)
*Rashid Mushkani,Hugo Berard,Toumadher Ammar,Cassandre Chatonnier,Shin Koseki*

Main category: cs.AI

TL;DR: 论文提出了一种基于设计正义和参与式AI的AI生命周期重构方法，强调共同生产、多样性和多学科合作，以减少AI对边缘化群体的负面影响。


<details>
  <summary>Details</summary>
Motivation: AI算法可能对文化边缘化群体产生不成比例的负面影响，现有方法（如伦理指南和技术解决方案）未能完全解决这一问题。

Method: 提出一个增强的AI生命周期，包括五个相互关联的阶段：共同框架、共同设计、共同实施、共同部署和共同维护，基于多学科研讨会和分布式权威理念。

Result: 新生命周期结合了伦理框架，并提出了扩展参与式治理的关键研究问题。

Conclusion: 重构AI生产流程以中心化共同生产和多样性，是减少AI对边缘化群体危害的关键。

Abstract: Despite efforts to mitigate the inherent risks and biases of artificial
intelligence (AI) algorithms, these algorithms can disproportionately impact
culturally marginalized groups. A range of approaches has been proposed to
address or reduce these risks, including the development of ethical guidelines
and principles for responsible AI, as well as technical solutions that promote
algorithmic fairness. Drawing on design justice, expansive learning theory, and
recent empirical work on participatory AI, we argue that mitigating these harms
requires a fundamental re-architecture of the AI production pipeline. This
re-design should center co-production, diversity, equity, inclusion (DEI), and
multidisciplinary collaboration. We introduce an augmented AI lifecycle
consisting of five interconnected phases: co-framing, co-design,
co-implementation, co-deployment, and co-maintenance. The lifecycle is informed
by four multidisciplinary workshops and grounded in themes of distributed
authority and iterative knowledge exchange. Finally, we relate the proposed
lifecycle to several leading ethical frameworks and outline key research
questions that remain for scaling participatory governance.

</details>


### [88] [Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation](https://arxiv.org/abs/2508.00143)
*Danielle R. Thomas,Conrad Borchers,Kenneth R. Koedinger*

Main category: cs.AI

TL;DR: 论文主张过度依赖人类评估者间一致性（IRR）会阻碍教育数据分类的进展，提出五种补充评估方法以提高数据质量和模型效果。


<details>
  <summary>Details</summary>
Motivation: 人类评估者存在偏见和不可靠性，传统IRR指标不足以确保标注数据的有效性，尤其是在教育AI领域。

Method: 提出五种补充评估方法，包括多标签标注方案、专家评估和闭环验证等。

Result: 这些方法能生成更有效的训练数据，提升模型对学生学习的促进作用。

Conclusion: 呼吁重新定义标注质量和真实标准，优先考虑有效性和教育影响，而非仅依赖共识。

Abstract: Humans can be notoriously imperfect evaluators. They are often biased,
unreliable, and unfit to define "ground truth." Yet, given the surging need to
produce large amounts of training data in educational applications using AI,
traditional inter-rater reliability (IRR) metrics like Cohen's kappa remain
central to validating labeled data. IRR remains a cornerstone of many machine
learning pipelines for educational data. Take, for example, the classification
of tutors' moves in dialogues or labeling open responses in machine-graded
assessments. This position paper argues that overreliance on human IRR as a
gatekeeper for annotation quality hampers progress in classifying data in ways
that are valid and predictive in relation to improving learning. To address
this issue, we highlight five examples of complementary evaluation methods,
such as multi-label annotation schemes, expert-based approaches, and
close-the-loop validity. We argue that these approaches are in a better
position to produce training data and subsequent models that produce improved
student learning and more actionable insights than IRR approaches alone. We
also emphasize the importance of external validity, for example, by
establishing a procedure of validating tutor moves and demonstrating that it
works across many categories of tutor actions (e.g., providing hints). We call
on the field to rethink annotation quality and ground truth--prioritizing
validity and educational impact over consensus alone.

</details>


### [89] [Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power](https://arxiv.org/abs/2508.00159)
*Jobst Heitzig,Ram Potham*

Main category: cs.AI

TL;DR: 论文探讨如何通过明确要求AI赋能人类并管理人机权力平衡，以促进安全和福祉。提出了一种参数化、可分解的目标函数，并通过算法计算其度量。


<details>
  <summary>Details</summary>
Motivation: 权力是AI安全中的关键概念，涉及权力寻求、人类权力丧失及权力平衡问题。同时，权力作为追求多样目标的能力对福祉至关重要。

Method: 采用部分公理化方法设计目标函数，考虑人类有限理性和社会规范，通过逆向归纳或多智能体强化学习计算该度量。

Result: 在多种典型情境中展示了最大化该度量的后果及其潜在子目标，表明其可能比直接基于效用的目标更安全。

Conclusion: 软性最大化人类权力聚合指标可能是AI系统的有益目标，比直接效用目标更安全。

Abstract: Power is a key concept in AI safety: power-seeking as an instrumental goal,
sudden or gradual disempowerment of humans, power balance in human-AI
interaction and international AI governance. At the same time, power as the
ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by
forcing AI agents explicitly to empower humans and to manage the power balance
between humans and AI agents in a desirable way. Using a principled, partially
axiomatic approach, we design a parametrizable and decomposable objective
function that represents an inequality- and risk-averse long-term aggregate of
human power. It takes into account humans' bounded rationality and social
norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or
approximating it via a form of multi-agent reinforcement learning from a given
world model. We exemplify the consequences of (softly) maximizing this metric
in a variety of paradigmatic situations and describe what instrumental
sub-goals it will likely imply. Our cautious assessment is that softly
maximizing suitable aggregate metrics of human power might constitute a
beneficial objective for agentic AI systems that is safer than direct
utility-based objectives.

</details>


### [90] [RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization](https://arxiv.org/abs/2508.00222)
*Yihong Dong,Xue Jiang,Yongding Tao,Huanyu Liu,Kechi Zhang,Lili Mou,Rongyu Cao,Yingwei Ma,Jue Chen,Binhua Li,Zhi Jin,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: RL-PLUS通过结合内部探索与外部数据，解决了RLVR的能力边界问题，提升了LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: RLVR因策略固有性和稀疏奖励限制了LLM的能力边界，甚至导致能力边界崩溃。

Method: RL-PLUS结合多重重要性采样和探索优势函数，优化推理路径。

Result: 在数学推理和分布外任务中表现优异，平均提升21.1%至69.2%。

Conclusion: RL-PLUS有效解决了能力边界崩溃问题，显著提升了推理性能。

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly
advanced the complex reasoning abilities of Large Language Models (LLMs).
However, it struggles to break through the inherent capability boundaries of
the base LLM, due to its inherently on-policy strategy with LLM's immense
action space and sparse reward. Further, RLVR can lead to the capability
boundary collapse, narrowing the LLM's problem-solving scope. To address this
problem, we propose RL-PLUS, a novel approach that synergizes internal
exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve
stronger reasoning capabilities and surpass the boundaries of base models.
RL-PLUS integrates two core components: Multiple Importance Sampling to address
for distributional mismatch from external data, and an Exploration-Based
Advantage Function to guide the model towards high-value, unexplored reasoning
paths. We provide both theoretical analysis and extensive experiments to
demonstrate the superiority and generalizability of our approach. The results
show that RL-PLUS achieves state-of-the-art performance compared with existing
RLVR methods on six math reasoning benchmarks and exhibits superior performance
on six out-of-distribution reasoning tasks. It also achieves consistent and
significant gains across diverse model families, with average relative
improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across
multiple benchmarks indicate that RL-PLUS effectively resolves the capability
boundary collapse problem.

</details>


### [91] [MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning](https://arxiv.org/abs/2508.00271)
*Hongjin Qian,Zheng Liu*

Main category: cs.AI

TL;DR: MetaAgent是一种基于学习实践原则的智能代理，通过不断自我改进和工具学习提升任务解决能力。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种能够通过实践和自我反思持续提升能力的智能代理，以解决知识发现任务中的挑战。

Method: MetaAgent从基础能力出发，通过生成自然语言请求、工具路由、自我反思和知识库构建，实现动态学习和工具使用优化。

Result: 在GAIA、WebWalkerQA和BrowseCamp等基准测试中，MetaAgent表现优于基线方法，并媲美端到端训练代理。

Conclusion: MetaAgent展示了自进化代理系统在通用知识发现任务中的潜力，无需调整模型参数或额外训练。

Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the
principle of learning-by-doing, where expertise is developed through hands-on
practice and continual self-improvement. MetaAgent starts with a minimal
workflow, equipped only with basic reasoning and adaptive help-seeking
abilities. When a knowledge gap is encountered, MetaAgent generates natural
language help requests, which are routed to the most suitable external tool by
a dedicated tool router. As MetaAgent solves tasks, it continually conducts
self-reflection and answer verification, distilling actionable experience into
concise texts that are dynamically incorporated into future task contexts.
Besides, MetaAgent autonomously builds in-house tools and a persistent
knowledge base by organizing its tool-use history, further enhancing its
ability to retrieve and integrate relevant information We term this continual,
data-driven process as \textit{meta tool learning}, through which MetaAgent
incrementally refines its reasoning and tool-use strategies, without changing
model parameters or requiring further post-training. Evaluated on challenging
knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,
MetaAgent consistently outperforms workflow-based baselines and matches or
exceeds end-to-end trained agents, demonstrating the promise of self-evolving
agentic systems for robust, general-purpose knowledge discovery. We provide our
source codes in https://github.com/qhjqhj00/MetaAgent.

</details>


### [92] [Mind the Gap: The Divergence Between Human and LLM-Generated Tasks](https://arxiv.org/abs/2508.00282)
*Yi-Long Lu,Jiajun Song,Chunhui Zhang,Wei Wang*

Main category: cs.AI

TL;DR: 论文比较了人类和LLM（GPT-4o）在任务生成中的差异，发现人类受心理驱动（如个人价值观和认知风格）影响，而LLM即使提供这些驱动也无法模拟人类行为，生成的任务更抽象、缺乏社交和物理性。尽管LLM的任务被认为更有趣和新颖，但其与人类认知存在本质差距。


<details>
  <summary>Details</summary>
Motivation: 探讨生成代理（如LLM）是否能模拟人类基于内在动机的任务生成行为。

Method: 通过任务生成实验比较人类和LLM（GPT-4o）的行为，分析心理驱动对任务生成的影响。

Result: 人类任务生成受心理驱动影响，而LLM生成的任务更抽象、缺乏社交和物理性，且与人类行为模式不符。

Conclusion: LLM与人类认知存在本质差距，需在设计更人性化代理时融入内在动机和物理基础。

Abstract: Humans constantly generate a diverse range of tasks guided by internal
motivations. While generative agents powered by large language models (LLMs)
aim to simulate this complex behavior, it remains uncertain whether they
operate on similar cognitive principles. To address this, we conducted a
task-generation experiment comparing human responses with those of an LLM agent
(GPT-4o). We find that human task generation is consistently influenced by
psychological drivers, including personal values (e.g., Openness to Change) and
cognitive style. Even when these psychological drivers are explicitly provided
to the LLM, it fails to reflect the corresponding behavioral patterns. They
produce tasks that are markedly less social, less physical, and thematically
biased toward abstraction. Interestingly, while the LLM's tasks were perceived
as more fun and novel, this highlights a disconnect between its linguistic
proficiency and its capacity to generate human-like, embodied goals.We conclude
that there is a core gap between the value-driven, embodied nature of human
cognition and the statistical patterns of LLMs, highlighting the necessity of
incorporating intrinsic motivation and physical grounding into the design of
more human-aligned agents.

</details>


### [93] [Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning](https://arxiv.org/abs/2508.00323)
*Jianyi Zhang,Xu Ji,Ziyin Zhou,Yuchen Zhou,Shubo Shi,Haoyu Wu,Zhen Li,Shizhao Liu*

Main category: cs.AI

TL;DR: ReasonBench是首个专注于结构化图形推理任务的评估基准，用于评估视觉语言模型（VLMs）在复杂图形推理中的表现，揭示了当前模型的局限性，并提出双重优化策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注简单图形，而VLMs在复杂图形推理和抽象问题解决方面表现不足，因此需要更全面的评估工具。

Method: 提出ReasonBench基准，包含1,613个真实世界智力测试问题，涵盖位置、属性、数量和多元素任务。评估11种主流VLMs，并提出DiaCoT和ReasonTune优化策略。

Result: 当前VLMs在复杂图形推理中存在显著局限性，优化策略使模型性能提升33.5%。

Conclusion: ReasonBench为VLMs的图形推理能力提供了全面评估，双重优化策略显著提升了模型性能。

Abstract: Evaluating the performance of visual language models (VLMs) in graphic
reasoning tasks has become an important research topic. However, VLMs still
show obvious deficiencies in simulating human-level graphic reasoning
capabilities, especially in complex graphic reasoning and abstract problem
solving, which are less studied and existing studies only focus on simple
graphics. To evaluate the performance of VLMs in complex graphic reasoning, we
propose ReasonBench, the first evaluation benchmark focused on structured
graphic reasoning tasks, which includes 1,613 questions from real-world
intelligence tests. ReasonBench covers reasoning dimensions related to
location, attribute, quantity, and multi-element tasks, providing a
comprehensive evaluation of the performance of VLMs in spatial, relational, and
abstract reasoning capabilities. We benchmark 11 mainstream VLMs (including
closed-source and open-source models) and reveal significant limitations of
current models. Based on these findings, we propose a dual optimization
strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability
of reasoning by decomposing layers, and ReasonTune enhances the task
adaptability of model reasoning through training, all of which improves VLM
performance by 33.5\%. All experimental data and code are in the repository:
https://huggingface.co/datasets/cistine/ReasonBench.

</details>


### [94] [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/abs/2508.00324)
*Yeonjun In,Wonjoong Kim,Sangwu Park,Chanyoung Park*

Main category: cs.AI

TL;DR: R1-Act是一种高效的后训练方法，通过结构化推理触发安全知识，显著提升大型推理模型的安全性，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 研究发现大型推理模型具备足够的安全知识，但在推理过程中未能激活，导致安全隐患。

Method: 提出R1-Act方法，通过结构化推理过程显式触发安全知识，仅需少量训练数据和计算资源。

Result: R1-Act在多个模型上表现出强大的安全性提升和推理性能保持，优于现有对齐方法。

Conclusion: R1-Act是一种高效、可扩展且实用的安全增强方法。

Abstract: Although large reasoning models (LRMs) have demonstrated impressive
capabilities on complex tasks, recent studies reveal that these models
frequently fulfill harmful user instructions, raising significant safety
concerns. In this paper, we investigate the underlying cause of LRM safety
risks and find that models already possess sufficient safety knowledge but fail
to activate it during reasoning. Based on this insight, we propose R1-Act, a
simple and efficient post-training method that explicitly triggers safety
knowledge through a structured reasoning process. R1-Act achieves strong safety
improvements while preserving reasoning performance, outperforming prior
alignment methods. Notably, it requires only 1,000 training examples and 90
minutes of training on a single RTX A6000 GPU. Extensive experiments across
multiple LRM backbones and sizes demonstrate the robustness, scalability, and
practical efficiency of our approach.

</details>


### [95] [CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding](https://arxiv.org/abs/2508.00378)
*Shixin Yi,Lin Shang*

Main category: cs.AI

TL;DR: CoRGI框架通过视觉验证改进视觉语言模型的多步推理，减少幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 解决Chain-of-Thought提示在视觉语言模型中产生的缺乏视觉依据的解释问题。

Method: 提出CoRGI框架，分三阶段：生成文本推理链、提取视觉证据、综合生成验证答案。

Result: 在VCR基准测试中提升Qwen-2.5VL和LLaVA-1.6的性能，生成更事实性和有帮助的解释。

Conclusion: 视觉证据对多模态推理的稳健性至关重要，CoRGI框架有效减少幻觉。

Abstract: Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in
vision-language models (VLMs), but it often produces explanations that are
linguistically fluent yet lack grounding in visual content. We observe that
such hallucinations arise in part from the absence of an explicit verification
mechanism during multi-step reasoning. To address this, we propose
\textbf{CoRGI}(\textbf{C}hain \textbf{o}f \textbf{R}easoning with
\textbf{G}rounded \textbf{I}nsights), a modular framework that introduces
visual verification into the reasoning process. CoRGI follows a three-stage
pipeline: it first generates a textual reasoning chain, then extracts
supporting visual evidence for each reasoning step via a dedicated module
(VEVM), and finally synthesizes the textual rationale with visual evidence to
generate a grounded, verified answer. The framework can be integrated with
existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR
benchmark and find that it improves reasoning performance on two representative
open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm
the contribution of each step in the verification module, and human evaluations
suggest that CoRGI leads to more factual and helpful explanations. We also
examine alternative designs for the visual verification step and discuss
potential limitations of post-hoc verification frameworks. These findings
highlight the importance of grounding intermediate reasoning steps in visual
evidence to enhance the robustness of multimodal reasoning.

</details>


### [96] [Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation](https://arxiv.org/abs/2508.00401)
*Riddhi J. Pitliya,Ozan Catal,Toon Van de Maele,Corrado Pezzato,Tim Verbelen*

Main category: cs.AI

TL;DR: 提出了一种基于心智理论（ToM）的多智能体协作新方法，通过主动推理实现，无需任务特定的共享生成模型或显式通信。


<details>
  <summary>Details</summary>
Motivation: 探索如何在多智能体协作中利用ToM，使智能体能够推理他人的信念和目标，从而更高效地合作。

Method: 扩展了基于推理树的规划算法，通过递归推理探索联合策略空间，智能体维护自身和他人信念及目标的独立表示。

Result: 在碰撞避免和觅食任务中，ToM智能体表现优于非ToM智能体，能减少碰撞和冗余努力。

Conclusion: 该方法不仅推动了人工智能的实际应用，还为ToM的计算机制提供了新见解。

Abstract: We present a novel approach to multi-agent cooperation by implementing theory
of mind (ToM) within active inference. ToM - the ability to understand that
others can have differing knowledge and goals - enables agents to reason about
others' beliefs while planning their own actions. Unlike previous active
inference approaches to multi-agent cooperation, our method neither relies on
task-specific shared generative models nor requires explicit communication,
while being generalisable. In our framework, the ToM-equipped agent maintains
distinct representations of its own and others' beliefs and goals. We extend
the sophisticated inference tree-based planning algorithm to systematically
explore joint policy spaces through recursive reasoning. Our approach is
evaluated through collision avoidance and foraging task simulations. Results
demonstrate that ToM-equipped agents cooperate better compared to non-ToM
counterparts by being able to avoid collisions and reduce redundant efforts.
Crucially, ToM agents accomplish this by inferring others' beliefs solely from
observable behaviour. This work advances practical applications in artificial
intelligence while providing computational insights into ToM.

</details>


### [97] [Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training](https://arxiv.org/abs/2508.00414)
*Tianqing Fang,Zhisong Zhang,Xiaoyang Wang,Rui Wang,Can Qin,Yuxuan Wan,Jun-Yu Ma,Ce Zhang,Jiaqi Chen,Xiyun Li,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: Cognitive Kernel-Pro 是一个完全开源且免费的 AI 代理框架，旨在推动高级 AI 代理的开发和评估。


<details>
  <summary>Details</summary>
Motivation: 当前 AI 代理系统多为闭源或依赖付费 API，限制了研究的可访问性和可复现性。

Method: 通过构建高质量的训练数据（查询、轨迹和可验证答案），并探索代理测试时的反思和投票策略。

Result: 在 GAIA 基准测试中，Cognitive Kernel-Pro 取得了开源代理中的最佳性能，8B 参数模型超越了 WebDancer 和 WebSailor。

Conclusion: Cognitive Kernel-Pro 为高性能、可访问的 AI 代理设定了新标准。

Abstract: General AI Agents are increasingly recognized as foundational frameworks for
the next generation of artificial intelligence, enabling complex reasoning, web
interaction, coding, and autonomous research capabilities. However, current
agent systems are either closed-source or heavily reliant on a variety of paid
APIs and proprietary tools, limiting accessibility and reproducibility for the
research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a
fully open-source and (to the maximum extent) free multi-module agent framework
designed to democratize the development and evaluation of advanced AI agents.
Within Cognitive Kernel-Pro, we systematically investigate the curation of
high-quality training data for Agent Foundation Models, focusing on the
construction of queries, trajectories, and verifiable answers across four key
domains: web, file, code, and general reasoning. Furthermore, we explore novel
strategies for agent test-time reflection and voting to enhance agent
robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving
state-of-the-art results among open-source and free agents. Notably, our
8B-parameter open-source model surpasses previous leading systems such as
WebDancer and WebSailor, establishing a new performance standard for
accessible, high-capability AI agents. Code is available at
https://github.com/Tencent/CognitiveKernel-Pro

</details>


### [98] [Thinking Machines: Mathematical Reasoning in the Age of LLMs](https://arxiv.org/abs/2508.00459)
*Andrea Asperti,Alberto Naibo,Claudio Sacerdoti Coen*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）在数学领域的应用，尤其是形式化数学证明中的挑战，分析了其与代码合成的差异，并提出了三个核心问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在编程和符号任务中表现出色，但在形式化数学证明中进展缓慢，这引发了对LLMs推理方式、监督方法及其内部状态表示的疑问。

Method: 文章综述了该领域的最新模型和基准，重点探讨了形式化与非形式化数学的训练权衡、证明生成脆弱性的原因，以及LLMs是否真正表示逻辑状态。

Result: 研究发现，LLMs在形式化数学证明中的表现不如代码合成，揭示了其推理和状态跟踪的局限性。

Conclusion: 文章旨在明确当前技术的边界，并提出可能的扩展方向，而非划定硬性界限。

Abstract: Large Language Models (LLMs) have shown remarkable abilities in structured
reasoning and symbolic tasks, with coding emerging as a particular area of
strength. This success has sparked growing interest in applying LLMs to
mathematics, both in informal problem-solving and formal theorem proving.
However, progress in formal mathematics has proven to be significantly more
difficult, despite surface-level similarities between programming and proof
construction. This discrepancy raises important questions about how LLMs
``reason'', how they are supervised, and whether they internally track a notion
of computational or deductive state. In this article, we address the
state-of-the-art of the discipline, focusing on recent models and benchmarks,
and explore three central issues at the intersection of machine learning and
mathematical cognition: (i) the trade-offs between formal and informal
mathematics as training domains; (ii) the deeper reasons why proof generation
remains more brittle than code synthesis; (iii) and the question of whether
LLMs represent, or merely mimic, a notion of evolving logical state. Our goal
is not to draw hard boundaries, but to identify where the current limits lie,
and how they might be extended.

</details>


### [99] [Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking](https://arxiv.org/abs/2508.00500)
*Haoyu Wang,Chris M. Poskitt,Jun Sun,Jiali Wei*

Main category: cs.AI

TL;DR: Pro2Guard是一个基于概率可达性分析的主动运行时安全框架，用于预测和预防LLM代理的不安全行为。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的安全系统（如AgentSpec）缺乏预见性，难以应对长期依赖和分布变化，因此需要一种更主动的安全框架。

Method: Pro2Guard将代理行为抽象为符号状态，并从执行轨迹中学习离散时间马尔可夫链（DTMC），在运行时通过估计到达不安全状态的概率来预测风险。

Result: 在家庭代理和自动驾驶场景中，Pro2Guard分别实现了93.6%和100%的不安全行为预测，并能提前干预。

Conclusion: Pro2Guard通过主动预测和干预，显著提升了LLM代理的安全性，同时保持了任务完成率。

Abstract: Large Language Model (LLM) agents exhibit powerful autonomous capabilities
across domains such as robotics, virtual assistants, and web automation.
However, their stochastic behavior introduces significant safety risks that are
difficult to anticipate. Existing rule-based enforcement systems, such as
AgentSpec, focus on developing reactive safety rules, which typically respond
only when unsafe behavior is imminent or has already occurred. These systems
lack foresight and struggle with long-horizon dependencies and distribution
shifts. To address these limitations, we propose Pro2Guard, a proactive runtime
enforcement framework grounded in probabilistic reachability analysis.
Pro2Guard abstracts agent behaviors into symbolic states and learns a
Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it
anticipates future risks by estimating the probability of reaching unsafe
states, triggering interventions before violations occur when the predicted
risk exceeds a user-defined threshold. By incorporating semantic validity
checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability
while approximating the underlying ground-truth model. We evaluate Pro2Guard
extensively across two safety-critical domains: embodied household agents and
autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early
on up to 93.6% of unsafe tasks using low thresholds, while configurable modes
(e.g., reflect) allow balancing safety with task success, maintaining up to
80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%
prediction of traffic law violations and collisions, anticipating risks up to
38.66 seconds ahead.

</details>


### [100] [MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models](https://arxiv.org/abs/2508.00576)
*Zhanliang Wang,Kai Wang*

Main category: cs.AI

TL;DR: MultiSHAP是一个模型无关的可解释性框架，用于量化多模态AI模型中视觉和文本元素之间的协同效应，适用于开源和闭源模型。


<details>
  <summary>Details</summary>
Motivation: 多模态AI模型的'黑盒'特性在高风险应用中限制了其部署，现有解释方法无法精确量化模态间的协同效应。

Method: 利用Shapley Interaction Index，将多模态预测归因于视觉和文本元素的成对交互，提供实例级和数据集级解释。

Result: 实验证明MultiSHAP能准确捕捉跨模态推理机制，并在实际案例中展示其实用性。

Conclusion: MultiSHAP为解释复杂多模态AI模型提供了通用解决方案，并可扩展至超过两种模态。

Abstract: Multimodal AI models have achieved impressive performance in tasks that
require integrating information from multiple modalities, such as vision and
language. However, their "black-box" nature poses a major barrier to deployment
in high-stakes applications where interpretability and trustworthiness are
essential. How to explain cross-modal interactions in multimodal AI models
remains a major challenge. While existing model explanation methods, such as
attention map and Grad-CAM, offer coarse insights into cross-modal
relationships, they cannot precisely quantify the synergistic effects between
modalities, and are limited to open-source models with accessible internal
weights. Here we introduce MultiSHAP, a model-agnostic interpretability
framework that leverages the Shapley Interaction Index to attribute multimodal
predictions to pairwise interactions between fine-grained visual and textual
elements (such as image patches and text tokens), while being applicable to
both open- and closed-source models. Our approach provides: (1) instance-level
explanations that reveal synergistic and suppressive cross-modal effects for
individual samples - "why the model makes a specific prediction on this input",
and (2) dataset-level explanation that uncovers generalizable interaction
patterns across samples - "how the model integrates information across
modalities". Experiments on public multimodal benchmarks confirm that MultiSHAP
faithfully captures cross-modal reasoning mechanisms, while real-world case
studies demonstrate its practical utility. Our framework is extensible beyond
two modalities, offering a general solution for interpreting complex multimodal
AI models.

</details>


### [101] [From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation](https://arxiv.org/abs/2508.00581)
*Ruiqing Ding,Qianfang Sun,Yongkang Leng,Hui Yin,Xiaojian Li*

Main category: cs.AI

TL;DR: 提出了一种多阶段LLM驱动框架，用于从复杂电子病历生成全面的预咨询问卷，解决了直接LLM方法在信息完整性、逻辑顺序和疾病级合成方面的不足。


<details>
  <summary>Details</summary>
Motivation: 预咨询是医疗保健的关键环节，但从复杂电子病历生成问卷具有挑战性，直接LLM方法在信息完整性和逻辑性上表现不佳。

Method: 分三阶段：1）提取原子断言；2）构建个人因果网络并合成疾病知识；3）生成个性化及标准化问卷。

Result: 在真实电子病历数据集上验证，方法在信息覆盖、诊断相关性、可理解性和生成时间上表现优越。

Conclusion: 该框架通过构建显式临床知识，显著提升了预咨询问卷的质量和实用性。

Abstract: Pre-consultation is a critical component of effective healthcare delivery.
However, generating comprehensive pre-consultation questionnaires from complex,
voluminous Electronic Medical Records (EMRs) is a challenging task. Direct
Large Language Model (LLM) approaches face difficulties in this task,
particularly regarding information completeness, logical order, and
disease-level synthesis. To address this issue, we propose a novel multi-stage
LLM-driven framework: Stage 1 extracts atomic assertions (key facts with
timing) from EMRs; Stage 2 constructs personal causal networks and synthesizes
disease knowledge by clustering representative networks from an EMR corpus;
Stage 3 generates tailored personal and standardized disease-specific
questionnaires based on these structured representations. This framework
overcomes limitations of direct methods by building explicit clinical
knowledge. Evaluated on a real-world EMR dataset and validated by clinical
experts, our method demonstrates superior performance in information coverage,
diagnostic relevance, understandability, and generation time, highlighting its
practical potential to enhance patient information collection.

</details>


### [102] [Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies](https://arxiv.org/abs/2508.00658)
*Chakattrai Sookkongwaree,Tattep Lakmuang,Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 该论文提出了一种多频带可变滞后格兰杰因果关系（MB-VLGC）框架，解决了传统方法无法处理频率依赖性因果延迟的问题。


<details>
  <summary>Details</summary>
Motivation: 传统格兰杰因果关系方法假设固定的时间滞后，而可变滞后方法（VLGC）虽然解决了时间延迟变化的问题，但忽略了因果延迟可能在不同频带上变化的情况。

Method: 论文提出了MB-VLGC框架，通过显式建模频率依赖性因果延迟，扩展了传统VLGC方法，并提供了理论证明和高效推理流程。

Result: 实验表明，MB-VLGC在合成和真实数据集上显著优于现有方法。

Conclusion: MB-VLGC框架具有广泛的适用性，适用于任何类型的时间序列数据。

Abstract: Understanding causal relationships in time series is fundamental to many
domains, including neuroscience, economics, and behavioral science. Granger
causality is one of the well-known techniques for inferring causality in time
series. Typically, Granger causality frameworks have a strong fix-lag
assumption between cause and effect, which is often unrealistic in complex
systems. While recent work on variable-lag Granger causality (VLGC) addresses
this limitation by allowing a cause to influence an effect with different time
lags at each time point, it fails to account for the fact that causal
interactions may vary not only in time delay but also across frequency bands.
For example, in brain signals, alpha-band activity may influence another region
with a shorter delay than slower delta-band oscillations. In this work, we
formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a
novel framework that generalizes traditional VLGC by explicitly modeling
frequency-dependent causal delays. We provide a formal definition of MB-VLGC,
demonstrate its theoretical soundness, and propose an efficient inference
pipeline. Extensive experiments across multiple domains demonstrate that our
framework significantly outperforms existing methods on both synthetic and
real-world datasets, confirming its broad applicability to any type of time
series data. Code and datasets are publicly available.

</details>


### [103] [Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI](https://arxiv.org/abs/2508.00665)
*Maryam Mosleh,Marie Devlin,Ellis Solaiman*

Main category: cs.AI

TL;DR: 提出了一种结合传统可解释AI技术与生成式AI模型的混合框架，旨在为教育领域提供多模态、个性化的解释，以增强透明度和用户体验。


<details>
  <summary>Details</summary>
Motivation: 当前自适应学习系统缺乏透明度，且现有可解释AI技术忽视用户角色和理解能力，需要一种更动态、用户中心的方法。

Method: 整合传统XAI技术与生成式AI模型，结合用户个性化需求，设计动态解释框架。

Result: 重新定义可解释性为动态沟通过程，提出框架设计并探讨教育中XAI的局限性及研究方向。

Conclusion: 目标是推动可解释AI在增强透明度的同时支持以用户为中心的体验。

Abstract: Artificial intelligence-driven adaptive learning systems are reshaping
education through data-driven adaptation of learning experiences. Yet many of
these systems lack transparency, offering limited insight into how decisions
are made. Most explainable AI (XAI) techniques focus on technical outputs but
neglect user roles and comprehension. This paper proposes a hybrid framework
that integrates traditional XAI techniques with generative AI models and user
personalisation to generate multimodal, personalised explanations tailored to
user needs. We redefine explainability as a dynamic communication process
tailored to user roles and learning goals. We outline the framework's design,
key XAI limitations in education, and research directions on accuracy,
fairness, and personalisation. Our aim is to move towards explainable AI that
enhances transparency while supporting user-centred experiences.

</details>


### [104] [Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations](https://arxiv.org/abs/2508.00674)
*Banan Alkhateeb,Ellis Solaiman*

Main category: cs.AI

TL;DR: 论文提出了一种用户分段的上下文感知解释系统，通过可视化方法提升社交媒体推荐的可解释性，适应不同用户需求。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体推荐系统的解释性不足，缺乏与用户特定需求的匹配，导致用户对推荐的理解和信任度下降。

Method: 设计了一个视觉解释系统，提供多样化的解释方法，包括技术详细版和简化版，适应不同用户（如专家和普通用户）。

Result: 系统首次在同一流程中联合调整解释风格（视觉与数字）和粒度（专家与普通），并通过30名X用户的公开试点验证其效果。

Conclusion: 提出的框架能有效提升用户对推荐的理解和信任，为社交媒体推荐系统的可解释性提供了新方向。

Abstract: Social media platforms today strive to improve user experience through AI
recommendations, yet the value of such recommendations vanishes as users do not
understand the reasons behind them. This issue arises because explainability in
social media is general and lacks alignment with user-specific needs. In this
vision paper, we outline a user-segmented and context-aware explanation layer
by proposing a visual explanation system with diverse explanation methods. The
proposed system is framed by the variety of user needs and contexts, showing
explanations in different visualized forms, including a technically detailed
version for AI experts and a simplified one for lay users. Our framework is the
first to jointly adapt explanation style (visual vs. numeric) and granularity
(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will
validate its impact on decision-making and trust.

</details>


### [105] [Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics](https://arxiv.org/abs/2508.00784)
*Tom Or,Omri Azencot*

Main category: cs.AI

TL;DR: 论文提出了一种基于大型预训练多模态模型的通用生成内容检测方法，通过利用其潜在编码区分真实与虚假信息，实现了跨模态的高效检测。


<details>
  <summary>Details</summary>
Motivation: 生成模型在多个领域表现出色，但被恶意用于传播虚假信息，现有检测工具泛化能力有限，亟需通用且稳定的检测方法。

Method: 利用大型预训练多模态模型的潜在编码，训练线性分类器进行生成内容检测，适用于音频和图像等多种模态。

Result: 该方法在跨模态检测中表现优异，计算高效且适用于少样本场景，性能超越或匹配现有基线方法。

Conclusion: 通过预训练多模态模型的特征提取能力，实现了高效、通用的生成内容检测，为虚假信息识别提供了新思路。

Abstract: Generative models achieve remarkable results in multiple data domains,
including images and texts, among other examples. Unfortunately, malicious
users exploit synthetic media for spreading misinformation and disseminating
deepfakes. Consequently, the need for robust and stable fake detectors is
pressing, especially when new generative models appear everyday. While the
majority of existing work train classifiers that discriminate between real and
fake information, such tools typically generalize only within the same family
of generators and data modalities, yielding poor results on other generative
classes and data domains. Towards a universal classifier, we propose the use of
large pre-trained multi-modal models for the detection of generative content.
Effectively, we show that the latent code of these models naturally captures
information discriminating real from fake. Building on this observation, we
demonstrate that linear classifiers trained on these features can achieve
state-of-the-art results across various modalities, while remaining
computationally efficient, fast to train, and effective even in few-shot
settings. Our work primarily focuses on fake detection in audio and images,
achieving performance that surpasses or matches that of strong baseline
methods.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [106] [From Dynamic Programs to Greedy Algorithms](https://arxiv.org/abs/2508.00776)
*Dieter van Melkebeek*

Main category: cs.DS

TL;DR: 论文展示了如何从动态规划的通用解法中简单推导出经典贪心算法，适用于区间调度、背包问题和最短路径问题。


<details>
  <summary>Details</summary>
Motivation: 为本科算法课程提供一种替代方法，通过动态规划的扩展和单调性推导贪心算法的正确性。

Method: 通过扩展Bellman方程并利用单调性，确定在特定限制下哪些项能产生最优解。

Result: 成功推导出区间调度、背包问题和最短路径问题的贪心算法。

Conclusion: 该方法为理解贪心算法的正确性提供了新视角，尤其在区间调度中解释了排序规则的变化。

Abstract: We show for several computational problems how classical greedy algorithms
for special cases can be derived in a simple way from dynamic programs for the
general case: interval scheduling (restricted to unit weights), knapsack
(restricted to unit values), and shortest paths (restricted to nonnegative edge
lengths). Conceptually, we repeatedly expand the Bellman equations underlying
the dynamic program and use straightforward monotonicity properties to figure
out which terms yield the optimal value under the respective restrictions. The
approach offers an alternative for developing these greedy algorithms in
undergraduate algorithms courses and/or for arguing their correctness. In the
setting of interval scheduling, it elucidates the change in order from earliest
start time first for the memoized dynamic program to earliest finish time first
for the greedy algorithm.

</details>
