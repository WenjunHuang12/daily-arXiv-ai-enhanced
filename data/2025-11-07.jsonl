{"id": "2511.03810", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.03810", "abs": "https://arxiv.org/abs/2511.03810", "authors": ["Egor Gagushin", "Marios Mertzanidis", "Alexandros Psomas"], "title": "On the Existence of Fair Allocations for Goods and Chores under Dissimilar Preferences", "comment": null, "summary": "We study the fundamental problem of fairly allocating a multiset\n$\\mathcal{M}$ of $t$ types of indivisible items among $d$ groups of agents,\nwhere all agents within a group have identical additive valuations. Gorantla et\nal. [GMV23] showed that for every such instance, there exists a finite number\n$\\mu$ such that, if each item type appears at least $\\mu$ times, an envy-free\nallocation exists. Their proof is non-constructive and only provides explicit\nupper bounds on $\\mu$ for the cases of two groups ($d=2$) or two item types\n($t=2$).\n  In this work, we resolve one of the main open questions posed by Gorantla et\nal. [GMV23] by deriving explicit upper bounds on $\\mu$ that hold for arbitrary\nnumbers of groups and item types. We introduce a significantly simpler, yet\npowerful technique that not only yields constructive guarantees for indivisible\ngoods but also extends naturally to chores and continuous domains, leading to\nnew results in related fair division settings such as cake cutting."}
{"id": "2511.03968", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.03968", "abs": "https://arxiv.org/abs/2511.03968", "authors": ["Ioannis Anagnostides", "Maria-Florina Balcan", "Kiriaki Fragkia", "Tuomas Sandholm", "Emanuel Tewolde", "Brian Hu Zhang"], "title": "The Complexity of Equilibrium Refinements in Potential Games", "comment": "The abstract has been abridged due to arXiv length constraints", "summary": "The complexity of computing equilibrium refinements has been at the forefront\nof algorithmic game theory research, but it has remained open in the seminal\nclass of potential games; we close this fundamental gap in this paper.\n  We first establish that computing a pure-strategy perfect equilibrium is\n$\\mathsf{PLS}$-complete under different game representations -- including\nextensive-form games and general polytope games, thereby being polynomial-time\nequivalent to pure Nash equilibria. For normal-form proper equilibria, our main\nresult is that a perturbed (proper) best response can be computed efficiently\nin extensive-form games. As a byproduct, we establish\n$\\mathsf{FIXP}_a$-completeness of normal-form proper equilibria in\nextensive-form games, resolving a long-standing open problem. In stark\ncontrast, we show that computing a normal-form proper equilibrium in polytope\npotential games is both $\\mathsf{NP}$-hard and $\\mathsf{coNP}$-hard.\n  We next turn to more structured classes of games, namely symmetric network\ncongestion and symmetric matroid congestion games. For both classes, we show\nthat a perfect pure-strategy equilibrium can be computed in polynomial time,\nstrengthening the existing results for pure Nash equilibria. On the other hand,\nwe establish that, for a certain class of potential games, there is an\nexponential separation in the length of the best-response path between perfect\nand Nash equilibria.\n  Finally, for mixed strategies, we prove that computing a point geometrically\nnear a perfect equilibrium requires a doubly exponentially small perturbation\neven in $3$-player potential games in normal form. On the flip side, in the\nspecial case of polymatrix potential games, we show that equilibrium\nrefinements are amenable to perturbed gradient descent dynamics, thereby\nbelonging to the complexity class $\\mathsf{CLS}$."}
{"id": "2511.04465", "categories": ["cs.GT", "cs.AI", "cs.LG", "econ.TH"], "pdf": "https://arxiv.org/pdf/2511.04465", "abs": "https://arxiv.org/abs/2511.04465", "authors": ["Abheek Ghosh", "Tzeh Yuan Neoh", "Nicholas Teh", "Giannis Tyrovolas"], "title": "Fraud-Proof Revenue Division on Subscription Platforms", "comment": "Appears in the 42nd International Conference on Machine Learning\n  (ICML), 2025", "summary": "We study a model of subscription-based platforms where users pay a fixed fee\nfor unlimited access to content, and creators receive a share of the revenue.\nExisting approaches to detecting fraud predominantly rely on machine learning\nmethods, engaging in an ongoing arms race with bad actors. We explore revenue\ndivision mechanisms that inherently disincentivize manipulation. We formalize\nthree types of manipulation-resistance axioms and examine which existing rules\nsatisfy these. We show that a mechanism widely used by streaming platforms, not\nonly fails to prevent fraud, but also makes detecting manipulation\ncomputationally intractable. We also introduce a novel rule, ScaledUserProp,\nthat satisfies all three manipulation-resistance axioms. Finally, experiments\nwith both real-world and synthetic streaming data support ScaledUserProp as a\nfairer alternative compared to existing rules."}
{"id": "2511.04572", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2511.04572", "abs": "https://arxiv.org/abs/2511.04572", "authors": ["Yixin Tao", "Weiqiang Zheng"], "title": "Fisher Meets Lindahl: A Unified Duality Framework for Market Equilibrium", "comment": "51 pages. Abstract shortened to meet arXiv's requirement", "summary": "The Fisher market equilibrium for private goods and the Lindahl equilibrium\nfor public goods are classic and fundamental solution concepts for market\nequilibria. While Fisher market equilibria have been well-studied, the\ntheoretical foundations for Lindahl equilibria remain substantially\nunderdeveloped.\n  In this work, we propose a unified duality framework for market equilibria.\nWe show that Lindahl equilibria of a public goods market correspond to Fisher\nmarket equilibria in a dual Fisher market with dual utilities, and vice versa.\nThe dual utility is based on the indirect utility, and the correspondence\nbetween the two equilibria works by exchanging the roles of allocations and\nprices.\n  Using the duality framework, we address the gaps concerning the computation\nand dynamics for Lindahl equilibria and obtain new insights and developments\nfor Fisher market equilibria. First, we leverage this duality to analyze\nwelfare properties of Lindahl equilibria. For concave homogeneous utilities, we\nprove that a Lindahl equilibrium maximizes Nash Social Welfare (NSW). For\nconcave non-homogeneous utilities, we show that a Lindahl equilibrium achieves\n$(1/e)^{1/e}$ approximation to the optimal NSW, and the approximation ratio is\ntight. Second, we apply the duality framework to market dynamics, including\nproportional response dynamics (PRD) and t\\^atonnement. We obtain new market\ndynamics for the Lindahl equilibria from market dynamics in the dual Fisher\nmarket. We also use duality to extend PRD to markets with total complements\nutilities, the dual class of gross substitutes utilities. Finally, we apply the\nduality framework to markets with chores. We propose a program for private\nchores for general convex homogeneous disutilities that avoids the \"poles\"\nissue, whose KKT points correspond to Fisher market equilibria. We also\ninitiate the study of the Lindahl equilibrium for public chores."}
{"id": "2511.04247", "categories": ["cs.MM", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.04247", "abs": "https://arxiv.org/abs/2511.04247", "authors": ["Allie Tran", "Luca Rossetto"], "title": "On the Brittleness of CLIP Text Encoders", "comment": "Accepted for publication at MMM'26", "summary": "Multimodal co-embedding models, especially CLIP, have advanced the state of\nthe art in zero-shot classification and multimedia information retrieval in\nrecent years by aligning images and text in a shared representation space.\nHowever, such modals trained on a contrastive alignment can lack stability\ntowards small input perturbations. Especially when dealing with manually\nexpressed queries, minor variations in the query can cause large differences in\nthe ranking of the best-matching results. In this paper, we present a\nsystematic analysis of the effect of multiple classes of non-semantic query\nperturbations in an multimedia information retrieval scenario. We evaluate a\ndiverse set of lexical, syntactic, and semantic perturbations across multiple\nCLIP variants using the TRECVID Ad-Hoc Video Search queries and the V3C1 video\ncollection. Across models, we find that syntactic and semantic perturbations\ndrive the largest instabilities, while brittleness is concentrated in trivial\nsurface edits such as punctuation and case. Our results highlight robustness as\na critical dimension for evaluating vision-language models beyond benchmark\naccuracy."}
{"id": "2511.04080", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.04080", "abs": "https://arxiv.org/abs/2511.04080", "authors": ["Xiaolu Chen", "Yong Liao"], "title": "Caption Injection for Optimization in Generative Search Engine", "comment": null, "summary": "Generative Search Engines (GSEs) leverage Retrieval-Augmented Generation\n(RAG) techniques and Large Language Models (LLMs) to integrate multi-source\ninformation and provide users with accurate and comprehensive responses. Unlike\ntraditional search engines that present results in ranked lists, GSEs shift\nusers' attention from sequential browsing to content-driven subjective\nperception, driving a paradigm shift in information retrieval. In this context,\nenhancing the subjective visibility of content through Generative Search Engine\nOptimization (G-SEO) methods has emerged as a new research focus. With the\nrapid advancement of Multimodal Retrieval-Augmented Generation (MRAG)\ntechniques, GSEs can now efficiently integrate text, images, audio, and video,\nproducing richer responses that better satisfy complex information needs.\nExisting G-SEO methods, however, remain limited to text-based optimization and\nfail to fully exploit multimodal data. To address this gap, we propose Caption\nInjection, the first multimodal G-SEO approach, which extracts captions from\nimages and injects them into textual content, integrating visual semantics to\nenhance the subjective visibility of content in generative search scenarios. We\nsystematically evaluate Caption Injection on MRAMG, a benchmark for MRAG, under\nboth unimodal and multimodal settings. Experimental results show that Caption\nInjection significantly outperforms text-only G-SEO baselines under the G-Eval\nmetric, demonstrating the necessity and effectiveness of multimodal integration\nin G-SEO to improve user-perceived content visibility."}
{"id": "2511.04140", "categories": ["cs.DB", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.04140", "abs": "https://arxiv.org/abs/2511.04140", "authors": ["Zheng Li", "Weiyan Wang", "Ruiyuan Li", "Chao Chen", "Xianlei Long", "Linjiang Zheng", "Quanqing Xu", "Chuanhui Yang"], "title": "GPU-Based Floating-point Adaptive Lossless Compression", "comment": null, "summary": "Domains such as IoT (Internet of Things) and HPC (High Performance Computing)\ngenerate a torrential influx of floating-point time-series data. Compressing\nthese data while preserving their absolute fidelity is critical, and leveraging\nthe massive parallelism of modern GPUs offers a path to unprecedented\nthroughput. Nevertheless, designing such a high-performance GPU-based lossless\ncompressor faces three key challenges: 1) heterogeneous data movement\nbottlenecks, 2) precision-preserving conversion complexity, and 3)\nanomaly-induced sparsity degradation. To address these challenges, this paper\nproposes Falcon, a GPU-based Floating-point Adaptive Lossless COmpressioN\nframework. Specifically, Falcon first introduces a lightweight asynchronous\npipeline, which hides the I/O latency during the data transmission between the\nCPU and GPU. Then, we propose an accurate and fast float-to-integer\ntransformation method with theoretical guarantees, which eliminates the errors\ncaused by floating-point arithmetic. Moreover, we devise an adaptive sparse\nbit-plane lossless encoding strategy, which reduces the sparsity caused by\noutliers. Extensive experiments on 12 diverse datasets show that our\ncompression ratio improves by 9.1% over the most advanced CPU-based method,\nwith compression throughput 2.43X higher and decompression throughput 2.4X\nhigher than the fastest GPU-based competitors, respectively."}
{"id": "2511.03820", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.03820", "abs": "https://arxiv.org/abs/2511.03820", "authors": ["Zhiguo Ding", "Robert Schober", "H. V. Poor"], "title": "Environment Division Multiple Access (EDMA): A Feasibility Study via Pinching Antennas", "comment": null, "summary": "This paper exploits the dynamic features of wireless propagation environments\nas the basis for a new multiple access technique, termed environment division\nmultiple access (EDMA). In particular, with the proposed\npinching-antenna-assisted EDMA, the multi-user propagation environment is\nintelligently reconfigured to improve signal strength at intended receivers and\nsimultaneously suppress multiple-access interference, without requiring complex\nsignal processing, e.g., precoding, beamforming, or multi-user detection. The\nkey to creating a favorable propagation environment is to utilize the\ncapability of pinching antennas to reconfigure line-of-sight (LoS) links, e.g.,\npinching antennas are placed at specific locations, such that interference\nlinks are blocked on purpose. Based on a straightforward choice of\npinching-antenna locations, the ergodic sum-rate gain of EDMA over conventional\nmultiple access and the probability that EDMA achieves a larger instantaneous\nsum rate than the considered benchmarking scheme are derived in closed form.\nThe obtained analytical results demonstrate the significant potential of EDMA\nfor supporting multi-user communications. Furthermore, pinching antenna\nlocation optimization is also investigated, since the locations of pinching\nantennas are critical for reconfiguring LoS links and large-scale path losses.\nTwo low-complexity algorithms are developed for uplink and downlink\ntransmission, respectively, and simulation results are provided to show their\noptimality in comparison to exhaustive searches."}
{"id": "2511.03752", "categories": ["cs.DS", "cs.CC", "cs.FL", "cs.GT", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.03752", "abs": "https://arxiv.org/abs/2511.03752", "authors": ["Rick van der Heijden"], "title": "Attractors Is All You Need: Parity Games In Polynomial Time", "comment": null, "summary": "This paper provides a polynomial-time algorithm for solving parity games that\nruns in $\\mathcal{O}(n^{2}\\cdot(n + m))$ time-ending a search that has taken\ndecades. Unlike previous attractor-based algorithms, the presented algorithm\nonly removes regions with a determined winner. The paper introduces a new type\nof attractor that can guarantee finding the minimal dominion of a parity game.\nThe attractor runs in polynomial time and can peel the graph empty."}
{"id": "2511.03752", "categories": ["cs.DS", "cs.CC", "cs.FL", "cs.GT", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.03752", "abs": "https://arxiv.org/abs/2511.03752", "authors": ["Rick van der Heijden"], "title": "Attractors Is All You Need: Parity Games In Polynomial Time", "comment": null, "summary": "This paper provides a polynomial-time algorithm for solving parity games that\nruns in $\\mathcal{O}(n^{2}\\cdot(n + m))$ time-ending a search that has taken\ndecades. Unlike previous attractor-based algorithms, the presented algorithm\nonly removes regions with a determined winner. The paper introduces a new type\nof attractor that can guarantee finding the minimal dominion of a parity game.\nThe attractor runs in polynomial time and can peel the graph empty."}
{"id": "2511.04087", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.04087", "abs": "https://arxiv.org/abs/2511.04087", "authors": ["Ge Zhang", "Rohan Deepak Ajwani", "Tony Zheng", "Hongjian Gu", "Yaochen Hu", "Wei Guo", "Mark Coates", "Yingxue Zhang"], "title": "E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for E-Commerce", "comment": null, "summary": "Finding relevant products given a user query plays a pivotal role in an\ne-commerce platform, as it can spark shopping behaviors and result in revenue\ngains. The challenge lies in accurately predicting the correlation between\nqueries and products. Recently, mining the cross-features between queries and\nproducts based on the commonsense reasoning capacity of Large Language Models\n(LLMs) has shown promising performance. However, such methods suffer from high\ncosts due to intensive real-time LLM inference during serving, as well as human\nannotations and potential Supervised Fine Tuning (SFT). To boost efficiency\nwhile leveraging the commonsense reasoning capacity of LLMs for various\ne-commerce tasks, we propose the Efficient Commonsense-Augmented Recommendation\nEnhancer (E-CARE). During inference, models augmented with E-CARE can access\ncommonsense reasoning with only a single LLM forward pass per query by\nutilizing a commonsense reasoning factor graph that encodes most of the\nreasoning schema from powerful LLMs. The experiments on 2 downstream tasks show\nan improvement of up to 12.1% on precision@5."}
{"id": "2511.04148", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.04148", "abs": "https://arxiv.org/abs/2511.04148", "authors": ["Xiaobo Zhao", "Daniel E. Lucani"], "title": "EntroGD: Efficient Compression and Accurate Direct Analytics on Compressed Data", "comment": "6 pages, 7 figures", "summary": "Generalized Deduplication (GD) enables lossless compression with direct\nanalytics on compressed data by dividing data into \\emph{bases} and\n\\emph{deviations} and performing dictionary encoding on the former. However, GD\nalgorithms face scalability challenges for high-dimensional data. For example,\nthe GreedyGD algorithm relies on an iterative bit-selection process across\n$d$-dimensional data resulting in $O(nd^2)$ complexity for $n$ data rows to\nselect bits to be used as bases and deviations. Although the $n$ data rows can\nbe reduced during training at the expense of performance, highly dimensional\ndata still experiences a marked loss in performance. This paper introduces\nEntroGD, an entropy-guided GD framework that reduces complexity of the\nbit-selection algorithm to $O(nd)$. EntroGD operates considers a two-step\nprocess. First, it generates condensed samples to preserve analytic fidelity.\nSecond, it applies entropy-guided bit selection to maximize compression\nefficiency. Across 18 datasets of varying types and dimensionalities, EntroGD\nachieves compression performance comparable to GD-based and universal\ncompressors, while reducing configuration time by up to 53.5$\\times$ over\nGreedyGD and accelerating clustering by up to 31.6$\\times$ over the original\ndata with negligible accuracy loss by performing analytics on the condensed\nsamples, which are much fewer than original samples. Thus, EntroGD provides an\nefficient and scalable solution to performing analytics directly on compressed\ndata."}
{"id": "2511.03849", "categories": ["cs.IT", "cs.LG", "math.IT", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2511.03849", "abs": "https://arxiv.org/abs/2511.03849", "authors": ["Phuc Nguyen", "Josiah Couch", "Rahul Bansal", "Alexandra Morgan", "Chris Tam", "Miao Li", "Rima Arnaout", "Ramy Arnaout"], "title": "Which Similarity-Sensitive Entropy?", "comment": "21 pages, 8 figures", "summary": "A canonical step in quantifying a system is to measure its entropy. Shannon\nentropy and other traditional entropy measures capture only the information\nencoded in the frequencies of a system's elements. Recently, Leinster, Cobbold,\nand Reeve (LCR) introduced a method that also captures the rich information\nencoded in the similarities and differences among elements, yielding\nsimilarity-sensitive entropy. More recently, the Vendi score (VS) was\nintroduced as an alternative, raising the question of how LCR and VS compare,\nand which is preferable. Here we address these questions conceptually,\nanalytically, and experimentally, using 53 machine-learning datasets. We show\nthat LCR and VS can differ by orders of magnitude and can capture complementary\ninformation about a system, except in limiting cases. We demonstrate that both\nLCR and VS depend on how similarities are scaled and introduce the concept of\n``half distance'' to parameterize this dependence. We prove that VS provides an\nupper bound on LCR for several values of the R\\'enyi-Hill order parameter and\nconjecture that this bound holds for all values. We conclude that VS is\npreferable only when interpreting elements as linear combinations of a more\nfundamental set of ``ur-elements'' or when the system or dataset possesses a\nquantum-mechanical character. In the broader circumstance where one seeks\nsimply to capture the rich information encoded by similarity, LCR is favored;\nnevertheless, for certain half-distances the two methods can complement each\nother."}
{"id": "2511.03960", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2511.03960", "abs": "https://arxiv.org/abs/2511.03960", "authors": ["Qian Li", "Xin Lyu"], "title": "Multi-Pass Streaming Lower Bounds for Uniformity Testing", "comment": "18 pages", "summary": "We prove multi-pass streaming lower bounds for uniformity testing over a\ndomain of size $2m$. The tester receives a stream of $n$ i.i.d. samples and\nmust distinguish (i) the uniform distribution on $[2m]$ from (ii) a\nPaninski-style planted distribution in which, for each pair $(2i-1,2i)$, the\nprobabilities are biased left or right by $\\epsilon/2m$. We show that any\n$\\ell$-pass streaming algorithm using space $s$ and achieving constant\nadvantage must satisfy the tradeoff $sn\\ell=\\tilde{\\Omega}(m/\\epsilon^2)$. This\nextends the one-pass lower bound of Diakonikolas, Gouleakis, Kane, and Rao\n(2019) to multiple passes.\n  Our proof has two components. First, we develop a hybrid argument, inspired\nby Dinur (2020), that reduces streaming to two-player communication problems.\nThis reduction relies on a new perspective on hardness: we identify the source\nof hardness as uncertainty in the bias directions, rather than the collision\nlocations. Second, we prove a strong lower bound for a basic two-player\ncommunication task, in which Alice and Bob must decide whether two random sign\nvectors $Y^a,Y^b\\in\\{\\pm 1\\}^m$ are independent or identical, yet they cannot\nobserve the signs directly--only noisy local views of each coordinate. Our\ntechniques may be of independent use for other streaming problems with\nstochastic inputs."}
{"id": "2511.04172", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04172", "abs": "https://arxiv.org/abs/2511.04172", "authors": ["Mashrur Rahman", "Mantaqa abedin", "Monowar Zamil Abir", "Faizul Islam Ansari", "Adib Reza", "Farig Yousuf Sadeque", "Niloy Farhan"], "title": "Transforming Mentorship: An AI Powered Chatbot Approach to University Guidance", "comment": "11 pages", "summary": "University students face immense challenges during their undergraduate lives,\noften being deprived of personalized on-demand guidance that mentors fail to\nprovide at scale. Digital tools exist, but there is a serious lack of\ncustomized coaching for newcomers. This paper presents an AI-powered chatbot\nthat will serve as a mentor for the students of BRAC University. The main\ncomponent is a data ingestion pipeline that efficiently processes and updates\ninformation from diverse sources, such as CSV files and university webpages.\nThe chatbot retrieves information through a hybrid approach, combining BM25\nlexical ranking with ChromaDB semantic retrieval, and uses a Large Language\nModel, LLaMA-3.3-70B, to generate conversational responses. The generated text\nwas found to be semantically highly relevant, with a BERTScore of 0.831 and a\nMETEOR score of 0.809. The data pipeline was also very efficient, taking 106.82\nseconds for updates, compared to 368.62 seconds for new data. This chatbot will\nbe able to help students by responding to their queries, helping them to get a\nbetter understanding of university life, and assisting them to plan better\nroutines for their semester in the open-credit university."}
{"id": "2511.04221", "categories": ["cs.IR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.04221", "abs": "https://arxiv.org/abs/2511.04221", "authors": ["Carl Kugblenu", "Petri Vuorimaa"], "title": "Coordination-Free Lane Partitioning for Convergent ANN Search", "comment": "10 pages, 6 figures; arXiv preprint", "summary": "Production vector search systems often fan out each query across parallel\nlanes (threads, replicas, or shards) to meet latency service-level objectives\n(SLOs). In practice, these lanes rediscover the same candidates, so extra\ncompute does not increase coverage. We present a coordination-free lane\npartitioner that turns duplication into complementary work at the same cost and\ndeadline. For each query we (1) build a deterministic candidate pool sized to\nthe total top-k budget, (2) apply a per-query pseudorandom permutation, and (3)\nassign each lane a disjoint slice of positions. Lanes then return different\nresults by construction, with no runtime coordination.\n  At equal cost with four lanes (total candidate budget 64), on SIFT1M (1M SIFT\nfeature vectors) with Hierarchical Navigable Small World graphs (HNSW)\nrecall@10 rises from 0.249 to 0.999 while lane overlap falls from nearly 100%\nto 0%. On MS MARCO (8.8M passages) with HNSW, hit@10 improves from 0.200 to\n0.601 and Mean Reciprocal Rank at 10 (MRR@10) from 0.133 to 0.330. For inverted\nfile (IVF) indexes we see smaller but consistent gains (for example, +11% on MS\nMARCO) by de-duplicating list routing. A microbenchmark shows planner overhead\nof ~37 microseconds per query (mean at the main setting) with linear growth in\nthe number of merged candidates.\n  These results yield a simple operational guideline: size the per-query pool\nto the total budget, deterministically partition positions across lanes, and\nturn redundant fan-out into complementary coverage without changing budget or\ndeadline."}
{"id": "2511.04088", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.04088", "abs": "https://arxiv.org/abs/2511.04088", "authors": ["Pranav Joshi", "Daniel McMorrow", "Yihan Zhang", "Amitalok J. Budkuley", "Sidharth Jaggi"], "title": "Efficient and rate-optimal list-decoding in the presence of minimal feedback: Weldon and Slepian-Wolf in sheep's clothing", "comment": null, "summary": "Given a channel with length-$n$ inputs and outputs over the alphabet\n$\\{0,1,\\ldots,q-1\\}$, and of which a fraction $\\varrho \\in (0,1-1/q)$ of\nsymbols can be arbitrarily corrupted by an adversary, a fundamental problem is\nthat of communicating at rates close to the information-theoretically optimal\nvalues, while ensuring the receiver can infer that the transmitter's message is\nfrom a ``small\" set. While the existence of such codes is known, and\nconstructions with computationally tractable encoding/decoding procedures are\nknown for large $q$, we provide the first schemes that attain this performance\nfor any $q \\geq 2$, as long as low-rate feedback (asymptotically negligible\nrelative to the number of transmissions) from the receiver to the transmitter\nis available. For any sufficiently small $\\varepsilon > 0$ and $\\varrho \\in\n(1-1/q-\\Theta(\\sqrt{\\varepsilon})$ our minimal feedback scheme has the\nfollowing parameters: Rate $1-H_q(\\varrho) - \\varepsilon$ (i.e.,\n$\\varepsilon$-close to information-theoretically optimal -- here $H_q(\\varrho)$\nis the $q$-ary entropy function), list-size\n$\\exp(\\mathcal{O}(\\varepsilon^{-3/2}\\log^2(1/\\varepsilon))$, computational\ncomplexity of encoding/decoding\n$n^{\\mathcal{O}(\\varepsilon^{-1}\\log(1/\\varepsilon))}$, storage complexity\n$\\mathcal{O}(n^{\\eta+1}\\log n)$ for a code design parameter $\\eta>1$ that\ntrades off storage complexity with the probability of error. The error\nprobability is $\\mathcal{O}(n^{-\\eta})$, and the (vanishing) feedback rate is\n$\\mathcal{O}(1/ \\log n)$."}
{"id": "2511.03994", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.03994", "abs": "https://arxiv.org/abs/2511.03994", "authors": ["Mahek Desai", "Apoorva Rumale", "Marjan Asadinia"], "title": "HART: A Hybrid Addressing Scheme for Self-Balancing Binary Search Trees in Phase Change Memory (PCM)", "comment": null, "summary": "As DRAM and other transistor-based memory technologies approach their\nscalability limits, alternative storage solutions like Phase-Change Memory\n(PCM) are gaining attention for their scalability, fast access times, and zero\nleakage power. However, current memory-intensive algorithms, especially those\nused in big data systems, often overlook PCM's endurance limitations (10^6 to\n10^8 writes before degradation) and write asymmetry. Self-balancing binary\nsearch trees (BSTs), which are widely used for large-scale data management,\nwere developed without considering PCM's unique properties, leading to\npotential performance degradation. This paper introduces HART, a novel hybrid\naddressing scheme for self-balancing BSTs, designed to optimize PCM's\ncharacteristics. By combining DFATGray code addressing for deeper nodes with\nlinear addressing for shallower nodes, HART balances reduced bit flips during\nfrequent rotations at deeper levels with computational simplicity at shallow\nlevels. Experimental results on PCM-aware AVL trees demonstrate significant\nimprovements in performance, with a reduction in bit flips leading to enhanced\nendurance, increased lifetime, and lower write energy and latency. Notably,\nthese benefits are achieved without imposing substantial computational\noverhead, making HART an efficient solution for big data applications."}
{"id": "2511.04221", "categories": ["cs.IR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.04221", "abs": "https://arxiv.org/abs/2511.04221", "authors": ["Carl Kugblenu", "Petri Vuorimaa"], "title": "Coordination-Free Lane Partitioning for Convergent ANN Search", "comment": "10 pages, 6 figures; arXiv preprint", "summary": "Production vector search systems often fan out each query across parallel\nlanes (threads, replicas, or shards) to meet latency service-level objectives\n(SLOs). In practice, these lanes rediscover the same candidates, so extra\ncompute does not increase coverage. We present a coordination-free lane\npartitioner that turns duplication into complementary work at the same cost and\ndeadline. For each query we (1) build a deterministic candidate pool sized to\nthe total top-k budget, (2) apply a per-query pseudorandom permutation, and (3)\nassign each lane a disjoint slice of positions. Lanes then return different\nresults by construction, with no runtime coordination.\n  At equal cost with four lanes (total candidate budget 64), on SIFT1M (1M SIFT\nfeature vectors) with Hierarchical Navigable Small World graphs (HNSW)\nrecall@10 rises from 0.249 to 0.999 while lane overlap falls from nearly 100%\nto 0%. On MS MARCO (8.8M passages) with HNSW, hit@10 improves from 0.200 to\n0.601 and Mean Reciprocal Rank at 10 (MRR@10) from 0.133 to 0.330. For inverted\nfile (IVF) indexes we see smaller but consistent gains (for example, +11% on MS\nMARCO) by de-duplicating list routing. A microbenchmark shows planner overhead\nof ~37 microseconds per query (mean at the main setting) with linear growth in\nthe number of merged candidates.\n  These results yield a simple operational guideline: size the per-query pool\nto the total budget, deterministically partition positions across lanes, and\nturn redundant fan-out into complementary coverage without changing budget or\ndeadline."}
{"id": "2511.04135", "categories": ["cs.IT", "cs.CR", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.04135", "abs": "https://arxiv.org/abs/2511.04135", "authors": ["Chen Yuan", "Ruiqi Zhu"], "title": "List Decoding of Folded Reed-Solomon Codes Over Galois Ring", "comment": "32 pages", "summary": "List decoding of codes can be seen as the generalization of unique decoding\nof codes While list decoding over finite fields has been extensively studied,\nextending these results to more general algebraic structures such as Galois\nrings remains an important challenge. Due to recent progress in zero knowledge\nsystems, there is a growing demand to investigate the proximity gap of codes\nover Galois rings in Yizhou Yao and coauthors(2025), Alexander Golovne and\ncoauthors(2023), Yuanju Wei and coauthors(2025). The proximity gap is closely\nrelated to the decoding capability of codes. It was shown in Eli Ben-Sasson and\ncoauthors(2020) that the proximity gap for RS codes over finite field can be\nimproved to $1-\\sqrt{r}$ if one consider list decoding instead of unique\ndecoding. However, we know very little about RS codes over Galois ring which\nmight hinder the development of zero knowledge proof system for ring-based\narithmetic circuit. In this work, we first extend the list decoding procedure\nof Guruswami and Sudan to Reed-Solomon codes over Galois rings, which shows\nthat RS codes with rate $r$ can be list decoded up to radius $1-\\sqrt{r}$.\nThen, we investigate the list decoding of folded Reed-Solomon codes over Galois\nrings. We show that the list decoding radius of folded Reed-Solomon codes can\nreach the Singlton bound as its counterpart over finite field. Finally, we\nimprove the list size of our folded Reed-Solomon code to\n$O(\\frac{1}{\\varepsilon^2})$ by extending recent work in Shashank\nSrivastava(2025) to Galois Rings."}
{"id": "2511.04107", "categories": ["cs.DS", "cs.DM"], "pdf": "https://arxiv.org/pdf/2511.04107", "abs": "https://arxiv.org/abs/2511.04107", "authors": ["Chengu Wang"], "title": "Depth-13 Sorting Networks for 28 Channels", "comment": "9 pages, 3 figures", "summary": "We establish new depth upper bounds for sorting networks on 27 and 28\nchannels, improving the previous best bound of 14 to 13. Our 28-channel network\nis constructed with reflectional symmetry by combining high-quality prefixes of\n16- and 12-channel networks, extending them greedily one comparator at a time,\nand using a SAT solver to complete the remaining layers."}
{"id": "2511.04237", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04237", "abs": "https://arxiv.org/abs/2511.04237", "authors": ["Zefeng Li", "Ning Yang"], "title": "Denoised Recommendation Model with Collaborative Signal Decoupling", "comment": null, "summary": "Although the collaborative filtering (CF) algorithm has achieved remarkable\nperformance in recommendation systems, it suffers from suboptimal\nrecommendation performance due to noise in the user-item interaction matrix.\nNumerous noise-removal studies have improved recommendation models, but most\nexisting approaches conduct denoising on a single graph. This may cause\nattenuation of collaborative signals: removing edges between two nodes can\ninterrupt paths between other nodes, weakening path-dependent collaborative\ninformation. To address these limitations, this study proposes a novel\nGNN-based CF model called DRCSD for denoising unstable interactions. DRCSD\nincludes two core modules: a collaborative signal decoupling module (decomposes\nsignals into distinct orders by structural characteristics) and an order-wise\ndenoising module (performs targeted denoising on each order). Additionally, the\ninformation aggregation mechanism of traditional GNN-based CF models is\nmodified to avoid cross-order signal interference until the final pooling\noperation. Extensive experiments on three public real-world datasets show that\nDRCSD has superior robustness against unstable interactions and achieves\nstatistically significant performance improvements in recommendation accuracy\nmetrics compared to state-of-the-art baseline models."}
{"id": "2511.04471", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.04471", "abs": "https://arxiv.org/abs/2511.04471", "authors": ["Ali Bemani", "Nassar Ksairi", "Marios Kountouris"], "title": "Affine Frequency Division Multiplexing: From Communication to Sensing", "comment": null, "summary": "Affine Frequency Division Multiplexing (AFDM) has been proposed as an\neffective waveform for achieving the full diversity of doubly-dispersive\n(delay-Doppler) channels. While this property is closely related to range and\nvelocity estimation in sensing, this article focuses on other AFDM features\nthat are particularly relevant for addressing two challenges in integrated\nsensing and communication (ISAC) systems: (1) maintaining receiver complexity\nand energy consumption at acceptable levels while supporting the large\nbandwidths required for high delay/range resolution, and (2) mitigating\ninterference in multiradar environments. In monostatic sensing, where direct\ntransmitter-receiver leakage is a major impairment, we show that AFDM-based\nISAC receivers can address the first challenge through their compatibility with\nlow-complexity self-interference cancellation (SIC) schemes and reduced\nsampling rates via analog dechirping. In bistatic sensing, where such analog\nsolutions may not be feasible, we demonstrate that AFDM supports sub-Nyquist\nsampling without requiring hardware modifications while preserving delay\nresolution. Finally, we show that the second challenge can be addressed by\nleveraging the resource-assignment flexibility of the discrete affine Fourier\ntransform (DAFT) underlying the AFDM waveform."}
{"id": "2511.04258", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.04258", "abs": "https://arxiv.org/abs/2511.04258", "authors": ["Balagopal Komarath", "Anant Kumar", "Akash Pareek"], "title": "Counting Patterns in Degenerate Graphs in Constant Space", "comment": null, "summary": "For an arbitrary, fixed graph (pattern graph), we study the algorithmic\ncomplexity of counting homomorphisms, subgraph isomorphisms, and induced\nsubgraph isomorphisms from the pattern graph to $n$-vertex, $d$-degenerate\ngraphs as input. Recent work by Bressan (Algorithmica, 2021) has shown that\nthis problem has efficient dynamic programming algorithms using a graph\nparameter called DAG treewidth. Bressan used DAG treewidth to design a fast\nalgorithm for counting homomorphisms, subgraph isomorphisms, and induced\nsubgraph isomorphisms that use polynomial space. Bera, Gishboliner, Levanzov,\nSeshadhri, and Shapira (SODA, 2021) provided a characterization of graphs with\nDAG treewidth one.\n  In this paper, we introduce a new graph parameter called DAG treedepth and\nshow that it yields efficient divide and conquer algorithms that use only\nconstant space (in the unit-cost RAM model). Specifically, we show:\n  An algorithm for counting subgraphs isomorphic to sparse pattern graphs using\nonly constant space.\n  We derive an induced minor-based characterization for graphs of DAG treedepth\nup to two.\n  For pattern graphs upto nine vertices, the induced subgraphs can be counted\nin $O(n^3)$ time using constant space.\n  An algorithm for counting induced subgraphs that matches the running time\ngiven by Bressan but only uses constant space.\n  Apart from the DAG treedepth result, we also focus on DAG treewidth. For DAG\ntreewidth, we show that we can count homomorphisms, subgraph isomorphisms, and\ninduced subgraph isomorphisms faster than Bressan's algorithm (2021). We\nfurther show that for all pattern graphs up to 11 vertices, we can count\ninduced subgraphs in quadratic time."}
{"id": "2511.04541", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04541", "abs": "https://arxiv.org/abs/2511.04541", "authors": ["Baptiste Bonin", "Maxime Heuillet", "Audrey Durand"], "title": "LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems", "comment": null, "summary": "Modeling user preferences across domains remains a key challenge in slate\nrecommendation (i.e. recommending an ordered sequence of items) research. We\ninvestigate how Large Language Models (LLM) can effectively act as world models\nof user preferences through pairwise reasoning over slates. We conduct an\nempirical study involving several LLMs on three tasks spanning different\ndatasets. Our results reveal relationships between task performance and\nproperties of the preference function captured by LLMs, hinting towards areas\nfor improvement and highlighting the potential of LLMs as world models in\nrecommender systems."}
{"id": "2511.04630", "categories": ["cs.IT", "cs.NI", "cs.SY", "eess.SP", "eess.SY", "math.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.04630", "abs": "https://arxiv.org/abs/2511.04630", "authors": ["Stavros Mitrolaris", "Subhankar Banerjee", "Sennur Ulukus"], "title": "Age of Job Completion Minimization with Stable Queues", "comment": null, "summary": "We consider a time-slotted job-assignment system with a central server, N\nusers and a machine which changes its state according to a Markov chain (hence\ncalled a Markov machine). The users submit their jobs to the central server\naccording to a stochastic job arrival process. For each user, the server has a\ndedicated job queue. Upon receiving a job from a user, the server stores that\njob in the corresponding queue. When the machine is not working on a job\nassigned by the server, the machine can be either in internally busy or in free\nstate, and the dynamics of these states follow a binary symmetric Markov chain.\nUpon sampling the state information of the machine, if the server identifies\nthat the machine is in the free state, it schedules a user and submits a job to\nthe machine from the job queue of the scheduled user. To maximize the number of\njobs completed per unit time, we introduce a new metric, referred to as the age\nof job completion. To minimize the age of job completion and the sampling cost,\nwe propose two policies and numerically evaluate their performance. For both of\nthese policies, we find sufficient conditions under which the job queues will\nremain stable."}
{"id": "2511.04343", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.04343", "abs": "https://arxiv.org/abs/2511.04343", "authors": ["Themistoklis Haris", "Fabian Spaeh", "Spyros Dragazis", "Charalampos Tsourakakis"], "title": "Estimating Hitting Times Locally At Scale", "comment": "Accepted for presentation at NeurIPS 2025", "summary": "Hitting times provide a fundamental measure of distance in random processes,\nquantifying the expected number of steps for a random walk starting at node $u$\nto reach node $v$. They have broad applications across domains such as network\ncentrality analysis, ranking and recommendation systems, and epidemiology. In\nthis work, we develop local algorithms for estimating hitting times between a\npair of vertices $u,v$ without accessing the full graph, overcoming scalability\nissues of prior global methods. Our first algorithm uses the key insight that\nhitting time computations can be truncated at the meeting time of two\nindependent random walks from $u$ and $v$. This leads to an efficient estimator\nanalyzed via the Kronecker product graph and Markov Chain Chernoff bounds. We\nalso present an algorithm extending the work of [Peng et al.; KDD 2021], that\nintroduces a novel adaptation of the spectral cutoff technique to account for\nthe asymmetry of hitting times. This adaptation captures the directionality of\nthe underlying random walk and requires non-trivial modifications to ensure\naccuracy and efficiency. In addition to the algorithmic upper bounds, we also\nprovide tight asymptotic lower bounds. We also reveal a connection between\nhitting time estimation and distribution testing, and validate our algorithms\nusing experiments on both real and synthetic data."}
{"id": "2511.04247", "categories": ["cs.MM", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.04247", "abs": "https://arxiv.org/abs/2511.04247", "authors": ["Allie Tran", "Luca Rossetto"], "title": "On the Brittleness of CLIP Text Encoders", "comment": "Accepted for publication at MMM'26", "summary": "Multimodal co-embedding models, especially CLIP, have advanced the state of\nthe art in zero-shot classification and multimedia information retrieval in\nrecent years by aligning images and text in a shared representation space.\nHowever, such modals trained on a contrastive alignment can lack stability\ntowards small input perturbations. Especially when dealing with manually\nexpressed queries, minor variations in the query can cause large differences in\nthe ranking of the best-matching results. In this paper, we present a\nsystematic analysis of the effect of multiple classes of non-semantic query\nperturbations in an multimedia information retrieval scenario. We evaluate a\ndiverse set of lexical, syntactic, and semantic perturbations across multiple\nCLIP variants using the TRECVID Ad-Hoc Video Search queries and the V3C1 video\ncollection. Across models, we find that syntactic and semantic perturbations\ndrive the largest instabilities, while brittleness is concentrated in trivial\nsurface edits such as punctuation and case. Our results highlight robustness as\na critical dimension for evaluating vision-language models beyond benchmark\naccuracy."}
{"id": "2511.04345", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.04345", "abs": "https://arxiv.org/abs/2511.04345", "authors": ["Kuowen Chen", "Nicole Wein", "Yiran Zhang"], "title": "A Polynomial-Time Algorithm for the Next-to-Shortest Path Problem on Positively Weighted Directed Graphs", "comment": null, "summary": "Given a graph and a pair of terminals $s$, $t$, the next-to-shortest path\nproblem asks for an $s\\!\\to \\!t$ (simple) path that is shortest among all not\nshortest $s\\!\\to \\!t$ paths (if one exists). This problem was introduced in\n1996, and soon after was shown to be NP-complete for directed graphs with\nnon-negative edge weights, leaving open the case of positive edge weights.\nSubsequent work investigated this open question, and developed polynomial-time\nalgorithms for the cases of undirected graphs and planar directed graphs. In\nthis work, we resolve this nearly 30-year-old open problem by providing an\nalgorithm for the next-to-shortest path problem on directed graphs with\npositive edge weights."}
{"id": "2511.04390", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.04390", "abs": "https://arxiv.org/abs/2511.04390", "authors": ["Kristóf Bérczi", "Vasilis Livanos", "José A. Soto", "Victor Verdugo"], "title": "Free-order secretary for two-sided independence systems", "comment": null, "summary": "The Matroid Secretary Problem is a central question in online optimization,\nmodeling sequential decision-making under combinatorial constraints. We\nintroduce a bipartite graph framework that unifies and extends several known\nformulations, including the bipartite matching, matroid intersection, and\nrandom-order matroid secretary problems. In this model, elements form a\nbipartite graph between agents and items, and the objective is to select a\nmatching that satisfies feasibility constraints on both sides, given by two\nindependence systems.\n  We study the free-order setting, where the algorithm may adaptively choose\nthe next element to reveal. For $k$-matroid intersection, we leverage a core\nlemma by (Feldman, Svensson and Zenklusen, 2022) to design an\n$\\Omega(1/k^2)$-competitive algorithm, extending known results for single\nmatroids. Building on this, we identify the structural property underlying our\napproach and introduce $k$-growth systems. We establish a generalized core\nlemma for $k$-growth systems, showing that a suitably defined set of critical\nelements retains a $\\Omega(1/k^2)$ fraction of the optimal weight. Using this\nlemma, we extend our $\\Omega(1/k^2)$-competitive algorithm to $k$-growth\nsystems for the edge-arrival model.\n  We then study the agent-arrival model, which presents unique challenges to\nour framework. We extend the core lemma to this model and then apply it to\nobtain an $\\Omega(\\beta/k^2)$-competitive algorithm for $k$-growth systems,\nwhere $\\beta$ denotes the competitiveness of a special type of order-oblivious\nalgorithm for the item-side constraint. Finally, we relax the matching\nassumption and extend our results to the case of multiple item selection, where\nagents have individual independence systems coupled by a global item-side\nconstraint. We obtain constant-competitive algorithms for fundamental cases\nsuch as partition matroids and $k$-matching constraints."}
{"id": "2511.04484", "categories": ["cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04484", "abs": "https://arxiv.org/abs/2511.04484", "authors": ["Tsubasa Harada", "Yasushi Kawase", "Hanna Sumita"], "title": "Online Algorithms for Repeated Optimal Stopping: Achieving Both Competitive Ratio and Regret Bounds", "comment": "33 pages", "summary": "We study the repeated optimal stopping problem, which generalizes the\nclassical optimal stopping problem with an unknown distribution to a setting\nwhere the same problem is solved repeatedly over $T$ rounds. In this framework,\nwe aim to design algorithms that guarantee a competitive ratio in each round\nwhile also achieving sublinear regret across all rounds.\n  Our primary contribution is a general algorithmic framework that achieves\nthese objectives simultaneously for a wide array of repeated optimal stopping\nproblems. The core idea is to dynamically select an algorithm for each round,\nchoosing between two candidates: (1) an empirically optimal algorithm derived\nfrom the history of observations, and (2) a sample-based algorithm with a\nproven competitive ratio guarantee. Based on this approach, we design an\nalgorithm that performs no worse than the baseline sample-based algorithm in\nevery round, while ensuring that the total regret is bounded by\n$\\tilde{O}(\\sqrt{T})$.\n  We demonstrate the broad applicability of our framework to canonical\nproblems, including the prophet inequality, the secretary problem, and their\nvariants under adversarial, random, and i.i.d. input models. For example, for\nthe repeated prophet inequality problem, our method achieves a\n$1/2$-competitive ratio from the second round on and an $\\tilde{O}(\\sqrt{T})$\nregret. Furthermore, we establish a regret lower bound of $\\Omega(\\sqrt{T})$\neven in the i.i.d. model, confirming that our algorithm's performance is almost\noptimal with respect to the number of rounds."}
{"id": "2511.04140", "categories": ["cs.DB", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.04140", "abs": "https://arxiv.org/abs/2511.04140", "authors": ["Zheng Li", "Weiyan Wang", "Ruiyuan Li", "Chao Chen", "Xianlei Long", "Linjiang Zheng", "Quanqing Xu", "Chuanhui Yang"], "title": "GPU-Based Floating-point Adaptive Lossless Compression", "comment": null, "summary": "Domains such as IoT (Internet of Things) and HPC (High Performance Computing)\ngenerate a torrential influx of floating-point time-series data. Compressing\nthese data while preserving their absolute fidelity is critical, and leveraging\nthe massive parallelism of modern GPUs offers a path to unprecedented\nthroughput. Nevertheless, designing such a high-performance GPU-based lossless\ncompressor faces three key challenges: 1) heterogeneous data movement\nbottlenecks, 2) precision-preserving conversion complexity, and 3)\nanomaly-induced sparsity degradation. To address these challenges, this paper\nproposes Falcon, a GPU-based Floating-point Adaptive Lossless COmpressioN\nframework. Specifically, Falcon first introduces a lightweight asynchronous\npipeline, which hides the I/O latency during the data transmission between the\nCPU and GPU. Then, we propose an accurate and fast float-to-integer\ntransformation method with theoretical guarantees, which eliminates the errors\ncaused by floating-point arithmetic. Moreover, we devise an adaptive sparse\nbit-plane lossless encoding strategy, which reduces the sparsity caused by\noutliers. Extensive experiments on 12 diverse datasets show that our\ncompression ratio improves by 9.1% over the most advanced CPU-based method,\nwith compression throughput 2.43X higher and decompression throughput 2.4X\nhigher than the fastest GPU-based competitors, respectively."}
