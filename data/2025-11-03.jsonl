{"id": "2510.26891", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.26891", "abs": "https://arxiv.org/abs/2510.26891", "authors": ["Martin Loebl", "Anetta Jedličková", "Jakub Černý"], "title": "Market Equilibria With Buying Rights", "comment": null, "summary": "We embed buying rights into a (repeated) Arrow-Debreu model to study the\nlong-term effects of regulation through buying rights on arising inequality.\nOur motivation stems from situations that typically call for regulatory\ninterventions, such as rationing, namely, distribution crises in which demand\nand supply are persistently misaligned. In such settings, scarce resources tend\nto become increasingly concentrated among more affluent individuals, while the\nneeds of the broader population remain unmet. While fully centralized\ndistribution may be logistically or politically unfeasible, issuing buying\nrights offers a more practical alternative: they can be implemented digitally,\ne.g., via tokens traded on online platforms, making them significantly easier\nto administer. We model a scenario in which a regulator periodically\ndistributes buying rights with the aim of promoting a more equitable\nallocation. Our contributions include (i) the definition of the (iterated)\nmarket where in each round the buying rights are distributed and then traded\nalongside the resource, (ii) the approximation algorithm of the market-clearing\nprices in every round, and (iii) the upper bound on \\textit{frustration} -- a\nnotion conceptually similar to the Price of Anarchy, but for systems regulated\nthrough buying rights, defined as the arising loss in fairness the individual\nbuyers have to take when the distribution is handled via the market."}
{"id": "2510.27008", "categories": ["cs.GT", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2510.27008", "abs": "https://arxiv.org/abs/2510.27008", "authors": ["Fabian Raoul Pieroth", "Ole Petersen", "Martin Bichler"], "title": "Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing", "comment": "This work was presented at the Conference on Information Systems and\n  Technology (CIST) in Atlanta, Georgia, USA, Oct 2025", "summary": "Predatory pricing -- where a firm strategically lowers prices to undermine\ncompetitors -- is a contentious topic in dynamic oligopoly theory, with\nscholars debating practical relevance and the existence of predatory\nequilibria. Although finite-horizon dynamic models have long been proposed to\ncapture the strategic intertemporal incentives of oligopolists, the existence\nand form of equilibrium strategies in settings that allow for firm exit\n(drop-outs following loss-making periods) have remained an open question. We\nfocus on the seminal dynamic oligopoly model by Selten (1965) that introduces\nthe subgame perfect equilibrium and analyzes smooth market sharing. Equilibrium\ncan be derived analytically in models that do not allow for dropouts, but not\nin models that can lead to predatory pricing. In this paper, we leverage recent\nadvances in deep reinforcement learning to compute and verify equilibria in\nfinite-horizon dynamic oligopoly games. Our experiments reveal two key\nfindings: first, state-of-the-art deep reinforcement learning algorithms\nreliably converge to equilibrium in both perfect- and imperfect-information\noligopoly models; second, when firms face asymmetric cost structures, the\nresulting equilibria exhibit predatory pricing behavior. These results\ndemonstrate that predatory pricing can emerge as a rational equilibrium\nstrategy across a broad variety of model settings. By providing equilibrium\nanalysis of finite-horizon dynamic oligopoly models with drop-outs, our study\nanswers a decade-old question and offers new insights for competition\nauthorities and regulators."}
{"id": "2510.26844", "categories": ["cs.IT", "cs.MM", "eess.IV", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.26844", "abs": "https://arxiv.org/abs/2510.26844", "authors": ["Bingyan Xie", "Jihong Park", "Yongpeng Wu", "Wenjun Zhang", "Tony Quek"], "title": "Multi-hop Parallel Image Semantic Communication for Distortion Accumulation Mitigation", "comment": null, "summary": "Existing semantic communication schemes primarily focus on single-hop\nscenarios, overlooking the challenges of multi-hop wireless image transmission.\nAs semantic communication is inherently lossy, distortion accumulates over\nmultiple hops, leading to significant performance degradation. To address this,\nwe propose the multi-hop parallel image semantic communication (MHPSC)\nframework, which introduces a parallel residual compensation link at each hop\nagainst distortion accumulation. To minimize the associated transmission\nbandwidth overhead, a coarse-to-fine residual compression scheme is designed. A\ndeep learning-based residual compressor first condenses the residuals, followed\nby the adaptive arithmetic coding (AAC) for further compression. A residual\ndistribution estimation module predicts the prior distribution for the AAC to\nachieve fine compression performances. This approach ensures robust multi-hop\nimage transmission with only a minor increase in transmission bandwidth.\nExperimental results confirm that MHPSC outperforms both existing semantic\ncommunication and traditional separated coding schemes."}
{"id": "2510.26861", "categories": ["cs.IR", "cs.CL", "H.3.3; I.2.7; I.2.10"], "pdf": "https://arxiv.org/pdf/2510.26861", "abs": "https://arxiv.org/abs/2510.26861", "authors": ["Teerapol Saengsukhiran", "Peerawat Chomphooyod", "Narabodee Rodjananant", "Chompakorn Chaksangchaichot", "Patawee Prakrankamanant", "Witthawin Sripheanpol", "Pak Lovichit", "SarChaksaana Nutanong", "Ekapol Chuangsuwanich"], "title": "Evaluating Perspectival Biases in Cross-Modal Retrieval", "comment": null, "summary": "Multimodal retrieval systems are expected to operate in a semantic space,\nagnostic to the language or cultural origin of the query. In practice, however,\nretrieval outcomes systematically reflect perspectival biases: deviations\nshaped by linguistic prevalence and cultural associations. We study two such\nbiases. First, prevalence bias refers to the tendency to favor entries from\nprevalent languages over semantically faithful entries in image-to-text\nretrieval. Second, association bias refers to the tendency to favor images\nculturally associated with the query over semantically correct ones in\ntext-to-image retrieval. Results show that explicit alignment is a more\neffective strategy for mitigating prevalence bias. However, association bias\nremains a distinct and more challenging problem. These findings suggest that\nachieving truly equitable multimodal systems requires targeted strategies\nbeyond simple data scaling and that bias arising from cultural association may\nbe treated as a more challenging problem than one arising from linguistic\nprevalence."}
{"id": "2510.26835", "categories": ["cs.DB", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26835", "abs": "https://arxiv.org/abs/2510.26835", "authors": ["Chen Wang", "Xunzhuo Liu", "Yue Zhu", "Alaa Youssef", "Priya Nagpurkar", "Huamin Chen"], "title": "Category-Aware Semantic Caching for Heterogeneous LLM Workloads", "comment": "13 pages including reference, position paper", "summary": "LLM serving systems process heterogeneous query workloads where different\ncategories exhibit different characteristics. Code queries cluster densely in\nembedding space while conversational queries distribute sparsely. Content\nstaleness varies from minutes (stock data) to months (code patterns). Query\nrepetition patterns range from power-law (code) to uniform (conversation),\nproducing long tail cache hit rate distributions: high-repetition categories\nachieve 40-60% hit rates while low-repetition or volatile categories achieve\n5-15% hit rates. Vector databases must exclude the long tail because remote\nsearch costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of\nproduction traffic uncached. Uniform cache policies compound this problem:\nfixed thresholds cause false positives in dense spaces and miss valid\nparaphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This\npaper presents category-aware semantic caching where similarity thresholds,\nTTLs, and quotas vary by query category. We present a hybrid architecture\nseparating in-memory HNSW search from external document storage, reducing miss\ncost from 30ms to 2ms. This reduction makes low-hit-rate categories\neconomically viable (break-even at 3-5% versus 15-20%), enabling cache coverage\nacross the entire workload distribution. Adaptive load-based policies extend\nthis framework to respond to downstream model load, dynamically adjusting\nthresholds and TTLs to reduce traffic to overloaded models by 9-17% in\ntheoretical projections."}
{"id": "2510.26844", "categories": ["cs.IT", "cs.MM", "eess.IV", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.26844", "abs": "https://arxiv.org/abs/2510.26844", "authors": ["Bingyan Xie", "Jihong Park", "Yongpeng Wu", "Wenjun Zhang", "Tony Quek"], "title": "Multi-hop Parallel Image Semantic Communication for Distortion Accumulation Mitigation", "comment": null, "summary": "Existing semantic communication schemes primarily focus on single-hop\nscenarios, overlooking the challenges of multi-hop wireless image transmission.\nAs semantic communication is inherently lossy, distortion accumulates over\nmultiple hops, leading to significant performance degradation. To address this,\nwe propose the multi-hop parallel image semantic communication (MHPSC)\nframework, which introduces a parallel residual compensation link at each hop\nagainst distortion accumulation. To minimize the associated transmission\nbandwidth overhead, a coarse-to-fine residual compression scheme is designed. A\ndeep learning-based residual compressor first condenses the residuals, followed\nby the adaptive arithmetic coding (AAC) for further compression. A residual\ndistribution estimation module predicts the prior distribution for the AAC to\nachieve fine compression performances. This approach ensures robust multi-hop\nimage transmission with only a minor increase in transmission bandwidth.\nExperimental results confirm that MHPSC outperforms both existing semantic\ncommunication and traditional separated coding schemes."}
{"id": "2510.27157", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.27157", "abs": "https://arxiv.org/abs/2510.27157", "authors": ["Min Hou", "Le Wu", "Yuxin Liao", "Yonghui Yang", "Zhen Zhang", "Changlong Zheng", "Han Wu", "Richang Hong"], "title": "A Survey on Generative Recommendation: Data, Model, and Tasks", "comment": null, "summary": "Recommender systems serve as foundational infrastructure in modern\ninformation ecosystems, helping users navigate digital content and discover\nitems aligned with their preferences. At their core, recommender systems\naddress a fundamental problem: matching users with items. Over the past\ndecades, the field has experienced successive paradigm shifts, from\ncollaborative filtering and matrix factorization in the machine learning era to\nneural architectures in the deep learning era. Recently, the emergence of\ngenerative models, especially large language models (LLMs) and diffusion\nmodels, have sparked a new paradigm: generative recommendation, which\nreconceptualizes recommendation as a generation task rather than discriminative\nscoring. This survey provides a comprehensive examination through a unified\ntripartite framework spanning data, model, and task dimensions. Rather than\nsimply categorizing works, we systematically decompose approaches into\noperational stages-data augmentation and unification, model alignment and\ntraining, task formulation and execution. At the data level, generative models\nenable knowledge-infused augmentation and agent-based simulation while unifying\nheterogeneous signals. At the model level, we taxonomize LLM-based methods,\nlarge recommendation models, and diffusion approaches, analyzing their\nalignment mechanisms and innovations. At the task level, we illuminate new\ncapabilities including conversational interaction, explainable reasoning, and\npersonalized content generation. We identify five key advantages: world\nknowledge integration, natural language understanding, reasoning capabilities,\nscaling laws, and creative generation. We critically examine challenges in\nbenchmark design, model robustness, and deployment efficiency, while charting a\nroadmap toward intelligent recommendation assistants that fundamentally reshape\nhuman-information interaction."}
{"id": "2510.26840", "categories": ["cs.DB", "cs.AI", "cs.FL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.26840", "abs": "https://arxiv.org/abs/2510.26840", "authors": ["Rocky Klopfenstein", "Yang He", "Andrew Tremante", "Yuepeng Wang", "Nina Narodytska", "Haoze Wu"], "title": "SpotIt: Evaluating Text-to-SQL Evaluation with Formal Verification", "comment": null, "summary": "Community-driven Text-to-SQL evaluation platforms play a pivotal role in\ntracking the state of the art of Text-to-SQL performance. The reliability of\nthe evaluation process is critical for driving progress in the field. Current\nevaluation methods are largely test-based, which involves comparing the\nexecution results of a generated SQL query and a human-labeled ground-truth on\na static test database. Such an evaluation is optimistic, as two queries can\ncoincidentally produce the same output on the test database while actually\nbeing different. In this work, we propose a new alternative evaluation\npipeline, called SpotIt, where a formal bounded equivalence verification engine\nactively searches for a database that differentiates the generated and\nground-truth SQL queries. We develop techniques to extend existing verifiers to\nsupport a richer SQL subset relevant to Text-to-SQL. A performance evaluation\nof ten Text-to-SQL methods on the high-profile BIRD dataset suggests that\ntest-based methods can often overlook differences between the generated query\nand the ground-truth. Further analysis of the verification results reveals a\nmore complex picture of the current Text-to-SQL evaluation."}
{"id": "2510.26988", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.26988", "abs": "https://arxiv.org/abs/2510.26988", "authors": ["Fardad Vakilipoor", "Johannes Konrad", "Maximilian Schäfer"], "title": "Inferring the Chemotaxis Distortion Function from Cellular Decision Strategies", "comment": null, "summary": "Cellular intelligence enables cells to process environmental signals and make\ncontext-dependent decisions, as exemplified by chemotaxis, where cells navigate\nchemical gradients despite noisy signaling pathways. To investigate how cells\ndeal with uncertainty, we apply an information-theoretic framework based on\nrate distortion theory (RDT). The Blahut-Arimoto algorithm (BAA) computes\noptimal decision strategies that minimize mutual information while satisfying\ndistortion constraints, balancing sensing accuracy with distortion constraint\nequivalent to resource cost. We propose the inverse Blahut-Arimoto algorithm\n(IBAA) to compute the distortion function, which quantifies the system's\ndecision-making criteria for realizing a decision strategy to map input signals\nto outputs. This general framework extends beyond chemotaxis to biological and\nengineered systems requiring efficient information processing under\nuncertainty. We validate the proposed IBAA by accurately estimating theoretical\ndistortion functions in a cellular apoptosis scenario. Additionally, using the\nlocal excitation global inhibition (LEGI) model to simulate chemotactic\nresponses, we compute the distortion functions from the cell's perspective. Our\nfinding reveals a state-dependent decision criteria by the cell."}
{"id": "2510.26938", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.26938", "abs": "https://arxiv.org/abs/2510.26938", "authors": ["Ajinkya Gaikwad", "Hitendra Kumar", "S. Padmapriya", "Praneet Kumar Patra", "Harsh Sanklecha", "Soumen Maity"], "title": "Inclusive and Exclusive Vertex Splitting into Specific Graph Classes: NP Hardness and Algorithms", "comment": null, "summary": "We study a family of graph modification problems called the F-Vertex\nSplitting problem. Given a graph G, the task is to determine whether G can be\ntransformed into a graph G-prime belonging to a graph class F through a\nsequence of at most k vertex splits. We investigate this problem for several\ntarget graph classes, namely constellations, cycle graphs, linear forests, and\nbipartite graphs. We analyze both inclusive and exclusive variants of vertex\nsplitting, as introduced by Abu-Khzam and collaborators (ISCO 2018). Our\nresults show that the F-Vertex Splitting problem is polynomial-time solvable\nwhen F is a cycle graph or a linear forest, for both variants. In contrast,\nwhen F is a constellation or a bipartite graph, the problem is NP-complete for\nboth variants."}
{"id": "2510.27232", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.27232", "abs": "https://arxiv.org/abs/2510.27232", "authors": ["Liyang He", "Zhenya Huang", "Cheng Yang", "Rui Li", "Zheng Zhang", "Kai Zhang", "Zhi Li", "Qi Liu", "Enhong Chen"], "title": "A Survey on Deep Text Hashing: Efficient Semantic Text Retrieval with Binary Representation", "comment": null, "summary": "With the rapid growth of textual content on the Internet, efficient\nlarge-scale semantic text retrieval has garnered increasing attention from both\nacademia and industry. Text hashing, which projects original texts into compact\nbinary hash codes, is a crucial method for this task. By using binary codes,\nthe semantic similarity computation for text pairs is significantly accelerated\nvia fast Hamming distance calculations, and storage costs are greatly reduced.\nWith the advancement of deep learning, deep text hashing has demonstrated\nsignificant advantages over traditional, data-independent hashing techniques.\nBy leveraging deep neural networks, these methods can learn compact and\nsemantically rich binary representations directly from data, overcoming the\nperformance limitations of earlier approaches. This survey investigates current\ndeep text hashing methods by categorizing them based on their core components:\nsemantic extraction, hash code quality preservation, and other key\ntechnologies. We then present a detailed evaluation schema with results on\nseveral popular datasets, followed by a discussion of practical applications\nand open-source tools for implementation. Finally, we conclude by discussing\nkey challenges and future research directions, including the integration of\ndeep text hashing with large language models to further advance the field. The\nproject for this survey can be accessed at\nhttps://github.com/hly1998/DeepTextHashing."}
{"id": "2510.26868", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.26868", "abs": "https://arxiv.org/abs/2510.26868", "authors": ["Reham Faqehi", "Haya Alhuraib", "Hamad Saiari", "Zyad Bamigdad"], "title": "The Impact of Data Compression in Real-Time and Historical Data Acquisition Systems on the Accuracy of Analytical Solutions", "comment": "9 pages", "summary": "In industrial and IoT environments, massive amounts of real-time and\nhistorical process data are continuously generated and archived. With sensors\nand devices capturing every operational detail, the volume of time-series data\nhas become a critical challenge for storage and processing systems. Efficient\ndata management is essential to ensure scalability, cost-effectiveness, and\ntimely analytics. To minimize storage expenses and optimize performance, data\ncompression algorithms are frequently utilized in data historians and\nacquisition systems. However, compression comes with trade-offs that may\ncompromise the accuracy and reliability of engineering analytics that depend on\nthis compressed data. Understanding these trade-offs is essential for\ndeveloping data strategies that support both operational efficiency and\naccurate, reliable analytics. This paper assesses the relation of common\ncompression mechanisms used in real-time and historical data systems and the\naccuracy of analytical solutions, including statistical analysis, anomaly\ndetection, and machine learning models. Through theoretical analysis, simulated\nsignal compression, and empirical assessment, we illustrate that excessive\ncompression can lose critical patterns, skew statistical measures, and diminish\npredictive accuracy. The study suggests optimum methods and best practices for\nstriking a compromise between analytical integrity and compression efficiency."}
{"id": "2510.27071", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.27071", "abs": "https://arxiv.org/abs/2510.27071", "authors": ["Dengming Xu", "Mengmeng LI"], "title": "Multilevel constructions of constant dimension codes based on one-factorization of complete graphs", "comment": null, "summary": "Constant dimension codes (CDCs) have become an important object in coding\ntheory due to their application in random network coding. The multilevel\nconstruction is one of the most effective ways to construct constant dimension\ncodes. The paper is devoted to constructing CDCs by the multilevel\nconstruction. Precisely, we first choose an appropriate skeleton code based on\nthe transformations of binary vectors related to the one-factorization of\ncomplete graphs; then we construct CDCs by using the chosen skeleton code,\nwhere quasi-pending blocks are used; finally, we calculate the dimensions by\nuse of known constructions of optimal Ferrers diagram rank metric codes. As\napplications, we improve the lower bounds of $\\overline{A}_q(n,8,6)$ for\n$16\\leq n\\leq 19.$"}
{"id": "2510.26968", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.26968", "abs": "https://arxiv.org/abs/2510.26968", "authors": ["Jackson Bibbens", "Cooper Sigrist", "Bo Sun", "Shahin Kamali", "Mohammad Hajiesmaili"], "title": "Green Bin Packing", "comment": "50 pages, 12 figures", "summary": "The online bin packing problem and its variants are regularly used to model\nserver allocation problems. Modern concerns surrounding sustainability and\novercommitment in cloud computing motivate bin packing models that capture\ncosts associated with highly utilized servers. In this work, we introduce the\ngreen bin packing problem, an online variant with a linear cost $\\beta$ for\nfilling above a fixed level $G$. For a given instance, the goal is to minimize\nthe sum of the number of opened bins and the linear cost. We show that when\n$\\beta G \\le 1$, classical online bin packing algorithms such as FirstFit or\nHarmonic perform well, and can achieve competitive ratios lower than in the\nclassic setting. However, when $\\beta G > 1$, new algorithmic solutions can\nimprove both worst-case and typical performance. We introduce variants of\nclassic online bin packing algorithms and establish theoretical bounds, as well\nas test their empirical performance."}
{"id": "2510.27274", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.27274", "abs": "https://arxiv.org/abs/2510.27274", "authors": ["Yu Lin", "Zhen Jia", "Philipp Christmann", "Xu Zhang", "Shengdong Du", "Tianrui Li"], "title": "Traceable Drug Recommendation over Medical Knowledge Graphs", "comment": "Accepted to MediKS@CIKM2025", "summary": "Drug recommendation (DR) systems aim to support healthcare professionals in\nselecting appropriate medications based on patients' medical conditions.\nState-of-the-art approaches utilize deep learning techniques for improving DR,\nbut fall short in providing any insights on the derivation process of\nrecommendations -- a critical limitation in such high-stake applications. We\npropose TraceDR, a novel DR system operating over a medical knowledge graph\n(MKG), which ensures access to large-scale and high-quality information.\nTraceDR simultaneously predicts drug recommendations and related evidence\nwithin a multi-task learning framework, enabling traceability of medication\nrecommendations. For covering a more diverse set of diseases and drugs than\nexisting works, we devise a framework for automatically constructing patient\nhealth records and release DrugRec, a new large-scale testbed for DR."}
{"id": "2510.27119", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.27119", "abs": "https://arxiv.org/abs/2510.27119", "authors": ["Qiyan Deng", "Jianhui Li", "Chengliang Chai", "Jinqi Liu", "Junzhi She", "Kaisen Jin", "Zhaoze Sun", "Yuhao Deng", "Jia Yuan", "Ye Yuan", "Guoren Wang", "Lei Cao"], "title": "Unstructured Data Analysis using LLMs: A Comprehensive Benchmark", "comment": null, "summary": "Nowadays, the explosion of unstructured data presents immense analytical\nvalue. Leveraging the remarkable capability of large language models (LLMs) in\nextracting attributes of structured tables from unstructured data, researchers\nare developing LLM-powered data systems for users to analyze unstructured\ndocuments as working with a database. These unstructured data analysis (UDA)\nsystems differ significantly in all aspects, including query interfaces, query\noptimization strategies, and operator implementations, making it unclear which\nperforms best in which scenario. Unfortunately, there does not exist a\ncomprehensive benchmark that offers high-quality, large-volume, and diverse\ndatasets as well as rich query workload to thoroughly evaluate such systems. To\nfill this gap, we present UDA-Bench, the first benchmark for unstructured data\nanalysis that meets all the above requirements. Specifically, we organize a\nteam with 30 graduate students that spends over in total 10,000 hours on\ncurating 5 datasets from various domains and constructing a relational database\nview from these datasets by manual annotation. These relational databases can\nbe used as ground truth to evaluate any of these UDA systems despite their\ndifferences in programming interfaces. Moreover, we design diverse queries to\nanalyze the attributes defined in the database schema, covering different types\nof analytical operators with varying selectivities and complexities. We conduct\nin-depth analysis of the key building blocks of existing UDA systems: query\ninterface, query optimization, operator design, and data processing. We run\nexhaustive experiments over the benchmark to fully evaluate these systems and\ndifferent techniques w.r.t. the above building blocks."}
{"id": "2510.27147", "categories": ["cs.IT", "cs.DC", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.27147", "abs": "https://arxiv.org/abs/2510.27147", "authors": ["Gaoyuan Zhang", "Ruisong Si", "Boyuan Li", "Zijian Li", "Baofeng Ji", "Chenqi Zhu", "Tony Q. S. Quek"], "title": "Secure Communication in the Presence of an RIS-Enhanced Eavesdropper in MIMO Networks", "comment": "13 pages, 15 figures", "summary": "We pay our attention towards secure and robust communication in the presence\nof a Reconfigurable Intelligent Surface (RIS)-enhanced mobile eavesdropping\nattacker in Multiple-Input Multiple-Output (MIMO)wireless\nnetworks.Specifically,we first provide a unifying framework that generalizes\nspecific intelligent wiretap model wherein the passive eavesdropper configured\nwith any number of antennas is potentially mobile and can actively optimize its\nreceived signal strength with the help of RIS by intelligently manipulating\nwiretap channel characteristics.To effectively mitigate this intractable\nthreat,we then propose a novel and lightweight secure communication scheme from\nthe perspective of information theory.The main idea is that the data processing\ncan in some cases be observed as communication channel,and a random\nbit-flipping scheme is then carefully involved for the legitimate transmitter\nto minimize the mutual information between the secret message and the passive\neavesdropper's received data.The Singular Value Decomposition (SVD)-based\nprecoding strategy is also implemented to optimize power allocation,and thus\nensure that the legitimate receiver is not subject to interference from this\nrandom bit-flipping.The corresponding results depict that our secure\ncommunication scheme is practically desired, which does not require any a prior\nknowledge of the eavesdropper's full instantaneous Channel State Information\n(ICSI). Furthermore,we consider the RIS optimization problem from the\neavesdropper's perspective,and provide RIS phase shift design solutions under\ndifferent attacking scenarios.Finally,the optimal detection schemes\nrespectively for the legitimate user and the eavesdropper are provided,and\ncomprehensive simulations are presented to verify our theoretical analysis and\nshow the effectiveness and robustness of our secure communication scheme across\na wide range of attacking scenarios."}
{"id": "2510.27330", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.27330", "abs": "https://arxiv.org/abs/2510.27330", "authors": ["Maximilian Probst Gutenberg", "Weixuan Yuan"], "title": "A Simple Deterministic Reduction From Gomory-Hu Tree to Maxflow and Expander Decomposition", "comment": "19 pages, 0 figures, submitted to the 17th Innovations in Theoretical\n  Computer Science Conference (ITCS 2026) on August 30th", "summary": "Given an undirected graph $G=(V,E,w)$, a Gomory-Hu tree $T$ (Gomory and Hu,\n1961) is a tree on $V$ that preserves all-pairs mincuts of $G$ exactly.\n  We present a simple and efficient randomized reduction from Gomory-Hu trees\nto polylog maxflow computations. On unweighted graphs, our reduction reduces to\nmaxflow computations on graphs of total instance size $\\tilde{O}(m)$ and the\nalgorithm requires only $\\tilde{O}(m)$ additional time. Our reduction is the\nfirst that is tight up to polylog factors. The reduction also seamlessly\nextends to weighted graphs, however, instance sizes and runtime increase to\n$\\tilde{O}(n^2)$.\n  Finally, we show how to extend our reduction to reduce Gomory-Hu trees for\nunweighted hypergraphs to maxflow in hypergraphs. Again, our reduction is the\nfirst that is tight up to polylog factors."}
{"id": "2510.27342", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.27342", "abs": "https://arxiv.org/abs/2510.27342", "authors": ["Alireza Gharahighehi", "Felipe Kenji Nakano", "Xuehua Yang", "Wenhan Cu", "Celine Vens"], "title": "Pairwise and Attribute-Aware Decision Tree-Based Preference Elicitation for Cold-Start Recommendation", "comment": null, "summary": "Recommender systems (RSs) are intelligent filtering methods that suggest\nitems to users based on their inferred preferences, derived from their\ninteraction history on the platform. Collaborative filtering-based RSs rely on\nusers past interactions to generate recommendations. However, when a user is\nnew to the platform, referred to as a cold-start user, there is no historical\ndata available, making it difficult to provide personalized recommendations. To\naddress this, rating elicitation techniques can be used to gather initial\nratings or preferences on selected items, helping to build an early\nunderstanding of the user's tastes. Rating elicitation approaches are generally\ncategorized into two types: non-personalized and personalized. Decision\ntree-based rating elicitation is a personalized method that queries users about\ntheir preferences at each node of the tree until sufficient information is\ngathered. In this paper, we propose an extension to the decision tree approach\nfor rating elicitation in the context of music recommendation. Our method: (i)\nelicits not only item ratings but also preferences on attributes such as genres\nto better cluster users, and (ii) uses item pairs instead of single items at\neach node to more effectively learn user preferences. Experimental results\ndemonstrate that both proposed enhancements lead to improved performance,\nparticularly with a reduced number of queries."}
{"id": "2510.27141", "categories": ["cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.27141", "abs": "https://arxiv.org/abs/2510.27141", "authors": ["Chunxiao Ye", "Xiao Yan", "Eric Lo"], "title": "Compass: General Filtered Search across Vector and Structured Data", "comment": null, "summary": "The increasing prevalence of hybrid vector and relational data necessitates\nefficient, general support for queries that combine high-dimensional vector\nsearch with complex relational filtering. However, existing filtered search\nsolutions are fundamentally limited by specialized indices, which restrict\narbitrary filtering and hinder integration with general-purpose DBMSs. This\nwork introduces \\textsc{Compass}, a unified framework that enables general\nfiltered search across vector and structured data without relying on new index\ndesigns. Compass leverages established index structures -- such as HNSW and IVF\nfor vector attributes, and B+-trees for relational attributes -- implementing a\nprincipled cooperative query execution strategy that coordinates candidate\ngeneration and predicate evaluation across modalities. Uniquely, Compass\nmaintains generality by allowing arbitrary conjunctions, disjunctions, and\nrange predicates, while ensuring robustness even with highly-selective or\nmulti-attribute filters. Comprehensive empirical evaluations demonstrate that\nCompass consistently outperforms NaviX, the only existing performant general\nframework, across diverse hybrid query workloads. It also matches the query\nthroughput of specialized single-attribute indices in their favorite settings\nwith only a single attribute involved, all while maintaining full generality\nand DBMS compatibility. Overall, Compass offers a practical and robust solution\nfor achieving truly general filtered search in vector database systems."}
{"id": "2510.27175", "categories": ["cs.IT", "cs.DC", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.27175", "abs": "https://arxiv.org/abs/2510.27175", "authors": ["Gaoyuan Zhang", "Gaolei Song", "Boyuan Li", "Zijian Li", "Baofeng Ji", "Ruijuan Zheng", "Guoqiang Zheng", "Tony Q. S. Quek"], "title": "Byzantine Attacks in RIS-Enhanced Cooperative Spectrum Sensing: A Decision Fusion Perspective", "comment": "16 pages, 12 figures", "summary": "From the perspective of hard decision fusion, we investigate Byzantine\nattacks in Reconfigurable Intelligent Surface (RIS)-enhanced and\ndecode-and-forward relay-assisted Cooperative Spectrum Sensing (CSS) for mobile\nCognitive Radio Networks (CRNs) in this paper. Specially, a RIS-enhanced and\ndecode-and-forward relay-assisted CSS configuration is first constructed under\ndynamic channel scenarios due to user mobility. Subsequently, the channel- and\nattack-aware hard decision fusion rules are developed, and the optimal\nchannel-aware Byzantine attack strategies are then developed under both\nsmall-scale and large-scale attacking scenarios. The corresponding results\ndepict that the optimal attack strategy does not require any a prior knowledge\nof the global instantaneous Channel State Information (ICSI) (e.g. false alarm\nprobability and detection probability of all the secondary users), although\nperfect acquisition of ICSI is clearly always not affordable from the attacker\nperspective, which is further exacerbated by the RIS and decode-and-forward\nrelays involved in CSS and the potential high mobility of secondary users that\nleads to fast fading channels. Furthermore, our counterintuitive results also\nindicate that, regardless of the attacker's awareness of the decision fusion\nrule, the optimal Byzantine attack can be achieved through a unifying\nframework, the explicit attack strategy may be not unique, and the attacking\neffectiveness is primarily determined by the fraction of the Byzantine nodes\nrather than the channel dynamics. That is, to make the channel-aware approach\nmore practical, the challenge that the heavy reliance on the global ICSI and\ndecision fusion rule in obtaining the Byzantine attacks is successfully\nrelaxed. Finally, we empirically validate our theoretical analysis through\nextensive simulations across a wide range of attacking scenarios."}
{"id": "2510.27588", "categories": ["cs.DS", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.27588", "abs": "https://arxiv.org/abs/2510.27588", "authors": ["Stefan Hermann", "Hans-Peter Lehmann", "Giorgio Vinciguerra", "Stefan Walzer"], "title": "Learned Static Function Data Structures", "comment": null, "summary": "We consider the task of constructing a data structure for associating a\nstatic set of keys with values, while allowing arbitrary output values for\nqueries involving keys outside the set. Compared to hash tables, these\nso-called static function data structures do not need to store the key set and\nthus use significantly less memory. Several techniques are known, with\ncompressed static functions approaching the zero-order empirical entropy of the\nvalue sequence. In this paper, we introduce learned static functions, which use\nmachine learning to capture correlations between keys and values. For each key,\na model predicts a probability distribution over the values, from which we\nderive a key-specific prefix code to compactly encode the true value. The\nresulting codeword is stored in a classic static function data structure. This\ndesign allows learned static functions to break the zero-order entropy barrier\nwhile still supporting point queries. Our experiments show substantial space\nsavings: up to one order of magnitude on real data, and up to three orders of\nmagnitude on synthetic data."}
{"id": "2510.27566", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.27566", "abs": "https://arxiv.org/abs/2510.27566", "authors": ["Yulong Hui", "Chao Chen", "Zhihang Fu", "Yihao Liu", "Jieping Ye", "Huanchen Zhang"], "title": "Interact-RAG: Reason and Interact with the Corpus, Beyond Black-Box Retrieval", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has significantly enhanced LLMs by\nincorporating external information. However, prevailing agentic RAG approaches\nare constrained by a critical limitation: they treat the retrieval process as a\nblack-box querying operation. This confines agents' actions to query issuing,\nhindering its ability to tackle complex information-seeking tasks. To address\nthis, we introduce Interact-RAG, a new paradigm that elevates the LLM agent\nfrom a passive query issuer into an active manipulator of the retrieval\nprocess. We dismantle the black-box with a Corpus Interaction Engine, equipping\nthe agent with a set of action primitives for fine-grained control over\ninformation retrieval. To further empower the agent on the entire RAG pipeline,\nwe first develop a reasoning-enhanced workflow, which enables both zero-shot\nexecution and the synthesis of interaction trajectories. We then leverage this\nsynthetic data to train a fully autonomous end-to-end agent via Supervised\nFine-Tuning (SFT), followed by refinement with Reinforcement Learning (RL).\nExtensive experiments across six benchmarks demonstrate that Interact-RAG\nsignificantly outperforms other advanced methods, validating the efficacy of\nour reasoning-interaction strategy."}
{"id": "2510.27168", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.27168", "abs": "https://arxiv.org/abs/2510.27168", "authors": ["Jing Chang", "Chang Liu", "Jinbin Huang", "Shuyuan Zheng", "Rui Mao", "Jianbin Qin"], "title": "ShapleyPipe: Hierarchical Shapley Search for Data Preparation Pipeline Construction", "comment": null, "summary": "Automated data preparation pipeline construction is critical for machine\nlearning success, yet existing methods suffer from two fundamental limitations:\nthey treat pipeline construction as black-box optimization without quantifying\nindividual operator contributions, and they struggle with the combinatorial\nexplosion of the search space ($N^M$ configurations for N operators and\npipeline length M). We introduce ShapleyPipe, a principled framework that\nleverages game-theoretic Shapley values to systematically quantify each\noperator's marginal contribution while maintaining full interpretability. Our\nkey innovation is a hierarchical decomposition that separates category-level\nstructure search from operator-level refinement, reducing the search complexity\nfrom exponential to polynomial. To make Shapley computation tractable, we\ndevelop: (1) a Multi-Armed Bandit mechanism for intelligent category evaluation\nwith provable convergence guarantees, and (2) Permutation Shapley values to\ncorrectly capture position-dependent operator interactions. Extensive\nevaluation on 18 diverse datasets demonstrates that ShapleyPipe achieves 98.1\\%\nof high-budget baseline performance while using 24\\% fewer evaluations, and\noutperforms the state-of-the-art reinforcement learning method by 3.6\\%. Beyond\nperformance gains, ShapleyPipe provides interpretable operator valuations\n($\\rho$=0.933 correlation with empirical performance) that enable data-driven\npipeline analysis and systematic operator library refinement."}
{"id": "2510.27185", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.27185", "abs": "https://arxiv.org/abs/2510.27185", "authors": ["Xu Gan", "Zhaolin Wang", "Yuanwei Liu"], "title": "Dual-Scale Antenna Deployment for Pinching Antenna Systems", "comment": null, "summary": "A dual-scale deployment (DSD) framework for pinching antenna systems (PASS)\nis proposed. 1) In the first coarse stage, the pinching antenna (PA) is\ntransferred over a large-scale range at the waveguide level. 2) The refinement\nstage performs small-scale relocation of the PA with high precision. Four PA\ndeployment protocols are provided in the proposed DSD framework. Then, a\npractical power consumption model is proposed, based on which the theoretical\nenergy efficiency formulas for PASS are derived. The transmit precoding, PA\nradiation power, and PA deployment are jointly optimized to maximize the energy\nefficiency under the provided PA deployment protocols. To solve this\nnon-convex, highly coupled problem, a low-complexity penalty-based alternating\noptimization algorithm is proposed. Simulation results validate the accuracy of\ntheoretical results and the convergence of the proposed algorithm. It is\ndemonstrated that: 1) PASS delivers about 70% higher energy efficiency than the\nconventional cell-free architecture and nearly twofold improvement relative to\nMIMO systems; 2) it is essential to specify the DSD resolution and deployment\nprotocol to achieve the maximum energy efficiency for PASS."}
{"id": "2510.27614", "categories": ["cs.DS", "cs.DB", "H.0; E.4; D.4"], "pdf": "https://arxiv.org/pdf/2510.27614", "abs": "https://arxiv.org/abs/2510.27614", "authors": ["Pedro Silva Gomes", "Carlos Baquero"], "title": "Rateless Bloom Filters: Set Reconciliation for Divergent Replicas with Variable-Sized Elements", "comment": "Under submission", "summary": "Set reconciliation protocols typically make two critical assumptions: they\nare designed for fixed-sized elements and they are optimized for when the\ndifference cardinality, d, is very small. When adapting to variable-sized\nelements, the current practice is to synchronize fixed-size element digests.\nHowever, when the number of differences is considerable, such as after a\nnetwork partition, this approach can be inefficient. Our solution is a\ntwo-stage hybrid protocol that introduces a preliminary Bloom filter step,\nspecifically designed for this regime. The novelty of this approach, however,\nis in solving a core technical challenge: determining the optimal Bloom filter\nsize without knowing d. Our solution is the Rateless Bloom Filter (RBF), a\ndynamic filter that naturally adapts to arbitrary symmetric differences,\nclosely matching the communication complexity of an optimally configured static\nfilter without requiring any prior parametrization. Our evaluation in sets of\nvariable-sized elements shows that for Jaccard indices below 85%, our RBF-IBLT\nhybrid protocol reduces the total communication cost by up to over 20% compared\nto the state-of-the-art."}
{"id": "2510.27238", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.27238", "abs": "https://arxiv.org/abs/2510.27238", "authors": ["Chuxuan Hu", "Maxwell Yang", "James Weiland", "Yeji Lim", "Suhas Palawala", "Daniel Kang"], "title": "DRAMA: Unifying Data Retrieval and Analysis for Open-Domain Analytic Queries", "comment": "Accepted to SIGMOD 2026", "summary": "Manually conducting real-world data analyses is labor-intensive and\ninefficient. Despite numerous attempts to automate data science workflows, none\nof the existing paradigms or systems fully demonstrate all three key\ncapabilities required to support them effectively: (1) open-domain data\ncollection, (2) structured data transformation, and (3) analytic reasoning.\n  To overcome these limitations, we propose DRAMA, an end-to-end paradigm that\nanswers users' analytic queries in natural language on large-scale open-domain\ndata. DRAMA unifies data collection, transformation, and analysis as a single\npipeline. To quantitatively evaluate system performance on tasks representative\nof DRAMA, we construct a benchmark, DRAMA-Bench, consisting of two categories\nof tasks: claim verification and question answering, each comprising 100\ninstances. These tasks are derived from real-world applications that have\ngained significant public attention and require the retrieval and analysis of\nopen-domain data. We develop DRAMA-Bot, a multi-agent system designed following\nDRAMA. It comprises a data retriever that collects and transforms data by\ncoordinating the execution of sub-agents, and a data analyzer that performs\nstructured reasoning over the retrieved data. We evaluate DRAMA-Bot on\nDRAMA-Bench together with five state-of-the-art baseline agents. DRAMA-Bot\nachieves 86.5% task accuracy at a cost of $0.05, outperforming all baselines\nwith up to 6.9 times the accuracy and less than 1/6 of the cost. DRAMA is\npublicly available at https://github.com/uiuc-kang-lab/drama."}
{"id": "2510.27349", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.27349", "abs": "https://arxiv.org/abs/2510.27349", "authors": ["Fan-Hao Lin", "Chi-Jui Sung", "Chu-Hsiang Huang", "Hui Chen", "Chao-Kai Wen", "Henk Wymeersch"], "title": "Cross-Band Channel Impulse Response Prediction: Leveraging 3.5 GHz Channels for Upper Mid-Band", "comment": "7 pages, 5 figures, 4 tables, this work has been submitted to IEEE\n  International Conference on Communications (ICC) 2026", "summary": "Accurate cross-band channel prediction is essential for 6G networks,\nparticularly in the upper mid-band (FR3, 7--24 GHz), where penetration loss and\nblockage are severe. Although ray tracing (RT) provides high-fidelity modeling,\nit remains computationally intensive, and high-frequency data acquisition is\ncostly. To address these challenges, we propose CIR-UNext, a deep learning\nframework designed to predict 7 GHz channel impulse responses (CIRs) by\nleveraging abundant 3.5 GHz CIRs. The framework integrates an RT-based dataset\npipeline with attention U-Net (AU-Net) variants for gain and phase prediction.\nThe proposed AU-Net-Aux model achieves a median gain error of 0.58 dB and a\nphase prediction error of 0.27 rad on unseen complex environments. Furthermore,\nwe extend CIR-UNext into a foundation model, Channel2ComMap, for throughput\nprediction in MIMO-OFDM systems, demonstrating superior performance compared\nwith existing approaches. Overall, CIR-UNext provides an efficient and scalable\nsolution for cross-band prediction, enabling applications such as localization,\nbeam management, digital twins, and intelligent resource allocation in 6G\nnetworks."}
{"id": "2510.27141", "categories": ["cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.27141", "abs": "https://arxiv.org/abs/2510.27141", "authors": ["Chunxiao Ye", "Xiao Yan", "Eric Lo"], "title": "Compass: General Filtered Search across Vector and Structured Data", "comment": null, "summary": "The increasing prevalence of hybrid vector and relational data necessitates\nefficient, general support for queries that combine high-dimensional vector\nsearch with complex relational filtering. However, existing filtered search\nsolutions are fundamentally limited by specialized indices, which restrict\narbitrary filtering and hinder integration with general-purpose DBMSs. This\nwork introduces \\textsc{Compass}, a unified framework that enables general\nfiltered search across vector and structured data without relying on new index\ndesigns. Compass leverages established index structures -- such as HNSW and IVF\nfor vector attributes, and B+-trees for relational attributes -- implementing a\nprincipled cooperative query execution strategy that coordinates candidate\ngeneration and predicate evaluation across modalities. Uniquely, Compass\nmaintains generality by allowing arbitrary conjunctions, disjunctions, and\nrange predicates, while ensuring robustness even with highly-selective or\nmulti-attribute filters. Comprehensive empirical evaluations demonstrate that\nCompass consistently outperforms NaviX, the only existing performant general\nframework, across diverse hybrid query workloads. It also matches the query\nthroughput of specialized single-attribute indices in their favorite settings\nwith only a single attribute involved, all while maintaining full generality\nand DBMS compatibility. Overall, Compass offers a practical and robust solution\nfor achieving truly general filtered search in vector database systems."}
{"id": "2510.27243", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.27243", "abs": "https://arxiv.org/abs/2510.27243", "authors": ["Jiachen Zhao", "Xiao Yan", "Eric Lo"], "title": "Approximate Diverse $k$-nearest Neighbor Search in Vector Database", "comment": null, "summary": "Approximate $k$-nearest neighbor search (A$k$-NNS) is a core operation in\nvector databases, underpinning applications such as retrieval-augmented\ngeneration (RAG) and image retrieval. In these scenarios, users often prefer\ndiverse result sets to minimize redundancy and enhance information value.\nHowever, existing greedy-based diverse methods frequently yield sub-optimal\nresults, failing to adequately approximate the optimal similarity score under\ncertain diversification level. Furthermore, there is a need for flexible\nalgorithms that can adapt to varying user-defined result sizes and diversity\nrequirements.\n  To address these challenges, we propose a novel approach that seamlessly\nintegrates result diversification into state-of-the-art (SOTA) A$k$-NNS\nmethods. Our approach introduces a progressive search framework, consisting of\niterative searching, diversification, and verification phases. Carefully\ndesigned diversification and verification steps enable our approach to\nefficiently approximate the optimal diverse result set according to\nuser-specified diversification levels without additional indexing overhead.\n  We evaluate our method on three million-scale benchmark datasets, LAION-art,\nDeep1M, and Txt2img, using latency, similarity, and recall as performance\nmetrics across a range of $k$ values and diversification thresholds.\nExperimental results demonstrate that our approach consistently retrieves\nnear-optimal diverse results with minimal latency overhead, particularly under\nmedium and high diversity settings."}
{"id": "2510.27358", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.27358", "abs": "https://arxiv.org/abs/2510.27358", "authors": ["S. T. Dougherty", "C. Fernández-Córdoba"], "title": "Weight Enumerators From Equivalence Relations and MacWilliams Identities", "comment": null, "summary": "In this paper, we consider codes over finite fields, finite abelian groups,\nand finite Frobenius rings. For such codes, the complete weight enumerator and\nthe Hamming weight enumerator serve as powerful tools. These two types of\nweight enumerators satisfy the MacWilliams relations. We define the weight\nenumerator of a code with respect to an equivalence relation and determine in\nwhich cases the MacWilliams relations hold for this weight enumerator. We also\nstudy some weight enumerators for specific equivalence relations."}
{"id": "2510.27238", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.27238", "abs": "https://arxiv.org/abs/2510.27238", "authors": ["Chuxuan Hu", "Maxwell Yang", "James Weiland", "Yeji Lim", "Suhas Palawala", "Daniel Kang"], "title": "DRAMA: Unifying Data Retrieval and Analysis for Open-Domain Analytic Queries", "comment": "Accepted to SIGMOD 2026", "summary": "Manually conducting real-world data analyses is labor-intensive and\ninefficient. Despite numerous attempts to automate data science workflows, none\nof the existing paradigms or systems fully demonstrate all three key\ncapabilities required to support them effectively: (1) open-domain data\ncollection, (2) structured data transformation, and (3) analytic reasoning.\n  To overcome these limitations, we propose DRAMA, an end-to-end paradigm that\nanswers users' analytic queries in natural language on large-scale open-domain\ndata. DRAMA unifies data collection, transformation, and analysis as a single\npipeline. To quantitatively evaluate system performance on tasks representative\nof DRAMA, we construct a benchmark, DRAMA-Bench, consisting of two categories\nof tasks: claim verification and question answering, each comprising 100\ninstances. These tasks are derived from real-world applications that have\ngained significant public attention and require the retrieval and analysis of\nopen-domain data. We develop DRAMA-Bot, a multi-agent system designed following\nDRAMA. It comprises a data retriever that collects and transforms data by\ncoordinating the execution of sub-agents, and a data analyzer that performs\nstructured reasoning over the retrieved data. We evaluate DRAMA-Bot on\nDRAMA-Bench together with five state-of-the-art baseline agents. DRAMA-Bot\nachieves 86.5% task accuracy at a cost of $0.05, outperforming all baselines\nwith up to 6.9 times the accuracy and less than 1/6 of the cost. DRAMA is\npublicly available at https://github.com/uiuc-kang-lab/drama."}
{"id": "2510.27588", "categories": ["cs.DS", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.27588", "abs": "https://arxiv.org/abs/2510.27588", "authors": ["Stefan Hermann", "Hans-Peter Lehmann", "Giorgio Vinciguerra", "Stefan Walzer"], "title": "Learned Static Function Data Structures", "comment": null, "summary": "We consider the task of constructing a data structure for associating a\nstatic set of keys with values, while allowing arbitrary output values for\nqueries involving keys outside the set. Compared to hash tables, these\nso-called static function data structures do not need to store the key set and\nthus use significantly less memory. Several techniques are known, with\ncompressed static functions approaching the zero-order empirical entropy of the\nvalue sequence. In this paper, we introduce learned static functions, which use\nmachine learning to capture correlations between keys and values. For each key,\na model predicts a probability distribution over the values, from which we\nderive a key-specific prefix code to compactly encode the true value. The\nresulting codeword is stored in a classic static function data structure. This\ndesign allows learned static functions to break the zero-order entropy barrier\nwhile still supporting point queries. Our experiments show substantial space\nsavings: up to one order of magnitude on real data, and up to three orders of\nmagnitude on synthetic data."}
{"id": "2510.27614", "categories": ["cs.DS", "cs.DB", "H.0; E.4; D.4"], "pdf": "https://arxiv.org/pdf/2510.27614", "abs": "https://arxiv.org/abs/2510.27614", "authors": ["Pedro Silva Gomes", "Carlos Baquero"], "title": "Rateless Bloom Filters: Set Reconciliation for Divergent Replicas with Variable-Sized Elements", "comment": "Under submission", "summary": "Set reconciliation protocols typically make two critical assumptions: they\nare designed for fixed-sized elements and they are optimized for when the\ndifference cardinality, d, is very small. When adapting to variable-sized\nelements, the current practice is to synchronize fixed-size element digests.\nHowever, when the number of differences is considerable, such as after a\nnetwork partition, this approach can be inefficient. Our solution is a\ntwo-stage hybrid protocol that introduces a preliminary Bloom filter step,\nspecifically designed for this regime. The novelty of this approach, however,\nis in solving a core technical challenge: determining the optimal Bloom filter\nsize without knowing d. Our solution is the Rateless Bloom Filter (RBF), a\ndynamic filter that naturally adapts to arbitrary symmetric differences,\nclosely matching the communication complexity of an optimally configured static\nfilter without requiring any prior parametrization. Our evaluation in sets of\nvariable-sized elements shows that for Jaccard indices below 85%, our RBF-IBLT\nhybrid protocol reduces the total communication cost by up to over 20% compared\nto the state-of-the-art."}
