{"id": "2512.02083", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2512.02083", "abs": "https://arxiv.org/abs/2512.02083", "authors": ["Sangam Balchandar Reddy"], "title": "On the Complexity of Signed Roman Domination", "comment": "38 pages, 7 figures, Submitted to Elsevier", "summary": "Given a graph $G = (V, E)$, a signed Roman dominating function is a function $f: V \\rightarrow \\{-1, 1, 2\\}$ such that for every vertex $u \\in V$: $\\sum_{v \\in N[u]} f(v) \\geq 1$ and for every vertex $u \\in V$ with $f(u) = -1$, there exists a vertex $v \\in N(u)$ with $f(v) = 2$. The weight of a signed Roman dominating function $f$ is $\\sum_{u \\in V} f(u)$. The objective of \\srd{} (SRD) problem is to compute a signed Roman dominating function with minimum weight. The problem is known to be NP-complete even when restricted to bipartite graphs and planar graphs. In this paper, we advance the complexity study by showing that the problem remains NP-complete on split graphs. In the realm of parameterized complexity, we prove that the problem is W[2]-hard parameterized by weight, even on bipartite graphs. We further show that the problem is W[1]-hard parameterized by feedback vertex set number (and hence also when parameterized by treewidth or clique-width). On the positive side, we present an FPT algorithm parameterized by neighbourhood diversity (and by vertex cover number). Finally, we complement this result by proving that the problem does not admit a polynomial kernel parameterized by vertex cover number unless coNP $\\subseteq$ NP/poly."}
{"id": "2512.02252", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.02252", "abs": "https://arxiv.org/abs/2512.02252", "authors": ["Adam Ganczorzand Tomasz Jurdzinski"], "title": "Optimal-Length Labeling Schemes and Fast Algorithms for k-gathering and k-broadcasting", "comment": "Accepted for SOFSEM 2026", "summary": "We consider basic communication tasks in arbitrary radio networks: $k$-broadcasting and $k$-gathering. In the case of $k$-broadcasting messages from $k$ sources have to get to all nodes in the network. The goal of $k$-gathering is to collect messages from $k$ source nodes in a designated sink node. We consider these problems in the framework of distributed algorithms with advice.\n  risko and Miller showed in 2021 that the optimal size of advice for $k$-broadcasting is $Θ(\\min(\\log Δ,$ $ \\log k))$, where $Δ$ is equal to the maximum degree of a vertex of the input communication graph. We show that the same bound $Θ(\\min(\\log Δ, \\log k))$ on the size of optimal labeling scheme holds also for the $k$-gathering problems. Moreover, we design fast algorithms for both problems with asymptotically optimal size of advice. For $k$-gathering our algorithm works in at most $D+k$ rounds, where $D$ is the diameter of the communication graph. This time bound is optimal even for centralized algorithms. We apply the $k$-gathering algorithm for $k$-broadcasting to achieve an algorithm working in time $O(D+\\log^2 n+k)$ rounds. We also exhibit a logarithmic time complexity gap between distributed algorithms with advice of optimal size and distributed algorithms with distinct arbitrary labels."}
{"id": "2512.02384", "categories": ["cs.DS", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.02384", "abs": "https://arxiv.org/abs/2512.02384", "authors": ["Amit Rajaraman", "David X. Wu"], "title": "Markov Chains Approximate Message Passing", "comment": "41 pages, 2 figures", "summary": "Markov chain Monte Carlo algorithms have long been observed to obtain near-optimal performance in various Bayesian inference settings. However, developing a supporting theory that make these studies rigorous has proved challenging.\n  In this paper, we study the classical spiked Wigner inference problem, where one aims to recover a planted Boolean spike from a noisy matrix measurement. We relate the recovery performance of Glauber dynamics on the annealed posterior to the performance of Approximate Message Passing (AMP), which is known to achieve Bayes-optimal performance. Our main results rely on the analysis of an auxiliary Markov chain called restricted Gaussian dynamics (RGD). Concretely, we establish the following results:\n  1. RGD can be reduced to an effective one-dimensional recursion which mirrors the evolution of the AMP iterates.\n  2. From a warm start, RGD rapidly converges to a fixed point in correlation space, which recovers Bayes-optimal performance when run on the posterior.\n  3. Conditioned on widely believed mixing results for the SK model, we recover the phase transition for non-trivial inference."}
{"id": "2512.02412", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.02412", "abs": "https://arxiv.org/abs/2512.02412", "authors": ["Arnav Burudgunte", "Paul Valiant", "Hongao Wang"], "title": "New Bounds for Circular Trace Reconstruction", "comment": null, "summary": "The ''trace reconstruction'' problem asks, given an unknown binary string $x$ and a channel that repeatedly returns ''traces'' of $x$ with each bit randomly deleted with some probability $p$, how many traces are needed to recover $x$? There is an exponential gap between the best known upper and lower bounds for this problem. Many variants of the model have been introduced in hopes of motivating or revealing new approaches to narrow this gap. We study the variant of circular trace reconstruction introduced by Narayanan and Ren (ITCS 2021), in which traces undergo a random cyclic shift in addition to random deletions.\n  We show an improved lower bound of $\\tildeΩ(n^5)$ for circular trace reconstruction. This contrasts with the (previously) best known lower bounds of $\\tildeΩ(n^3)$ in the circular case and $\\tildeΩ(n^{3/2})$ in the linear case. Our bound shows the indistinguishability of traces from two sparse strings $x,y$ that each have a constant number of nonzeros. Can this technique be extended significantly? How hard is it to reconstruct a sparse string $x$ under a cyclic deletion channel? We resolve these questions by showing, using Fourier techniques, that $\\tilde{O}(n^6)$ traces suffice for reconstructing any constant-sparse string in a circular deletion channel, in contrast to the upper bound of $\\exp(\\tilde{O}(n^{1/3}))$ for general strings in the circular deletion channel. This shows that new algorithms or new lower bounds must focus on non-constant-sparse strings."}
{"id": "2512.02474", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02474", "abs": "https://arxiv.org/abs/2512.02474", "authors": ["Haofeng Huang", "Ling Gai"], "title": "Q-BERT4Rec: Quantized Semantic-ID Representation Learning for Multimodal Recommendation", "comment": "Submitted to KDD2026", "summary": "Sequential recommendation plays a critical role in modern online platforms such as e-commerce, advertising, and content streaming, where accurately predicting users' next interactions is essential for personalization. Recent Transformer-based methods like BERT4Rec have shown strong modeling capability, yet they still rely on discrete item IDs that lack semantic meaning and ignore rich multimodal information (e.g., text and image). This leads to weak generalization and limited interpretability. To address these challenges, we propose Q-Bert4Rec, a multimodal sequential recommendation framework that unifies semantic representation and quantized modeling. Specifically, Q-Bert4Rec consists of three stages: (1) cross-modal semantic injection, which enriches randomly initialized ID embeddings through a dynamic transformer that fuses textual, visual, and structural features; (2) semantic quantization, which discretizes fused representations into meaningful tokens via residual vector quantization; and (3) multi-mask pretraining and fine-tuning, which leverage diverse masking strategies -- span, tail, and multi-region -- to improve sequential understanding. We validate our model on public Amazon benchmarks and demonstrate that Q-Bert4Rec significantly outperforms many strong existing methods, confirming the effectiveness of semantic tokenization for multimodal sequential recommendation. Our source code will be publicly available on GitHub after publishing."}
{"id": "2512.02021", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.02021", "abs": "https://arxiv.org/abs/2512.02021", "authors": ["Jun Kawasaki"], "title": "FCDB (Functorial-Categorical Database): A Compositional Framework for Information Preservation and Anti-Commutativity Reduction", "comment": "Primary category: cs.DB; secondary: cs.LO, cs.DS. Includes tables and a TikZ diagram. https://github.com/com-junkawasaki/fcdb", "summary": "Conventional database architectures often secure local consistency by discarding information, entangling correctness with loss. We introduce the Functorial-Categorical Database (FCDb), which models data operations as morphisms in a layered functor category and establishes a Complete Preserving Family (CPF) of projections spanning content invariance (CAS), capability, and ownership, with optional observational projections for local order (B+Tree), temporal history (append-only/LSM), and adjacency (Graph). We identify a minimal kernel (F_core = Own o Cap o CAS) that preserves information and collapses non-commutativity to the ethical grant/revoke boundary. Under adjoint lifts and a fibred structure, operational pairs commute in the categorical limit while ownership integrity and capability constraints are maintained. The framework connects to information geometry via projection interpretations and supports empirical validation without discarding semantic, temporal, or relational entropy."}
{"id": "2512.02533", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2512.02533", "abs": "https://arxiv.org/abs/2512.02533", "authors": ["Yijun Liu", "Wu Liu", "Xiaoyan Gu", "Allen He", "Weiping Wang", "Yongdong Zhang"], "title": "PopSim: Social Network Simulation for Social Media Popularity Prediction", "comment": null, "summary": "Accurately predicting the popularity of user-generated content (UGC) is essential for advancing social media analytics and recommendation systems. Existing approaches typically follow an inductive paradigm, where researchers train static models on historical data for popularity prediction. However, the UGC propagation is inherently a dynamic process, and static modeling based on historical features fails to capture the complex interactions and nonlinear evolution. In this paper, we propose PopSim, a novel simulation-based paradigm for social media popularity prediction (SMPP). Unlike the inductive paradigm, PopSim leverages the large language models (LLMs)-based multi-agent social network sandbox to simulate UGC propagation dynamics for popularity prediction. Specifically, to effectively model the UGC propagation process in the network, we design a social-mean-field-based agent interaction mechanism, which models the dual-channel and bidirectional individual-population interactions, enhancing agents' global perception and decision-making capabilities. In addition, we propose a multi-source information aggregation module that transforms heterogeneous social metadata into a uniform formulation for LLMs. Finally, propagation dynamics with multimodal information are fused to provide comprehensive popularity prediction. Extensive experiments on real-world datasets demonstrate that SimPop consistently outperforms the state-of-the-art methods, reducing prediction error by an average of 8.82%, offering a new perspective for research on the SMPP task."}
{"id": "2512.02149", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.02149", "abs": "https://arxiv.org/abs/2512.02149", "authors": ["Cristina Fernández-Córdoba", "Sergi Sánchez-Aragón", "Mercè Villanueva"], "title": "Weight distributions of simplex codes over finite chain rings and their Gray map images", "comment": "23 pages", "summary": "A linear code of length $n$ over a finite chain ring $R$ with residue field $\\F_q$ is a $R$-submodule of $R^n$. A $R$-linear code is a code over $\\F_q$ (not necessarily linear) which is the generalized Gray map image of a linear code over $R$. These codes can be seen as a generalization of the linear codes over $\\Z_{p^s}$ with $p$ prime and $s \\geq 1$. In this paper, we present the construction of linear simplex codes over $R$ and their corresponding $R$-linear simplex codes of type $α$ and $β$. Moreover, we show the fundamental parameters of these codes, including their minimum Hamming distance, as well as their complete weight distributions. We also study whether these simplex codes are optimal with respect to the Griesmer-type bound."}
{"id": "2512.02354", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2512.02354", "abs": "https://arxiv.org/abs/2512.02354", "authors": ["Aadityan Ganesh", "Clayton Thomas", "S. Matthew Weinberg"], "title": "Characterizing Off-Chain Influence Proof Transaction Fee Mechanisms", "comment": null, "summary": "Roughgarden (2020) initiates the study of Transaction Fee Mechanisms (TFMs), and posits that the on-chain game of a ``good'' TFM should be on-chain simple (OnCS), i.e., incentive compatible for users and the miner. Recent work of Ganesh, Thomas and Weinberg (2024) posits that they should additionally be Off-Chain Influence Proof (OffCIP), which means that the miner cannot achieve any additional revenue by separately conducting an off-chain auction to determine on-chain inclusion. They observe that a cryptographic second-price auction satisfies both properties, but leave open the question of whether other mechanisms (e.g, non-cryptographic) satisfy these properties.\n  In this paper, we characterize OffCIP TFMs: They are those satisfying a burn identity relating the burn rule to the allocation rule. In particular, we show that auction is OffCIP if and only if its (induced direct-revelation) allocation rule $\\bar{X}(\\cdot)$ and burn rule $\\bar{B}(\\cdot)$ (both of which take as input users' values $v_1, \\dots, v_n$) are truthful when viewing $\\big(\\bar{X}(\\cdot), \\bar{B}(\\cdot)\\big)$ as the allocation and pricing rule of a multi-item auction for a single additive buyer with values $\\big(\\varphi(v_1),\\ldots, \\varphi(v_n)\\big)$ equal to the users' virtual values.\n  Building on this burn identity, we characterize deterministic OffCIP and OnCS TFMs that do not use cryptography: They are posted-price mechanisms with specially-tuned burns. As a corollary, we show that such TFMs can only exist with infinite supply and prior-dependence. However, we show that for randomized TFMs, there are additional OnCS and OffCIP auctions that do not use cryptography (even when there is finite supply, under prior-dependence with a bounded prior distribution). Holistically, our results show that while OffCIP is a fairly stringent requirement, families of OffCIP mechanisms can be found for a variety of settings."}
{"id": "2512.02571", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.02571", "abs": "https://arxiv.org/abs/2512.02571", "authors": ["Kobe Grobben", "Phablo F. S. Moura", "Hande Yaman"], "title": "Approximation schemes for covering and packing mixed-integer programs with a fixed number of constraints", "comment": "20 pages", "summary": "This paper presents an algorithmic study of a class of covering mixed-integer linear programming problems which encompasses classic cover problems, including multidimensional knapsack, facility location and supplier selection problems. We first show some properties of the vertices of the associated polytope, which are then used to decompose the problem into instances of the multidimensional knapsack cover problem with a single continuous variable per dimension. The proposed decomposition is used to design a polynomial-time approximation scheme for the problem with a fixed number of constraints. To the best of our knowledge, this is the first approximation scheme for such a general class of covering mixed-integer programs. Moreover, we design a fully polynomial-time approximation scheme and an approximate linear programming formulation for the case with a single constraint. These results improve upon the previously best-known 2-approximation algorithm for the knapsack cover problem with a single continuous variable. Finally, we show a perfect compact formulation for the case where all variables have the same lower and upper bounds. Analogous results are derived for the packing and assignment variants of the problem."}
{"id": "2512.02502", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02502", "abs": "https://arxiv.org/abs/2512.02502", "authors": ["Luyao Niu", "Zhicheng Deng", "Boyang Li", "Nuoxian Huang", "Ruiqi Liu", "Wenjia Zhang"], "title": "AskNearby: An LLM-Based Application for Neighborhood Information Retrieval and Personalized Cognitive-Map Recommendations", "comment": null, "summary": "The \"15-minute city\" envisions neighborhoods where residents can meet daily needs via a short walk or bike ride. Realizing this vision requires not only physical proximity but also efficient and reliable access to information about nearby places, services, and events. Existing location-based systems, however, focus mainly on city-level tasks and neglect the spatial, temporal, and cognitive factors that shape localized decision-making. We conceptualize this gap as the Local Life Information Accessibility (LLIA) problem and introduce AskNearby, an AI-driven community application that unifies retrieval and recommendation within the 15-minute life circle. AskNearby integrates (i) a three-layer Retrieval-Augmented Generation (RAG) pipeline that synergizes graph-based, semantic-vector, and geographic retrieval with (ii) a cognitive-map model that encodes each user's neighborhood familiarity and preferences. Experiments on real-world community datasets demonstrate that AskNearby significantly outperforms LLM-based and map-based baselines in retrieval accuracy and recommendation quality, achieving robust performance in spatiotemporal grounding and cognitive-aware ranking. Real-world deployments further validate its effectiveness. By addressing the LLIA challenge, AskNearby empowers residents to more effectively discover local resources, plan daily activities, and engage in community life."}
{"id": "2512.02281", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.02281", "abs": "https://arxiv.org/abs/2512.02281", "authors": ["Yi Liu", "Chen Qian"], "title": "Trinity: Disaggregating Vector Search from Prefill-Decode Disaggregation in LLM Serving", "comment": null, "summary": "Prefill and decode (PD) disaggregation separates prompt prefill and token-by-token decode stages into distinct GPU pools and has become the dominant architecture for large-scale LLM serving in industry. Also, retrieval tasks via vector search remains entangled with the model inference process, like heterogeneous RAG requests and prompt answer caches, inflating tail latency. We are motivated to investigate how vector search should be orchestrated along with PD disaggregation with a dedicated deployment architecture without violating SLOs in various retrieval workloads. We present Trinity, a practical framework that consolidates all retrieval into a single, shared vector-search GPU pool and make it work with PD disaggregated LLM serving in match. Trinity introduces (1) a novel architecture for deploying GPU-based vector search service in PD disaggregation. (2) Continuous batching for vector search that make full used of GPUs under heterogeneous queries; (3) Stage-aware scheduling that preempts vector search requests between both decode and prefill tasks."}
{"id": "2512.02584", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2512.02584", "abs": "https://arxiv.org/abs/2512.02584", "authors": ["Xiang Yuan", "Xinrong Chen", "Haochen Li", "Hang Yang", "Guanyu Wang", "Weiping Li", "Tong Mo"], "title": "Stepwise Schema-Guided Prompting Framework with Parameter Efficient Instruction Tuning for Multimedia Event Extraction", "comment": "Accepted by 2025 IEEE International Conference on Multimedia and Expo", "summary": "Multimedia Event Extraction (MEE) has become an important task in information extraction research as news today increasingly prefers to contain multimedia content. Current MEE works mainly face two challenges: (1) Inadequate extraction framework modeling for handling complex and flexible multimedia event structure; (2) The absence of multimodal-aligned training data for effective knowledge transfer to MEE task. In this work, we propose a Stepwise Schema-Guided Prompting Framework (SSGPF) using Multimodal Large Language Model (MLLM) as backbone for adaptive structure capturing to solve MEE task. At the initial step of SSGPF, we design Event Type Schema Guided Prompting (ETSGP) for event detection, then we devise Argument Role Schema Guided Prompting (ARSGP) that contains multi-step prompts with text-bridged grounding technique for argument extraction. We construct a weakly-aligned multimodal event labeled dataset based on existing unimodal event annotations, then conduct parameter efficient instruction tuning with LoRA on LLaVA-v1.5-7B under SSGPF. Experiments on the M2E2 benchmark demonstrate that SSGPF significantly outperforms current SOTA baselines by 5.8 percent F1 on event detection and 8.4 percent F1 on argument extraction."}
{"id": "2512.02255", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.02255", "abs": "https://arxiv.org/abs/2512.02255", "authors": ["Kunnathully Sadanandan Sanila", "Rickard Nilsson", "Emad Ibrahim", "Neelakandan Rajamohan"], "title": "Low-Power Double RIS-Assisted Mobile LEO Satellite Communications", "comment": "Published in 2025 IEEE Wireless Communications and Networking Conference (WCNC)", "summary": "We propose a low-power mobile low earth orbit (LEO) satellite communication architecture, employing double reconfigurable intelligent surfaces (RIS) to enhance energy efficiency and signal performance. With a distance between RISs that satisfies the far-field requirement, this architecture positions one small RIS each in the near-field of the satellite's antenna and the user on the ground. Moreover, we develop a path loss model for the double-RIS communication link, considering the near-field and far-field effects. Further, with the help of dual-stage beamforming, the proposed system maximizes the signal power and minimizes power consumption. Simulation results show that the proposed architecture can reduce the power consumption with 40 dB in the uplink, with a small $0.25^2$ $\\text{m}^2$ RIS near the user, to communicate in energy-constrained LEO satellite communication circumstances."}
{"id": "2512.02427", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2512.02427", "abs": "https://arxiv.org/abs/2512.02427", "authors": ["Hossein Nekouyan", "Bo Sun", "Raouf Boutaba", "Xiaoqi Tan"], "title": "Posted Pricing for Online Selection: Limited Price Changes and Risk Sensitivity", "comment": null, "summary": "Posted-price mechanisms (PPMs) are a widely adopted strategy for online resource allocation due to their simplicity, intuitive nature, and incentive compatibility. To manage the uncertainty inherent in online settings, PPMs commonly employ dynamically increasing prices. While this adaptive pricing achieves strong performance, it introduces practical challenges: dynamically changing prices can lead to fairness concerns stemming from price discrimination and incur operational costs associated with frequent updates. This paper addresses these issues by investigating posted pricing constrained by a limited, pre-specified number of allowed price changes, denoted by $Δ$. We further extend this framework by incorporating a second critical dimension: risk sensitivity. Instead of evaluating performance based solely on expectation, we utilize a tail-risk objective-specifically, the Conditional Value at Risk (CVaR) of the total social welfare, parameterized by a risk level $δ\\in [0, 1]$.\n  We formally introduce a novel problem class kSelection-$(δ,Δ)$ in online adversarial selection and propose a correlated PPM that utilizes a single random seed to correlate posted prices. This correlation scheme is designed to address both the limited price changes and simultaneously enhance the tail performance of the online algorithm. Our subsequent analysis provides performance guarantees under these joint constraints, revealing a clear trade-off between the number of allowed price changes and the algorithm's risk sensitivity. We also establish optimality results for several important special cases of the problem."}
{"id": "2512.02758", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.02758", "abs": "https://arxiv.org/abs/2512.02758", "authors": ["Klaus Jansen", "Lis Pirotton", "Malte Tutas"], "title": "The Support of Bin Packing is Exponential", "comment": null, "summary": "Consider the classical Bin Packing problem with $d$ different item sizes $s_i$ and amounts of items $a_i.$ The support of a Bin Packing solution is the number of differently filled bins. In this work, we show that the lower bound on the support of this problem is $2^{Ω(d)}$. Our lower bound matches the upper bound of $2^d$ given by Eisenbrand and Shmonin [Oper.Research Letters '06] up to a constant factor. This result has direct implications for the time complexity of several Bin Packing algorithms, such as Goemans and Rothvoss [SODA '14], Jansen and Klein [SODA '17] and Jansen and Solis-Oba [IPCO '10]. To achieve our main result, we develop a technique to aggregate equality constrained ILPs with many constraints into an equivalent ILP with one constraint. Our technique contrasts existing aggregation techniques as we manage to integrate upper bounds on variables into the resulting constraint. We believe this technique can be useful for solving general ILPs or the $d$-dimensional knapsack problem."}
{"id": "2512.03025", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03025", "abs": "https://arxiv.org/abs/2512.03025", "authors": ["Chenji Lu", "Zhuo Chen", "Hui Zhao", "Zhiyuan Zeng", "Gang Zhao", "Junjie Ren", "Ruicong Xu", "Haoran Li", "Songyan Liu", "Pengjie Wang", "Jian Xu", "Bo Zheng"], "title": "LORE: A Large Generative Model for Search Relevance", "comment": null, "summary": "Achievement. We introduce LORE, a systematic framework for Large Generative Model-based relevance in e-commerce search. Deployed and iterated over three years, LORE achieves a cumulative +27\\% improvement in online GoodRate metrics. This report shares the valuable experience gained throughout its development lifecycle, spanning data, features, training, evaluation, and deployment. Insight. While existing works apply Chain-of-Thought (CoT) to enhance relevance, they often hit a performance ceiling. We argue this stems from treating relevance as a monolithic task, lacking principled deconstruction. Our key insight is that relevance comprises distinct capabilities: knowledge and reasoning, multi-modal matching, and rule adherence. We contend that a qualitative-driven decomposition is essential for breaking through current performance bottlenecks. Contributions. LORE provides a complete blueprint for the LLM relevance lifecycle. Key contributions include: (1) A two-stage training paradigm combining progressive CoT synthesis via SFT with human preference alignment via RL. (2) A comprehensive benchmark, RAIR, designed to evaluate these core capabilities. (3) A query frequency-stratified deployment strategy that efficiently transfers offline LLM capabilities to the online system. LORE serves as both a practical solution and a methodological reference for other vertical domains."}
{"id": "2512.02289", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.02289", "abs": "https://arxiv.org/abs/2512.02289", "authors": ["Lindsey Linxi Wei", "Shreya Shankar", "Sepanta Zeighami", "Yeounoh Chung", "Fatma Ozcan", "Aditya G. Parameswaran"], "title": "Multi-Objective Agentic Rewrites for Unstructured Data Processing", "comment": "22 pages, 6 figures, 9 tables", "summary": "One year ago, we open-sourced DocETL, a declarative system for LLM-powered data processing that, as of November 2025, has 3.2K GitHub stars and users across domains (e.g., journalism, law, medicine, policy, finance, and urban planning). In DocETL, users build pipelines by composing operators described in natural language, also known as semantic operators, with an LLM executing each operator's logic. However, due to complexity in the operator or the data it operates on, LLMs often give inaccurate results. To address this challenge, DocETL introduced rewrite directives, or abstract rules that guide LLM agents in rewriting pipelines by decomposing operators or data. For example, decomposing a single filter(\"is this email sent from an executive and discussing fraud?\") into the conjunction of two separate semantic filters may improve accuracy. However, DocETL only optimizes for accuracy, not cost. How do we optimize for both?\n  We present MOAR (Multi-Objective Agentic Rewrites), a new optimizer for DocETL. To target cost optimization, we introduce two new categories of directives and extend all three existing categories with new ones, bringing the total to over 30 directives -- more than doubling what DocETL originally had. Moreover, since operators can interact with each other unpredictably due to LLM behavior, optimizing operators or sub-pipelines individually can yield suboptimal overall plans. Recognizing this, we design a new global search algorithm that explores rewrites in the context of entire pipelines. Since the space of rewrites is infinite -- pipelines can be rewritten in many ways, and each rewritten pipeline can itself be rewritten -- our algorithm adapts a multi-armed bandit framework to prioritize which pipelines to rewrite. Across six workloads, MOAR achieves 27% higher accuracy than ABACUS, the next-best optimizer, while matching its best accuracy at 55% of its cost."}
{"id": "2512.02257", "categories": ["cs.IT", "math.RT"], "pdf": "https://arxiv.org/pdf/2512.02257", "abs": "https://arxiv.org/abs/2512.02257", "authors": ["Ryan Leal", "Jingtong Sun", "Juan Pablo Vigneaux"], "title": "Entropies associated with orbits of finite groups", "comment": null, "summary": "For certain groups, parabolic subgroups appear as stabilizers of flags of sets or vector spaces. Quotients by these parabolic subgroups represent orbits of flags, and their cardinalities asymptotically reveal entropies (as rates of exponential or superexponential growth). The multiplicative \"chain rules\" that involve these cardinalities induce, asymptotically, additive analogues for entropies. Many traditional formulas in information theory correspond to quotients of symmetric groups, which are a particular kind of reflection group; in this case, the cardinalities of orbits are given by multinomial coefficients and are asymptotically related to Shannon entropy. One can treat similarly quotients of the general linear groups over a finite field; in this case, the cardinalities of orbits are given by $q$-multinomials and are asymptotically related to the Tsallis 2-entropy. In this contribution, we consider other finite reflection groups as well as the symplectic group as an example of a classical group over a finite field (groups of Lie type). In both cases, the groups are classified by Dynkin diagrams into infinite series of similar groups $A_n$, $B_n$, $C_n$, $D_n$ and a finite number of exceptional ones. The $A_n$ series consists of the symmetric groups (reflection case) and general linear groups (Lie case). Some of the other series, studied here from an information-theoretic perspective for the first time, are linked to new entropic functionals."}
{"id": "2512.02690", "categories": ["cs.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.02690", "abs": "https://arxiv.org/abs/2512.02690", "authors": ["Ruichen Luo", "Sebastian U. Stich", "Krishnendu Chatterjee"], "title": "Monotone Near-Zero-Sum Games: A Generalization of Convex-Concave Minimax", "comment": null, "summary": "Zero-sum and non-zero-sum (aka general-sum) games are relevant in a wide range of applications. While general non-zero-sum games are computationally hard, researchers focus on the special class of monotone games for gradient-based algorithms. However, there is a substantial gap between the gradient complexity of monotone zero-sum and monotone general-sum games. Moreover, in many practical scenarios of games the zero-sum assumption needs to be relaxed. To address these issues, we define a new intermediate class of monotone near-zero-sum games that contains monotone zero-sum games as a special case. Then, we present a novel algorithm that transforms the near-zero-sum games into a sequence of zero-sum subproblems, improving the gradient-based complexity for the class. Finally, we demonstrate the applicability of this new class to model practical scenarios of games motivated from the literature."}
{"id": "2512.02929", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2512.02929", "abs": "https://arxiv.org/abs/2512.02929", "authors": ["Yueyang Pan", "Meihao Liao", "Rong-Hua Li"], "title": "BD-Index: Scalable Biharmonic Distance Queries on Large Graphs via Divide-and-Conquer Indexing", "comment": null, "summary": "Biharmonic distance (\\bd) is a powerful graph distance metric with many applications, including identifying critical links in road networks and mitigating over-squashing problem in \\gnn. However, computing \\bd\\ is extremely difficult, especially on large graphs. In this paper, we focus on the problem of \\emph{single-pair} \\bd\\ query. Existing methods mainly rely on random walk-based approaches, which work well on some graphs but become inefficient when the random walk cannot mix rapidly.To overcome this issue, we first show that the biharmonic distance between two nodes $s,t$, denoted by $b(s,t)$, can be interpreted as the distance between two random walk distributions starting from $s$ and $t$. To estimate these distributions, the required random walk length is large when the underlying graph can be easily cut into smaller pieces. Inspired by this observation, we present novel formulas of \\bd to represent $b(s,t)$ by independent random walks within two node sets $\\mathcal{V}_s$, $\\mathcal{V}_t$ separated by a small \\emph{cut set} $\\mathcal{V}_{cut}$, where $\\mathcal{V}_s\\cup\\mathcal{V}_t\\cup\\mathcal{V}_{cut}=\\mathcal{V}$ is the set of graph nodes. Building upon this idea, we propose \\bindex, a novel index structure which follows a divide-and-conquer strategy. The graph is first cut into pieces so that each part can be processed easily. Then, all the required random walk probabilities can be deterministically computed in a bottom-top manner. When a query comes, only a small part of the index needs to be accessed. We prove that \\bindex\\ requires $O(n\\cdot h)$ space, can be built in $O(n\\cdot h\\cdot (h+d_{max}))$ time, and answers each query in $O(n\\cdot h)$ time, where $h$ is the height of a hierarchy partition tree and $d_{max}$ is the maximum degree, which are both usually much smaller than $n$."}
{"id": "2512.02444", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02444", "abs": "https://arxiv.org/abs/2512.02444", "authors": ["Ning Wang", "Sainyam Galhotra"], "title": "QJoin: Transformation-aware Joinable Data Discovery Using Reinforcement Learning", "comment": null, "summary": "Discovering which tables in large, heterogeneous repositories can be joined and by what transformations is a central challenge in data integration and data discovery. Traditional join discovery methods are largely designed for equi-joins, which assume that join keys match exactly or nearly so. These techniques, while efficient in clean, well-normalized databases, fail in open or federated settings where identifiers are inconsistently formatted, embedded, or split across multiple columns. Approximate or fuzzy joins alleviate minor string variations but cannot capture systematic transformations. We introduce QJoin, a reinforcement-learning framework that learns and reuses transformation strategies across join tasks. QJoin trains an agent under a uniqueness-aware reward that balances similarity with key distinctiveness, enabling it to explore concise, high-value transformation chains. To accelerate new joins, we introduce two reuse mechanisms: (i) agent transfer, which initializes new policies from pretrained agents, and (ii) transformation reuse, which caches successful operator sequences for similar column clusters. On the AutoJoin Web benchmark (31 table pairs), QJoin achieves an average F1-score of 91.0%. For 19,990 join tasks in NYC+Chicago open datasets, Qjoin reduces runtime by up to 7.4% (13,747 s) by using reusing. These results demonstrate that transformation learning and reuse can make join discovery both more accurate and more efficient."}
{"id": "2512.02325", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.02325", "abs": "https://arxiv.org/abs/2512.02325", "authors": ["Guodong Wang", "Hongwei Liu", "Jinquan Luo"], "title": "New Constructions of Non-GRS MDS Codes, Recovery and Determination Algorithms for GRS Codes", "comment": null, "summary": "In this paper, we propose a new method for constructing a class of non-GRS MDS codes. The lengths of these codes can reach up to $\\frac{q+3}{2}$ (for finite fields of odd characteristic) and $\\frac{q+4}{2}$ (for even characteristic), respectively. Owing to their special structure, we can use the Cauchy matrix method to obtain the necessary and sufficient conditions for these codes to be MDS codes and non-GRS MDS codes. Additionally, the inequivalence between these codes and twisted GRS codes is analyzed. Furthermore, we analyze the relationships among several existing classes of codes used for constructing non-GRS MDS codes, propose explicit constructions, and discuss the lengths of non-GRS MDS codes based on these constructions. Finally, we design two efficient algorithms to address two main problems in GRS code research, i.e., determining whether an unknown code $C$ is a GRS code from its generator matrix $G$, and recovering the key vectors $\\bmα$ and $\\bm{v}$ such that $C = \\GRS_{n,k}(\\bmα, \\bm{v})$ if $C$ is indeed a GRS code. A computational complexity comparison of the proposed algorithms ($O(nk+n)$) with that of the Sidelnikov-Shestakov attack (exceeding $O(qk^2n+qk^3)$) shows that our methods offer superior computational efficiency."}
{"id": "2512.02427", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2512.02427", "abs": "https://arxiv.org/abs/2512.02427", "authors": ["Hossein Nekouyan", "Bo Sun", "Raouf Boutaba", "Xiaoqi Tan"], "title": "Posted Pricing for Online Selection: Limited Price Changes and Risk Sensitivity", "comment": null, "summary": "Posted-price mechanisms (PPMs) are a widely adopted strategy for online resource allocation due to their simplicity, intuitive nature, and incentive compatibility. To manage the uncertainty inherent in online settings, PPMs commonly employ dynamically increasing prices. While this adaptive pricing achieves strong performance, it introduces practical challenges: dynamically changing prices can lead to fairness concerns stemming from price discrimination and incur operational costs associated with frequent updates. This paper addresses these issues by investigating posted pricing constrained by a limited, pre-specified number of allowed price changes, denoted by $Δ$. We further extend this framework by incorporating a second critical dimension: risk sensitivity. Instead of evaluating performance based solely on expectation, we utilize a tail-risk objective-specifically, the Conditional Value at Risk (CVaR) of the total social welfare, parameterized by a risk level $δ\\in [0, 1]$.\n  We formally introduce a novel problem class kSelection-$(δ,Δ)$ in online adversarial selection and propose a correlated PPM that utilizes a single random seed to correlate posted prices. This correlation scheme is designed to address both the limited price changes and simultaneously enhance the tail performance of the online algorithm. Our subsequent analysis provides performance guarantees under these joint constraints, revealing a clear trade-off between the number of allowed price changes and the algorithm's risk sensitivity. We also establish optimality results for several important special cases of the problem."}
{"id": "2512.02463", "categories": ["cs.DB", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.02463", "abs": "https://arxiv.org/abs/2512.02463", "authors": ["Puneet Arya", "Ojas Sahasrabudhe", "Adwaiya Srivastav", "Partha Pratim Das", "Maya Ramanath"], "title": "A Datalake for Data-driven Social Science Research", "comment": null, "summary": "Social science research increasingly demands data-driven insights, yet researchers often face barriers such as lack of technical expertise, inconsistent data formats, and limited access to reliable datasets.Social science research increasingly demands data-driven insights, yet researchers often face barriers such as lack of technical expertise, inconsistent data formats, and limited access to reliable datasets. In this paper, we present a Datalake infrastructure tailored to the needs of interdisciplinary social science research. Our system supports ingestion and integration of diverse data types, automatic provenance and version tracking, role-based access control, and built-in tools for visualization and analysis. We demonstrate the utility of our Datalake using real-world use cases spanning governance, health, and education. A detailed walkthrough of one such use case -- analyzing the relationship between income, education, and infant mortality -- shows how our platform streamlines the research process while maintaining transparency and reproducibility. We argue that such infrastructure can democratize access to advanced data science practices, especially for NGOs, students, and grassroots organizations. The Datalake continues to evolve with plans to support ML pipelines, mobile access, and citizen data feedback mechanisms."}
{"id": "2512.02332", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.02332", "abs": "https://arxiv.org/abs/2512.02332", "authors": ["Yuqing Zhu", "Yuan-Hsun Lo", "Yan Lin", "Yijin Zhang"], "title": "Age of Information for Constrained Scheduling with Imperfect Feedback", "comment": null, "summary": "This paper considers a downlink system where an access point sends the monitored status of multiple sources to multiple users. By jointly accounting for imperfect feedback and constrained transmission rate, which are key limited factors in practical systems, we aim to design scheduling algorithms to optimize the age of information (AoI) over the infinite time horizon. For zero feedback under the generate-at-will traffic, we derive a closed-form lower bound of achievable AoI, which, to the best of our knowledge, reflects the impact of zero feedback for the first time, and propose a policy that achieves this bound in many cases by jointly applying rate splitting and modular arithmetic. For zero feedback under the Bernoulli traffic, we develop a drift-plus-penalty (DPP) policy with a threshold structure based on the theory of Lyapunov optimization and provide a closed-form performance guarantee. Furthermore, we extend the design of this DPP policy to support general imperfect feedback without increasing the online computational complexity. Numerical results verify our theoretical analysis and the AoI advantage of the proposed policies over state-of-the-art policies."}
{"id": "2512.02491", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02491", "abs": "https://arxiv.org/abs/2512.02491", "authors": ["Yarden Gabbay", "Haoquan Guan", "Shaull Almagor", "El Kindi Rezig", "Brit Youngmann", "Babak Salimi"], "title": "Stress-Testing Causal Claims via Cardinality Repairs", "comment": null, "summary": "Causal analyses derived from observational data underpin high-stakes decisions in domains such as healthcare, public policy, and economics. Yet such conclusions can be surprisingly fragile: even minor data errors - duplicate records, or entry mistakes - may drastically alter causal relationships. This raises a fundamental question: how robust is a causal claim to small, targeted modifications in the data? Addressing this question is essential for ensuring the reliability, interpretability, and reproducibility of empirical findings. We introduce SubCure, a framework for robustness auditing via cardinality repairs. Given a causal query and a user-specified target range for the estimated effect, SubCure identifies a small set of tuples or subpopulations whose removal shifts the estimate into the desired range. This process not only quantifies the sensitivity of causal conclusions but also pinpoints the specific regions of the data that drive those conclusions. We formalize this problem under both tuple- and pattern-level deletion settings and show both are NP-complete. To scale to large datasets, we develop efficient algorithms that incorporate machine unlearning techniques to incrementally update causal estimates without retraining from scratch. We evaluate SubCure across four real-world datasets covering diverse application domains. In each case, it uncovers compact, high-impact subsets whose removal significantly shifts the causal conclusions, revealing vulnerabilities that traditional methods fail to detect. Our results demonstrate that cardinality repair is a powerful and general-purpose tool for stress-testing causal analyses and guarding against misleading claims rooted in ordinary data imperfections."}
{"id": "2512.02353", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.02353", "abs": "https://arxiv.org/abs/2512.02353", "authors": ["Ruizhe Wang", "Hong Ren", "Cunhua Pan", "Ruisong Weng", "Jiangzhou Wang"], "title": "A Cyclic Shift Embedded Pilot based Channel Estimation for Multi-User MIMO-OTFS systems with fractional delay and Doppler", "comment": null, "summary": "Orthogonal time frequency space (OTFS) modulation has been proposed to meet the demand for reliable communication in high-mobility scenarios for future wireless networks. However, in multi-user OTFS systems, conventional embedded pilot schemes require independent pilot allocation for each user, leading to linearly increasing pilot overhead. To address these issues, in this paper, we investigate the uplink channel estimation and pilot design for multi-user multiple-input multiple-output (MIMO)-OTFS systems. We propose a multi-dimensional decomposition-based channel estimation algorithm. Specifically, the proposed algorithm first estimates the angles of arrivals (AoAs) via subspace decomposition-based method. A spatial projection matrix, constructed from the estimated AOAs, decouples the received signal by propagation path subspace, effectively mitigating inter-path interference. The remaining fractional delay and Doppler can be obtained by a compressed sensing (CS)-based off-grid channel estimation method. Furthermore, to reduce the pilot overhead in multi-user OTFS systems, this paper proposes a novel cyclic shift embedded pilot (CSEP) structure, which can reuse users through cyclic shift-orthogonality of Zadoff-Chu (ZC) sequences. Compared with conventional embedded pilot structures, the CSEP structure can save over 30\\% of pilot overhead. Finally, an imporved channel estimation method based on the CSEP structure is proposed. Simulation results demonstrate that it achieves superior performance in channel estimation. Moreover, the proposed CSEP structure and channel estimation algorithm achieve a favorable balance between computational complexity, estimation accuracy, and bit error rate (BER) performance."}
{"id": "2512.02862", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.02862", "abs": "https://arxiv.org/abs/2512.02862", "authors": ["Jigao Luo", "Nils Boeschen", "Muhammad El-Hindi", "Carsten Binnig"], "title": "PystachIO: Efficient Distributed GPU Query Processing with PyTorch over Fast Networks & Fast Storage", "comment": null, "summary": "The AI hardware boom has led modern data centers to adopt HPC-style architectures centered on distributed, GPU-centric computation. Large GPU clusters interconnected by fast RDMA networks and backed by high-bandwidth NVMe storage enable scalable computation and rapid access to storage-resident data. Tensor computation runtimes (TCRs), such as PyTorch, originally designed for AI workloads, have recently been shown to accelerate analytical workloads. However, prior work has primarily considered settings where the data fits in aggregated GPU memory. In this paper, we systematically study how TCRs can support scalable, distributed query processing for large-scale, storage-resident OLAP workloads. Although TCRs provide abstractions for network and storage I/O, naive use often underutilizes GPU and I/O bandwidth due to insufficient overlap between computation and data movement. As a core contribution, we present PystachIO, a PyTorch-based distributed OLAP engine that combines fast network and storage I/O with key optimizations to maximize GPU, network, and storage utilization. Our evaluation shows up to 3x end-to-end speedups over existing distributed GPU-based query processing approaches."}
{"id": "2512.02397", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.02397", "abs": "https://arxiv.org/abs/2512.02397", "authors": ["Emanuele Bossi", "C. Tyler Diggans", "Abd AlRahman R. AlMomani"], "title": "Boltzmann-Shannon Index: A Geometric-Aware Measure of Clustering Balance", "comment": null, "summary": "We introduce the Boltzmann-Shannon Index (BSI), a normalized measure for clustered continuous data that captures the interaction between frequency-based and geometry-based probability distributions. Building on ideas from geometric coarse-graining and information theory, the BSI quantifies how well a partition reflects both the population of each cluster and its effective geometric extent. We illustrate its behavior on synthetic Gaussian mixtures, the Iris benchmark, and a high-imbalance resource-allocation scenario, showing that the index provides a coherent assessment even when traditional metrics give incomplete or misleading signals. Moreover, in resource-allocation settings, we demonstrate that BSI not only detects severe density-geometry inconsistency with high sensitivity, but also offers a smooth, optimization-ready objective that naturally favors allocations balancing demographic weight with each group's effective spread in the outcome space, while providing a smooth, gradient-friendly regularizer that can be easily embedded in modern policy-making and algorithmic governance optimization frameworks."}
{"id": "2512.02936", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.02936", "abs": "https://arxiv.org/abs/2512.02936", "authors": ["H. R. Paz"], "title": "From Administrative Chaos to Analytical Cohorts: A Three-Stage Normalisation Pipeline for Longitudinal University Administrative Records", "comment": "21 pages, 2 figures , 3 tables", "summary": "The growing use of longitudinal university administrative records in data-driven decision-making often overlooks a critical layer: how raw, inconsistent data are normalised before modelling. This article presents a three-stage normalisation pipeline for a dataset of 24,133 engineering students at a Latin American public university, spanning four decades (1980-2019). The pipeline comprises: (i) N1 CENSAL, harmonising demographics into a single person-level layer; (ii) N1b IDENTITY RESOLUTION, consolidating duplicate identifiers into a canonical ID while preserving an audit trail; and (iii) N1c GEO and SECONDARY-SCHOOL NORMALISATION, which builds reference tables, classifies school types (state national, state provincial, private secular, private religious), and flags irrecoverable cases as DATA_MISSING. The pipeline preserves 100% of students, achieves full geocoding, and yields valid school types for 56.6% of the population. The remaining 43.4% are identified as structurally missing due to legacy enrolment practices rather than stochastic non-response. Forensic analysis (chi-square, logistic regression) shows missingness is highly predictable from entry decade and geography, confirming a structural, historically induced mechanism. The article contributes: (a) a transparent, reproducible normalisation pipeline tailored to higher education; (b) a framework for treating structurally missing information without speculative imputation; and (c) guidance on defining analytically coherent cohorts (full population vs. secondary-school-informed subcohorts) for downstream learning analytics and policy evaluation."}
{"id": "2512.02449", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.02449", "abs": "https://arxiv.org/abs/2512.02449", "authors": ["Brendon McBain", "Yi Hong", "Emanuele Viterbo"], "title": "Optimal Handover Strategies in LEO Satellite Networks", "comment": "13 pages, 4 figures. Submitted to IEEE Transactions on Communications", "summary": "Existing theoretical analyses of satellite mega-constellations often rely on restrictive assumptions, such as short serving times, or lack tractability when evaluating realistic handover strategies. Motivated by these limitations, this paper develops a general analytical framework for accurately characterising the ergodic capacity of low Earth orbit (LEO) satellite networks under arbitrary handover strategies. Specifically, we model the transmission link as shadowed-Rician fading and introduce the persistent satellite channel, wherein the channel process is governed by an i.i.d. renewal process under mild assumptions of uncoordinated handover decisions and knowledge of satellite ephemeris and fading parameters. Within this framework, we derive the ergodic capacity (persistent capacity) of the persistent satellite channel using renewal theory and establish its relation to the non-persistent capacity studied in prior work. To address computational challenges, we present closed-form upper and lower bounds on persistent capacity. The optimal handover problem is formulated as a non-linear fractional program, obtaining an explicit decision rule via a variant of Dinkelbach's algorithm. We further demonstrate that a simpler handover strategy maximising serving capacity closely approximates the optimal strategy, providing practical insights for designing high-throughput LEO satellite communication systems."}
{"id": "2512.02461", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.02461", "abs": "https://arxiv.org/abs/2512.02461", "authors": ["Peng Zhang", "Jian Dang", "Miaowen Wen", "Ziyang Liu", "Chen Zhao", "Huaifeng Shi", "Chengsheng Pan", "Zaichen Zhang"], "title": "Artificial Noise Aided Physical Layer Security for Near-Field MIMO with Fluid Antenna Systems", "comment": null, "summary": "With the evolution of wireless systems toward large-scale arrays and high-frequency reconfigurable architectures, fluid antenna systems (FAS) operating in the near-field (NF) regime provide new degrees of freedom (DoF) for physical layer security (PLS). This paper proposes an artificial-noise (AN)-aided PLS scheme for NF fluid-antenna multiple-input multiple-output (FA-MIMO) systems, with joint beamforming (BF) and AN design for both compact and large arrays. An alternating-optimization (AO) framework addresses the sparsity-constrained non-convex design by splitting it into a continuous BF/AN joint-design subproblem and a discrete FAS port-selection subproblem. Closed-form fully digital BF/AN solutions are obtained via a generalized spectral water-filling procedure within a block coordinate descent (BCD) surrogate and realized by a hardware-efficient hybrid beamforming (HBF) architecture that embeds AN in the baseband without extra radio-frequency (RF) chains. For FAS port selection, a row-energy based prune--refit rule, aligned with Karush--Kuhn--Tucker (KKT) conditions of a group-sparsity surrogate, enables efficient active-port determination. Simulation results confirm that the proposed design exploits the geometry and position-domain DoF of FAS and significantly improves secrecy performance, particularly for non-extremely-large arrays where NF beam focusing alone is inadequate."}
{"id": "2512.02468", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.02468", "abs": "https://arxiv.org/abs/2512.02468", "authors": ["Ioannis Krikidis", "Valentin Gilbert"], "title": "Quantum Optimization in Wireless Communication Systems: Principles and Applications", "comment": "IEEE Communications Magazine, 2026", "summary": "Quantum optimization is poised to play a transformative role in the design of next-generation wireless communication systems by addressing key computational and technological challenges. This paper provides an overview of the principles of adiabatic quantum computing, the foundation of quantum optimization, and explores its two primary computational models: quantum annealing and the gate-based quantum approximate optimization algorithm. By highlighting their core features, performance benefits, limitations, and distinctions, we position these methods as promising tools for advancing wireless communication system design. As a case study, we examine the design of passive reconfigurable intelligent surface beamforming with binary phase-shift resolution, supported by experimental results obtained from real-world quantum hardware."}
{"id": "2512.02582", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.02582", "abs": "https://arxiv.org/abs/2512.02582", "authors": ["Bach Hung Luu", "Sinh Cong Lam", "Nam Hoang Nguyen"], "title": "Deep Q-Learning-Driven Power Control for Enhanced Noma User Performance", "comment": "16 pages, 5 figures", "summary": "Cell-edge users (CEUs) in cellular networks typically suffer from poor channel conditions due to long distances from serving base stations and physical obstructions, resulting in much lower data rates compared to cell-center users (CCUs). This paper proposes an Unmanned Aerial Vehicles (UAV)-assisted cellular network with intelligent power control to address the performance gap between CEUs and CCUs. Unlike conventional approaches that either deploy UAVs for all users or use no UAV assistance, our model uses a distance-based criterion where only users beyond a reference distance receive UAV relay assistance. Each UAV operates as an amplify-and-forward relay, enabling assisted users to receive signals from both the base station and the UAV simultaneously, thereby achieving diversity gain. To optimize transmission power allocation across base stations, we employ a Deep Q-Network (DQN) learning framework that learns power control policies without requiring accurate channel models. Simulation results show that the proposed approach achieves a peak average rate of 2.28 bps/Hz at the optimal reference distance of 400m, which represents a 3.6% improvement compared to networks without UAV assistance and 0.9% improvement compared to networks where all users receive UAV support. The results also reveal that UAV altitude and reference distance are critical factors affecting system performance, with lower altitudes providing better performance."}
{"id": "2512.02747", "categories": ["cs.IT", "math.CO", "math.NT"], "pdf": "https://arxiv.org/pdf/2512.02747", "abs": "https://arxiv.org/abs/2512.02747", "authors": ["Jiaxu Hu", "Kenneth J. Roche"], "title": "Digit-Indexed q-ary SEC-DED Codes with Near-Hamming Overhead", "comment": "13 pages, 1 figure, 3 tables. Interactive demo: https://sltracer.github.io/ECC_Paper_Website_Demo/index_SEC_TED_en.html", "summary": "We present a simple $q$-ary family of single-error-correcting, double-error-detecting (SEC--DED) linear codes whose parity checks are tied directly to the base-$p$ ($q=p$ prime) digits of the coordinate index. For blocklength $n=p^r$ the construction uses only $r+1$ parity checks -- \\emph{near-Hamming} overhead -- and admits an index-based decoder that runs in a single pass with constant-time location and magnitude recovery from the syndromes. Based on the prototype, we develop two extensions: Code A1, which removes specific redundant trits to achieve higher information rate and support variable-length encoding; and Code A2, which incorporates two group-sum checks together with a 3-wise XOR linear independence condition on index subsets, yielding a ternary distance-4 (SEC--TED) variant. Furthermore, we demonstrate how the framework generalizes via $n$-wise XOR linearly independent sets to construct codes with distance $d = n + 1$, notably recovering the ternary Golay code for $n = 5$ -- showing both structural generality and a serendipitous link to optimal classical codes.\n  Our contribution is not optimality but \\emph{implementational simplicity} and an \\emph{array-friendly} structure: the checks are digitwise and global sums, the mapping from syndromes to error location is explicit, and the SEC--TED upgrade is modular. We position the scheme against classical $q$-ary Hamming and SPC/product-code baselines and provide a small comparison of parity overhead, decoding work, and two-error behavior."}
{"id": "2512.02767", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.02767", "abs": "https://arxiv.org/abs/2512.02767", "authors": ["Rostislav Matveev", "Andrei Romashchenko"], "title": "Structural Properties of Entropic Vectors and Stability of the Ingleton Inequality", "comment": "25 pages", "summary": "We study constrained versions of the Ingleton inequality in the entropic setting and quantify its stability under small violations of conditional independence. Although the classical Ingleton inequality fails for general entropy profiles, it is known to hold under certain exact independence constraints. We focus on the regime where selected conditional mutual information terms are small (but not zero), and the inequality continues to hold up to controlled error terms. A central technical tool is a structural lemma that materializes part of the mutual information between two random variables, implicitly capturing the effect of infinitely many non-Shannon--type inequalities. This leads to conceptually transparent proofs without explicitly invoking such infinite families. Some of our bounds recover, in a unified way, what can also be deduced from the infinite families of inequalities of Matúš (2007) and of Dougherty--Freiling--Zeger (2011), while others appear to be new."}
{"id": "2512.02941", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.02941", "abs": "https://arxiv.org/abs/2512.02941", "authors": ["Wittawat Kositwattanarerk", "Gretchen L. Matthews", "Emily McMillon", "Tunchanok Yutitumsatit"], "title": "Pseudocodewords of quantum, quasi-cyclic, and spatially-coupled LDPC codes: a fundamental cone perspective", "comment": null, "summary": "While low-density parity-check (LDPC) codes are near capacity-achieving when paired with iterative decoders, these decoders may not output a codeword due to the existence of pseudocodewords. Thus, pseudocodewords have been studied to give insight into the performance of modern decoders including iterative and linear programming decoders. These pseudocodewords are found to be dependent on the parity-check matrix of the code and the particular decoding algorithm used. In this paper, we consider LP decoding, which has been linked to graph cover decoding, providing functions which capture these pseudocodewords. In particular, we analyze the underlying structure of pseudocodewords from quantum stabilizer codes that arise from LP decoding, quasi-cyclic LDPC codes, and spatially-coupled LDPC codes."}
