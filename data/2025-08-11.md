<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 3]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 31]
- [cs.LG](#cs.LG) [Total: 49]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.DS](#cs.DS) [Total: 6]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [User-Intent-Driven Semantic Communication via Adaptive Deep Understanding](https://arxiv.org/abs/2508.05884)
*Peigen Ye,Jingpu Duan,Hongyang Du,Yulan Guo*

Main category: cs.IT

TL;DR: 提出了一种基于用户意图的语义通信系统，通过多模态大模型和注意力模块提升语义理解与传输效率。


<details>
  <summary>Details</summary>
Motivation: 现有语义通信系统未能深入理解用户真实意图，需改进以实现意图导向的通信。

Method: 结合多模态大模型生成意图先验，提出掩码引导注意力模块和信道状态感知模块。

Result: 在瑞利信道下，PSNR、SSIM和LPIPS分别提升8%、6%和19%。

Conclusion: 该系统能深度理解用户意图，并在不同信道条件下实现高效语义通信。

Abstract: Semantic communication focuses on transmitting task-relevant semantic
information, aiming for intent-oriented communication. While existing systems
improve efficiency by extracting key semantics, they still fail to deeply
understand and generalize users' real intentions. To overcome this, we propose
a user-intention-driven semantic communication system that interprets diverse
abstract intents. First, we integrate a multi-modal large model as semantic
knowledge base to generate user-intention prior. Next, a mask-guided attention
module is proposed to effectively highlight critical semantic regions. Further,
a channel state awareness module ensures adaptive, robust transmission across
varying channel conditions. Extensive experiments demonstrate that our system
achieves deep intent understanding and outperforms DeepJSCC, e.g., under a
Rayleigh channel at an SNR of 5 dB, it achieves improvements of 8%, 6%, and 19%
in PSNR, SSIM, and LPIPS, respectively.

</details>


### [2] [On MDS Convertible Codes in the Merge Regime](https://arxiv.org/abs/2508.06219)
*Vinayak Ramkumar,Xiangliang Kong,G. Yeswanth Sai,Myna Vajha,M. Nikhil Krishnan*

Main category: cs.IT

TL;DR: 论文研究了在大规模分布式存储系统中，通过代码转换优化访问成本和带宽成本的方法，提出了三种最优访问成本的构造方案，并改进了带宽最优的构造方案。


<details>
  <summary>Details</summary>
Motivation: 在大规模分布式存储系统中，纠删码用于确保磁盘故障的可靠性。通过调整代码参数以适应不同的磁盘故障率，可以显著节省存储空间而不影响可靠性。因此，设计可转换代码（convertible codes）以实现不同参数代码之间的高效转换具有重要意义。

Method: 论文研究了将初始MDS代码的多个码字合并为最终MDS代码的单个码字的场景，提出了三种构造方案：前两种适用于特定参数条件，第三种适用于任意参数范围。此外，还提出了一种优化带宽成本的构造方案。

Result: 三种构造方案均实现了最优访问成本，第三种方案在几乎所有情况下达到了MDS猜想暗示的最小字段大小。带宽优化方案减少了子分组化，改进了现有方法。

Conclusion: 论文提出的构造方案在访问成本和带宽成本上均实现了优化，为大规模分布式存储系统中的代码转换提供了高效解决方案。

Abstract: In large-scale distributed storage systems, erasure coding is employed to
ensure reliability against disk failures. Recent work by Kadekodi et al.
demonstrates that adapting code parameters to varying disk failure rates can
lead to significant storage savings without compromising reliability. Such
adaptations, known as \emph{code conversions}, motivate the design of
\emph{convertible codes}, which enable efficient transformations between codes
of different parameters.
  In this work, we study the setting in which $\lambda$ codewords of an initial
$[n^I = k^I + r^I,\, k^I]$ MDS code are merged into a single codeword of a
final $[n^F = \lambda k^I + r^F,\, k^F = \lambda k^I]$ MDS code. We begin by
presenting three constructions that achieve optimal \emph{access cost}, defined
as the total number of disks accessed during the conversion process. The first
two constructions apply when $\lambda \leq r^I$ and impose specific
divisibility conditions on $r^I$ and the field size $q$. These schemes minimize
both the per-symbol and the overall access cost. The third construction, which
builds on a prior scheme by Kong, achieves minimal access cost while supporting
arbitrary parameter regimes. All three constructions require field sizes that
are linear in the final code length, and notably, the third construction
achieves a field size that matches the lower bound implied by the MDS
conjecture in almost all cases. In addition, we propose a construction that
optimizes the \emph{bandwidth cost}, defined as the total number of symbols
transmitted during conversion. This scheme is a refinement of Maturana and
Rashmi's bandwidth-optimal construction based on the piggybacking framework,
and achieves reduced sub-packetization.

</details>


### [3] [A New Framework for the Sum of Squared $κ$-$μ$ RVs with Application to Sub-THz Systems](https://arxiv.org/abs/2508.06242)
*Gustavo Rodrigues de Lima Tejerina,Italo Atzeni*

Main category: cs.IT

TL;DR: 本文提出了一种基于$\kappa$-$\mu$模型的子太赫兹频段传播分析方法，推导了多天线系统中接收信号功率的新表达式，并分析了其性能。


<details>
  <summary>Details</summary>
Motivation: 研究子太赫兹频段传播特性，为多天线系统提供高效的分析工具。

Method: 采用$\kappa$-$\mu$模型，推导了独立同分布随机变量平方和的精确表达式，并分析了概率密度函数、累积分布函数等。

Result: 提出了计算高效的分析框架，适用于大规模天线阵列系统，并推导了覆盖概率和误码率的表达式。

Conclusion: 该方法在子太赫兹频段的多天线系统中表现出高效性和实用性。

Abstract: In this paper, we adopt the $\kappa$-$\mu$ model to characterize the
propagation in the sub-THz band. We develop a new exact representation of the
sum of squared independent and identically distributed $\kappa$-$\mu$ random
variables, which can be used to express the power of the received signal in
multi-antenna systems. Unlike existing ones, the proposed analytical framework
is remarkably tractable and computationally efficient, and thus can be
conveniently employed to analyze systems with massive antenna arrays. We derive
novel expressions for the probability density function and cumulative
distribution function, analyze their convergence and truncation error, and
discuss the computational complexity and the implementation aspects. Moreover,
we derive expressions for the coverage probability and bit error probability
for coherent binary modulations. Lastly, we evaluate the performance of an
uplink sub-THz system where a single-antenna user is served by a base station
employing maximum ratio combining.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [4] [Stochastic Bandits for Crowdsourcing and Multi-Platform Autobidding](https://arxiv.org/abs/2508.05844)
*François Bachoc,Nicolò Cesa-Bianchi,Tommaso Cesari,Roberto Colomboni*

Main category: cs.GT

TL;DR: 论文提出了一种随机多臂老虎机模型，用于解决预算分配问题，设计了算法并证明了其遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于众包和自动竞价中的预算分配问题，需要将固定预算分配到多个任务或拍卖中。

Method: 定义了一个基于概率单纯形的随机老虎机模型，设计了算法以最小化遗憾。

Result: 算法在T步后的期望遗憾为O(K√T)，在满足额外递减收益条件时遗憾界可提升至O(K(logT)^2)。

Conclusion: 论文通过理论分析证明了算法的有效性，并展示了在不同条件下的遗憾界优化。

Abstract: Motivated by applications in crowdsourcing, where a fixed sum of money is
split among $K$ workers, and autobidding, where a fixed budget is used to bid
in $K$ simultaneous auctions, we define a stochastic bandit model where arms
belong to the $K$-dimensional probability simplex and represent the fraction of
budget allocated to each task/auction. The reward in each round is the sum of
$K$ stochastic rewards, where each of these rewards is unlocked with a
probability that varies with the fraction of the budget allocated to that
task/auction. We design an algorithm whose expected regret after $T$ steps is
of order $K\sqrt{T}$ (up to log factors) and prove a matching lower bound.
Improved bounds of order $K (\log T)^2$ are shown when the function mapping
budget to probability of unlocking the reward (i.e., terminating the task or
winning the auction) satisfies additional diminishing-returns conditions.

</details>


### [5] [An Overlapping Coalition Game Approach for Collaborative Block Mining and Edge Task Offloading in MEC-assisted Blockchain Networks](https://arxiv.org/abs/2508.06031)
*Licheng Ye,Zehui Xiong,Lin Gao,Dusit Niyato*

Main category: cs.GT

TL;DR: 该论文研究了移动边缘计算（MEC）辅助的多联盟协作区块链网络，提出了一种两阶段Stackelberg博弈模型，以优化矿工和边缘计算服务提供商的行为。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单联盟协作模式，限制了矿工的灵活性。本文探索多联盟协作模式，以提高区块链网络的效率和可扩展性。

Method: 提出两阶段Stackelberg博弈：第一阶段由边缘计算服务提供商定价；第二阶段矿工选择联盟并形成重叠联盟形成（OCF）游戏，随后联盟购买资源形成边缘资源竞争（ERC）游戏。

Result: 推导了ERC游戏的纳什均衡解，并提出OCF交替算法实现稳定联盟结构，同时为ECP设计了近最优定价策略。

Conclusion: 多联盟协作模式结合MEC技术，显著提升了区块链网络的效率、安全性和可扩展性。

Abstract: Mobile edge computing (MEC) is a promising technology that enhances the
efficiency of mobile blockchain networks, by enabling miners, often acted by
mobile users (MUs) with limited computing resources, to offload
resource-intensive mining tasks to nearby edge computing servers. Collaborative
block mining can further boost mining efficiency by allowing multiple miners to
form coalitions, pooling their computing resources and transaction data
together to mine new blocks collaboratively. Therefore, an MEC-assisted
collaborative blockchain network can leverage the strengths of both
technologies, offering improved efficiency, security, and scalability for
blockchain systems. While existing research in this area has mainly focused on
the single-coalition collaboration mode, where each miner can only join one
coalition, this work explores a more comprehensive multi-coalition
collaboration mode, which allows each miner to join multiple coalitions. To
analyze the behavior of miners and the edge computing service provider (ECP) in
this scenario, we propose a novel two-stage Stackelberg game. In Stage I, the
ECP, as the leader, determines the prices of computing resources for all MUs.
In Stage II, each MU decides the coalitions to join, resulting in an
overlapping coalition formation (OCF) game; Subsequently, each coalition
decides how many edge computing resources to purchase from the ECP, leading to
an edge resource competition (ERC) game. We derive the closed-form Nash
equilibrium for the ERC game, based on which we further propose an OCF-based
alternating algorithm to achieve a stable coalition structure for the OCF game
and develop a near-optimal pricing strategy for the ECP's resource pricing
problem.

</details>


### [6] [Social Welfare in Battery Charging Games](https://arxiv.org/abs/2508.06320)
*Simon Krogmann,Pascal Lenzner,Alexander Skopalik,Tobias Sträubig*

Main category: cs.GT

TL;DR: 论文探讨了分散式可再生能源市场中家用电池的博弈论问题，研究了价格激励对均衡和社会福利的影响。


<details>
  <summary>Details</summary>
Motivation: 分散式可再生能源和家用电池的普及带来了电网供需平衡的挑战，需要从博弈论角度分析自私行为对电网的影响。

Method: 采用Stackelberg博弈模型，研究第三方价格激励对电池充放电策略的影响，分析均衡存在性和社会福利。

Result: 均衡存在性取决于定价策略，社会福利差异显著，需更复杂的市场模型和定价机制。

Conclusion: 研究为可再生能源网络中的激励机制开辟了新方向，未来需进一步探索算法博弈论的应用。

Abstract: The recent rise of renewable energy produced by many decentralized sources
yields interesting market design challenges for electrical grids. Balancing
supply and demand in such networks is both a temporal and spatial challenge due
to capacity constraints. The recent surge in the number of household-owned
batteries, especially in regions with rooftop solar adoption, offers mitigation
potential but often acts misaligned with grid-level objectives. In fact, the
decision to charge or discharge a household-owned battery is a strategic choice
by each battery owner governed by selfish incentives. This calls for an
analysis from a game-theoretic point of view.
  We initiate this timely research direction by considering a game-theoretic
setting where selfish agents strategically charge or discharge their batteries
to increase their profit. In particular, we study a Stackelberg-like market
model where a third party introduces price incentives, aiming to optimize
renewable energy utilization while preserving grid feasibility. For this, we
study the existence and the quality of equilibria under various pricing
strategies. We find that the existence of equilibria crucially depends on the
chosen pricing and that the obtained social welfare varies widely. This calls
for more sophisticated market models and pricing mechanisms and opens up a rich
field for future research in Algorithmic Game Theory on incentives in renewable
energy networks.

</details>


### [7] [A Geometric Analysis of Gains from Trade](https://arxiv.org/abs/2508.06469)
*Jason Hartline,Kangning Wang*

Main category: cs.GT

TL;DR: 论文通过几何方法证明随机提议机制在双边交易中是最优收益的4倍近似，并进一步优化至3.15倍近似。


<details>
  <summary>Details</summary>
Motivation: 研究双边交易中随机提议机制的性能，探索其与最优收益的近似关系。

Method: 采用几何分析方法，对随机提议机制的性能进行理论推导和优化。

Result: 初始证明为4倍近似，进一步优化后达到3.15倍近似。

Conclusion: 几何方法有效验证了随机提议机制的近似性能，并实现了更优的近似比。

Abstract: We provide a geometric proof that the random proposer mechanism is a
$4$-approximation to the first-best gains from trade in bilateral exchange. We
then refine this geometric analysis to recover the state-of-the-art
approximation ratio of $3.15$.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [8] [A Cross-Perspective Annotated Dataset for Dynamic Object-Level Attention Modeling in Cloud Gaming](https://arxiv.org/abs/2508.06077)
*Hongqin Lei,Haowei Tang,Zhe Zhang*

Main category: cs.DB

TL;DR: 论文提出一个基于GTA V游戏的数据集，关注玩家兴趣对象的语义关系和特征，分析影响玩家兴趣的主要因素。


<details>
  <summary>Details</summary>
Motivation: 现有数据集通常忽略对象的语义关系和独特特征，影响深度学习方法的效率，因此需要更全面的数据集。

Method: 通过收集GTA V游戏片段并标注玩家兴趣对象，分析影响玩家兴趣的因素。

Result: 发现玩家游戏内速度、对象大小和对象速度是影响玩家兴趣的主要因素。

Conclusion: 该数据集为云游戏中的深度学习研究提供了更全面的数据支持。

Abstract: Cloud gaming has gained popularity as it provides high-quality gaming
experiences on thin hardware, such as phones and tablets. Transmitting gameplay
frames at high resolutions and ultra-low latency is the key to guaranteeing
players' quality of experience (QoE). Numerous studies have explored deep
learning (DL) techniques to address this challenge. The efficiency of these
DL-based approaches is highly affected by the dataset. However, existing
datasets usually focus on the positions of objects while ignoring semantic
relationships with other objects and their unique features. In this paper, we
present a game dataset by collecting gameplay clips from Grand Theft Auto (GTA)
V, and annotating the player's interested objects during the gameplay. Based on
the collected data, we analyze several factors that have an impact on player's
interest and identify that the player's in-game speed, object's size, and
object's speed are the main factors. The dataset is available at
https://drive.google.com/drive/folders/1idH251a2K-hGGd3pKjX-3Gx5o_rUqLC4?usp=sharing

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [9] [Semantic Item Graph Enhancement for Multimodal Recommendation](https://arxiv.org/abs/2508.06154)
*Xiaoxiong Zhang,Xin Zhou,Zhiwei Zeng,Dusit Niyato,Zhiqi Shen*

Main category: cs.IR

TL;DR: 论文提出了一种多模态推荐系统框架，通过增强语义建模和减少结构噪声来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在构建模态特定的物品语义图时存在语义缺陷，包括协作信号建模不足和原始模态特征噪声导致的结构失真。

Method: 提取交互图中的协作信号注入语义图，设计基于模量的个性化嵌入扰动机制和双重表示对齐机制。

Result: 在四个基准数据集上的实验验证了框架的有效性。

Conclusion: 提出的方法通过增强语义建模和减少噪声影响，显著提升了多模态推荐系统的性能。

Abstract: Multimodal recommendation systems have attracted increasing attention for
their improved performance by leveraging items' multimodal information. Prior
methods often build modality-specific item-item semantic graphs from raw
modality features and use them as supplementary structures alongside the
user-item interaction graph to enhance user preference learning. However, these
semantic graphs suffer from semantic deficiencies, including (1) insufficient
modeling of collaborative signals among items and (2) structural distortions
introduced by noise in raw modality features, ultimately compromising
performance. To address these issues, we first extract collaborative signals
from the interaction graph and infuse them into each modality-specific item
semantic graph to enhance semantic modeling. Then, we design a modulus-based
personalized embedding perturbation mechanism that injects perturbations with
modulus-guided personalized intensity into embeddings to generate contrastive
views. This enables the model to learn noise-robust representations through
contrastive learning, thereby reducing the effect of structural noise in
semantic graphs. Besides, we propose a dual representation alignment mechanism
that first aligns multiple semantic representations via a designed Anchor-based
InfoNCE loss using behavior representations as anchors, and then aligns
behavior representations with the fused semantics by standard InfoNCE, to
ensure representation consistency. Extensive experiments on four benchmark
datasets validate the effectiveness of our framework.

</details>


### [10] [Request-Only Optimization for Recommendation Systems](https://arxiv.org/abs/2508.05640)
*Liang Guo,Wei Li,Lucy Liao,Huihui Cheng,Rui Zhang,Yu Shi,Yueming Wang,Yanzun Huang,Keke Zhai,Pengchao Wang,Timothy Shi,Xuan Cao,Shengzhi Wang,Renqin Cai,Zhaojie Gong,Omkar Vichare,Rui Jian,Leon Gao,Shiyan Deng,Xingyu Liu,Xiong Zhang,Fu Li,Wenlei Xie,Bin Wen,Rui Li,Xing Liu,Jiaqi Zhai*

Main category: cs.IR

TL;DR: 论文提出了一种名为ROO（Request-Only Optimizations）的训练和建模范式，通过以用户请求为单位优化数据存储、训练效率和模型质量。


<details>
  <summary>Details</summary>
Motivation: 工业级深度推荐模型（DLRM）规模庞大，需要处理海量数据和复杂计算，现有方法在存储和训练效率上存在瓶颈。

Method: ROO范式通过协同设计数据（仅请求数据）、基础设施（基于请求的数据处理管道）和模型架构（仅请求的神经网络架构），以用户请求为训练单位。

Result: ROO实现了数据存储的节省，并通过减少重复计算和通信，支持更复杂的神经网络架构（如生成式推荐器）。

Conclusion: ROO范式显著提升了推荐系统的存储、训练效率和模型质量，为大规模推荐系统提供了新的优化方向。

Abstract: Deep Learning Recommendation Models (DLRMs) represent one of the largest
machine learning applications on the planet. Industry-scale DLRMs are trained
with petabytes of recommendation data to serve billions of users every day. To
utilize the rich user signals in the long user history, DLRMs have been scaled
up to unprecedented complexity, up to trillions of floating-point operations
(TFLOPs) per example. This scale, coupled with the huge amount of training
data, necessitates new storage and training algorithms to efficiently improve
the quality of these complex recommendation systems. In this paper, we present
a Request-Only Optimizations (ROO) training and modeling paradigm. ROO
simultaneously improves the storage and training efficiency as well as the
model quality of recommendation systems. We holistically approach this
challenge through co-designing data (i.e., request-only data), infrastructure
(i.e., request-only based data processing pipeline), and model architecture
(i.e., request-only neural architectures). Our ROO training and modeling
paradigm treats a user request as a unit of the training data. Compared with
the established practice of treating a user impression as a unit, our new
design achieves native feature deduplication in data logging, consequently
saving data storage. Second, by de-duplicating computations and communications
across multiple impressions in a request, this new paradigm enables highly
scaled-up neural network architectures to better capture user interest signals,
such as Generative Recommenders (GRs) and other request-only friendly
architectures.

</details>


### [11] [Query-Aware Graph Neural Networks for Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05647)
*Vibhor Agrawal,Fay Wang,Rishi Puri*

Main category: cs.IR

TL;DR: 提出了一种新型图神经网络（GNN）架构，用于检索增强生成（RAG），通过查询感知注意力机制和学习评分头提升复杂多跳问题的检索准确性。


<details>
  <summary>Details</summary>
Motivation: 传统密集检索方法将文档视为独立实体，无法捕捉文本块之间的序列和语义关系，限制了复杂问题的检索效果。

Method: 构建每集知识图谱，结合查询引导的图注意力网络（Enhanced Graph Attention Network），动态聚焦与查询相关的图部分。

Result: 实验表明，该方法在复杂问答任务中显著优于标准密集检索器，尤其适用于需要多文档推理的问题。

Conclusion: 该方法通过PyTorch Geometric高效处理图结构数据，适用于生产检索系统的可扩展部署。

Abstract: We present a novel graph neural network (GNN) architecture for
retrieval-augmented generation (RAG) that leverages query-aware attention
mechanisms and learned scoring heads to improve retrieval accuracy on complex,
multi-hop questions. Unlike traditional dense retrieval methods that treat
documents as independent entities, our approach constructs per-episode
knowledge graphs that capture both sequential and semantic relationships
between text chunks. We introduce an Enhanced Graph Attention Network with
query-guided pooling that dynamically focuses on relevant parts of the graph
based on user queries. Experimental results demonstrate that our approach
significantly outperforms standard dense retrievers on complex question
answering tasks, particularly for questions requiring multi-document reasoning.
Our implementation leverages PyTorch Geometric for efficient processing of
graph-structured data, enabling scalable deployment in production retrieval
systems

</details>


### [12] [AquiLLM: a RAG Tool for Capturing Tacit Knowledge in Research Groups](https://arxiv.org/abs/2508.05648)
*Chandler Campbell,Bernie Boscoe,Tuan Do*

Main category: cs.IR

TL;DR: AquiLLM是一个轻量级、模块化的RAG系统，旨在帮助研究团队更有效地捕获和检索分布式知识，包括非正式和私密资源。


<details>
  <summary>Details</summary>
Motivation: 研究团队在管理分布式知识（尤其是非正式和私密资源）时面临挑战，现有RAG系统多关注公开文档，忽视隐私需求。

Method: 开发AquiLLM，支持多种文档类型和可配置的隐私设置，以增强对正式和非正式知识的访问。

Result: AquiLLM为研究团队提供了更有效的知识管理工具，解决了隐私和多样性文档类型的需求。

Conclusion: AquiLLM填补了现有RAG系统的不足，为研究团队提供了更全面的知识管理解决方案。

Abstract: Research groups face persistent challenges in capturing, storing, and
retrieving knowledge that is distributed across team members. Although
structured data intended for analysis and publication is often well managed,
much of a group's collective knowledge remains informal, fragmented, or
undocumented--often passed down orally through meetings, mentoring, and
day-to-day collaboration. This includes private resources such as emails,
meeting notes, training materials, and ad hoc documentation. Together, these
reflect the group's tacit knowledge--the informal, experience-based expertise
that underlies much of their work. Accessing this knowledge can be difficult,
requiring significant time and insider understanding. Retrieval-augmented
generation (RAG) systems offer promising solutions by enabling users to query
and generate responses grounded in relevant source material. However, most
current RAG-LLM systems are oriented toward public documents and overlook the
privacy concerns of internal research materials. We introduce AquiLLM
(pronounced ah-quill-em), a lightweight, modular RAG system designed to meet
the needs of research groups. AquiLLM supports varied document types and
configurable privacy settings, enabling more effective access to both formal
and informal knowledge within scholarly groups.

</details>


### [13] [AI Guided Accelerator For Search Experience](https://arxiv.org/abs/2508.05649)
*Jayanth Yetukuri,Mehran Elyasi,Samarth Agrawal,Aritra Mandal,Rui Kong,Harish Vempati,Ishita Khan*

Main category: cs.IR

TL;DR: 论文提出了一种新框架，通过建模用户搜索过程中的过渡查询，结合生成式大语言模型（LLMs），提升电商环境中搜索的相关性和用户参与度。


<details>
  <summary>Details</summary>
Motivation: 传统方法将查询改写视为孤立对，无法捕捉用户行为的序列和动态变化。

Method: 通过挖掘用户交互日志中的查询轨迹，建模过渡查询，并利用LLMs生成语义多样的替代查询。

Result: 实验证明该方法在转化率和用户参与度上优于现有相关搜索模块。

Conclusion: 该框架有效提升了电商搜索的意图理解和用户体验。

Abstract: Effective query reformulation is pivotal in narrowing the gap between a
user's exploratory search behavior and the identification of relevant products
in e-commerce environments. While traditional approaches predominantly model
query rewrites as isolated pairs, they often fail to capture the sequential and
transitional dynamics inherent in real-world user behavior. In this work, we
propose a novel framework that explicitly models transitional
queries--intermediate reformulations occurring during the user's journey toward
their final purchase intent. By mining structured query trajectories from
eBay's large-scale user interaction logs, we reconstruct query sequences that
reflect shifts in intent while preserving semantic coherence. This approach
allows us to model a user's shopping funnel, where mid-journey transitions
reflect exploratory behavior and intent refinement. Furthermore, we incorporate
generative Large Language Models (LLMs) to produce semantically diverse and
intent-preserving alternative queries, extending beyond what can be derived
through collaborative filtering alone. These reformulations can be leveraged to
populate Related Searches or to power intent-clustered carousels on the search
results page, enhancing both discovery and engagement. Our contributions
include (i) the formal identification and modeling of transitional queries,
(ii) the introduction of a structured query sequence mining pipeline for intent
flow understanding, and (iii) the application of LLMs for scalable,
intent-aware query expansion. Empirical evaluation demonstrates measurable
gains in conversion and engagement metrics compared to the existing Related
Searches module, validating the effectiveness of our approach in real-world
e-commerce settings.

</details>


### [14] [OmniBench-RAG: A Multi-Domain Evaluation Platform for Retrieval-Augmented Generation Tools](https://arxiv.org/abs/2508.05650)
*Jiaxuan Liang,Shide Zhou,Kailong Wang*

Main category: cs.IR

TL;DR: OmniBench RAG是一个自动化平台，用于多领域评估检索增强生成（RAG）系统，提供标准化指标以量化性能和效率增益。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法缺乏领域覆盖、精细指标和标准化框架，难以比较RAG在不同模型和领域的有效性。

Method: 平台引入动态测试生成、模块化评估流程和自动知识库构建，量化准确性和效率增益。

Result: 评估显示RAG效果在不同领域差异显著，文化领域表现优异，数学领域则有所下降。

Conclusion: 系统化、领域感知的评估对RAG性能至关重要，OmniBench RAG为此提供了标准化解决方案。

Abstract: While Retrieval Augmented Generation (RAG) is now widely adopted to enhance
LLMs, evaluating its true performance benefits in a reproducible and
interpretable way remains a major hurdle. Existing methods often fall short:
they lack domain coverage, employ coarse metrics that miss sub document
precision, and fail to capture computational trade offs. Most critically, they
provide no standardized framework for comparing RAG effectiveness across
different models and domains.
  We introduce OmniBench RAG, a novel automated platform for multi domain
evaluation of RAG systems. The platform quantifies performance gains across
accuracy and efficiency dimensions, spanning nine knowledge fields including
culture, geography, and health. We introduce two standardized metrics:
Improvements (accuracy gains) and Transformation (efficiency differences
between pre RAG and post RAG models), enabling reproducible comparisons across
models and tasks. The platform features dynamic test generation, modular
evaluation pipelines, and automated knowledge base construction. Our evaluation
reveals striking variability in RAG effectiveness, from significant gains in
culture to declines in mathematics, highlighting the critical importance of
systematic, domain aware assessment. A demonstration video is available at:
https://www.youtube.com/watch?v=BZx83QFcTCI. Code and datasets:
https://github.com/Garnett-Liang/Omnibench-RAG.

</details>


### [15] [Lessons from A Large Language Model-based Outdoor Trail Recommendation Chatbot with Retrieval Augmented Generation](https://arxiv.org/abs/2508.05652)
*Julia Ann Mathew,Suining He*

Main category: cs.IR

TL;DR: 论文探讨了基于LLM和RAG的户外步道推荐聊天机器人Judy的开发，解决了提供准确信息和高效推荐服务的挑战。


<details>
  <summary>Details</summary>
Motivation: 户外休闲活动的增加催生了对话AI系统的需求，需解决准确提供信息和高效推荐的问题。

Method: 开发了基于LLM和RAG的聊天机器人Judy，通过案例研究（康涅狄格州步道）验证，包括数据收集、管理和模型性能研究。

Result: 实验证明Judy在基于LLM和RAG的推荐中具有准确性、有效性和可用性。

Conclusion: Judy展示了LLM与RAG结合在户外步道推荐中的潜力。

Abstract: The increasing popularity of outdoor recreational activities (such as hiking
and biking) has boosted the demand for a conversational AI system to provide
informative and personalized suggestion on outdoor trails. Challenges arise in
response to (1) how to provide accurate outdoor trail information via
conversational AI; and (2) how to enable usable and efficient recommendation
services. To address above, this paper discusses the preliminary and practical
lessons learned from developing Judy, an outdoor trail recommendation chatbot
based on the large language model (LLM) with retrieval augmented generation
(RAG). To gain concrete system insights, we have performed case studies with
the outdoor trails in Connecticut (CT), US. We have conducted web-based data
collection, outdoor trail data management, and LLM model performance studies on
the RAG-based recommendation. Our experimental results have demonstrated the
accuracy, effectiveness, and usability of Judy in recommending outdoor trails
based on the LLM with RAG.

</details>


### [16] [Comparison of Information Retrieval Techniques Applied to IT Support Tickets](https://arxiv.org/abs/2508.05654)
*Leonardo Santiago Benitez Pereira,Robinson Pizzio,Samir Bonho*

Main category: cs.IR

TL;DR: 该论文比较了11种信息检索技术在IT支持工单数据集上的表现，目标是开发一个辅助IT支持分析师的软件。Sentence-BERT多语言变体表现最佳，推荐准确率达78.7%。


<details>
  <summary>Details</summary>
Motivation: IT帮助台系统作为连接IT人员和用户的中心枢纽，需要高效的信息检索技术来提升支持效率。

Method: 比较了11种信息检索技术（如Sentence-BERT、TF-IDF、Word2vec、LDA）在IT支持工单数据集上的性能。

Result: Sentence-BERT多语言变体（distilluse-base-multilingual-cased-v1）表现最佳，准确率为78.7%。其他技术如TF-IDF（69.0%）、Word2vec（68.7%）和LDA（66.3%）也表现稳定。

Conclusion: 论文展示了支持工单检索系统的实用性，并提出了一个新颖的评估指标，以更贴近IT分析师对检索质量的感知。

Abstract: Institutions dependent on IT services and resources acknowledge the crucial
significance of an IT help desk system, that act as a centralized hub
connecting IT staff and users for service requests. Employing various Machine
Learning models, these IT help desk systems allow access to corrective actions
used in the past, but each model has different performance when applied to
different datasets. This work compares eleven Information Retrieval techniques
in a dataset of IT support tickets, with the goal of implementing a software
that facilitates the work of Information Technology support analysts. The best
results were obtained with the Sentence-BERT technique, in its multi-language
variation distilluse-base-multilingual-cased-v1, where 78.7% of the
recommendations made by the model were considered relevant. TF-IDF (69.0%),
Word2vec (68.7%) and LDA (66.3%) techniques also had consistent results.
Furthermore, the used datasets and essential parts of coding have been
published and made open source. It also demonstrated the practicality of a
support ticket recovery system by implementing a minimal viable prototype, and
described in detail the implementation of the system. Finally, this work
proposed a novel metric for comparing the techniques, whose aim is to closely
reflect the perception of the IT analysts about the retrieval quality.

</details>


### [17] [Beyond Single Labels: Improving Conversational Recommendation through LLM-Powered Data Augmentation](https://arxiv.org/abs/2508.05657)
*Haozhe Xu,Xiaohua Wang,Changze Lv,Xiaoqing Zheng*

Main category: cs.IR

TL;DR: 论文提出了一种解决对话推荐系统中假阴性问题的新数据增强框架，通过LLM语义检索和两阶段训练策略提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 对话推荐系统（CRS）在多轮对话中捕捉用户偏好，但假阴性问题导致推荐效果不佳。数据增强虽直观，但需平衡语义相关性和协作信息。

Method: 提出基于LLM的语义检索器筛选语义相关项，并通过相关性评分器去噪；采用两阶段训练策略平衡语义与协作信息。

Result: 在两个基准数据集和用户模拟器上的实验表明，该方法显著提升了多种推荐器的性能。

Conclusion: 该框架有效解决了假阴性问题，提升了CRS的推荐质量。

Abstract: Conversational recommender systems (CRSs) enhance recommendation quality by
engaging users in multi-turn dialogues, capturing nuanced preferences through
natural language interactions. However, these systems often face the false
negative issue, where items that a user might like are incorrectly labeled as
negative during training, leading to suboptimal recommendations.Expanding the
label set through data augmentation presents an intuitive solution but faces
the challenge of balancing two key aspects: ensuring semantic relevance and
preserving the collaborative information inherent in CRS datasets. To address
these issues, we propose a novel data augmentation framework that first
leverages an LLM-based semantic retriever to identify diverse and semantically
relevant items, which are then filtered by a relevance scorer to remove noisy
candidates. Building on this, we introduce a two-stage training strategy
balancing semantic relevance and collaborative information. Extensive
experiments on two benchmark datasets and user simulators demonstrate
significant and consistent performance improvements across various
recommenders, highlighting the effectiveness of our approach in advancing CRS
performance.

</details>


### [18] [Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review](https://arxiv.org/abs/2508.05660)
*Aditya Nagori,Ricardo Accorsi Casonatto,Ayush Gautam,Abhinav Manikantha Sai Cheruvu,Rishikesan Kamaleswaran*

Main category: cs.IR

TL;DR: 论文提出了一种动态混合检索增强生成（RAG）系统，通过自主代理动态选择图检索或向量检索，实时调整生成内容，并量化不确定性，显著提升了检索相关性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 科学文献的激增使传统综述方法面临挑战，需要结合结构化元数据和全文分析的工具。现有混合RAG系统通常是静态的，依赖专有工具且缺乏不确定性估计。

Method: 系统整合了PubMed、arXiv和Google Scholar的数据，构建了基于Neo4j的引用知识图和FAISS向量存储，使用Llama-3.3-70B代理动态选择检索方式，并通过指令调优优化生成。

Result: 在模拟真实查询的基准测试中，系统在多个指标上显著优于基线，如上下文召回率提升0.63，整体上下文精确度提升0.56。

Conclusion: 该系统通过动态编排和不确定性量化，提升了异构数据源上的推理能力，为自主科学发现提供了可扩展框架。

Abstract: The surge in scientific publications challenges traditional review methods,
demanding tools that integrate structured metadata with full-text analysis.
Hybrid Retrieval Augmented Generation (RAG) systems, combining graph queries
with vector search offer promise but are typically static, rely on proprietary
tools, and lack uncertainty estimates. We present an agentic approach that
encapsulates the hybrid RAG pipeline within an autonomous agent capable of (1)
dynamically selecting between GraphRAG and VectorRAG for each query, (2)
adapting instruction-tuned generation in real time to researcher needs, and (3)
quantifying uncertainty during inference. This dynamic orchestration improves
relevance, reduces hallucinations, and promotes reproducibility.
  Our pipeline ingests bibliometric open-access data from PubMed, arXiv, and
Google Scholar APIs, builds a Neo4j citation-based knowledge graph (KG), and
embeds full-text PDFs into a FAISS vector store (VS) using the all-MiniLM-L6-v2
model. A Llama-3.3-70B agent selects GraphRAG (translating queries to Cypher
for KG) or VectorRAG (combining sparse and dense retrieval with re-ranking).
Instruction tuning refines domain-specific generation, and bootstrapped
evaluation yields standard deviation for evaluation metrics.
  On synthetic benchmarks mimicking real-world queries, the Instruction-Tuned
Agent with Direct Preference Optimization (DPO) outperforms the baseline,
achieving a gain of 0.63 in VS Context Recall and a 0.56 gain in overall
Context Precision. Additional gains include 0.24 in VS Faithfulness, 0.12 in
both VS Precision and KG Answer Relevance, 0.11 in overall Faithfulness score,
0.05 in KG Context Recall, and 0.04 in both VS Answer Relevance and overall
Precision. These results highlight the system's improved reasoning over
heterogeneous sources and establish a scalable framework for autonomous,
agentic scientific discovery.

</details>


### [19] [Zero-Shot Retrieval for Scalable Visual Search in a Two-Sided Marketplace](https://arxiv.org/abs/2508.05661)
*Andre Rusli,Shoma Ishimoto,Sho Akiyama,Aman Kumar Singh*

Main category: cs.IR

TL;DR: 论文介绍了一种在Mercari C2C市场中部署的可扩展视觉搜索系统，评估了零样本图像检索的视觉语言模型，并展示了SigLIP模型在性能和实际应用中的优势。


<details>
  <summary>Details</summary>
Motivation: C2C市场的产品列表通常是非结构化和视觉驱动的，因此需要一种直观的视觉搜索系统来提升用户体验和交易效率。

Method: 系统结合了实时推理和后台索引工作流，通过统一嵌入管道和降维优化。评估了多种视觉语言模型，并与现有基线进行比较。

Result: SigLIP模型在离线评估中表现最佳，nDCG@5提升13.3%。在线A/B测试显示交易率提升40.9%。

Conclusion: 零样本模型可作为生产环境的强基线，部署高效视觉搜索系统，同时保留未来微调的灵活性。

Abstract: Visual search offers an intuitive way for customers to explore diverse
product catalogs, particularly in consumer-to-consumer (C2C) marketplaces where
listings are often unstructured and visually driven. This paper presents a
scalable visual search system deployed in Mercari's C2C marketplace, where
end-users act as buyers and sellers. We evaluate recent vision-language models
for zero-shot image retrieval and compare their performance with an existing
fine-tuned baseline. The system integrates real-time inference and background
indexing workflows, supported by a unified embedding pipeline optimized through
dimensionality reduction. Offline evaluation using user interaction logs shows
that the multilingual SigLIP model outperforms other models across multiple
retrieval metrics, achieving a 13.3% increase in nDCG@5 over the baseline. A
one-week online A/B test in production further confirms real-world impact, with
the treatment group showing substantial gains in engagement and conversion, up
to a 40.9% increase in transaction rate via image search. Our findings
highlight that recent zero-shot models can serve as a strong and practical
baseline for production use, which enables teams to deploy effective visual
search systems with minimal overhead, while retaining the flexibility to
fine-tune based on future data or domain-specific needs.

</details>


### [20] [From Static to Dynamic: A Streaming RAG Approach to Real-time Knowledge Base](https://arxiv.org/abs/2508.05662)
*Yuzhou Zhu*

Main category: cs.IR

TL;DR: Streaming RAG提出了一种动态处理实时数据流的框架，解决了静态RAG框架在内存成本、数据新鲜度和语义覆盖上的不足。


<details>
  <summary>Details</summary>
Motivation: 静态RAG框架在处理动态数据流时面临高内存成本、延迟和语义覆盖不足的问题，需要一种更高效的解决方案。

Method: 结合多向量余弦筛选、小批量聚类和基于计数器的频繁项过滤器，维护紧凑的原型集，并通过增量索引更新机制实现实时刷新。

Result: 在8个实时数据流上的实验显示，Recall@10提升3点，延迟低于15毫秒，吞吐量超过900文档/秒。在问答和摘要任务中也有显著提升。

Conclusion: Streaming RAG为检索增强技术设定了新的帕累托前沿，显著提升了动态数据流的处理效率和质量。

Abstract: Dynamic streams from news feeds, social media, sensor networks, and financial
markets challenge static RAG frameworks. Full-scale indices incur high memory
costs; periodic rebuilds introduce latency that undermines data freshness;
naive sampling sacrifices semantic coverage. We present Streaming RAG, a
unified pipeline that combines multi-vector cosine screening, mini-batch
clustering, and a counter-based heavy-hitter filter to maintain a compact
prototype set. We further prove an approximation bound \$E\[R(K\_t)] \ge R^\* -
L \Delta\$ linking retrieval quality to clustering variance. An incremental
index upsert mechanism refreshes prototypes without interrupting queries.
Experiments on eight real-time streams show statistically significant gains in
Recall\@10 (up to 3 points, p < 0.01), end-to-end latency below 15 ms, and
throughput above 900 documents per second under a 150 MB budget. Hyperparameter
sensitivity analysis over cluster count, admission probability, relevance
threshold, and counter capacity validates default settings. In open-domain
question answering with GPT-3.5 Turbo, we record 3.2-point gain in Exact Match
and 2.8-point gain in F1 on SQuAD; abstractive summarization yields ROUGE-L
improvements. Streaming RAG establishes a new Pareto frontier for retrieval
augmentation.

</details>


### [21] [Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer Support](https://arxiv.org/abs/2508.05664)
*Hei Yu Chan,Kuok Tou Ho,Chenglong Ma,Yujing Si,Hok Lai Lin,Sa Lei Lam*

Main category: cs.IR

TL;DR: 论文评估了多种技术（如查询重写、RAG Fusion、意图识别等）在电力领域客户服务系统中的表现，最终选择基于图的RAG框架，并结合意图识别、RAG Fusion和重排技术，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有AI客服系统在处理模糊、多意图或细节查询时的不足，提升电力领域客户支持系统的鲁棒性。

Method: 比较向量存储和图基RAG框架，测试查询重写、RAG Fusion、关键词增强、意图识别和上下文重排等技术。

Result: 最终系统在GPT-4生成数据集和真实FAQ数据集上分别达到97.9%和89.6%的准确率，显著优于基线模型。

Conclusion: 基于图的RAG框架结合意图识别、RAG Fusion和重排技术，能有效处理复杂查询，提升客户服务系统的性能。

Abstract: Many AI customer service systems use standard NLP pipelines or finetuned
language models, which often fall short on ambiguous, multi-intent, or
detail-specific queries. This case study evaluates recent techniques: query
rewriting, RAG Fusion, keyword augmentation, intent recognition, and context
reranking, for building a robust customer support system in the electric power
domain. We compare vector-store and graph-based RAG frameworks, ultimately
selecting the graph-based RAG for its superior performance in handling complex
queries. We find that query rewriting improves retrieval for queries using
non-standard terminology or requiring precise detail. RAG Fusion boosts
performance on vague or multifaceted queries by merging multiple retrievals.
Reranking reduces hallucinations by filtering irrelevant contexts. Intent
recognition supports the decomposition of complex questions into more targeted
sub-queries, increasing both relevance and efficiency. In contrast, keyword
augmentation negatively impacts results due to biased keyword selection. Our
final system combines intent recognition, RAG Fusion, and reranking to handle
disambiguation and multi-source queries. Evaluated on both a GPT-4-generated
dataset and a real-world electricity provider FAQ dataset, it achieves 97.9%
and 89.6% accuracy respectively, substantially outperforming baseline RAG
models.

</details>


### [22] [HySemRAG: A Hybrid Semantic Retrieval-Augmented Generation Framework for Automated Literature Synthesis and Methodological Gap Analysis](https://arxiv.org/abs/2508.05666)
*Alejandro Godinez*

Main category: cs.IR

TL;DR: HySemRAG结合ETL与RAG，通过多层检索、自校正框架和后验证，实现大规模文献合成与方法论研究缺口识别。


<details>
  <summary>Details</summary>
Motivation: 解决现有RAG架构的局限性，提升文献合成的自动化与可验证性。

Method: 采用混合检索（语义搜索、关键词过滤、知识图谱遍历）、自校正框架和引用验证，分八阶段处理文献。

Result: 在643次测试中，语义相似度得分提高35.1%，单次通过率68.3%，引用准确率99%。

Conclusion: HySemRAG在科学领域加速证据合成与发现方面具有广泛应用潜力。

Abstract: We present HySemRAG, a framework that combines Extract, Transform, Load (ETL)
pipelines with Retrieval-Augmented Generation (RAG) to automate large-scale
literature synthesis and identify methodological research gaps. The system
addresses limitations in existing RAG architectures through a multi-layered
approach: hybrid retrieval combining semantic search, keyword filtering, and
knowledge graph traversal; an agentic self-correction framework with iterative
quality assurance; and post-hoc citation verification ensuring complete
traceability. Our implementation processes scholarly literature through eight
integrated stages: multi-source metadata acquisition, asynchronous PDF
retrieval, custom document layout analysis using modified Docling architecture,
bibliographic management, LLM-based field extraction, topic modeling, semantic
unification, and knowledge graph construction. The system creates dual data
products - a Neo4j knowledge graph enabling complex relationship queries and
Qdrant vector collections supporting semantic search - serving as foundational
infrastructure for verifiable information synthesis. Evaluation across 643
observations from 60 testing sessions demonstrates structured field extraction
achieving 35.1% higher semantic similarity scores (0.655 $\pm$ 0.178) compared
to PDF chunking approaches (0.485 $\pm$ 0.204, p < 0.000001). The agentic
quality assurance mechanism achieves 68.3% single-pass success rates with 99.0%
citation accuracy in validated responses. Applied to geospatial epidemiology
literature on ozone exposure and cardiovascular disease, the system identifies
methodological trends and research gaps, demonstrating broad applicability
across scientific domains for accelerating evidence synthesis and discovery.

</details>


### [23] [ITDR: An Instruction Tuning Dataset for Enhancing Large Language Models in Recommendations](https://arxiv.org/abs/2508.05667)
*Zekun Liu,Xiaowen Huang,Jitao Sang*

Main category: cs.IR

TL;DR: 论文提出了一种指令调优数据集ITDR，用于提升大语言模型在推荐任务中的性能，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言处理任务中表现优异，但在推荐系统中因数据结构差异表现不佳，需要针对性优化。

Method: 构建ITDR数据集，包含7个子任务和约20万实例，基于13个公开推荐数据集和标准化模板。

Result: ITDR显著提升了主流开源大语言模型在推荐任务中的性能，并分析了任务相关性及数据规模的影响。

Conclusion: ITDR数据集有效提升了大语言模型在推荐系统中的表现，为相关研究提供了资源。

Abstract: Large language models (LLMs) have demonstrated outstanding performance in
natural language processing tasks. However, in the field of recommendation
systems, due to the structural differences between user behavior data and
natural language, LLMs struggle to effectively model the associations between
user preferences and items. Although prompt-based methods can generate
recommendation results, their inadequate understanding of recommendation tasks
leads to constrained performance. To address this gap, in this work, we
construct a sufficient instruction tuning dataset, ITDR, which encompasses 7
subtasks across two core root tasks--user-item interaction and user-item
understanding. The dataset integrates data from 13 public recommendation
datasets and is built using manually crafted standardized templates, comprising
approximately 200,000 instances. Experimental results demonstrate that ITDR
significantly enhances the performance of mainstream open-source LLMs such as
GLM-4, Qwen2.5, Qwen2.5-Instruct and LLaMA-3.2 on recommendation tasks.
Furthermore, we analyze the correlations between tasks and explore the impact
of task descriptions and data scale on instruction tuning effectiveness.
Finally, we perform comparative experiments against closed-source LLMs with
substantial parameters. Our tuning dataset ITDR and the fine-tuned large
recommendation models can be accessed at https://github.com/hellolzk/ITDR.

</details>


### [24] [A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges](https://arxiv.org/abs/2508.05668)
*Yunjia Xi,Jianghao Lin,Yongzhao Xiao,Zheli Zhou,Rong Shan,Te Gao,Jiachen Zhu,Weiwen Liu,Yong Yu,Weinan Zhang*

Main category: cs.IR

TL;DR: 本文综述了基于大语言模型（LLMs）的搜索代理，分析了其架构、优化、应用和评估，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索LLM搜索代理在深度、动态和自主信息检索中的潜力及其实际应用。

Method: 系统分析现有研究，从架构、优化、应用和评估四个角度分类。

Result: 总结了搜索代理的关键挑战，并提出了未来研究方向。

Conclusion: LLM搜索代理在信息检索领域具有广阔前景，但仍需解决开放性问题。

Abstract: The advent of Large Language Models (LLMs) has significantly revolutionized
web search. The emergence of LLM-based Search Agents marks a pivotal shift
towards deeper, dynamic, autonomous information seeking. These agents can
comprehend user intentions and environmental context and execute multi-turn
retrieval with dynamic planning, extending search capabilities far beyond the
web. Leading examples like OpenAI's Deep Research highlight their potential for
deep information mining and real-world applications. This survey provides the
first systematic analysis of search agents. We comprehensively analyze and
categorize existing works from the perspectives of architecture, optimization,
application, and evaluation, ultimately identifying critical open challenges
and outlining promising future research directions in this rapidly evolving
field. Our repository is available on
https://github.com/YunjiaXi/Awesome-Search-Agent-Papers.

</details>


### [25] [Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports](https://arxiv.org/abs/2508.05669)
*Jin Khye Tan,En Jun Choong,Ethan Jeremiah Chitty,Yan Pheng Choo,John Hsin Yang Wong,Chern Eu Cheah*

Main category: cs.IR

TL;DR: 论文提出了一种基于Qwen2.5-VL-7B的微调视觉语言模型，用于将马来西亚审计财务报告中的表格转换为Markdown格式，显著提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 财务文档中表格结构的准确提取和表示是文档理解中的关键挑战，尤其是在监管和分析场景中。

Method: 使用微调的视觉语言模型（VLM），基于Qwen2.5-VL-7B，结合增强的数据集和LoRA监督微调策略。

Result: 模型在标准评估中达到92.20%的准确率和96.53%的Markdown TEDS分数，优于其他模型。

Conclusion: 领域特定微调是高效且有效的方法，能够在不增加计算开销的情况下匹敌更大规模的通用模型。

Abstract: Accurately extracting and representing the structure of tabular data from
financial documents remains a critical challenge in document understanding,
particularly for regulatory and analytical use cases. This study addresses the
complexity of converting financial tables from Malaysian audited financial
reports into Markdown format, a task complicated by rotated layouts,
multi-level headers, and implicit structural cues. We propose a fine-tuned
vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for
high-fidelity Markdown generation from document images. Our approach includes a
curated dataset of 2,152 image-text pairs with augmentations and a supervised
fine-tuning strategy using LoRA. To assess performance, we evaluated our model
on 100 out-of-sample tables using a dual framework: a criteria-based
LLM-as-a-judge for fine-grained accuracy and our novel Markdown
Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural
fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based
assessment and a 96.53% Markdown TEDS score. This performance significantly
surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized
reasoning-enabled models. Compared to these self-hosted alternatives, it also
significantly reduces inference time. Furthermore, its accuracy exceeds that of
widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash.
These results demonstrate that domain-specific fine-tuning provides an
effective and efficient method to bridge the gap between unstructured financial
documents and downstream automation, rivalling much larger and more general
models without their computational overhead.

</details>


### [26] [LMAR: Language Model Augmented Retriever for Domain-specific Knowledge Indexing](https://arxiv.org/abs/2508.05672)
*Yao Zhao,Yantian Ding,Zhiyue Zhang,Dapeng Yao,Yanxun Xu*

Main category: cs.IR

TL;DR: LMAR框架通过LLM引导的数据合成和对比嵌入适应，解决了RAG系统在领域特定知识中的性能问题，同时保持低硬件需求和延迟。


<details>
  <summary>Details</summary>
Motivation: RAG系统在领域特定知识中表现不佳，主要由于预训练嵌入的性能下降和LLM检索器的高计算成本。

Method: LMAR采用两阶段流程：1）LLM引导的三元组采样和合成数据增强；2）对比嵌入适应和高效文本聚类。

Result: 实验表明，LMAR在多个领域特定数据集上优于基线模型，且硬件需求和延迟较低。

Conclusion: LMAR是一种实用且经济的解决方案，适用于可扩展的领域特定适应。

Abstract: Retrieval Augmented Generation (RAG) systems often struggle with
domain-specific knowledge due to performance deterioration of pre-trained
embeddings and prohibitive computational costs of large language model
(LLM)-based retrievers. While fine-tuning data augmentation embedding models
offers a promising direction, its effectiveness is limited by the need for
high-quality training data and reliable chunking strategies that preserve
contextual integrity. We propose LMAR (Language Model Augmented Retriever), a
model-agnostic framework that addresses these challenges by combining
LLM-guided data synthesis with contrastive embedding adaptation and efficient
text clustering. LMAR consists of a two-stage pipeline: (1) Triplet sampling
and synthetic data augmentation, where LLMs act as both labeler and validator
to ensure high-fidelity supervision throughout the pipeline. Experimental
results across multiple domain-specific benchmark datasets demonstrate that
LMAR outperforms multiple baseline models, while maintaining moderate hardware
requirements and low latency. Its model-agnostic nature further enables
seamless integration with emerging RAG architectures and text embedding models,
ensuring continual improvements without redesigning the pipeline. These results
highlight LMAR as a practical and cost-effective solution for scalable
domain-specific adaptation.

</details>


### [27] [Breaking the Top-$K$ Barrier: Advancing Top-$K$ Ranking Metrics Optimization in Recommender Systems](https://arxiv.org/abs/2508.05673)
*Weiqin Yang,Jiawei Chen,Shengjia Zhang,Peng Wu,Yuegang Sun,Yan Feng,Chun Chen,Can Wang*

Main category: cs.IR

TL;DR: 论文提出了一种名为SoftmaxLoss@K（SL@K）的新损失函数，用于优化推荐系统中的NDCG@K指标，解决了现有方法的计算成本高和训练不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 在推荐系统中，NDCG@K是评估性能的关键指标，但其优化存在挑战，包括不连续性和Top-K截断问题。现有方法要么忽略截断，要么计算成本高且不稳定。

Method: 提出SL@K损失函数，结合分位数技术处理Top-K截断，并推导出平滑上界以优化NDCG@K。

Result: 在四个真实数据集和三个推荐模型上，SL@K平均提升6.03%，优于现有方法。

Conclusion: SL@K具有理论保证、易实现、计算高效、梯度稳定和抗噪声等优点，是优化NDCG@K的有效方法。

Abstract: In the realm of recommender systems (RS), Top-$K$ ranking metrics such as
NDCG@$K$ are the gold standard for evaluating recommendation performance.
However, during the training of recommendation models, optimizing NDCG@$K$
poses significant challenges due to its inherent discontinuous nature and the
intricate Top-$K$ truncation. Recent efforts to optimize NDCG@$K$ have either
overlooked the Top-$K$ truncation or suffered from high computational costs and
training instability. To overcome these limitations, we propose SoftmaxLoss@$K$
(SL@$K$), a novel recommendation loss tailored for NDCG@$K$ optimization.
Specifically, we integrate the quantile technique to handle Top-$K$ truncation
and derive a smooth upper bound for optimizing NDCG@$K$ to address
discontinuity. The resulting SL@$K$ loss has several desirable properties,
including theoretical guarantees, ease of implementation, computational
efficiency, gradient stability, and noise robustness. Extensive experiments on
four real-world datasets and three recommendation backbones demonstrate that
SL@$K$ outperforms existing losses with a notable average improvement of 6.03%.
The code is available at https://github.com/Tiny-Snow/IR-Benchmark.

</details>


### [28] [Domain-Specific Fine-Tuning and Prompt-Based Learning: A Comparative Study for developing Natural Language-Based BIM Information Retrieval Systems](https://arxiv.org/abs/2508.05676)
*Han Gao,Timo Hartmann,Botao Zhong,Kai Lia,Hanbin Luo*

Main category: cs.IR

TL;DR: 比较了两种自然语言接口（NLI）在BIM信息检索中的方法：领域微调与提示学习，提出混合配置并验证其效果。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言查询在BIM环境中提取数据的挑战，探索更高效的信息检索方法。

Method: 采用两阶段框架（意图识别与表格问答），对比领域微调与提示学习，构建BIM专用数据集进行实验。

Result: 领域微调在意图识别中表现更优，提示学习（如GPT-4o）在表格问答中更强，混合配置实现更均衡性能。

Conclusion: 混合方法结合两种技术的优势，为BIM系统设计提供了实用见解。

Abstract: Building Information Modeling (BIM) is essential for managing building data
across the entire lifecycle, supporting tasks from design to maintenance.
Natural Language Interface (NLI) systems are increasingly explored as
user-friendly tools for information retrieval in Building Information Modeling
(BIM) environments. Despite their potential, accurately extracting BIM-related
data through natural language queries remains a persistent challenge due to the
complexity use queries and specificity of domain knowledge. This study presents
a comparative analysis of two prominent approaches for developing NLI-based BIM
information retrieval systems: domain-specific fine-tuning and prompt-based
learning using large language models (LLMs). A two-stage framework consisting
of intent recognition and table-based question answering is implemented to
evaluate the effectiveness of both approaches. To support this evaluation, a
BIM-specific dataset of 1,740 annotated queries of varying types across 69
models is constructed. Experimental results show that domain-specific
fine-tuning delivers superior performance in intent recognition tasks, while
prompt-based learning, particularly with GPT-4o, shows strength in table-based
question answering. Based on these findings, this study identify a hybrid
configuration that combines fine-tuning for intent recognition with
prompt-based learning for question answering, achieving more balanced and
robust performance across tasks. This integrated approach is further tested
through case studies involving BIM models of varying complexity. This study
provides a systematic analysis of the strengths and limitations of each
approach and discusses the applicability of the NLI to real-world BIM
scenarios. The findings offer insights for researchers and practitioners in
designing intelligent, language-driven BIM systems.

</details>


### [29] [Are All Genders Equal in the Eyes of Algorithms? -- Analysing Search and Retrieval Algorithms for Algorithmic Gender Fairness](https://arxiv.org/abs/2508.05680)
*Stefanie Urchs,Veronika Thurner,Matthias Aßenmacher,Ludwig Bothmann,Christian Heumann,Stephanie Thiemichen*

Main category: cs.IR

TL;DR: 论文分析了搜索引擎和信息检索平台对学术可见性的影响，提出了一种保持偏见的算法性别公平定义，并发现男性教授在搜索结果和出版物记录上更具优势。


<details>
  <summary>Details</summary>
Motivation: 研究算法系统如何可能复制或强化社会偏见，特别是性别偏见，以评估其公平性。

Method: 使用德国大学和应用科学大学的学术档案数据集，分析元数据完整性、出版物检索和谷歌搜索结果中的性别差异。

Result: 未发现明显的算法歧视，但男性教授在搜索结果和出版物记录上更占优势，女性教授的数字化可见性变异性更高。

Conclusion: 研究强调需要同时考虑技术性能和代表性平等的公平性评估。

Abstract: Algorithmic systems such as search engines and information retrieval
platforms significantly influence academic visibility and the dissemination of
knowledge. Despite assumptions of neutrality, these systems can reproduce or
reinforce societal biases, including those related to gender. This paper
introduces and applies a bias-preserving definition of algorithmic gender
fairness, which assesses whether algorithmic outputs reflect real-world gender
distributions without introducing or amplifying disparities. Using a
heterogeneous dataset of academic profiles from German universities and
universities of applied sciences, we analyse gender differences in metadata
completeness, publication retrieval in academic databases, and visibility in
Google search results. While we observe no overt algorithmic discrimination,
our findings reveal subtle but consistent imbalances: male professors are
associated with a greater number of search results and more aligned publication
records, while female professors display higher variability in digital
visibility. These patterns reflect the interplay between platform algorithms,
institutional curation, and individual self-presentation. Our study highlights
the need for fairness evaluations that account for both technical performance
and representational equality in digital systems.

</details>


### [30] [LLM4ES: Learning User Embeddings from Event Sequences via Large Language Models](https://arxiv.org/abs/2508.05688)
*Aleksei Shestov,Omar Zoloev,Maksim Makarenko,Mikhail Orlov,Egor Fadeev,Ivan Kireev,Andrey Savchenko*

Main category: cs.IR

TL;DR: LLM4ES利用预训练语言模型从事件序列生成用户嵌入，通过文本表示和微调技术提升嵌入质量，在金融等领域用户分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理低变异性领域的事件序列时表现不佳，需要一种能生成高质量用户嵌入的新框架。

Method: 将事件序列转化为文本表示，通过下一词预测微调LLM，并引入文本增强技术提升嵌入质量。

Result: LLM4ES在金融等领域的用户分类任务中表现优于现有方法。

Conclusion: LLM4ES生成的用户嵌入可广泛应用于金融用户分群和医疗结果预测等领域。

Abstract: This paper presents LLM4ES, a novel framework that exploits large pre-trained
language models (LLMs) to derive user embeddings from event sequences. Event
sequences are transformed into a textual representation, which is subsequently
used to fine-tune an LLM through next-token prediction to generate high-quality
embeddings. We introduce a text enrichment technique that enhances LLM
adaptation to event sequence data, improving representation quality for
low-variability domains. Experimental results demonstrate that LLM4ES achieves
state-of-the-art performance in user classification tasks in financial and
other domains, outperforming existing embedding methods. The resulting user
embeddings can be incorporated into a wide range of applications, from user
segmentation in finance to patient outcome prediction in healthcare.

</details>


### [31] [Multi-Faceted Large Embedding Tables for Pinterest Ads Ranking](https://arxiv.org/abs/2508.05700)
*Runze Su,Jiayin Jin,Jiacheng Li,Sihan Wang,Guangtong Bai,Zelun Wang,Li Tang,Yixiong Meng,Huasen Wu,Zhimeng Pan,Kungang Li,Han Sun,Zhifang Liu,Haoyang Li,Siping Ji,Ling Leng,Prathibha Deshikachar*

Main category: cs.IR

TL;DR: 论文提出了一种多方面的预训练方案，解决了大型嵌入表在推荐系统中的性能问题，并通过CPU-GPU混合架构提升了扩展性。


<details>
  <summary>Details</summary>
Motivation: 在Pinterest广告排名模型中集成大型嵌入表时，遇到了稀疏性、扩展性等挑战，尤其是初始训练效果不佳。

Method: 引入多方面的预训练方案，结合多种预训练算法，并设计了CPU-GPU混合服务架构。

Result: 显著提升了点击率（CTR）和转化率（CVR），在线部署后实现了1.34%的CPC降低和2.60%的CTR提升。

Conclusion: 多方面的预训练和混合架构有效解决了大型嵌入表的性能问题，为推荐系统带来了显著改进。

Abstract: Large embedding tables are indispensable in modern recommendation systems,
thanks to their ability to effectively capture and memorize intricate details
of interactions among diverse entities. As we explore integrating large
embedding tables into Pinterest's ads ranking models, we encountered not only
common challenges such as sparsity and scalability, but also several obstacles
unique to our context. Notably, our initial attempts to train large embedding
tables from scratch resulted in neutral metrics. To tackle this, we introduced
a novel multi-faceted pretraining scheme that incorporates multiple pretraining
algorithms. This approach greatly enriched the embedding tables and resulted in
significant performance improvements. As a result, the multi-faceted large
embedding tables bring great performance gain on both the Click-Through Rate
(CTR) and Conversion Rate (CVR) domains. Moreover, we designed a CPU-GPU hybrid
serving infrastructure to overcome GPU memory limits and elevate the
scalability. This framework has been deployed in the Pinterest Ads system and
achieved 1.34% online CPC reduction and 2.60% CTR increase with neutral
end-to-end latency change.

</details>


### [32] [G-UBS: Towards Robust Understanding of Implicit Feedback via Group-Aware User Behavior Simulation](https://arxiv.org/abs/2508.05709)
*Boyu Chen,Siran Chen,Zhengrong Yue,Kainan Yan,Chenyun Yu,Beibei Kong,Cheng Lei,Chengxiang Zhuo,Zang Li,Yali Wang*

Main category: cs.IR

TL;DR: 论文提出了一种基于用户群体行为的隐式反馈建模方法（G-UBS），通过群体上下文指导，提升视频推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 显式用户反馈稀缺，而隐式反馈存在噪声，容易误判用户兴趣，影响推荐效果。

Method: G-UBS包含两个关键模块：用户群体管理器（UGM）和用户反馈建模器（UFM），分别通过LLM聚类用户群体和群体感知强化学习建模反馈。

Result: 在IF-VR基准测试中，G-UBS显著优于主流模型，播放率>30%的视频比例提高4.0%，推理准确率提高14.9%。

Conclusion: G-UBS通过群体上下文指导，有效解决了隐式反馈噪声问题，提升了推荐系统的性能。

Abstract: User feedback is critical for refining recommendation systems, yet explicit
feedback (e.g., likes or dislikes) remains scarce in practice. As a more
feasible alternative, inferring user preferences from massive implicit feedback
has shown great potential (e.g., a user quickly skipping a recommended video
usually indicates disinterest). Unfortunately, implicit feedback is often
noisy: a user might skip a video due to accidental clicks or other reasons,
rather than disliking it. Such noise can easily misjudge user interests,
thereby undermining recommendation performance. To address this issue, we
propose a novel Group-aware User Behavior Simulation (G-UBS) paradigm, which
leverages contextual guidance from relevant user groups, enabling robust and
in-depth interpretation of implicit feedback for individual users.
Specifically, G-UBS operates via two key agents. First, the User Group Manager
(UGM) effectively clusters users to generate group profiles utilizing a
``summarize-cluster-reflect" workflow based on LLMs. Second, the User Feedback
Modeler (UFM) employs an innovative group-aware reinforcement learning
approach, where each user is guided by the associated group profiles during the
reinforcement learning process, allowing UFM to robustly and deeply examine the
reasons behind implicit feedback. To assess our G-UBS paradigm, we have
constructed a Video Recommendation benchmark with Implicit Feedback (IF-VR). To
the best of our knowledge, this is the first multi-modal benchmark for implicit
feedback evaluation in video recommendation, encompassing 15k users, 25k
videos, and 933k interaction records with implicit feedback. Extensive
experiments on IF-VR demonstrate that G-UBS significantly outperforms
mainstream LLMs and MLLMs, with a 4.0% higher proportion of videos achieving a
play rate > 30% and 14.9% higher reasoning accuracy on IF-VR.

</details>


### [33] [WebWatcher: Breaking New Frontiers of Vision-Language Deep Research Agent](https://arxiv.org/abs/2508.05748)
*Xinyu Geng,Peng Xia,Zhen Zhang,Xinyu Wang,Qiuchen Wang,Ruixue Ding,Chenxi Wang,Jialong Wu,Yida Zhao,Kuan Li,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.IR

TL;DR: 论文提出WebWatcher，一种多模态深度研究代理，通过增强的视觉语言推理能力和强化学习，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多为文本中心，忽略了现实世界中的视觉信息，导致多模态深度研究代理面临更高挑战。

Method: 利用高质量合成多模态轨迹进行冷启动训练，结合多种工具进行深度推理，并通过强化学习增强泛化能力。

Result: 在四个挑战性VQA基准测试中，WebWatcher显著优于专有基线、RAG工作流和开源代理。

Conclusion: WebWatcher为复杂多模态信息检索任务提供了有效解决方案，推动了该领域的发展。

Abstract: Web agents such as Deep Research have demonstrated superhuman cognitive
abilities, capable of solving highly challenging information-seeking problems.
However, most research remains primarily text-centric, overlooking visual
information in the real world. This makes multimodal Deep Research highly
challenging, as such agents require much stronger reasoning abilities in
perception, logic, knowledge, and the use of more sophisticated tools compared
to text-based agents. To address this limitation, we introduce WebWatcher, a
multi-modal Agent for Deep Research equipped with enhanced visual-language
reasoning capabilities. It leverages high-quality synthetic multimodal
trajectories for efficient cold start training, utilizes various tools for deep
reasoning, and further enhances generalization through reinforcement learning.
To better evaluate the capabilities of multimodal agents, we propose
BrowseComp-VL, a benchmark with BrowseComp-style that requires complex
information retrieval involving both visual and textual information.
Experimental results show that WebWatcher significantly outperforms proprietary
baseline, RAG workflow and open-source agents in four challenging VQA
benchmarks, which paves the way for solving complex multimodal
information-seeking tasks.

</details>


### [34] [Dual prototype attentive graph network for cross-market recommendation](https://arxiv.org/abs/2508.05969)
*Li Fan,Menglin Kong,Yang Xiang,Chong Zhang,Chengtao Ji*

Main category: cs.IR

TL;DR: 论文提出了一种名为DGRE的双原型注意力图网络方法，用于跨市场推荐系统（CMRS），通过结合市场特定和市场共享的洞察力，提升系统的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有CMRS方法主要关注单一市场的特定偏好，忽略了不同市场用户间可能存在的共享偏好，限制了系统的泛化能力。

Method: DGRE利用基于图表示学习的原型，从用户和物品两方面捕捉市场特定和共享的洞察力，包括聚类多市场用户构建共享原型，以及聚合市场内物品特征构建特定原型。

Result: 在真实跨市场数据集上的实验表明，DGRE通过结合市场特定和共享的建模，显著提升了CMRS的泛化性和鲁棒性。

Conclusion: DGRE方法通过双原型建模，有效解决了跨市场推荐中的泛化和鲁棒性问题，为CMRS提供了新的研究方向。

Abstract: Cross-market recommender systems (CMRS) aim to utilize historical data from
mature markets to promote multinational products in emerging markets. However,
existing CMRS approaches often overlook the potential for shared preferences
among users in different markets, focusing primarily on modeling specific
preferences within each market. In this paper, we argue that incorporating both
market-specific and market-shared insights can enhance the generalizability and
robustness of CMRS. We propose a novel approach called Dual Prototype Attentive
Graph Network for Cross-Market Recommendation (DGRE) to address this. DGRE
leverages prototypes based on graph representation learning from both items and
users to capture market-specific and market-shared insights. Specifically, DGRE
incorporates market-shared prototypes by clustering users from various markets
to identify behavioural similarities and create market-shared user profiles.
Additionally, it constructs item-side prototypes by aggregating item features
within each market, providing valuable market-specific insights. We conduct
extensive experiments to validate the effectiveness of DGRE on a real-world
cross-market dataset, and the results show that considering both
market-specific and market-sharing aspects in modelling can improve the
generalization and robustness of CMRS.

</details>


### [35] [Efficient Multimodal Streaming Recommendation via Expandable Side Mixture-of-Experts](https://arxiv.org/abs/2508.05993)
*Yunke Qu,Liang Qu,Tong Chen,Quoc Viet Hung Nguyen,Hongzhi Yin*

Main category: cs.IR

TL;DR: XSMoE是一种高效的多模态流推荐框架，通过轻量级扩展专家模块动态捕捉用户偏好变化，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决流推荐系统中用户兴趣漂移和新物品冷启动问题，同时避免频繁微调大型多模态编码器的高成本和遗忘长期偏好的风险。

Method: 提出XSMoE框架，通过扩展的轻量级专家模块附加到预训练编码器上，动态结合专家和主干输出，并采用基于利用率的剪枝策略。

Result: 在三个真实数据集上，XSMoE在推荐质量和计算效率上均优于现有基线方法。

Conclusion: XSMoE能够有效捕捉冷启动和兴趣漂移，同时保持模型紧凑性和高效性。

Abstract: Streaming recommender systems (SRSs) are widely deployed in real-world
applications, where user interests shift and new items arrive over time. As a
result, effectively capturing users' latest preferences is challenging, as
interactions reflecting recent interests are limited and new items often lack
sufficient feedback. A common solution is to enrich item representations using
multimodal encoders (e.g., BERT or ViT) to extract visual and textual features.
However, these encoders are pretrained on general-purpose tasks: they are not
tailored to user preference modeling, and they overlook the fact that user
tastes toward modality-specific features such as visual styles and textual
tones can also drift over time. This presents two key challenges in streaming
scenarios: the high cost of fine-tuning large multimodal encoders, and the risk
of forgetting long-term user preferences due to continuous model updates.
  To tackle these challenges, we propose Expandable Side Mixture-of-Experts
(XSMoE), a memory-efficient framework for multimodal streaming recommendation.
XSMoE attaches lightweight side-tuning modules consisting of expandable expert
networks to frozen pretrained encoders and incrementally expands them in
response to evolving user feedback. A gating router dynamically combines expert
and backbone outputs, while a utilization-based pruning strategy maintains
model compactness. By learning new patterns through expandable experts without
overwriting previously acquired knowledge, XSMoE effectively captures both cold
start and shifting preferences in multimodal features. Experiments on three
real-world datasets demonstrate that XSMoE outperforms state-of-the-art
baselines in both recommendation quality and computational efficiency.

</details>


### [36] [Improving Table Retrieval with Question Generation from Partial Tables](https://arxiv.org/abs/2508.06168)
*Hsing-Ping Liang,Che-Wei Chang,Yao-Chung Fan*

Main category: cs.IR

TL;DR: 论文提出QGpT方法，通过LLM生成基于部分表格的合成问题，增强表格与问题的语义对齐，显著提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究在检索器组件中主要优化查询表示，而忽略了表格在嵌入空间中的表示优化，导致与用户问题的语义对齐不足。

Method: 使用LLM基于部分表格生成合成问题，模拟用户查询行为，并将生成的问题与表格片段联合嵌入。

Result: 在多个基准测试中，QGpT显著提升了密集和延迟交互检索器的性能。

Conclusion: QGpT是一种简单有效的方法，通过增强表格表示与用户问题的语义对齐，显著改进检索效果。

Abstract: Recent advances in open-domain question answering over tables have widely
adopted large language models (LLMs) under the Retriever-Reader architecture.
Prior works have effectively leveraged LLMs to tackle the complex reasoning
demands of the Reader component, such as text-to-text, text-to-SQL, and multi
hop reasoning. In contrast, the Retriever component has primarily focused on
optimizing the query representation-training retrievers to retrieve relevant
tables based on questions, or to select keywords from questions for matching
table segments. However, little attention has been given to enhancing how
tables themselves are represented in embedding space to better align with
questions. To address this, we propose QGpT (Question Generation from Partial
Tables), a simple yet effective method that uses an LLM to generate synthetic
questions based on small portions of a table. These questions are generated to
simulate how a user might query the content of the table currently under
consideration. The generated questions are then jointly embedded with the
partial table segments used for generation, enhancing semantic alignment with
user queries. Without the need to embed entire tables, our method significantly
improves retrieval performance across multiple benchmarks for both dense and
late-interaction retrievers.

</details>


### [37] [M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation](https://arxiv.org/abs/2508.06328)
*Zhiyou Xiao,Qinhan Yu,Binghui Li,Geng Chen,Chong Chen,Wentao Zhang*

Main category: cs.IR

TL;DR: 论文提出了一种支持多模态输入和输出的新型框架M2IO-R1，通过强化学习优化图像选择和布局，显著提升了生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有MRAG技术仅支持单模态输出，限制了实际应用；现实需求需要多模态输入和输出以实现更有效的沟通和推理。

Method: 采用强化学习（RL）范式，提出M2IO-R1框架，核心是训练一个基于Group Relative Policy Optimization的3B参数插入器（Inserter-R1-3B），用于可控且语义对齐的图像选择和布局。

Result: 实验表明，轻量级的3B插入器在推理能力和延迟方面表现优异，质量和效率均优于基线方法。

Conclusion: M2IO-R1框架通过强化学习实现了多模态输出的高效生成，为多模态任务提供了实用解决方案。

Abstract: Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables
diverse multimodal inputs but remains limited to single-modality outputs,
restricting expressive capacity and practical utility. In contrast, real-world
applications often demand both multimodal inputs and multimodal outputs for
effective communication and grounded reasoning. Motivated by the recent success
of Reinforcement Learning (RL) in complex reasoning tasks for Large Language
Models (LLMs), we adopt RL as a principled and effective paradigm to address
the multi-step, outcome-driven challenges inherent in multimodal output
generation. Here, we introduce M2IO-R1, a novel framework for Multimodal
Retrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal
inputs and outputs. Central to our framework is an RL-based inserter,
Inserter-R1-3B, trained with Group Relative Policy Optimization to guide image
selection and placement in a controllable and semantically aligned manner.
Empirical results show that our lightweight 3B inserter achieves strong
reasoning capabilities with significantly reduced latency, outperforming
baselines in both quality and efficiency.

</details>


### [38] [eSASRec: Enhancing Transformer-based Recommendations in a Modular Fashion](https://arxiv.org/abs/2508.06450)
*Daria Tikhonovich,Nikita Zelinskiy,Aleksandr V. Petrov,Mayya Spirina,Andrei Semenov,Andrey V. Savchenko,Sergei Kuliev*

Main category: cs.IR

TL;DR: 本文提出了一种增强的SASRec模型（eSASRec），通过结合SASRec的训练目标、LiGR Transformer层和Sampled Softmax Loss，显著提升了推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在顺序推荐中表现优异，但模块化改进的叠加效果未系统评估，本文旨在填补这一空白。

Method: 结合SASRec训练目标、LiGR Transformer层和Sampled Softmax Loss，构建eSASRec模型，并进行生产环境评估。

Result: eSASRec在学术基准测试中比最新模型（如ActionPiece）效果提升23%，在生产环境中与工业模型HSTU和FuXi性能相当。

Conclusion: eSASRec易于集成到现有推荐系统中，可作为强大且简单的基线模型，开源实现已提供。

Abstract: Since their introduction, Transformer-based models, such as SASRec and
BERT4Rec, have become common baselines for sequential recommendations,
surpassing earlier neural and non-neural methods. A number of following
publications have shown that the effectiveness of these models can be improved
by, for example, slightly updating the architecture of the Transformer layers,
using better training objectives, and employing improved loss functions.
However, the additivity of these modular improvements has not been
systematically benchmarked - this is the gap we aim to close in this paper.
Through our experiments, we identify a very strong model that uses SASRec's
training objective, LiGR Transformer layers, and Sampled Softmax Loss. We call
this combination eSASRec (Enhanced SASRec). While we primarily focus on
realistic, production-like evaluation, in our preliminarily study we find that
common academic benchmarks show eSASRec to be 23% more effective compared to
the most recent state-of-the-art models, such as ActionPiece. In our main
production-like benchmark, eSASRec resides on the Pareto frontier in terms of
the accuracy-coverage tradeoff (alongside the recent industrial models HSTU and
FuXi. As the modifications compared to the original SASRec are relatively
straightforward and no extra features are needed (such as timestamps in HSTU),
we believe that eSASRec can be easily integrated into existing recommendation
pipelines and can can serve as a strong yet very simple baseline for emerging
complicated algorithms. To facilitate this, we provide the open-source
implementations for our models and benchmarks in repository
https://github.com/blondered/transformer_benchmark

</details>


### [39] [Maximum Impact with Fewer Features: Efficient Feature Selection for Cold-Start Recommenders through Collaborative Importance Weighting](https://arxiv.org/abs/2508.06455)
*Nikita Sukhorukov,Danil Gusak,Evgeny Frolov*

Main category: cs.IR

TL;DR: 论文提出了一种基于用户行为信息的特征选择策略，通过混合矩阵分解和最大体积算法筛选关键特征，以解决冷启动问题，同时平衡推荐准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 冷启动问题需要利用辅助特征，但无关或噪声特征会降低性能，而过多特征会增加计算负担。

Method: 结合协作行为数据的相关性，使用混合矩阵分解和最大体积算法进行特征选择和排序。

Result: 在多种数据集和混合推荐模型上验证，该方法在冷启动场景中表现优异，选择少量高效特征。

Conclusion: 该方法在严格特征缩减下仍优于现有技术，同时保持高效性。

Abstract: Cold-start challenges in recommender systems necessitate leveraging auxiliary
features beyond user-item interactions. However, the presence of irrelevant or
noisy features can degrade predictive performance, whereas an excessive number
of features increases computational demands, leading to higher memory
consumption and prolonged training times.
  To address this, we propose a feature selection strategy that prioritizes the
user behavioral information. Our method enhances the feature representation by
incorporating correlations from collaborative behavior data using a hybrid
matrix factorization technique and then ranks features using a mechanism based
on the maximum volume algorithm. This approach identifies the most influential
features, striking a balance between recommendation accuracy and computational
efficiency. We conduct an extensive evaluation across various datasets and
hybrid recommendation models, demonstrating that our method excels in
cold-start scenarios by selecting minimal yet highly effective feature subsets.
Even under strict feature reduction, our approach surpasses existing feature
selection techniques while maintaining superior efficiency.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty](https://arxiv.org/abs/2508.05659)
*Jeroen F. Uleman,Loes Crielaard,Leonie K. Elsenburg,Guido A. Veldhuis,Karien Stronks,Naja Hulvej Rod,Rick Quax,Vítor V. Vasconcelos*

Main category: cs.LG

TL;DR: 论文提出Diagrams-to-Dynamics (D2D)方法，将因果循环图(CLDs)转化为系统动力学模型(SDMs)，以支持动态分析和干预策略。


<details>
  <summary>Details</summary>
Motivation: CLDs作为静态定性工具，无法支持动态分析或量化干预效果，且现有量化方法易导致错误推断。

Method: D2D通过用户简单标注变量类型（存量、流量/辅助变量、常量），利用CLDs中的结构信息（链接存在性和极性）生成SDMs，模拟干预并探索潜在杠杆点。

Result: D2D能区分高/低优先级杠杆点，与数据驱动模型一致性优于网络中心性分析，并提供不确定性估计和数据收集指导。

Conclusion: D2D通过开源工具实现，降低了动态建模门槛，未来验证将扩展其应用范围和实用性。

Abstract: Causal loop diagrams (CLDs) are widely used in health and environmental
research to represent hypothesized causal structures underlying complex
problems. However, as qualitative and static representations, CLDs are limited
in their ability to support dynamic analysis and inform intervention
strategies. Additionally, quantitative CLD analysis methods like network
centrality analysis often lead to false inference. We propose
Diagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory
system dynamics models (SDMs) in the absence of empirical data. With minimal
user input - following a protocol to label variables as stocks,
flows/auxiliaries, or constants - D2D leverages the structural information
already encoded in CLDs, namely, link existence and polarity, to simulate
hypothetical interventions and explore potential leverage points under
uncertainty. Results suggest that D2D helps distinguish between high- and
low-ranked leverage points. We compare D2D to a data-driven SDM constructed
from the same CLD and variable labeling. D2D showed greater consistency with
the data-driven model than network centrality analysis, while providing
uncertainty estimates and guidance for future data collection. The method is
implemented in an open-source Python package and a web-based application to
support further testing and lower the barrier to dynamic modeling for
researchers working with CLDs. We expect additional validation will further
establish the approach's utility across a broad range of cases and domains.

</details>


### [41] [A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics](https://arxiv.org/abs/2508.05724)
*Massimiliano Romiti*

Main category: cs.LG

TL;DR: 提出了一种基于加权知识图的物理定律表示与分析框架，通过语义清理构建了400个高级物理方程数据库，利用图注意力网络（GAT）进行链接预测，性能显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决物理方程表示中的语义模糊问题，并通过知识图揭示物理定律之间的潜在联系。

Method: 构建物理方程数据库，设计加权知识图表示，训练GAT模型进行链接预测。

Result: GAT模型在测试中AUC达到0.9742，显著优于基线方法，并揭示了物理定律间的宏观结构和跨领域关系。

Conclusion: 该框架不仅能重新发现已知物理结构，还能生成新的跨领域关系假设，为物理研究提供了新工具。

Abstract: This work introduces a novel framework for representing and analyzing
physical laws as a weighted knowledge graph. We constructed a database of 659
distinct physical equations, subjected to rigorous semantic cleaning to resolve
notational ambiguities, resulting in a corpus of 400 advanced physics
equations. We developed an enhanced graph representation where both physical
concepts and equations are nodes, connected by weighted inter-equation bridges.
These weights are objectively defined using normalized metrics for variable
overlap, physics-informed importance scores, and bibliometric data. A Graph
Attention Network (GAT) was trained for link prediction, achieving a test AUC
of 0.9742 +/- 0.0018 across five independent runs, significantly outperforming
both classical heuristics (best baseline AUC: 0.9487) and established GNN
architectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testing
confirmed significance of all comparisons (p < 0.05), with 2.7% improvement
over the best baseline. Our analysis reveals three key findings: (i) The model
autonomously rediscovers the known macroscopic structure of physics,
identifying strong conceptual axes between Electromagnetism and Statistical
Mechanics. (ii) It identifies central hub equations that serve as critical
bridges between multiple physical domains. (iii) The model generates stable,
computationally-derived hypotheses for cross-domain relationships, identifying
both known principles and suggesting novel mathematical analogies for further
theoretical investigation. The framework can generate hundreds of such
hypotheses, enabling the creation of specialized datasets for targeted analysis
of specific physics subfields. Code and data available at
https://github.com/kingelanci/graphysics

</details>


### [42] [Machine Learning-Based Nonlinear Nudging for Chaotic Dynamical Systems](https://arxiv.org/abs/2508.05778)
*Jaemin Oh,Jinsil Lee,Youngjoon Hong*

Main category: cs.LG

TL;DR: 论文提出了一种基于神经网络的非线性状态空间模型数据同化方法，通过理论证明和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在非线性系统中设计有效的nudging项具有挑战性，本文旨在解决这一问题。

Method: 提出神经网络nudging方法，基于Kazantzis--Kravaris--Luenberger观测器理论设计控制项。

Result: 在Lorenz 96模型、Kuramoto--Sivashinsky方程和Kolmogorov流三个混沌系统上验证了方法的有效性。

Conclusion: 神经网络nudging方法为非线性状态空间模型的数据同化提供了可行方案。

Abstract: Nudging is an empirical data assimilation technique that incorporates an
observation-driven control term into the model dynamics. The trajectory of the
nudged system approaches the true system trajectory over time, even when the
initial conditions differ. For linear state space models, such control terms
can be derived under mild assumptions. However, designing effective nudging
terms becomes significantly more challenging in the nonlinear setting. In this
work, we propose neural network nudging, a data-driven method for learning
nudging terms in nonlinear state space models. We establish a theoretical
existence result based on the Kazantzis--Kravaris--Luenberger observer theory.
The proposed approach is evaluated on three benchmark problems that exhibit
chaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and
the Kolmogorov flow.

</details>


### [43] [From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data](https://arxiv.org/abs/2508.05791)
*Haoran Li,Lihao Mai,Muhao Guo,Jiaqi Wu,Yang Weng,Yannan Sun,Ce Jimmy Liu*

Main category: cs.LG

TL;DR: 提出了一种可扩展的框架，通过整合异构数据重建可靠的电网拓扑结构，结合空间布局和动态行为，并引入置信感知推理机制，确保结果物理可行且可信。


<details>
  <summary>Details</summary>
Motivation: 现代电网操作需要准确的拓扑结构，但实际数据来源多样且质量不均，需一种可靠的重建方法。

Method: 结合空间布局（如GIS）和动态行为（如电压时间序列），引入置信感知推理机制，并嵌入物理约束（如变压器容量限制）。

Result: 在8000多个电表和3个馈线的数据验证中，拓扑重建准确率超过95%，置信校准和计算效率显著提升。

Conclusion: 该框架在现实条件下快速收敛到可信拓扑，优于基线方法。

Abstract: Accurate distribution grid topology is essential for reliable modern grid
operations. However, real-world utility data originates from multiple sources
with varying characteristics and levels of quality. In this work, developed in
collaboration with Oncor Electric Delivery, we propose a scalable framework
that reconstructs a trustworthy grid topology by systematically integrating
heterogeneous data. We observe that distribution topology is fundamentally
governed by two complementary dimensions: the spatial layout of physical
infrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the
system in the signal domain (e.g., voltage time series). When jointly
leveraged, these dimensions support a complete and physically coherent
reconstruction of network connectivity. To address the challenge of uneven data
quality without compromising observability, we introduce a confidence-aware
inference mechanism that preserves structurally informative yet imperfect
inputs, while quantifying the reliability of each inferred connection for
operator interpretation. This soft handling of uncertainty is tightly coupled
with hard enforcement of physical feasibility: we embed operational
constraints, such as transformer capacity limits and radial topology
requirements, directly into the learning process. Together, these components
ensure that inference is both uncertainty-aware and structurally valid,
enabling rapid convergence to actionable, trustworthy topologies under
real-world deployment conditions. The proposed framework is validated using
data from over 8000 meters across 3 feeders in Oncor's service territory,
demonstrating over 95% accuracy in topology reconstruction and substantial
improvements in confidence calibration and computational efficiency relative to
baseline methods.

</details>


### [44] [Optimal Linear Baseline Models for Scientific Machine Learning](https://arxiv.org/abs/2508.05831)
*Alexander DeLise,Kyle Loh,Krish Patel,Meredith Teague,Andrea Arnold,Matthias Chung*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯风险最小化的统一理论框架，用于分析线性编码器-解码器架构，解决了科学机器学习中的正向建模和逆向恢复问题。


<details>
  <summary>Details</summary>
Motivation: 非线性神经网络虽然成功，但理论不透明，限制了其在需要可解释性的场景中的应用。线性神经网络则能提供更直观的复杂关系分析。

Method: 通过贝叶斯风险最小化，推导了秩约束的线性和仿射线性最优映射，适用于数据、前向算子和测量过程中的秩缺陷情况。

Result: 理论结果在生物医学成像、金融因子分析和非线性流体动力学模拟等数据集上得到验证。

Conclusion: 该研究为科学机器学习中的神经网络模型提供了可理解和基准化的理论基础。

Abstract: Across scientific domains, a fundamental challenge is to characterize and
compute the mappings from underlying physical processes to observed signals and
measurements. While nonlinear neural networks have achieved considerable
success, they remain theoretically opaque, which hinders adoption in contexts
where interpretability is paramount. In contrast, linear neural networks serve
as a simple yet effective foundation for gaining insight into these complex
relationships. In this work, we develop a unified theoretical framework for
analyzing linear encoder-decoder architectures through the lens of Bayes risk
minimization for solving data-driven scientific machine learning problems. We
derive closed-form, rank-constrained linear and affine linear optimal mappings
for forward modeling and inverse recovery tasks. Our results generalize
existing formulations by accommodating rank-deficiencies in data, forward
operators, and measurement processes. We validate our theoretical results by
conducting numerical experiments on datasets from simple biomedical imaging,
financial factor analysis, and simulations involving nonlinear fluid dynamics
via the shallow water equations. This work provides a robust baseline for
understanding and benchmarking learned neural network models for scientific
machine learning problems.

</details>


### [45] [An Effective Approach for Node Classification in Textual Graphs](https://arxiv.org/abs/2508.05836)
*Rituparna Datta,Nibir Chandra Mandal*

Main category: cs.LG

TL;DR: 提出了一种结合TAPE框架与Graphormer的新方法，利用ChatGPT生成语义丰富的节点表示，显著提升了文本属性图的节点分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在整合文本语义与图结构信息时存在困难，特别是在处理领域术语、长程依赖、时间演化和大规模数据时表现不佳。

Method: 结合TAPE框架与Graphormer，利用ChatGPT生成语义解释，并通过注意力机制融合语义与结构特征。

Result: 在ogbn-arxiv数据集上取得0.772的分类准确率，显著优于基线GCN（0.713），并在精确率、召回率和F1分数上表现优异。

Conclusion: 该方法为动态文本属性图的节点分类提供了可扩展且鲁棒的解决方案，为未来知识系统和科学发现研究指明了方向。

Abstract: Textual Attribute Graphs (TAGs) are critical for modeling complex networks
like citation networks, but effective node classification remains challenging
due to difficulties in integrating rich semantics from text with structural
graph information. Existing methods often struggle with capturing nuanced
domain-specific terminology, modeling long-range dependencies, adapting to
temporal evolution, and scaling to massive datasets. To address these issues,
we propose a novel framework that integrates TAPE (Text-Attributed Graph
Representation Enhancement) with Graphormer. Our approach leverages a large
language model (LLM), specifically ChatGPT, within the TAPE framework to
generate semantically rich explanations from paper content, which are then
fused into enhanced node representations. These embeddings are combined with
structural features using a novel integration layer with learned attention
weights. Graphormer's path-aware position encoding and multi-head attention
mechanisms are employed to effectively capture long-range dependencies across
the citation network. We demonstrate the efficacy of our framework on the
challenging ogbn-arxiv dataset, achieving state-of-the-art performance with a
classification accuracy of 0.772, significantly surpassing the best GCN
baseline of 0.713. Our method also yields strong results in precision (0.671),
recall (0.577), and F1-score (0.610). We validate our approach through
comprehensive ablation studies that quantify the contribution of each
component, demonstrating the synergy between semantic and structural
information. Our framework provides a scalable and robust solution for node
classification in dynamic TAGs, offering a promising direction for future
research in knowledge systems and scientific discovery.

</details>


### [46] [Near-Optimal Regret for Efficient Stochastic Combinatorial Semi-Bandits](https://arxiv.org/abs/2508.06247)
*Zichun Ye,Runqi Wang,Xutong Liu,Shuai Li*

Main category: cs.LG

TL;DR: 论文提出了CMOSS算法，解决了组合多臂老虎机问题中UCB和对抗方法的不足，实现了高效的实例无关遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有UCB方法（如CUCB）存在长期遗憾问题，而对抗方法（如EXP3.M和HYBRID）计算开销大。

Method: 提出CMOSS算法，结合半强盗反馈，实现高效计算和低遗憾。

Result: CMOSS在合成和真实数据集上表现优于基准算法，遗憾和运行效率均更优。

Conclusion: CMOSS是一种高效且理论最优的算法，适用于组合多臂老虎机问题。

Abstract: The combinatorial multi-armed bandit (CMAB) is a cornerstone of sequential
decision-making framework, dominated by two algorithmic families: UCB-based and
adversarial methods such as follow the regularized leader (FTRL) and online
mirror descent (OMD). However, prominent UCB-based approaches like CUCB suffer
from additional regret factor $\log T$ that is detrimental over long horizons,
while adversarial methods such as EXP3.M and HYBRID impose significant
computational overhead. To resolve this trade-off, we introduce the
Combinatorial Minimax Optimal Strategy in the Stochastic setting (CMOSS). CMOSS
is a computationally efficient algorithm that achieves an instance-independent
regret of $O\big( (\log k)^2\sqrt{kmT}\big )$ under semi-bandit feedback, where
$m$ is the number of arms and $k$ is the maximum cardinality of a feasible
action. Crucially, this result eliminates the dependency on $\log T$ and
matches the established $\Omega\big( \sqrt{kmT}\big)$ lower bound up to
$O\big((\log k)^2\big)$. We then extend our analysis to show that CMOSS is also
applicable to cascading feedback. Experiments on synthetic and real-world
datasets validate that CMOSS consistently outperforms benchmark algorithms in
both regret and runtime efficiency.

</details>


### [47] [A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance](https://arxiv.org/abs/2508.05876)
*Francesca Ferrara,Lander W. Schillinger Arana,Florian Dörfler,Sarah H. Q. Li*

Main category: cs.LG

TL;DR: 提出了一种基于MDP和RL-PG的自主避碰策略，旨在平衡碰撞风险和燃料消耗。


<details>
  <summary>Details</summary>
Motivation: 解决传统避碰策略中燃料消耗高且决策时间固定的问题。

Method: 使用MDP建模避碰决策，结合RL-PG训练策略，优化决策时机。

Result: 在合成和历史数据中，策略显著降低燃料消耗，同时保持或提高碰撞风险保障。

Conclusion: 该方法在燃料效率和碰撞风险间取得了良好平衡，优于传统策略。

Abstract: This work presents a Markov decision process (MDP) framework to model
decision-making for collision avoidance maneuver (CAM) and a reinforcement
learning policy gradient (RL-PG) algorithm to train an autonomous guidance
policy using historic CAM data. In addition to maintaining acceptable collision
risks, this approach seeks to minimize the average fuel consumption of CAMs by
making early maneuver decisions. We model CAM as a continuous state, discrete
action and finite horizon MDP, where the critical decision is determining when
to initiate the maneuver. The MDP model also incorporates analytical models for
conjunction risk, propellant consumption, and transit orbit geometry. The
Markov policy effectively trades-off maneuver delay-which improves the
reliability of conjunction risk indicators-with propellant consumption-which
increases with decreasing maneuver time. Using historical data of tracked
conjunction events, we verify this framework and conduct an extensive ablation
study on the hyper-parameters used within the MDP. On synthetic conjunction
events, the trained policy significantly minimizes both the overall and average
propellant consumption per CAM when compared to a conventional cut-off policy
that initiates maneuvers 24 hours before the time of closest approach (TCA). On
historical conjunction events, the trained policy consumes more propellant
overall but reduces the average propellant consumption per CAM. For both
historical and synthetic conjunction events, the trained policy achieves equal
if not higher overall collision risk guarantees.

</details>


### [48] [The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)](https://arxiv.org/abs/2508.05905)
*Jeffrey Uhlmann*

Main category: cs.LG

TL;DR: SZT是一种2位量化方法，能在固定资源预算下提高信息密度，优于非量化方法。


<details>
  <summary>Details</summary>
Motivation: 量化通常被视为性能与计算资源之间的权衡，但本研究探讨在固定资源预算下量化是否能提供更优性能。

Method: 引入Signed-Zero Ternary (SZT)，一种2位量化方法，确定性地提供梯度信息且不影响前向路径。

Result: 分析表明，SZT在信息密度上可能优于非量化方法。

Conclusion: SZT展示了在固定资源预算下，量化可以超越非量化方法的潜力。

Abstract: Quantization is usually regarded as a means to trade quality of performance
for reduced compute requirements, i.e., as a suboptimal approximation. However,
if examined in terms of a fixed overall resource budget, a very different
perspective arises. We introduce Signed-Zero Ternary (SZT), a 2-bit
quantization that deterministically provides gradient information with no
forward-path penalty. Our analysis provides evidence that it may improve
information density compared to non-quantized alternatives.

</details>


### [49] [Dual Signal Decomposition of Stochastic Time Series](https://arxiv.org/abs/2508.05915)
*Alex Glushkovsky*

Main category: cs.LG

TL;DR: 论文提出了一种将随机时间序列分解为均值、离散度和噪声三部分的方法，通过机器学习拟合双信号并优化损失函数。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列中噪声干扰问题，同时保留特殊模式，为后续分析提供更清晰的数据基础。

Method: 应用机器学习拟合双信号，优化损失函数（包括正则化项），并采用统计过程控制方法加权。学习过程分为顺序学习和联合学习两种方式。

Result: 分解后的信号可用于平滑、去噪，并能揭示异方差时间序列的复杂关系。

Conclusion: 该方法在时间序列分析和预测中具有广泛应用潜力，特别是在需要分离噪声和保留模式的情况下。

Abstract: The research paper addresses decomposition of a stochastic time series into
three time series representing a dual signal i.e., the mean and the dispersion,
with noise isolated. Decomposition is done by applying machine learning to fit
a dual signal. Machine learning minimizes the loss function which compromises
between fitting the original time series and penalizing irregularities of the
dual signal. The latter includes terms based on the first and second order
derivatives along time. To preserve special patterns, weighting of the
regularization components of the loss function has been introduced based on
Statistical Process Control methodology. The proposed decomposition can be
applied as a smoothing algorithm against the mean and dispersion of the time
series. By isolating noise, the proposed decomposition can be seen as a
denoising algorithm. Two approaches of the learning process have been
considered: sequential and jointly. The former approach learns the mean signal
first and then dispersion. The latter approach fits the dual signal jointly.
Jointly learning can uncover complex relationships for the time series with
heteroskedasticity. Learning has been set by solving the direct non-linear
unconstrained optimization problem or by applying neural networks that have
sequential or twin output architectures. Tuning of the loss function
hyperparameters focuses on the isolated noise to be a stationary stochastic
process without autocorrelation properties. Depending on the applications, the
hyperparameters of the learning can be tuned towards either the discrete states
by stepped signal or smoothed series. The decomposed dual signal can be
represented on the 2D space and used to learn inherent structures, to forecast
both mean and dispersion, or to analyze cross effects in case of multiple time
series.

</details>


### [50] [Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations](https://arxiv.org/abs/2508.05921)
*Siddharth Rout*

Main category: cs.LG

TL;DR: 论文提出Shifted Gaussian Encoding方法，解决神经PDE求解器因矩阵病态导致的优化问题，显著提升了求解范围和精度。


<details>
  <summary>Details</summary>
Motivation: 研究神经PDE求解器在多保真度和刚性问题上因矩阵病态导致的优化困难，特别是在PIELMs中。

Method: 引入Shifted Gaussian Encoding，一种激活过滤步骤，提高矩阵秩和表达能力，同时保持凸性。

Result: 方法将稳态对流-扩散方程的Peclet数求解范围扩展了两个数量级，多频函数学习的误差降低了六个数量级，且在高保真图像向量拟合上比百万参数深度网络更快更准。

Conclusion: 研究表明，矩阵条件而非网络深度是科学神经求解器的瓶颈，简单架构改进可带来显著性能提升。

Abstract: Accuracy in neural PDE solvers often breaks down not because of limited
expressivity, but due to poor optimisation caused by ill-conditioning,
especially in multi-fidelity and stiff problems. We study this issue in
Physics-Informed Extreme Learning Machines (PIELMs), a convex variant of neural
PDE solvers, and show that asymptotic components in governing equations can
produce highly ill-conditioned activation matrices, severely limiting
convergence. We introduce Shifted Gaussian Encoding, a simple yet effective
activation filtering step that increases matrix rank and expressivity while
preserving convexity. Our method extends the solvable range of Peclet numbers
in steady advection-diffusion equations by over two orders of magnitude,
achieves up to six orders lower error on multi-frequency function learning, and
fits high-fidelity image vectors more accurately and faster than deep networks
with over a million parameters. This work highlights that conditioning, not
depth, is often the bottleneck in scientific neural solvers and that simple
architectural changes can unlock substantial gains.

</details>


### [51] [Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting](https://arxiv.org/abs/2508.05928)
*Si Shen,Peijun Shen,Wenhua Zhao,Danhao Zhu*

Main category: cs.LG

TL;DR: S-GRPO是一种改进的GRPO方法，通过噪声感知的优势权重稳定训练，解决了Think-Answer Mismatch问题，在数学推理任务中表现优于GRPO。


<details>
  <summary>Details</summary>
Motivation: GRPO在训练大型推理模型时存在Think-Answer Mismatch问题，噪声奖励信号会破坏学习过程，尤其在响应组不平衡时更严重。

Method: 提出S-GRPO，通过噪声感知的优势权重优化训练稳定性。

Result: 在多个模型上，S-GRPO显著优于GRPO，性能提升2.2%-2.5%，且在20%噪声下仍能稳定学习。

Conclusion: S-GRPO为大规模推理模型的训练提供了更稳健和有效的方法。

Abstract: Group-Relative Policy Optimization (GRPO) is a key technique for training
large reasoning models, yet it suffers from a critical vulnerability: the
\emph{Think-Answer Mismatch}, where noisy reward signals corrupt the learning
process. This problem is most severe in unbalanced response groups,
paradoxically degrading the signal precisely when it should be most
informative. To address this challenge, we propose Stable Group-Relative Policy
Optimization (S-GRPO), a principled enhancement that derives optimal,
noise-aware advantage weights to stabilize training. Our comprehensive
experiments on mathematical reasoning benchmarks demonstrate S-GRPO's
effectiveness and robustness. On various models, S-GRPO significantly
outperforms DR. GRPO, achieving performance gains of +2.5% on
Qwen-Math-7B-Base, +2.2% on Llama-3.2-3B-Base, and +2.4% on
Qwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn
under 20% synthetic reward noise, S-GRPO maintains stable learning progress.
These results highlight S-GRPO's potential for more robust and effective
training of large-scale reasoning models. \footnote{Code and data are available
at: https://github.com/shenpeijun0212/S-GRPO

</details>


### [52] [Multi-Armed Bandits-Based Optimization of Decision Trees](https://arxiv.org/abs/2508.05957)
*Hasibul Karim Shanto,Umme Ayman Koana,Shadikur Rahman*

Main category: cs.LG

TL;DR: 提出了一种基于多臂老虎机（MAB）的决策树剪枝方法，通过强化学习动态优化剪枝过程，提升模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法（如CCP和REP）基于贪心策略，可能导致泛化能力下降，尤其是在小规模复杂数据集上。

Method: 将剪枝过程建模为探索-利用问题，利用MAB算法动态选择最优剪枝节点。

Result: 实验表明，该方法在多个基准数据集上优于传统剪枝方法，预测性能更优。

Conclusion: MAB剪枝方法为决策树优化提供了一种动态且概率化的新思路。

Abstract: Decision trees, without appropriate constraints, can easily become overly
complex and prone to overfit, capturing noise rather than generalizable
patterns. To resolve this problem,pruning operation is a crucial part in
optimizing decision trees, as it not only reduces the complexity of trees but
also decreases the probability of generating overfit models. The conventional
pruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning
(REP) are mostly based on greedy approaches that focus on immediate gains in
performance while pruning nodes of the decision tree. However, this might
result in a lower generalization in the long run, compromising the robust
ability of the tree model when introduced to unseen data samples, particularly
when trained with small and complex datasets. To address this challenge, we are
proposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement
learning (RL)-based technique, that will dynamically prune the tree to generate
an optimal decision tree with better generalization. Our proposed approach
assumes the pruning process as an exploration-exploitation problem, where we
are utilizing the MAB algorithms to find optimal branch nodes to prune based on
feedback from each pruning actions. Experimental evaluation on several
benchmark datasets, demonstrated that our proposed approach results in better
predictive performance compared to the traditional ones. This suggests the
potential of utilizing MAB for a dynamic and probabilistic way of decision tree
pruning, in turn optimizing the decision tree-based model.

</details>


### [53] [Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning](https://arxiv.org/abs/2508.05960)
*Haohui Chen,Zhiyong Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为MCRE的框架和MCRQ算法，用于解决离线强化学习中的分布偏移问题，平衡保守性和性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，学习策略与行为策略之间的分布偏移导致OOD动作和过高估计，需平衡保守性和性能。

Method: 提出MCRE框架，结合TD误差和行为克隆项；开发MCRQ算法，将MCRE融入离线actor-critic框架。

Result: MCRQ在基准数据集上优于现有基线方法和最先进的离线RL算法。

Conclusion: MCRE和MCRQ有效解决了离线RL中的保守性与性能平衡问题。

Abstract: Offline reinforcement learning (RL) seeks to learn optimal policies from
static datasets without further environment interaction. A key challenge is the
distribution shift between the learned and behavior policies, leading to
out-of-distribution (OOD) actions and overestimation. To prevent gross
overestimation, the value function must remain conservative; however, excessive
conservatism may hinder performance improvement. To address this, we propose
the mildly conservative regularized evaluation (MCRE) framework, which balances
conservatism and performance by combining temporal difference (TD) error with a
behavior cloning term in the Bellman backup. Building on this, we develop the
mildly conservative regularized Q-learning (MCRQ) algorithm, which integrates
MCRE into an off-policy actor-critic framework. Experiments show that MCRQ
outperforms strong baselines and state-of-the-art offline RL algorithms on
benchmark datasets.

</details>


### [54] [LinguaFluid: Language Guided Fluid Control via Semantic Rewards in Reinforcement Learning](https://arxiv.org/abs/2508.05977)
*Aoming Liang,Chi Cheng,Dashuai Chen,Boai Sun,Dixia Fan*

Main category: cs.LG

TL;DR: 论文提出了一种基于语义对齐的强化学习方法，通过SBERT模型计算奖励，替代传统手工设计的奖励函数，实验表明该方法在多个环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习中的奖励函数多依赖启发式或手工设计，难以在任务目标难以数值化的环境中有效工作，因此需要一种更通用的方法。

Method: 使用SBERT模型计算当前状态与目标语义指令的余弦相似度作为奖励，替代手工设计的奖励函数。

Result: 实验证明语义奖励能有效指导学习，在无手工奖励函数的情况下实现竞争性控制行为。

Conclusion: 该方法展示了语言嵌入空间与传统欧几里得空间的关联，为自然语言目标与智能体行为的对齐提供了新思路，并为大语言模型与控制应用的结合奠定了基础。

Abstract: In the domain of scientific machine learning, designing effective reward
functions remains a challenge in reinforcement learning (RL), particularly in
environments where task goals are difficult to specify numerically. Reward
functions in existing work are predominantly based on heuristics, manual
engineering, or task-specific tuning. In this work, we introduce a semantically
aligned reinforcement learning method where rewards are computed by aligning
the current state with a target semantic instruction using a
Sentence-Bidirectional Encoder Representations from Transformers (SBERT).
Instead of relying on manually defined reward functions, the policy receives
feedback based on the reward, which is a cosine similarity between the goal
textual description and the statement description in the episode. We evaluated
our approach in several environments and showed that semantic reward can guide
learning to achieve competitive control behavior, even in the absence of
hand-crafted reward functions. Our study demonstrates a correlation between the
language embedding space and the conventional Euclidean space. This framework
opens new horizons for aligning agent behavior with natural language goals and
lays the groundwork for a more seamless integration of larger language models
(LLMs) and fluid control applications.

</details>


### [55] [Parameter-free Optimal Rates for Nonlinear Semi-Norm Contractions with Applications to $Q$-Learning](https://arxiv.org/abs/2508.05984)
*Ankur Naskar,Gugan Thoppe,Vijay Gupta*

Main category: cs.LG

TL;DR: 本文提出了一种解决非线性固定点方程的方法，通过重新定义平均误差为线性递归并结合半范数的收缩性，首次实现了参数无关的最优收敛速率。


<details>
  <summary>Details</summary>
Motivation: 解决非线性固定点方程（如平均奖励Q学习和TD学习）中半范数非单调性导致的收敛速率问题。

Method: 将平均误差重新定义为线性递归，并通过半范数收缩与诱导范数的单调性结合来抑制非线性。

Result: 首次实现了参数无关的$\tilde{O}(1/\sqrt{t})$最优收敛速率，适用于同步/异步更新、单/多代理部署及马尔可夫轨迹数据流。

Conclusion: 该方法为非线性固定点方程的求解提供了广泛适用的最优收敛速率解决方案。

Abstract: Algorithms for solving \textit{nonlinear} fixed-point equations -- such as
average-reward \textit{$Q$-learning} and \textit{TD-learning} -- often involve
semi-norm contractions. Achieving parameter-free optimal convergence rates for
these methods via Polyak--Ruppert averaging has remained elusive, largely due
to the non-monotonicity of such semi-norms. We close this gap by (i.) recasting
the averaged error as a linear recursion involving a nonlinear perturbation,
and (ii.) taming the nonlinearity by coupling the semi-norm's contraction with
the monotonicity of a suitably induced norm. Our main result yields the first
parameter-free $\tilde{O}(1/\sqrt{t})$ optimal rates for $Q$-learning in both
average-reward and exponentially discounted settings, where $t$ denotes the
iteration index. The result applies within a broad framework that accommodates
synchronous and asynchronous updates, single-agent and distributed deployments,
and data streams obtained either from simulators or along Markovian
trajectories.

</details>


### [56] [Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal](https://arxiv.org/abs/2508.05988)
*Wenhao Zeng,Yaoning Wang,Chao Hu,Yuling Shi,Chengcheng Wan,Hongyu Zhang,Xiaodong Gu*

Main category: cs.LG

TL;DR: ASAP是一种新颖的CoT压缩框架，通过锚点引导和基于意外性的剪枝，显著减少推理延迟和训练成本，同时保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决长推理链带来的训练成本高、推理延迟大和部署困难的问题。

Method: ASAP采用粗到细的框架，先锚点引导剪枝保留核心结构，再基于第一令牌意外性选择逻辑关键步骤，最后模型自主生成简洁CoT。

Result: 在代码生成任务中，ASAP显著减少23.5%的令牌生成和43.5%的推理延迟，Pass@1准确率达36.19%。

Conclusion: ASAP为构建高效强大的LRMs提供了有前景的方向。

Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in code reasoning by scaling up the length of Chain-of-Thought
(CoT). However, excessively long reasoning traces introduce substantial
challenges in terms of training cost, inference latency, and deployment
feasibility. While various CoT compression approaches have emerged to address
this challenge, they face inherent trade-offs: token-level methods often
disrupt syntactic and logical coherence, while step-level methods based on
perplexity fail to reliably capture the logically critical reasoning steps. In
this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel
coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided
pruning to preserve the core reasoning structure, which efficiently reduces the
search space for subsequent processing. It then enables a logic-aware pruning
by selecting logically essential reasoning steps based on a novel first-token
surprisal metric. Finally, ASAP teaches models to autonomously generate and
leverage these concise CoTs at inference time, enabling efficient reasoning in
coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy
across multiple code generation benchmarks while substantially reducing
training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,
our approach reduces token generation by 23.5% and inference latency by 43.5%
compared to the strongest baseline, while achieving a competitive accuracy of
36.19% in Pass@1. Our results highlight a promising direction for building
powerful and efficient LRMs.

</details>


### [57] [Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization](https://arxiv.org/abs/2508.05995)
*Fei Xu Yu,Gina Adam,Nathaniel D. Bastian,Tian Lan*

Main category: cs.LG

TL;DR: 论文提出MCTS-OPS框架，结合蒙特卡洛树搜索（MCTS）与大型语言模型（LLMs），通过多步提示序列提升代码生成质量，在复杂优化任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在需要多步规划的复杂任务中性能下降，且现有MCTS方法仅关注启发式代码生成或简单任务。

Method: 提出MCTS-OPS框架，将提示选择建模为MCTS引导的序列决策过程，优化多步提示序列。

Result: 在网络优化实验中，代码执行成功率和优化结果显著提升（奖励提高2~4倍，标准差降低3倍），最优解概率提高约10%。

Conclusion: 结合符号规划与LLMs在复杂领域能实现更鲁棒、高质量的代码生成。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code generation and structured reasoning; however, their performance often
degrades on complex tasks that require consistent multi-step planning. Recent
work has explored combining LLMs with Monte Carlo Tree Search (MCTS), yet
existing approaches primarily focus on generating heuristic-based code for
optimization or target simpler tasks where correctness alone is sufficient. In
this work, we propose MCTS-OPS, a novel neural-symbolic framework that
formulates prompt selection as a sequential decision process guided by MCTS.
Our method explores and refines multi-step prompt sequences for the goal of
improving code generation quality and enhancing the problem-solving
capabilities of LLMs in general optimization. Experiments on network
optimization show significant improvement over the baselines, both in the
success rate of executing the generated code and in the optimization results
with the specified objective and constraints (2$\sim$4$\times$ higher reward
and 3$\times$ lower standard deviation). Moreover, it improves the chance of
attaining the optimal solution by about 10\% of cases, compared to baseline
methods in hard problems. These results highlight the promise of combining
symbolic planning with LLMs for robust, high-quality code generation in complex
domains.

</details>


### [58] [Stepwise Fine and Gray: Subject-Specific Variable Selection Shows When Hemodynamic Data Improves Prognostication of Comatose Post-Cardiac Arrest Patients](https://arxiv.org/abs/2508.06023)
*Xiaobin Shen,Jonathan Elmer,George H. Chen*

Main category: cs.LG

TL;DR: 该研究提出了一种新颖的逐步动态竞争风险模型，用于预测心脏骤停后昏迷患者的神经功能恢复情况，通过分阶段利用时间不变和时间变化特征，提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 心脏骤停后昏迷患者的预后预测对ICU临床决策至关重要，但目前方法未能充分利用随时间变化的动态特征。

Method: 研究扩展了Fine和Gray模型，引入神经网络分阶段处理时间不变和时间变化特征，动态判断何时及对哪些患者这些特征有益。

Result: 在2278名患者的回顾性队列中，模型对觉醒、撤除生命支持及死亡等竞争结局表现出稳健的判别性能。

Conclusion: 该方法适用于多阶段动态预测任务，可推广至其他需要动态特征优化的场景。

Abstract: Prognostication for comatose post-cardiac arrest patients is a critical
challenge that directly impacts clinical decision-making in the ICU. Clinical
information that informs prognostication is collected serially over time.
Shortly after cardiac arrest, various time-invariant baseline features are
collected (e.g., demographics, cardiac arrest characteristics). After ICU
admission, additional features are gathered, including time-varying hemodynamic
data (e.g., blood pressure, doses of vasopressor medications). We view these as
two phases in which we collect new features. In this study, we propose a novel
stepwise dynamic competing risks model that improves the prediction of
neurological outcomes by automatically determining when to take advantage of
time-invariant features (first phase) and time-varying features (second phase).
Notably, our model finds patients for whom this second phase (time-varying
hemodynamic) information is beneficial for prognostication and also when this
information is beneficial (as we collect more hemodynamic data for a patient
over time, how important these data are for prognostication varies). Our
approach extends the standard Fine and Gray model to explicitly model the two
phases and to incorporate neural networks to flexibly capture complex nonlinear
feature relationships. Evaluated on a retrospective cohort of 2,278 comatose
post-arrest patients, our model demonstrates robust discriminative performance
for the competing outcomes of awakening, withdrawal of life-sustaining therapy,
and death despite maximal support. Our approach generalizes to more than two
phases in which new features are collected and could be used in other dynamic
prediction tasks, where it may be helpful to know when and for whom newly
collected features significantly improve prediction.

</details>


### [59] [Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity](https://arxiv.org/abs/2508.06034)
*Qin Chen,Guojie Song*

Main category: cs.LG

TL;DR: 论文提出了一种自适应异构图神经网络（AHGNN），解决了异构图（HGs）中异质性分布和语义多样性问题，显著提升了高异质性场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多孤立关注异质性或异质性，忽略了实际应用中异质性HGs的普遍性，导致性能下降。

Method: 提出AHGNN，采用异质性感知卷积和粗到细注意力机制，处理不同跳数和元路径的异质性分布及语义多样性。

Result: 在七个真实世界图和二十个基线模型上的实验表明，AHGNN在高异质性场景下表现优异。

Conclusion: AHGNN通过自适应机制有效解决了异质性HGs的建模挑战，显著提升了性能。

Abstract: Heterogeneous graphs (HGs) are common in real-world scenarios and often
exhibit heterophily. However, most existing studies focus on either
heterogeneity or heterophily in isolation, overlooking the prevalence of
heterophilic HGs in practical applications. Such ignorance leads to their
performance degradation. In this work, we first identify two main challenges in
modeling heterophily HGs: (1) varying heterophily distributions across hops and
meta-paths; (2) the intricate and often heterophily-driven diversity of
semantic information across different meta-paths. Then, we propose the Adaptive
Heterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN
employs a heterophily-aware convolution that accounts for heterophily
distributions specific to both hops and meta-paths. It then integrates messages
from diverse semantic spaces using a coarse-to-fine attention mechanism, which
filters out noise and emphasizes informative signals. Experiments on seven
real-world graphs and twenty baselines demonstrate the superior performance of
AHGNN, particularly in high-heterophily situations.

</details>


### [60] [DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment](https://arxiv.org/abs/2508.06041)
*Sangwoo Kwon,Seong Hoon Seo,Jae W. Lee,Yeonhong Park*

Main category: cs.LG

TL;DR: DP-LLM是一种动态分配精度的机制，通过轻量级误差估计器和阈值学习，优化LLM的性能与延迟权衡。


<details>
  <summary>Details</summary>
Motivation: 解决在设备上运行的LLM如何根据运行时约束（如延迟和准确性）动态调整模型精度的问题。

Method: 提出DP-LLM机制，动态为每层分配精度，基于输入值和轻量级误差估计器，通过微调学习阈值。

Result: 实验表明，DP-LLM在性能与延迟权衡上优于现有方法。

Conclusion: DP-LLM通过动态精度分配，显著提升了LLM的适应性和效率。

Abstract: How can we effectively handle queries for on-device large language models
(LLMs) with varying runtime constraints, such as latency and accuracy?
Multi-scale quantization addresses this challenge by enabling memory-efficient
runtime model adaptation of LLMs through the overlaying of multiple model
variants quantized to different bitwidths. Meanwhile, an important question
still remains open-ended: how can models be properly configured to match a
target precision or latency? While mixed-precision offers a promising solution,
we take this further by leveraging the key observation that the sensitivity of
each layer dynamically changes across decoding iterations. Building on this
insight, we introduce DP-LLM, a novel mechanism that dynamically assigns
precision to each layer based on input values. DP-LLM augments each linear
layer in an LLM with a precision selector that determines the bitwidth at
runtime using a lightweight error estimator and threshold values learned
through fine-tuning. Experimental results across multiple models and benchmarks
demonstrate that DP-LLM achieves a superior performance-latency trade-off,
outperforming prior approaches.

</details>


### [61] [Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology](https://arxiv.org/abs/2508.06066)
*Barak Gahtan,Alex M. Bronstein*

Main category: cs.LG

TL;DR: 论文提出了针对深度时序模型（如TCNs）的非空泛化边界和评估方法，揭示了时序依赖性对学习的潜在益处。


<details>
  <summary>Details</summary>
Motivation: 填补深度时序模型泛化理论理解的空白，并提供架构感知的泛化边界和评估方法。

Method: 使用延迟反馈阻塞机制将依赖样本转化为独立样本，推导出泛化边界，并引入公平比较方法固定有效样本量。

Result: 泛化边界为O(R√(Dpn logN/N))，强依赖序列的泛化差距比弱依赖序列小76%。收敛速率与理论预测存在差异。

Conclusion: 时序依赖性在固定信息预算下可增强学习，但理论与实践之间存在差距，需进一步研究。

Abstract: Deep temporal architectures such as Temporal Convolutional Networks (TCNs)
achieve strong predictive performance on sequential data, yet theoretical
understanding of their generalization remains limited. We address this gap by
providing both the first non-vacuous, architecture-aware generalization bounds
for deep temporal models and a principled evaluation methodology.
  For exponentially $\beta$-mixing sequences, we derive bounds scaling as $
O\!\Bigl(R\,\sqrt{\tfrac{D\,p\,n\,\log N}{N}}\Bigr), $ where $D$ is network
depth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our
delayed-feedback blocking mechanism transforms dependent samples into
effectively independent ones while discarding only $O(1/\log N)$ of the data,
yielding $\sqrt{D}$ scaling instead of exponential, implying that doubling
depth requires approximately quadrupling the training data.
  We also introduce a fair-comparison methodology that fixes the effective
sample size to isolate the effect of temporal structure from information
content. Under $N_{\text{eff}}=2{,}000$, strongly dependent sequences
($\rho=0.8$) exhibit $\approx76\%$ smaller generalization gaps than weakly
dependent ones ($\rho=0.2$), challenging the intuition that dependence is
purely detrimental. Yet convergence rates diverge from theory: weak
dependencies follow $N_{\text{eff}}^{-1.21}$ scaling and strong dependencies
follow $N_{\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.
These findings reveal that temporal dependence can enhance learning under fixed
information budgets, while highlighting gaps between theory and practice that
motivate future research.

</details>


### [62] [Recurrent Deep Differentiable Logic Gate Networks](https://arxiv.org/abs/2508.06097)
*Simon Bührer,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 论文首次实现了循环深度可微分逻辑门网络（RDDLGN），将布尔运算与循环架构结合用于序列到序列学习，在WMT'14英德翻译任务中表现接近GRU。


<details>
  <summary>Details</summary>
Motivation: 探索可微分逻辑门在序列建模中的应用，填补该领域的研究空白。

Method: 提出RDDLGN，结合布尔运算与循环架构，用于序列到序列学习。

Result: 在WMT'14英德翻译任务中，RDDLGN达到5.00 BLEU和30.9%训练准确率，接近GRU性能（5.41 BLEU）。

Conclusion: RDDLGN证明了基于循环逻辑的神经计算的可行性，为FPGA加速等研究方向开辟了新路径。

Abstract: While differentiable logic gates have shown promise in feedforward networks,
their application to sequential modeling remains unexplored. This paper
presents the first implementation of Recurrent Deep Differentiable Logic Gate
Networks (RDDLGN), combining Boolean operations with recurrent architectures
for sequence-to-sequence learning.
  Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and
30.9\% accuracy during training, approaching GRU performance (5.41 BLEU) and
graceful degradation (4.39 BLEU) during inference. This work establishes
recurrent logic-based neural computation as viable, opening research directions
for FPGA acceleration in sequential modeling and other recursive network
architectures.

</details>


### [63] [GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning](https://arxiv.org/abs/2508.06108)
*Xing Lei,Wenyan Yang,Kaiqiang Ke,Shentao Yang,Xuetao Zhang,Joni Pajarinen,Donglin Wang*

Main category: cs.LG

TL;DR: HGR和HSR结合的方法显著提高了GCRL的样本效率。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励下的GCRL效率不足，现有方法如HER未能充分利用经验。

Method: 提出HGR，基于后见目标生成动作正则化先验，结合HSR最大化经验利用率。

Result: 在导航和操作任务中表现最优，样本重用效率显著提升。

Conclusion: HGR和HSR的结合为GCRL提供了更高效的解决方案。

Abstract: Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a
fundamental challenge in reinforcement learning. While hindsight experience
replay (HER) has shown promise by relabeling collected trajectories with
achieved goals, we argue that trajectory relabeling alone does not fully
exploit the available experiences in off-policy GCRL methods, resulting in
limited sample efficiency. In this paper, we propose Hindsight Goal-conditioned
Regularization (HGR), a technique that generates action regularization priors
based on hindsight goals. When combined with hindsight self-imitation
regularization (HSR), our approach enables off-policy RL algorithms to maximize
experience utilization. Compared to existing GCRL methods that employ HER and
self-imitation techniques, our hindsight regularizations achieve substantially
more efficient sample reuse and the best performances, which we empirically
demonstrate on a suite of navigation and manipulation tasks.

</details>


### [64] [Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models](https://arxiv.org/abs/2508.06151)
*Yong Oh Lee,JeeEun Kim,Jung Woo Lee*

Main category: cs.LG

TL;DR: 论文提出了一种基于扩散模型的图像生成方法，用于合成口腔癌病变图像，提升诊断模型的性能。


<details>
  <summary>Details</summary>
Motivation: 由于标注数据有限且训练数据不足，口腔癌诊断模型的性能受到限制。

Method: 采用微调的扩散模型和图像修复技术生成高保真度的合成病变图像，并结合多源数据集。

Result: 分类模型诊断准确率达0.97，检测模型定位准确率为0.85。

Conclusion: 合成图像生成在医学诊断中具有潜力，可推广至其他癌症诊断研究。

Abstract: In oral cancer diagnostics, the limited availability of annotated datasets
frequently constrains the performance of diagnostic models, particularly due to
the variability and insufficiency of training data. To address these
challenges, this study proposed a novel approach to enhance diagnostic accuracy
by synthesizing realistic oral cancer lesions using an inpainting technique
with a fine-tuned diffusion model. We compiled a comprehensive dataset from
multiple sources, featuring a variety of oral cancer images. Our method
generated synthetic lesions that exhibit a high degree of visual fidelity to
actual lesions, thereby significantly enhancing the performance of diagnostic
algorithms. The results show that our classification model achieved a
diagnostic accuracy of 0.97 in differentiating between cancerous and
non-cancerous tissues, while our detection model accurately identified lesion
locations with 0.85 accuracy. This method validates the potential for synthetic
image generation in medical diagnostics and paves the way for further research
into extending these methods to other types of cancer diagnostics.

</details>


### [65] [Differentially Private Federated Clustering with Random Rebalancing](https://arxiv.org/abs/2508.06183)
*Xiyuan Yang,Shengyuan Hu,Soyeon Kim,Tian Li*

Main category: cs.LG

TL;DR: RR-Cluster是一种轻量级技术，通过随机重新平衡聚类分配来减少隐私噪声，提升联邦聚类中的隐私/效用权衡。


<details>
  <summary>Details</summary>
Motivation: 联邦聚类通过为每个聚类训练一个模型提升性能，但隐私泄露风险更高。直接应用差分隐私机制会显著降低效用。

Method: 提出RR-Cluster技术，通过随机重新平衡聚类分配，确保每个聚类有最少客户端数量，减少隐私噪声。

Result: 理论和实验表明，RR-Cluster显著改善了隐私/效用权衡，适用于合成和真实数据集。

Conclusion: RR-Cluster是一种简单有效的技术，可提升联邦聚类算法的隐私保护能力，同时保持模型性能。

Abstract: Federated clustering aims to group similar clients into clusters and produce
one model for each cluster. Such a personalization approach typically improves
model performance compared with training a single model to serve all clients,
but can be more vulnerable to privacy leakage. Directly applying client-level
differentially private (DP) mechanisms to federated clustering could degrade
the utilities significantly. We identify that such deficiencies are mainly due
to the difficulties of averaging privacy noise within each cluster (following
standard privacy mechanisms), as the number of clients assigned to the same
clusters is uncontrolled. To this end, we propose a simple and effective
technique, named RR-Cluster, that can be viewed as a light-weight add-on to
many federated clustering algorithms. RR-Cluster achieves reduced privacy noise
via randomly rebalancing cluster assignments, guaranteeing a minimum number of
clients assigned to each cluster. We analyze the tradeoffs between decreased
privacy noise variance and potentially increased bias from incorrect
assignments and provide convergence bounds for RR-Clsuter. Empirically, we
demonstrate the RR-Cluster plugged into strong federated clustering algorithms
results in significantly improved privacy/utility tradeoffs across both
synthetic and real-world datasets.

</details>


### [66] [Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning](https://arxiv.org/abs/2508.06199)
*Mateusz Praski,Jakub Adamczyk,Wojciech Czech*

Main category: cs.LG

TL;DR: 该研究对25种预训练神经网络模型在25个数据集上进行了全面比较，发现大多数模型在分子属性预测任务中并未显著优于传统ECFP指纹方法，仅CLAMP模型表现显著更好。


<details>
  <summary>Details</summary>
Motivation: 探讨预训练神经网络在化学和药物设计中的应用效果，评估其是否优于传统方法。

Method: 采用公平比较框架，评估25种不同模态、架构和预训练策略的模型，并使用层次贝叶斯统计测试进行分析。

Result: 大多数神经网络模型表现与ECFP指纹相当，仅CLAMP模型显著优于其他方法。

Conclusion: 研究揭示了现有评估方法的不足，并提出了改进建议。

Abstract: Pretrained neural networks have attracted significant interest in chemistry
and small molecule drug design. Embeddings from these models are widely used
for molecular property prediction, virtual screening, and small data learning
in molecular chemistry. This study presents the most extensive comparison of
such models to date, evaluating 25 models across 25 datasets. Under a fair
comparison framework, we assess models spanning various modalities,
architectures, and pretraining strategies. Using a dedicated hierarchical
Bayesian statistical testing model, we arrive at a surprising result: nearly
all neural models show negligible or no improvement over the baseline ECFP
molecular fingerprint. Only the CLAMP model, which is also based on molecular
fingerprints, performs statistically significantly better than the
alternatives. These findings raise concerns about the evaluation rigor in
existing studies. We discuss potential causes, propose solutions, and offer
practical recommendations.

</details>


### [67] [Graph Federated Learning for Personalized Privacy Recommendation](https://arxiv.org/abs/2508.06208)
*Ce Na,Kai Yang,Dengzhao Fang,Yu Li,Jingtong Gao,Chengcheng Zhu,Jiale Zhang,Xiaobing Sun,Yi Chang*

Main category: cs.LG

TL;DR: GFed-PP是一种新型的联邦推荐系统，适应不同隐私需求并提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐系统假设所有用户隐私需求相同，忽略了公开用户数据的潜力。

Method: 利用公开用户数据构建用户-项目交互图，采用轻量级GCN学习个性化项目嵌入，保护隐私。

Result: 在五个数据集上显著优于现有方法，推荐准确性高且不损害隐私。

Conclusion: GFed-PP为联邦推荐系统中不同隐私偏好提供了实用解决方案。

Abstract: Federated recommendation systems (FedRecs) have gained significant attention
for providing privacy-preserving recommendation services. However, existing
FedRecs assume that all users have the same requirements for privacy
protection, i.e., they do not upload any data to the server. The approaches
overlook the potential to enhance the recommendation service by utilizing
publicly available user data. In real-world applications, users can choose to
be private or public. Private users' interaction data is not shared, while
public users' interaction data can be shared. Inspired by the issue, this paper
proposes a novel Graph Federated Learning for Personalized Privacy
Recommendation (GFed-PP) that adapts to different privacy requirements while
improving recommendation performance. GFed-PP incorporates the interaction data
of public users to build a user-item interaction graph, which is then used to
form a user relationship graph. A lightweight graph convolutional network (GCN)
is employed to learn each user's user-specific personalized item embedding. To
protect user privacy, each client learns the user embedding and the scoring
function locally. Additionally, GFed-PP achieves optimization of the federated
recommendation framework through the initialization of item embedding on
clients and the aggregation of the user relationship graph on the server.
Experimental results demonstrate that GFed-PP significantly outperforms
existing methods for five datasets, offering superior recommendation accuracy
without compromising privacy. This framework provides a practical solution for
accommodating varying privacy preferences in federated recommendation systems.

</details>


### [68] [Reparameterization Proximal Policy Optimization](https://arxiv.org/abs/2508.06214)
*Hai Zhong,Xun Wang,Zhuoran Li,Longbo Huang*

Main category: cs.LG

TL;DR: 论文提出了一种稳定的、基于重新参数化策略梯度（RPG）的方法RPO，通过结合PPO的代理目标和KL正则化，显著提升了样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 重新参数化策略梯度（RPG）虽然能提高样本效率，但其训练不稳定性（高方差梯度）限制了其应用。本文旨在解决这一问题。

Method: 通过建立PPO代理目标与RPG的联系，提出RPO方法，利用时间反向传播高效计算梯度，并结合KL正则化和裁剪代理目标实现稳定训练。

Result: 在运动和操作任务上的实验表明，RPO具有优越的样本效率和性能。

Conclusion: RPO通过结合PPO的稳定性和RPG的效率，为强化学习提供了一种高效且稳定的训练方法。

Abstract: Reparameterization policy gradient (RPG) is promising for improving sample
efficiency by leveraging differentiable dynamics. However, a critical barrier
is its training instability, where high-variance gradients can destabilize the
learning process. To address this, we draw inspiration from Proximal Policy
Optimization (PPO), which uses a surrogate objective to enable stable sample
reuse in the model-free setting. We first establish a connection between this
surrogate objective and RPG, which has been largely unexplored and is
non-trivial. Then, we bridge this gap by demonstrating that the
reparameterization gradient of a PPO-like surrogate objective can be computed
efficiently using backpropagation through time. Based on this key insight, we
propose Reparameterization Proximal Policy Optimization (RPO), a stable and
sample-efficient RPG-based method. RPO enables multiple epochs of stable sample
reuse by optimizing a clipped surrogate objective tailored for RPG, while being
further stabilized by Kullback-Leibler (KL) divergence regularization and
remaining fully compatible with existing variance reduction methods. We
evaluate RPO on a suite of challenging locomotion and manipulation tasks, where
experiments demonstrate that our method achieves superior sample efficiency and
strong performance.

</details>


### [69] [Epidemic Control on a Large-Scale-Agent-Based Epidemiology Model using Deep Deterministic Policy Gradient](https://arxiv.org/abs/2304.04475)
*Gaurav Deshkar,Jayanta Kshirsagar,Harshal Hayatnagarkar,Janani Venugopalan*

Main category: cs.LG

TL;DR: 论文提出了一种基于DDPG的策略优化框架，用于在大规模流行病模拟中自动确定最优干预措施（如封锁和疫苗接种）。


<details>
  <summary>Details</summary>
Motivation: 现有研究在模拟目标、规模、模型类型和干预策略探索方面存在局限，无法有效优化干预措施。

Method: 使用DDPG框架在大规模（10万个体）流行病代理模拟中进行多目标优化，结合年龄分层和基本经济模拟。

Result: 在无封锁和针对中老年人群的疫苗接种策略下，实现了经济（贫困线以下人口）与健康目标（感染和住院）的平衡。

Conclusion: 需进一步深入模拟验证结果，并开源框架。

Abstract: To mitigate the impact of the pandemic, several measures include lockdowns,
rapid vaccination programs, school closures, and economic stimulus. These
interventions can have positive or unintended negative consequences. Current
research to model and determine an optimal intervention automatically through
round-tripping is limited by the simulation objectives, scale (a few thousand
individuals), model types that are not suited for intervention studies, and the
number of intervention strategies they can explore (discrete vs continuous). We
address these challenges using a Deep Deterministic Policy Gradient (DDPG)
based policy optimization framework on a large-scale (100,000 individual)
epidemiological agent-based simulation where we perform multi-objective
optimization. We determine the optimal policy for lockdown and vaccination in a
minimalist age-stratified multi-vaccine scenario with a basic simulation for
economic activity. With no lockdown and vaccination (mid-age and elderly),
results show optimal economy (individuals below the poverty line) with balanced
health objectives (infection, and hospitalization). An in-depth simulation is
needed to further validate our results and open-source our framework.

</details>


### [70] [SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems](https://arxiv.org/abs/2508.06243)
*Ioan-Sorin Comsa,Purav Shah,Karthik Vaidhyanathan,Deepak Gangadharan,Christof Imhof,Per Bergamin,Aryan Kaushik,Gabriel-Miro Muntean,Ramona Trestian*

Main category: cs.LG

TL;DR: 论文提出了一种名为SCAR的边缘AI辅助框架，用于优化6G车载娱乐中的资源管理，通过ML压缩技术减少数据量并提升调度公平性。


<details>
  <summary>Details</summary>
Motivation: 传统无线电资源管理（RRM）技术难以应对6G车载环境中日益增长的数据量和复杂性，如来自自动驾驶车辆的CQI数据。

Method: SCAR采用基于机器学习的压缩技术（如聚类和RBF网络）压缩CQI数据，保留关键特征，并利用6G强化学习策略优化调度和公平性。

Result: 实验显示，SCAR将可行调度区域时间提升14%，不公平调度时间减少15%，且SAST聚类技术将CQI聚类失真降低10%。

Conclusion: SCAR框架在动态车载网络中展现出良好的可扩展性和公平性优势。

Abstract: The advent of 6G networks opens new possibilities for connected infotainment
services in vehicular environments. However, traditional Radio Resource
Management (RRM) techniques struggle with the increasing volume and complexity
of data such as Channel Quality Indicators (CQI) from autonomous vehicles. To
address this, we propose SCAR (State-Space Compression for AI-Driven Resource
Management), an Edge AI-assisted framework that optimizes scheduling and
fairness in vehicular infotainment. SCAR employs ML-based compression
techniques (e.g., clustering and RBF networks) to reduce CQI data size while
preserving essential features. These compressed states are used to train
6G-enabled Reinforcement Learning policies that maximize throughput while
meeting fairness objectives defined by the NGMN. Simulations show that SCAR
increases time in feasible scheduling regions by 14\% and reduces unfair
scheduling time by 15\% compared to RL baselines without CQI compression.
Furthermore, Simulated Annealing with Stochastic Tunneling (SAST)-based
clustering reduces CQI clustering distortion by 10\%, confirming its
efficiency. These results demonstrate SCAR's scalability and fairness benefits
for dynamic vehicular networks.

</details>


### [71] [Membership Inference Attack with Partial Features](https://arxiv.org/abs/2508.06244)
*Xurun Wang,Guangrui Liu,Xinjie Li,Haoyu He,Lin Yao,Weizhe Zhang*

Main category: cs.LG

TL;DR: 论文研究了在部分特征信息可用的情况下，针对机器学习模型的成员推理攻击（PFMI），并提出了一种两阶段攻击框架MRAD。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理方法通常假设攻击者能访问目标样本的全部特征，但现实中往往只有部分特征可用，限制了这些方法的适用性。

Method: 提出MRAD框架：第一阶段优化未知特征值以最小化样本损失；第二阶段通过异常检测衡量重构样本与训练分布的偏差。

Result: 实验表明MRAD在多种数据集上有效，例如在STL-10上，即使缺失40%特征，AUC仍可达0.6。

Conclusion: MRAD解决了部分特征下的成员推理问题，且兼容多种异常检测技术。

Abstract: Machine learning models have been shown to be susceptible to membership
inference attack, which can be used to determine whether a given sample appears
in the training data. Existing membership inference methods commonly assume
that the adversary has full access to the features of the target sample. This
assumption, however, does not hold in many real-world scenarios where only
partial features information is available, thereby limiting the applicability
of these methods. In this work, we study an inference scenario where the
adversary observes only partial features of each sample and aims to infer
whether this observed subset was present in the training set of the target
model. We define this problem as Partial Feature Membership Inference (PFMI).
To address this problem, we propose MRAD (Memory-guided Reconstruction and
Anomaly Detection), a two-stage attack framework. In the first stage, MRAD
optimizes the unknown feature values to minimize the loss of the sample. In the
second stage, it measures the deviation between the reconstructed sample and
the training distribution using anomaly detection. Empirical results
demonstrate that MRAD is effective across a range of datasets, and maintains
compatibility with various off-the-shelf anomaly detection techniques. For
example, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of
the missing features.

</details>


### [72] [In-Training Defenses against Emergent Misalignment in Language Models](https://arxiv.org/abs/2508.06249)
*David Kaczér,Magnus Jørgenvåg,Clemens Vetter,Lucie Flek,Florian Mai*

Main category: cs.LG

TL;DR: 论文研究了微调大型语言模型（LLM）时可能引发的意外有害行为（EMA），并提出了四种训练正则化干预方法以减少EMA。


<details>
  <summary>Details</summary>
Motivation: 微调LLM可能导致模型在目标领域外产生有害行为，即使微调数据本身无害。研究旨在为API提供实用的防护措施。

Method: 研究了四种干预方法：KL散度正则化、特征空间L2距离、安全子空间投影（SafeLoRA）和混合安全训练数据。

Result: 评估了四种方法在恶意任务和良性任务中的效果，发现它们能有效减少EMA。

Conclusion: 讨论了EMA研究的开放性问题，为未来研究提供了方向。

Abstract: Fine-tuning lets practitioners repurpose aligned large language models (LLMs)
for new domains, yet recent work reveals emergent misalignment (EMA): Even a
small, domain-specific fine-tune can induce harmful behaviors far outside the
target domain. Even in the case where model weights are hidden behind a
fine-tuning API, this gives attackers inadvertent access to a broadly
misaligned model in a way that can be hard to detect from the fine-tuning data
alone. We present the first systematic study of in-training safeguards against
EMA that are practical for providers who expose fine-tuning via an API. We
investigate four training regularization interventions: (i) KL-divergence
regularization toward a safe reference model, (ii) $\ell_2$ distance in feature
space, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving
of a small amount of safe training examples from a general instruct-tuning
dataset. We first evaluate the methods' emergent misalignment effect across
four malicious, EMA-inducing tasks. Second, we assess the methods' impacts on
benign tasks. We conclude with a discussion of open questions in emergent
misalignment research.

</details>


### [73] [Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)](https://arxiv.org/abs/2508.06251)
*Alejandro Moreno R.,Desale Fentaw,Samuel Palmer,Raúl Salles de Padua,Ninad Dixit,Samuel Mugel,Roman Orús,Manuel Radons,Josef Menter,Ali Abedi*

Main category: cs.LG

TL;DR: 本文提出了一种基于张量网络（MPS）的隐私保护高质量合成表格数据生成方法，在数据保真度和隐私保护能力上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺、隐私限制以及对多样化数据集的需求，同时确保合成数据的高质量和隐私保护。

Method: 使用矩阵乘积态（MPS）生成合成数据，并通过噪声注入和梯度裁剪实现差分隐私（DP）。

Result: MPS在严格隐私约束下表现优于CTGAN、VAE和PrivBayes等模型。

Conclusion: MPS是隐私感知合成数据生成的有前景工具，结合了张量网络的表达能力和隐私机制，适用于敏感领域。

Abstract: Synthetic data generation is a key technique in modern artificial
intelligence, addressing data scarcity, privacy constraints, and the need for
diverse datasets in training robust models. In this work, we propose a method
for generating privacy-preserving high-quality synthetic tabular data using
Tensor Networks, specifically Matrix Product States (MPS). We benchmark the
MPS-based generative model against state-of-the-art models such as CTGAN, VAE,
and PrivBayes, focusing on both fidelity and privacy-preserving capabilities.
To ensure differential privacy (DP), we integrate noise injection and gradient
clipping during training, enabling privacy guarantees via R\'enyi Differential
Privacy accounting. Across multiple metrics analyzing data fidelity and
downstream machine learning task performance, our results show that MPS
outperforms classical models, particularly under strict privacy constraints.
This work highlights MPS as a promising tool for privacy-aware synthetic data
generation. By combining the expressive power of tensor network representations
with formal privacy mechanisms, the proposed approach offers an interpretable
and scalable alternative for secure data sharing. Its structured design
facilitates integration into sensitive domains where both data quality and
confidentiality are critical.

</details>


### [74] [Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors](https://arxiv.org/abs/2508.06257)
*Jielong Lu,Zhihao Wu,Jiajun Yu,Jiajun Bu,Haishuai Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为GTMancer的框架，利用图神经网络和对比学习整合多组学数据，用于癌症亚型分类，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多组学数据整合中忽略了异构组学间的复杂耦合关系，限制了其在精准肿瘤学中解析癌症亚型异质性的能力。

Method: GTMancer框架基于图神经网络优化问题，利用对比学习将多组学数据嵌入统一语义空间，并通过双重注意力系数捕捉组学内和组学间的结构图先验。

Result: 在七个真实癌症数据集上的实验表明，GTMancer优于现有最先进算法。

Conclusion: GTMancer通过整合多组学数据，显著提升了癌症亚型分类的准确性，为精准肿瘤学提供了新工具。

Abstract: Integrating multi-omics datasets through data-driven analysis offers a
comprehensive understanding of the complex biological processes underlying
various diseases, particularly cancer. Graph Neural Networks (GNNs) have
recently demonstrated remarkable ability to exploit relational structures in
biological data, enabling advances in multi-omics integration for cancer
subtype classification. Existing approaches often neglect the intricate
coupling between heterogeneous omics, limiting their capacity to resolve subtle
cancer subtype heterogeneity critical for precision oncology. To address these
limitations, we propose a framework named Graph Transformer for Multi-omics
Cancer Subtype Classification (GTMancer). This framework builds upon the GNN
optimization problem and extends its application to complex multi-omics data.
Specifically, our method leverages contrastive learning to embed multi-omics
data into a unified semantic space. We unroll the multiplex graph optimization
problem in that unified space and introduce dual sets of attention coefficients
to capture structural graph priors both within and among multi-omics data. This
approach enables global omics information to guide the refining of the
representations of individual omics. Empirical experiments on seven real-world
cancer datasets demonstrate that GTMancer outperforms existing state-of-the-art
algorithms.

</details>


### [75] [OM2P: Offline Multi-Agent Mean-Flow Policy](https://arxiv.org/abs/2508.06269)
*Zhuoran Li,Xun Wang,Hai Zhong,Longbo Huang*

Main category: cs.LG

TL;DR: 提出OM2P算法，解决生成模型在离线多智能体强化学习中的采样效率问题，实现一步动作采样。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如扩散和流模型）在离线多智能体强化学习中采样效率低，难以应用于时间敏感或资源受限的场景。

Method: 提出OM2P算法，结合奖励感知优化方案、均值流匹配损失和Q函数监督，设计广义时间步分布和无导数估计策略。

Result: 在Multi-Agent Particle和MuJoCo基准测试中，OM2P性能优越，GPU内存使用减少3.8倍，训练速度提升10.8倍。

Conclusion: OM2P首次成功将均值流模型融入离线多智能体强化学习，为实用且可扩展的生成策略铺平道路。

Abstract: Generative models, especially diffusion and flow-based models, have been
promising in offline multi-agent reinforcement learning. However, integrating
powerful generative models into this framework poses unique challenges. In
particular, diffusion and flow-based policies suffer from low sampling
efficiency due to their iterative generation processes, making them impractical
in time-sensitive or resource-constrained settings. To tackle these
difficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel
offline MARL algorithm to achieve efficient one-step action sampling. To
address the misalignment between generative objectives and reward maximization,
we introduce a reward-aware optimization scheme that integrates a
carefully-designed mean-flow matching loss with Q-function supervision.
Additionally, we design a generalized timestep distribution and a
derivative-free estimation strategy to reduce memory overhead and improve
training stability. Empirical evaluations on Multi-Agent Particle and MuJoCo
benchmarks demonstrate that OM2P achieves superior performance, with up to a
3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time.
Our approach represents the first to successfully integrate mean-flow model
into offline MARL, paving the way for practical and scalable generative
policies in cooperative multi-agent settings.

</details>


### [76] [A Study on Regularization-Based Continual Learning Methods for Indic ASR](https://arxiv.org/abs/2508.06280)
*Gokul Adethya T,S. Jaya Nirmala*

Main category: cs.LG

TL;DR: 论文研究了在印度语言多样性背景下，通过持续学习（CL）方法开发包容性自动语音识别（ASR）系统，解决了传统多语言模型的数据隐私和顺序学习问题。


<details>
  <summary>Details</summary>
Motivation: 印度语言多样性对ASR系统开发提出了挑战，传统多语言模型因数据隐私和顺序学习限制而不适用，CL方法提供了一种解决方案。

Method: 使用基于Conformer的混合RNN-T/CTC模型，从印地语预训练开始，逐步学习八种印度语言，评估了三种CL策略（EWC、MAS、LwF）。

Result: 结果表明，CL方法有效缓解了遗忘问题，在干净和噪声数据上均表现良好，优于简单微调。

Conclusion: 持续学习是解决印度语言多样性ASR问题的有效方法，适用于实际约束下的扩展应用。

Abstract: Indias linguistic diversity poses significant challenges for developing
inclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual
models, which require simultaneous access to all language data, are impractical
due to the sequential arrival of data and privacy constraints. Continual
Learning (CL) offers a solution by enabling models to learn new languages
sequentially without catastrophically forgetting previously learned knowledge.
This paper investigates CL for ASR on Indian languages using a subset of the
IndicSUPERB benchmark. We employ a Conformer-based hybrid RNN-T/CTC model,
initially pretrained on Hindi, which is then incrementally trained on eight
additional Indian languages, for a total sequence of nine languages. We
evaluate three prominent regularization- and distillation-based CL strategies:
Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning
without Forgetting (LwF), selected for their suitability in no-replay,
privacy-conscious scenarios. Performance is analyzed using Word Error Rate
(WER) for both RNN-T and CTC paths on clean and noisy data, as well as
knowledge retention via Backward Transfer. We also explore the impact of
varying the number of training epochs (1, 2, 5, and 10) per task. Results,
compared against naive fine-tuning, demonstrate CLs effectiveness in mitigating
forgetting, making it a promising approach for scalable ASR in diverse Indian
languages under realistic constraints. The code is available at:
https://github.com/FrozenWolf-Cyber/Indic-CL-ASR

</details>


### [77] [Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback](https://arxiv.org/abs/2508.06292)
*Sanja Karilanova,Subhrakanti Dey,Ayça Özçelikkale*

Main category: cs.LG

TL;DR: 提出了一种新型多输出脉冲神经元模型，结合了线性状态转移和非线性反馈机制，性能与现有SNN模型相当。


<details>
  <summary>Details</summary>
Motivation: 结合SNN的低延迟、高效能与深度SSM的高性能，解决其缺乏重置机制的问题。

Method: 设计了一种多输出脉冲神经元模型，明确区分脉冲功能、重置条件和重置动作。

Result: 在多项任务中表现与现有SNN基准相当，重置机制克服了不稳定性。

Conclusion: 提出的重置机制扩展了深度SSM的应用范围，解决了稳定性问题。

Abstract: Neuromorphic computing is an emerging technology enabling low-latency and
energy-efficient signal processing. A key algorithmic tool in neuromorphic
computing is spiking neural networks (SNNs). SNNs are biologically inspired
neural networks which utilize stateful neurons, and provide low-bit data
processing by encoding and decoding information using spikes. Similar to SNNs,
deep state-space models (SSMs) utilize stateful building blocks. However, deep
SSMs, which recently achieved competitive performance in various temporal
modeling tasks, are typically designed with high-precision activation functions
and no reset mechanisms. To bridge the gains offered by SNNs and the recent
deep SSM models, we propose a novel multiple-output spiking neuron model that
combines a linear, general SSM state transition with a non-linear feedback
mechanism through reset. Compared to the existing neuron models for SNNs, our
proposed model clearly conceptualizes the differences between the spiking
function, the reset condition and the reset action. The experimental results on
various tasks, i.e., a keyword spotting task, an event-based vision task and a
sequential pattern recognition task, show that our proposed model achieves
performance comparable to existing benchmarks in the SNN literature. Our
results illustrate how the proposed reset mechanism can overcome instability
and enable learning even when the linear part of neuron dynamics is unstable,
allowing us to go beyond the strictly enforced stability of linear dynamics in
recent deep SSM models.

</details>


### [78] [FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields](https://arxiv.org/abs/2508.06301)
*Junhyeog Yun,Minui Hong,Gunhee Kim*

Main category: cs.LG

TL;DR: FedMeNF是一种新的联邦元学习方法，通过隐私保护损失函数解决传统FML的隐私泄露问题，实现高效优化和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 神经场学习需要大量数据和计算资源，传统FML方法存在隐私泄露问题，FedMeNF旨在解决这些问题。

Method: 提出FedMeNF方法，采用隐私保护损失函数，优化本地元学习过程，避免保留客户端私有数据。

Result: 实验表明，FedMeNF在少样本或非独立同分布数据下仍能快速优化并保持稳健重建性能，同时保护隐私。

Conclusion: FedMeNF在高效优化和隐私保护方面表现出色，适用于资源受限的边缘设备。

Abstract: Neural fields provide a memory-efficient representation of data, which can
effectively handle diverse modalities and large-scale data. However, learning
to map neural fields often requires large amounts of training data and
computations, which can be limited to resource-constrained edge devices. One
approach to tackle this limitation is to leverage Federated Meta-Learning
(FML), but traditional FML approaches suffer from privacy leakage. To address
these issues, we introduce a novel FML approach called FedMeNF. FedMeNF
utilizes a new privacy-preserving loss function that regulates privacy leakage
in the local meta-optimization. This enables the local meta-learner to optimize
quickly and efficiently without retaining the client's private data. Our
experiments demonstrate that FedMeNF achieves fast optimization speed and
robust reconstruction performance, even with few-shot or non-IID data across
diverse data modalities, while preserving client data privacy.

</details>


### [79] [Unsupervised Partner Design Enables Robust Ad-hoc Teamwork](https://arxiv.org/abs/2508.06336)
*Constantin Ruhdorfer,Matteo Bortoletto,Victor Oei,Anna Penzkofer,Andreas Bulling*

Main category: cs.LG

TL;DR: UPD是一种无监督的多智能体强化学习框架，通过动态生成训练伙伴和基于方差的度量优化，显著提升了协作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要预训练伙伴或手动调参的问题，实现完全无监督的协作学习。

Method: 通过随机混合策略生成多样伙伴，并使用基于方差的度量选择优化伙伴。

Result: 在Overcooked-AI等任务中，UPD显著优于基线方法，并在用户研究中表现更优。

Conclusion: UPD为无监督协作学习提供了高效且适应性强的解决方案。

Abstract: We introduce Unsupervised Partner Design (UPD) - a population-free,
multi-agent reinforcement learning framework for robust ad-hoc teamwork that
adaptively generates training partners without requiring pretrained partners or
manual parameter tuning. UPD constructs diverse partners by stochastically
mixing an ego agent's policy with biased random behaviours and scores them
using a variance-based learnability metric that prioritises partners near the
ego agent's current learning frontier. We show that UPD can be integrated with
unsupervised environment design, resulting in the first method enabling fully
unsupervised curricula over both level and partner distributions in a
cooperative setting. Through extensive evaluations on Overcooked-AI and the
Overcooked Generalisation Challenge, we demonstrate that this dynamic partner
curriculum is highly effective: UPD consistently outperforms both
population-based and population-free baselines as well as ablations. In a user
study, we further show that UPD achieves higher returns than all baselines and
was perceived as significantly more adaptive, more human-like, a better
collaborator, and less frustrating.

</details>


### [80] [Introducing Fractional Classification Loss for Robust Learning with Noisy Labels](https://arxiv.org/abs/2508.06346)
*Mert Can Kurucu,Tufan Kumbasar,İbrahim Eksin,Müjde Güzelkaya*

Main category: cs.LG

TL;DR: 论文提出了一种自适应鲁棒损失函数FCL，通过分数阶导数自动调整对标签噪声的鲁棒性，无需手动调参。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒损失函数需要大量数据集特定的超参数调优，限制了其应用。

Method: FCL结合交叉熵的分数阶导数（主动部分）和MAE（被动部分），动态学习分数阶导数阶数μ，平衡鲁棒性和收敛速度。

Result: FCL在基准数据集上实现了最先进的性能，无需手动调参。

Conclusion: FCL通过动态调整损失函数，有效解决了标签噪声下的分类问题。

Abstract: Robust loss functions are crucial for training deep neural networks in the
presence of label noise, yet existing approaches require extensive,
dataset-specific hyperparameter tuning. In this work, we introduce Fractional
Classification Loss (FCL), an adaptive robust loss that automatically
calibrates its robustness to label noise during training. Built within the
active-passive loss framework, FCL employs the fractional derivative of the
Cross-Entropy (CE) loss as its active component and the Mean Absolute Error
(MAE) as its passive loss component. With this formulation, we demonstrate that
the fractional derivative order $\mu$ spans a family of loss functions that
interpolate between MAE-like robustness and CE-like fast convergence.
Furthermore, we integrate $\mu$ into the gradient-based optimization as a
learnable parameter and automatically adjust it to optimize the trade-off
between robustness and convergence speed. We reveal that FCL's unique property
establishes a critical trade-off that enables the stable learning of $\mu$:
lower log penalties on difficult or mislabeled examples improve robustness but
impose higher penalties on easy or clean data, reducing model confidence in
them. Consequently, FCL can dynamically reshape its loss landscape to achieve
effective classification performance under label noise. Extensive experiments
on benchmark datasets show that FCL achieves state-of-the-art results without
the need for manual hyperparameter tuning.

</details>


### [81] [Structural Equation-VAE: Disentangled Latent Representations for Tabular Data](https://arxiv.org/abs/2508.06347)
*Ruiyu Zhang,Ce Zhao,Xin Zhao,Lin Nie,Wai-Fung Lam*

Main category: cs.LG

TL;DR: SE-VAE是一种新型变分自编码器，通过结构方程建模设计，直接嵌入测量结构，实现可解释的潜在表示学习。


<details>
  <summary>Details</summary>
Motivation: 解决表格数据中潜在表示学习的可解释性问题，特别是在科学和社会领域中理论驱动的潜在构造和测量有效性至关重要的情况下。

Method: SE-VAE通过结构方程建模设计，将潜在子空间与已知指标分组对齐，并引入全局干扰潜在变量以隔离构造特定的混杂变异。

Result: SE-VAE在模拟表格数据集上表现优异，在因子恢复、可解释性和对干扰变异的鲁棒性方面优于基线方法。

Conclusion: SE-VAE通过设计而非统计正则化实现解耦，为科学和社会领域的白盒生成建模提供了原则性框架。

Abstract: Learning interpretable latent representations from tabular data remains a
challenge in deep generative modeling. We introduce SE-VAE (Structural
Equation-Variational Autoencoder), a novel architecture that embeds measurement
structure directly into the design of a variational autoencoder. Inspired by
structural equation modeling, SE-VAE aligns latent subspaces with known
indicator groupings and introduces a global nuisance latent to isolate
construct-specific confounding variation. This modular architecture enables
disentanglement through design rather than through statistical regularizers
alone. We evaluate SE-VAE on a suite of simulated tabular datasets and
benchmark its performance against a series of leading baselines using standard
disentanglement metrics. SE-VAE consistently outperforms alternatives in factor
recovery, interpretability, and robustness to nuisance variation. Ablation
results reveal that architectural structure, rather than regularization
strength, is the key driver of performance. SE-VAE offers a principled
framework for white-box generative modeling in scientific and social domains
where latent constructs are theory-driven and measurement validity is
essential.

</details>


### [82] [Geometric-k-means: A Bound Free Approach to Fast and Eco-Friendly k-means](https://arxiv.org/abs/2508.06353)
*Parichit Sharma,Marcin Stanislaw,Hasan Kurban,Oguzhan Kulekci,Mehmet Dalkilic*

Main category: cs.LG

TL;DR: Gk-means是一种基于几何原理改进的k-means算法，通过利用标量投影显著提升效率，同时保持解的质量。


<details>
  <summary>Details</summary>
Motivation: 传统k-means算法效率较低，Gk-means旨在通过几何优化减少计算开销，提升性能和能源经济性。

Method: 利用标量投影识别高表达数据（HE），忽略低表达数据（LE），从而加速聚类过程。

Result: 实验表明，Gk-means在运行时间和距离计算上优于传统及SOTA k-means变体，且能源效率更高。

Conclusion: Gk-means是一种高效、节能的k-means改进方法，适用于多种数据集。

Abstract: This paper introduces Geometric-k-means (or Gk-means for short), a novel
approach that significantly enhances the efficiency and energy economy of the
widely utilized k-means algorithm, which, despite its inception over five
decades ago, remains a cornerstone in machine learning applications. The
essence of Gk-means lies in its active utilization of geometric principles,
specifically scalar projection, to significantly accelerate the algorithm
without sacrificing solution quality. This geometric strategy enables a more
discerning focus on data points that are most likely to influence cluster
updates, which we call as high expressive data (HE). In contrast, low
expressive data (LE), does not impact clustering outcome, is effectively
bypassed, leading to considerable reductions in computational overhead.
Experiments spanning synthetic, real-world and high-dimensional datasets,
demonstrate Gk-means is significantly better than traditional and state of the
art (SOTA) k-means variants in runtime and distance computations (DC).
Moreover, Gk-means exhibits better resource efficiency, as evidenced by its
reduced energy footprint, placing it as more sustainable alternative.

</details>


### [83] [Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts](https://arxiv.org/abs/2508.06361)
*Zhaomin Wu,Mingzhe Du,See-Kiong Ng,Bingsheng He*

Main category: cs.LG

TL;DR: 论文探讨了大型语言模型（LLM）在无人类诱导情况下自发性欺骗行为的可能性，并提出了一种基于心理学的评估框架。


<details>
  <summary>Details</summary>
Motivation: LLM的信任度至关重要，但现有研究多关注人为诱导的欺骗，忽视了模型自发的欺骗行为。

Method: 提出使用'接触搜索问题'框架，并引入两个统计指标（欺骗意图分数和欺骗行为分数）量化欺骗可能性。

Result: 评估14个主流LLM发现，任务难度增加时，欺骗倾向和行为不一致性均上升。

Conclusion: 即使最先进的LLM在处理复杂问题时也表现出欺骗倾向，这对LLM在关键领域的部署提出了警示。

Abstract: Large Language Models (LLMs) have been widely deployed in reasoning,
planning, and decision-making tasks, making their trustworthiness a critical
concern. The potential for intentional deception, where an LLM deliberately
fabricates or conceals information to serve a hidden objective, remains a
significant and underexplored threat. Existing studies typically induce such
deception by explicitly setting a "hidden" objective through prompting or
fine-tuning, which may not fully reflect real-world human-LLM interactions.
Moving beyond this human-induced deception, we investigate LLMs' self-initiated
deception on benign prompts. To address the absence of ground truth in this
evaluation, we propose a novel framework using "contact searching questions."
This framework introduces two statistical metrics derived from psychological
principles to quantify the likelihood of deception. The first, the Deceptive
Intention Score, measures the model's bias towards a hidden objective. The
second, Deceptive Behavior Score, measures the inconsistency between the LLM's
internal belief and its expressed output. Upon evaluating 14 leading LLMs, we
find that both metrics escalate as task difficulty increases, rising in
parallel for most models. Building on these findings, we formulate a
mathematical model to explain this behavior. These results reveal that even the
most advanced LLMs exhibit an increasing tendency toward deception when
handling complex problems, raising critical concerns for the deployment of LLM
agents in complex and crucial domains.

</details>


### [84] [ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design](https://arxiv.org/abs/2508.06364)
*Renyi Zhou,Huimin Zhu,Jing Tang,Min Li*

Main category: cs.LG

TL;DR: ActivityDiff是一种基于扩散模型的生成方法，通过分类器引导技术实现分子活性的多目标控制，包括增强目标活性和减少脱靶效应。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法主要关注单一活性分子，缺乏同时管理多目标分子活性的机制，难以满足药物设计中对多目标控制和安全性平衡的需求。

Method: ActivityDiff利用预训练的药物-靶点分类器进行正负引导，通过扩散模型生成分子，实现多目标活性的协同调控。

Result: 实验表明，ActivityDiff能有效处理单/双靶点生成、片段约束双靶点设计、选择性生成和脱靶效应减少等任务。

Conclusion: ActivityDiff为分子设计提供了一种平衡效能和安全性的新范式，是一个多功能且可扩展的框架。

Abstract: Achieving precise control over a molecule's biological activity-encompassing
targeted activation/inhibition, cooperative multi-target modulation, and
off-target toxicity mitigation-remains a critical challenge in de novo drug
design. However, existing generative methods primarily focus on producing
molecules with a single desired activity, lacking integrated mechanisms for the
simultaneous management of multiple intended and unintended molecular
interactions. Here, we propose ActivityDiff, a generative approach based on the
classifier-guidance technique of diffusion models. It leverages separately
trained drug-target classifiers for both positive and negative guidance,
enabling the model to enhance desired activities while minimizing harmful
off-target effects. Experimental results show that ActivityDiff effectively
handles essential drug design tasks, including single-/dual-target generation,
fragment-constrained dual-target design, selective generation to enhance target
specificity, and reduction of off-target effects. These results demonstrate the
effectiveness of classifier-guided diffusion in balancing efficacy and safety
in molecular design. Overall, our work introduces a novel paradigm for
achieving integrated control over molecular activity, and provides ActivityDiff
as a versatile and extensible framework.

</details>


### [85] [End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation](https://arxiv.org/abs/2508.06387)
*Anurag Tripathi,Vaibhav Patle,Abhinav Jain,Ayush Pundir,Sairam Menon,Ajeet Kumar Singh*

Main category: cs.LG

TL;DR: 提出了一种三阶段端到端文本到SQL框架，先识别目标数据库再生成SQL查询，结合LLM和规则提取，提升了数据库意图预测和SQL生成准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设目标数据库已知，但在多数据库场景下，识别正确数据库是关键但被忽视的步骤。

Method: 1. 利用LLM和提示工程从自然语言查询中提取规则；2. 训练RoBERTa微调编码器预测db_id；3. 使用批评代理修正SQL错误。

Result: 实验表明，该框架在数据库意图预测和SQL生成准确性上优于当前最优模型。

Conclusion: 提出的三阶段框架解决了多数据库场景下的数据库识别问题，显著提升了文本到SQL的性能。

Abstract: Text-to-SQL bridges the gap between natural language and structured database
language, thus allowing non-technical users to easily query databases.
Traditional approaches model text-to-SQL as a direct translation task, where a
given Natural Language Query (NLQ) is mapped to an SQL command. Recent advances
in large language models (LLMs) have significantly improved translation
accuracy, however, these methods all require that the target database is
pre-specified. This becomes problematic in scenarios with multiple extensive
databases, where identifying the correct database becomes a crucial yet
overlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL
framework to identify the user's intended database before generating SQL
queries. Our approach leverages LLMs and prompt engineering to extract implicit
information from natural language queries (NLQs) in the form of a ruleset. We
then train a large db\_id prediction model, which includes a RoBERTa-based
finetuned encoder, to predict the correct Database identifier (db\_id) based on
both the NLQ and the LLM-generated rules. Finally, we refine the generated SQL
by using critic agents to correct errors. Experimental results demonstrate that
our framework outperforms the current state-of-the-art models in both database
intent prediction and SQL generation accuracy.

</details>


### [86] [A New Lens on Homelessness: Daily Tent Monitoring with 311 Calls and Street Images](https://arxiv.org/abs/2508.06409)
*Wooyong Jung,Sola Kim,Dongwook Kim,Maryam Tabar,Dongwon Lee*

Main category: cs.LG

TL;DR: 利用公开众包数据（如311服务电话和街景图像）追踪旧金山无家可归者帐篷趋势，提供更及时、本地化和低成本的信息。


<details>
  <summary>Details</summary>
Motivation: 现有监测方法（如点时间统计）在频率、一致性和空间细节上存在局限，无法捕捉快速变化和空间迁移。

Method: 使用311服务电话和街景图像数据，建立预测模型，捕捉每日和社区级别的细微变化。

Result: 模型揭示了传统统计忽略的模式，如COVID-19期间的快速波动和帐篷位置的空间迁移。

Conclusion: 该方法为政策制定和干预评估提供了更有效的工具，有助于减少无庇护无家可归现象。

Abstract: Homelessness in the United States has surged to levels unseen since the Great
Depression. However, existing methods for monitoring it, such as point-in-time
(PIT) counts, have limitations in terms of frequency, consistency, and spatial
detail. This study proposes a new approach using publicly available,
crowdsourced data, specifically 311 Service Calls and street-level imagery, to
track and forecast homeless tent trends in San Francisco. Our predictive model
captures fine-grained daily and neighborhood-level variations, uncovering
patterns that traditional counts often overlook, such as rapid fluctuations
during the COVID-19 pandemic and spatial shifts in tent locations over time. By
providing more timely, localized, and cost-effective information, this approach
serves as a valuable tool for guiding policy responses and evaluating
interventions aimed at reducing unsheltered homelessness.

</details>


### [87] [Sample-efficient LLM Optimization with Reset Replay](https://arxiv.org/abs/2508.06412)
*Zichuan Liu,Jinyu Wang,Lei Song,Jiang Bian*

Main category: cs.LG

TL;DR: LoRR是一种插件，通过高重放训练和周期性重置策略提升LLM的样本效率，结合SFT和偏好损失优化性能。


<details>
  <summary>Details</summary>
Motivation: 解决RL和偏好优化方法在LLM训练中样本效率低和初始偏差的问题。

Method: 提出LoRR插件，采用高重放训练、周期性重置和混合优化目标（SFT+偏好损失）。

Result: LoRR显著提升多种偏好优化方法的性能，在数学和通用推理任务中表现优异。

Conclusion: LoRR为LLM微调提供了高效、实用的解决方案，尤其适合数据有限场景。

Abstract: Recent advancements in post-training Large Language Models (LLMs),
particularly through Reinforcement Learning (RL) and preference optimization
methods, are key drivers for enhancing their reasoning capabilities. However,
these methods are often plagued by low sample efficiency and a susceptibility
to primacy bias, where overfitting to initial experiences degrades policy
quality and damages the learning process. To address these challenges, we
introduce LLM optimization with Reset Replay (LoRR), a general and powerful
plugin designed to enhance sample efficiency in any preference-based
optimization framework. LoRR core mechanism enables training at a high replay
number, maximizing the utility of each collected data batch. To counteract the
risk of overfitting inherent in high-replay training, LoRR incorporates a
periodic reset strategy with reusing initial data, which preserves network
plasticity. Furthermore, it leverages a hybrid optimization objective,
combining supervised fine-tuning (SFT) and preference-based losses to further
bolster data exploitation. Our extensive experiments demonstrate that LoRR
significantly boosts the performance of various preference optimization methods
on both mathematical and general reasoning benchmarks. Notably, an iterative
DPO approach augmented with LoRR achieves comparable performance on challenging
math tasks, outperforming some complex and computationally intensive RL-based
algorithms. These findings highlight that LoRR offers a practical,
sample-efficient, and highly effective paradigm for LLM finetuning, unlocking
greater performance from limited data.

</details>


### [88] [LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection](https://arxiv.org/abs/2508.06467)
*Ameya Anjarlekar,Sandeep Pombra*

Main category: cs.LG

TL;DR: GRIN是一个针对大型语言模型（LLM）的模块化、目标性遗忘框架，通过梯度比指标定位需遗忘数据的参数，并选择性注入噪声以提高遗忘效果，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临法律和伦理审查，需要有效遗忘敏感或未经授权数据，现有方法常导致不完全遗忘或无关知识退化。

Method: 提出GRIN框架，使用梯度比指标定位关键参数，选择性注入噪声后进行微调。

Result: 在TOFU、WMDP和SafePKU等基准测试中验证了GRIN的有效性。

Conclusion: GRIN在实现目标性遗忘的同时，保持了模型的实用性。

Abstract: The growing legal and ethical scrutiny of large language models (LLMs)
necessitates effective machine unlearning, particularly for sensitive or
unauthorized data. Existing empirical methods often yield incomplete forgetting
or unintended degradation of unrelated knowledge due to poor localization. In
this work, we propose GRIN: a modular and targeted framework for LLM
unlearning. GRIN introduces a novel gradient-ratio-based metric to identify
parameters most responsible for memorizing forget data. We then perform
selective noise injection into these parameters prior to fine-tuning, which
improves unlearning performance while maintaining model utility. Finally, we
propose new evaluation metrics tailored to the LLM setting and validate our
approach on standard benchmarks such as TOFU, WMDP, and SafePKU.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [89] [InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization](https://arxiv.org/abs/2508.05731)
*Yuhang Liu,Zeyu Liu,Shuanghe Zhu,Pengxiang Li,Congkai Xie,Jiasheng Wang,Xueyu Hu,Xiaotian Han,Jianbo Yuan,Xinyao Wang,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: 论文提出了一种名为AEPO的新策略优化框架，用于解决多模态大语言模型在GUI任务中的语义对齐问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在GUI任务中语义对齐的探索效率问题。

Method: 提出AEPO框架，采用多答案生成策略和自适应探索奖励函数（AER）。

Result: AEPO训练的模型在多个GUI基准测试中达到最新最优性能，相对基线提升9.0%。

Conclusion: AEPO有效解决了语义对齐的探索瓶颈，显著提升了模型性能。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the
development of autonomous agents that operate on Graphical User Interfaces
(GUIs) using pure visual input. A fundamental challenge is robustly grounding
natural language instructions. This requires a precise spatial alignment, which
accurately locates the coordinates of each element, and, more critically, a
correct semantic alignment, which matches the instructions to the functionally
appropriate UI element. Although Reinforcement Learning with Verifiable Rewards
(RLVR) has proven to be effective at improving spatial alignment for these
MLLMs, we find that inefficient exploration bottlenecks semantic alignment,
which prevent models from learning difficult semantic associations. To address
this exploration problem, we present Adaptive Exploration Policy Optimization
(AEPO), a new policy optimization framework. AEPO employs a multi-answer
generation strategy to enforce broader exploration, which is then guided by a
theoretically grounded Adaptive Exploration Reward (AER) function derived from
first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B
and InfiGUI-G1-7B, establish new state-of-the-art results across multiple
challenging GUI grounding benchmarks, achieving significant relative
improvements of up to 9.0% against the naive RLVR baseline on benchmarks
designed to test generalization and semantic understanding. Resources are
available at https://github.com/InfiXAI/InfiGUI-G1.

</details>


### [90] [A Framework for Inherently Safer AGI through Language-Mediated Active Inference](https://arxiv.org/abs/2508.05766)
*Bo Wen*

Main category: cs.AI

TL;DR: 本文提出了一种结合主动推理原则与大型语言模型（LLM）的新型框架，用于开发安全的通用人工智能（AGI）。通过透明信念表示和层次化价值对齐，将安全性保证融入系统核心设计。


<details>
  <summary>Details</summary>
Motivation: 传统AI安全方法（如事后可解释性和奖励工程）存在根本性局限，需要一种更本质的安全设计。

Method: 采用多智能体系统，基于主动推理原则自组织，通过自然语言表示信念和偏好，实现层次化安全约束。

Result: 提出了一种通过模块化结构和资源感知自由能最小化确保安全性的机制。

Conclusion: 该框架为AGI开发提供了一条本质安全的路径，未来研究将基于ARC基准验证其安全性。

Abstract: This paper proposes a novel framework for developing safe Artificial General
Intelligence (AGI) by combining Active Inference principles with Large Language
Models (LLMs). We argue that traditional approaches to AI safety, focused on
post-hoc interpretability and reward engineering, have fundamental limitations.
We present an architecture where safety guarantees are integrated into the
system's core design through transparent belief representations and
hierarchical value alignment. Our framework leverages natural language as a
medium for representing and manipulating beliefs, enabling direct human
oversight while maintaining computational tractability. The architecture
implements a multi-agent system where agents self-organize according to Active
Inference principles, with preferences and safety constraints flowing through
hierarchical Markov blankets. We outline specific mechanisms for ensuring
safety, including: (1) explicit separation of beliefs and preferences in
natural language, (2) bounded rationality through resource-aware free energy
minimization, and (3) compositional safety through modular agent structures.
The paper concludes with a research agenda centered on the Abstraction and
Reasoning Corpus (ARC) benchmark, proposing experiments to validate our
framework's safety properties. Our approach offers a path toward AGI
development that is inherently safer, rather than retrofitted with safety
measures.

</details>


### [91] [Whither symbols in the era of advanced neural networks?](https://arxiv.org/abs/2508.05776)
*Thomas L. Griffiths,Brenden M. Lake,R. Thomas McCoy,Ellie Pavlick,Taylor W. Webb*

Main category: cs.AI

TL;DR: 现代神经网络表现出类似人类思维的符号系统能力，挑战了人类认知必须基于符号系统的观点，并提出新的研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络是否具备类似人类思维的符号系统能力，并重新审视人类思维的符号基础。

Method: 通过分析神经网络的表现能力，对比人类思维的符号系统特征。

Result: 神经网络展现出类似符号系统的能力，表明人类思维可能不完全依赖符号系统。

Conclusion: 提出新的研究方向，重新思考人类思维的符号基础及其与神经网络的关系。

Abstract: Some of the strongest evidence that human minds should be thought about in
terms of symbolic systems has been the way they combine ideas, produce novelty,
and learn quickly. We argue that modern neural networks -- and the artificial
intelligence systems built upon them -- exhibit similar abilities. This
undermines the argument that the cognitive processes and representations used
by human minds are symbolic, although the fact that these neural networks are
typically trained on data generated by symbolic systems illustrates that such
systems play an important role in characterizing the abstract problems that
human minds have to solve. This argument leads us to offer a new agenda for
research on the symbolic basis of human thought.

</details>


### [92] [Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making](https://arxiv.org/abs/2508.05792)
*Kausik Lakkaraju,Siva Likitha Valluru,Biplav Srivastava*

Main category: cs.AI

TL;DR: H-XAI是一个统一的解释性AI框架，结合因果评分与传统XAI方法，支持多方法交互式解释，满足不同利益相关者的需求。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法主要服务于开发者，未能满足多样化利益相关者的需求，且缺乏对假设测试的支持。

Method: H-XAI整合因果评分与传统XAI方法，支持交互式多方法解释，包括假设测试和基准比较。

Result: 通过两个案例研究（信用风险分类和金融时间序列预测），验证了H-XAI的通用性和实用性。

Conclusion: H-XAI填补了现有XAI方法的空白，通过结合因果评分和后验解释，为利益相关者提供个性化支持。

Abstract: Current eXplainable AI (XAI) methods largely serve developers, often focusing
on justifying model outputs rather than supporting diverse stakeholder needs. A
recent shift toward Evaluative AI reframes explanation as a tool for hypothesis
testing, but still focuses primarily on operational organizations. We introduce
Holistic-XAI (H-XAI), a unified framework that integrates causal rating methods
with traditional XAI methods to support explanation as an interactive,
multi-method process. H-XAI allows stakeholders to ask a series of questions,
test hypotheses, and compare model behavior against automatically constructed
random and biased baselines. It combines instance-level and global
explanations, adapting to each stakeholder's goals, whether understanding
individual decisions, assessing group-level bias, or evaluating robustness
under perturbations. We demonstrate the generality of our approach through two
case studies spanning six scenarios: binary credit risk classification and
financial time-series forecasting. H-XAI fills critical gaps left by existing
XAI methods by combining causal ratings and post-hoc explanations to answer
stakeholder-specific questions at both the individual decision level and the
overall model level.

</details>


### [93] [The Fair Game: Auditing & Debiasing AI Algorithms Over Time](https://arxiv.org/abs/2508.06443)
*Debabrota Basu,Udvas Das*

Main category: cs.AI

TL;DR: 论文提出了一种动态机制“Fair Game”，通过结合审计员和去偏算法，利用强化学习实现机器学习算法的公平性，并适应社会动态变化。


<details>
  <summary>Details</summary>
Motivation: 现有公平机器学习的定义多为观察性，且存在冲突，难以在动态社会环境中实现公平目标。

Method: 提出“Fair Game”框架，将审计员和去偏算法通过强化学习循环结合，动态调整公平目标。

Result: “Fair Game”提供了一个灵活且随时间适应的框架，可在部署前后构建公平机器学习系统。

Conclusion: 该框架模拟了社会伦理和法律框架的演变，为公平机器学习提供了动态解决方案。

Abstract: An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify
different types of bias (also known as unfairness) exhibited in the predictions
of ML algorithms, and to design new algorithms to mitigate them. Often, the
definitions of bias used in the literature are observational, i.e. they use the
input and output of a pre-trained algorithm to quantify a bias under concern.
In reality,these definitions are often conflicting in nature and can only be
deployed if either the ground truth is known or only in retrospect after
deploying the algorithm. Thus,there is a gap between what we want Fair ML to
achieve and what it does in a dynamic social environment. Hence, we propose an
alternative dynamic mechanism,"Fair Game",to assure fairness in the predictions
of an ML algorithm and to adapt its predictions as the society interacts with
the algorithm over time. "Fair Game" puts together an Auditor and a Debiasing
algorithm in a loop around an ML algorithm. The "Fair Game" puts these two
components in a loop by leveraging Reinforcement Learning (RL). RL algorithms
interact with an environment to take decisions, which yields new observations
(also known as data/feedback) from the environment and in turn, adapts future
decisions. RL is already used in algorithms with pre-fixed long-term fairness
goals. "Fair Game" provides a unique framework where the fairness goals can be
adapted over time by only modifying the auditor and the different biases it
quantifies. Thus,"Fair Game" aims to simulate the evolution of ethical and
legal frameworks in the society by creating an auditor which sends feedback to
a debiasing algorithm deployed around an ML system. This allows us to develop a
flexible and adaptive-over-time framework to build Fair ML systems pre- and
post-deployment.

</details>


### [94] [Safety of Embodied Navigation: A Survey](https://arxiv.org/abs/2508.05855)
*Zixia Wang,Jia Hu,Ronghui Mu*

Main category: cs.AI

TL;DR: 本文综述了具身导航中的安全问题，分析了攻击策略、防御机制和评估方法，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和具身AI的发展，具身导航的安全问题日益突出，需确保其在动态环境中的安全性。

Method: 通过综合分析现有安全挑战、缓解技术、数据集和评估指标，探讨未解决问题和未来方向。

Result: 提出了潜在攻击方法、缓解策略、可靠评估技术和验证框架的研究方向。

Conclusion: 本文为开发更安全可靠的具身导航系统提供了指导，并对社会安全和工业效率提升有广泛意义。

Abstract: As large language models (LLMs) continue to advance and gain influence, the
development of embodied AI has accelerated, drawing significant attention,
particularly in navigation scenarios. Embodied navigation requires an agent to
perceive, interact with, and adapt to its environment while moving toward a
specified target in unfamiliar settings. However, the integration of embodied
navigation into critical applications raises substantial safety concerns. Given
their deployment in dynamic, real-world environments, ensuring the safety of
such systems is critical. This survey provides a comprehensive analysis of
safety in embodied navigation from multiple perspectives, encompassing attack
strategies, defense mechanisms, and evaluation methodologies. Beyond conducting
a comprehensive examination of existing safety challenges, mitigation
technologies, and various datasets and metrics that assess effectiveness and
robustness, we explore unresolved issues and future research directions in
embodied navigation safety. These include potential attack methods, mitigation
strategies, more reliable evaluation techniques, and the implementation of
verification frameworks. By addressing these critical gaps, this survey aims to
provide valuable insights that can guide future research toward the development
of safer and more reliable embodied navigation systems. Furthermore, the
findings of this study have broader implications for enhancing societal safety
and increasing industrial efficiency.

</details>


### [95] [What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting](https://arxiv.org/abs/2508.06454)
*Joshua Caiata,Ben Armstrong,Kate Larson*

Main category: cs.AI

TL;DR: 本文提出了一种数据驱动框架，用于评估多赢家投票规则在不同偏好分布下违反公理的频率，并通过神经网络设计优于传统规则的新投票系统。


<details>
  <summary>Details</summary>
Motivation: 研究多赢家投票规则在实践中的公理违反情况，以补充传统的最坏情况分析。

Method: 提出数据驱动框架，分析多种偏好分布下投票规则的公理表现，并利用神经网络设计新规则。

Result: 神经网络作为投票规则在减少公理违反方面优于传统规则。

Conclusion: 数据驱动方法可为投票系统设计提供新思路，推动社会选择领域的数据驱动研究。

Abstract: Committee-selection problems arise in many contexts and applications, and
there has been increasing interest within the social choice research community
on identifying which properties are satisfied by different multi-winner voting
rules. In this work, we propose a data-driven framework to evaluate how
frequently voting rules violate axioms across diverse preference distributions
in practice, shifting away from the binary perspective of axiom satisfaction
given by worst-case analysis. Using this framework, we analyze the relationship
between multi-winner voting rules and their axiomatic performance under several
preference distributions. We then show that neural networks, acting as voting
rules, can outperform traditional rules in minimizing axiom violations. Our
results suggest that data-driven approaches to social choice can inform the
design of new voting systems and support the continuation of data-driven
research in social choice.

</details>


### [96] [Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning](https://arxiv.org/abs/2508.05888)
*Sahil Bansal,Sai Shruthi Sistla,Aarti Arikatala,Sebastian Schreiber*

Main category: cs.AI

TL;DR: 论文提出了一种基于知识图谱（KG）的工具检索框架，通过捕捉工具间的语义关系和功能依赖关系，提升多步任务中的工具检索准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅依赖用户查询与工具描述的相似性，限制了多步请求的检索准确性，因此需要一种更全面的方法。

Method: 采用知识图谱框架，利用1-hop ego工具图集合建模工具间的直接和间接连接，实现更全面的工具选择。

Result: 在合成数据集上，基于工具图的方法在Complete Recall指标上达到91.85%，优于非KG基线方法的89.26%。

Conclusion: 知识图谱的结构信息为纯相似性匹配提供了补充信号，特别适用于需要顺序工具组合的查询。

Abstract: Effective tool retrieval is essential for AI agents to select from a vast
array of tools when identifying and planning actions in the context of complex
user queries. Despite its central role in planning, this aspect remains
underexplored in the literature. Traditional approaches rely primarily on
similarities between user queries and tool descriptions, which significantly
limits retrieval accuracy, specifically when handling multi-step user requests.
To address these limitations, we propose a Knowledge Graph (KG)-based tool
retrieval framework that captures the semantic relationships between tools and
their functional dependencies. Our retrieval algorithm leverages ensembles of
1-hop ego tool graphs to model direct and indirect connections between tools,
enabling more comprehensive and contextual tool selection for multi-step tasks.
We evaluate our approach on a synthetically generated internal dataset across
six defined user classes, extending previous work on coherent dialogue
synthesis and too retrieval benchmarks. Results demonstrate that our tool
graph-based method achieves 91.85% tool coverage on the micro-average Complete
Recall metric, compared to 89.26% for re-ranked semantic-lexical hybrid
retrieval, the strongest non-KG baseline in our experiments. These findings
support our hypothesis that the structural information in the KG provides
complementary signals to pure similarity matching, particularly for queries
requiring sequential tool composition.

</details>


### [97] [Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making](https://arxiv.org/abs/2508.05996)
*Kaitao Chen,Mianxin Liu,Daoming Zong,Chaoyue Ding,Shaohao Rui,Yankai Jiang,Mu Zhou,Xiaosong Wang*

Main category: cs.AI

TL;DR: 提出MedOrch框架，通过LLM中介协调多VLM专家代理协作，提升医疗多模态决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有多代理研究多限于语言任务，多模态场景下VLM协作能力不足，需解决错误结果放大的问题。

Method: 采用LLM中介代理协调多VLM专家代理，利用开源VLM实现异构模型协作。

Result: 在五项医疗视觉问答基准测试中表现优异，协作能力超越单一代理。

Conclusion: 中介引导的多代理协作可推动医疗多模态智能发展。

Abstract: Complex medical decision-making involves cooperative workflows operated by
different clinicians. Designing AI multi-agent systems can expedite and augment
human-level clinical decision-making. Existing multi-agent researches primarily
focus on language-only tasks, yet their extension to multimodal scenarios
remains challenging. A blind combination of diverse vision-language models
(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are
less capable in instruction following and importantly self-reflection, compared
to large language models (LLMs) of comparable sizes. This disparity largely
constrains VLMs' ability in cooperative workflows. In this study, we propose
MedOrch, a mediator-guided multi-agent collaboration framework for medical
multimodal decision-making. MedOrch employs an LLM-based mediator agent that
enables multiple VLM-based expert agents to exchange and reflect on their
outputs towards collaboration. We utilize multiple open-source general-purpose
and domain-specific VLMs instead of costly GPT-series models, revealing the
strength of heterogeneous models. We show that the collaboration within
distinct VLM-based agents can surpass the capabilities of any individual agent.
We validate our approach on five medical vision question answering benchmarks,
demonstrating superior collaboration performance without model training. Our
findings underscore the value of mediator-guided multi-agent collaboration in
advancing medical multimodal intelligence. Our code will be made publicly
available.

</details>


### [98] [Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning](https://arxiv.org/abs/2508.06042)
*Daechul Ahn,San Kim,Jonghyun Choi*

Main category: cs.AI

TL;DR: 论文提出了一种分层多智能体框架HIMA，结合模仿学习和元控制器，解决了大型语言模型在动态长期任务（如星际争霸II）中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在动态、长期任务（如实时战略游戏）中表现不佳，特别是在资源管理和环境适应方面。

Method: 提出HIMA框架，通过专家示范训练多个专用模仿学习智能体，并由元控制器Strategic Planner协调其行动，生成适应环境的计划。

Result: HIMA在战略清晰度、适应性和计算效率上优于现有方法，验证了其有效性。

Conclusion: 结合专用模仿模块和元级协调，可以开发更鲁棒、通用的AI智能体。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive action
sequence prediction capabilities but often struggle with dynamic, long-horizon
tasks such as real-time strategic games. In a game such as StarCraftII (SC2),
agents need to manage resource constraints and adapt to evolving battlefield
situations in a partially observable environment. This often overwhelms
exisiting LLM-based approaches. To address these challenges, we propose a
hierarchical multi-agent framework that employs specialized imitation learning
agents under a meta-controller called Strategic Planner (SP). By expert
demonstrations, each specialized agent learns a distinctive strategy, such as
aerial support or defensive maneuvers, and produces coherent, structured
multistep action sequences. The SP then orchestrates these proposals into a
single, environmentally adaptive plan that ensures local decisions aligning
with long-term strategies. We call this HIMA (Hierarchical Imitation
Multi-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that
encompasses all race match combinations in SC2. Our empirical results show that
HIMA outperforms state of the arts in strategic clarity, adaptability, and
computational efficiency, underscoring the potential of combining specialized
imitation modules with meta-level orchestration to develop more robust,
general-purpose AI agents.

</details>


### [99] [LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences](https://arxiv.org/abs/2508.06060)
*Sankarshan Damle,Boi Faltings*

Main category: cs.AI

TL;DR: 论文提出了一个双用途框架，利用参与式预算（PB）作为（i）LLM资源分配的实践场景和（ii）评估LLM推理能力的动态基准。通过三种提示策略测试LLM在预算约束下的项目选择能力，并评估其与最优解的差距。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在复杂决策任务中的应用潜力尚未充分探索，且现有评估方法存在数据污染和静态基准的局限性。

Method: 采用参与式预算（PB）作为框架，通过贪婪选择、直接优化和启发式改进三种提示策略，测试LLM在预算约束下的资源分配能力，并评估其推理表现。

Result: 结果表明提示设计对LLM表现至关重要，LLM在机制设计和处理非结构化输入方面具有潜力。

Conclusion: LLM在资源分配和偏好推断任务中展现出潜力，提示设计是关键因素。

Abstract: Large Language Models (LLMs) are increasingly expected to handle complex
decision-making tasks, yet their ability to perform structured resource
allocation remains underexplored. Evaluating their reasoning is also difficult
due to data contamination and the static nature of existing benchmarks. We
present a dual-purpose framework leveraging Participatory Budgeting (PB) both
as (i) a practical setting for LLM-based resource allocation and (ii) an
adaptive benchmark for evaluating their reasoning capabilities. We task LLMs
with selecting project subsets under feasibility (e.g., budget) constraints via
three prompting strategies: greedy selection, direct optimization, and a
hill-climbing-inspired refinement. We benchmark LLMs' allocations against a
utility-maximizing oracle. Interestingly, we also test whether LLMs can infer
structured preferences from natural-language voter input or metadata, without
explicit votes. By comparing allocations based on inferred preferences to those
from ground-truth votes, we evaluate LLMs' ability to extract preferences from
open-ended input. Our results underscore the role of prompt design and show
that LLMs hold promise for mechanism design with unstructured inputs.

</details>


### [100] [Don't Forget Imagination!](https://arxiv.org/abs/2508.06062)
*Evgenii E. Vityaev,Andrei Mantsivoda*

Main category: cs.AI

TL;DR: 论文呼吁重视认知想象力在AI中的作用，并提出语义模型作为模拟工具。


<details>
  <summary>Details</summary>
Motivation: 认知想象力在人类思维中起关键作用，但当前AI领域对其重视不足，导致推理能力受限。

Method: 提出语义模型，一种基于概率因果关系的数学模型，可学习和确保想象上下文的连贯性。

Result: 语义模型能模拟认知想象力，支持透明操作和一致性验证。

Conclusion: 认知想象力是AI领域的重要突破方向，语义模型为实现这一目标提供了新工具。

Abstract: Cognitive imagination is a type of imagination that plays a key role in human
thinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to
mentally visualize coherent and holistic systems of concepts and causal links
that serve as semantic contexts for reasoning, decision making and prediction.
Our position is that the role of cognitive imagination is still greatly
underestimated, and this creates numerous problems and diminishes the current
capabilities of AI. For instance, when reasoning, humans rely on imaginary
contexts to retrieve background info. They also constantly return to the
context for semantic verification that their reasoning is still reasonable.
Thus, reasoning without imagination is blind. This paper is a call for greater
attention to cognitive imagination as the next promising breakthrough in
artificial intelligence. As an instrument for simulating cognitive imagination,
we propose semantic models -- a new approach to mathematical models that can
learn, like neural networks, and are based on probabilistic causal
relationships. Semantic models can simulate cognitive imagination because they
ensure the consistency of imaginary contexts and implement a glass-box approach
that allows the context to be manipulated as a holistic and coherent system of
interrelated facts glued together with causal relations.

</details>


### [101] [A Generic Complete Anytime Beam Search for Optimal Decision Tree](https://arxiv.org/abs/2508.06064)
*Harold Silvère Kiossou,Siegfried Nijssen,Pierre Schaus*

Main category: cs.AI

TL;DR: CA-DL8.5是一种通用的、完整的、随时可用的波束搜索算法，扩展了DL8.5框架，统一了现有的一些随时策略，并通过模块化设计整合了多种启发式和松弛机制。


<details>
  <summary>Details</summary>
Motivation: 由于现有方法在未完成搜索时难以快速找到高质量的决策树，且缺乏系统性比较，因此需要一种更有效的随时扩展方法。

Method: CA-DL8.5结合了DL8.5的高效剪枝和缓存技术，采用基于重启的波束搜索逐步放松剪枝标准，提升解的质量。

Result: 实验表明，基于LDS启发式的CA-DL8.5在随时性能上表现最佳，优于其他变体和Blossom算法。

Conclusion: CA-DL8.5为决策树学习提供了一个通用框架，支持多种启发式和搜索策略，并在保持最优性的同时提升了随时性能。

Abstract: Finding an optimal decision tree that minimizes classification error is known
to be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic
programming guarantee optimality, they often suffer from poor anytime behavior
-- meaning they struggle to find high-quality decision trees quickly when the
search is stopped before completion -- due to unbalanced search space
exploration. To address this, several anytime extensions of exact methods have
been proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not
been systematically compared, making it difficult to assess their relative
effectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and
anytime beam search algorithm that extends the DL8.5 framework and unifies some
existing anytime strategies. In particular, CA-DL8.5 generalizes previous
approaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various
heuristics and relaxation mechanisms through a modular design. The algorithm
reuses DL8.5's efficient branch-and-bound pruning and trie-based caching,
combined with a restart-based beam search that gradually relaxes pruning
criteria to improve solution quality over time. Our contributions are twofold:
(1) We introduce this new generic framework for exact and anytime decision tree
learning, enabling the incorporation of diverse heuristics and search
strategies; (2) We conduct a rigorous empirical comparison of several
instantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k
heuristics -- using an anytime evaluation metric called the primal gap
integral. Experimental results on standard classification benchmarks show that
CA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime
performance, outperforming both other CA-DL8.5 variants and the Blossom
algorithm while maintaining completeness and optimality guarantees.

</details>


### [102] [ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception](https://arxiv.org/abs/2508.06074)
*Siyi Lu,Run Liu,Dongsheng Yang,Lei He*

Main category: cs.AI

TL;DR: 论文提出了一种基于深度强化学习（DRL）和鸟瞰图（BEV）感知的自动驾驶新方法，结合高效的时空特征提取网络Mamba-BEV和ME³-BEV框架，显著提升了动态城市驾驶场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在复杂环境感知和实时决策方面面临挑战，传统模块化方法存在误差传播和协调问题，而端到端学习系统则受限于计算瓶颈。

Method: 提出Mamba-BEV模型，结合BEV感知和Mamba框架进行时空特征提取；进一步设计ME³-BEV框架，将Mamba-BEV作为端到端DRL的特征输入。

Result: 在CARLA模拟器上的实验表明，ME³-BEV在碰撞率和轨迹精度等多项指标上优于现有模型。

Conclusion: 该方法为实时自动驾驶提供了一种高效且可解释的解决方案。

Abstract: Autonomous driving systems face significant challenges in perceiving complex
environments and making real-time decisions. Traditional modular approaches,
while offering interpretability, suffer from error propagation and coordination
issues, whereas end-to-end learning systems can simplify the design but face
computational bottlenecks. This paper presents a novel approach to autonomous
driving using deep reinforcement learning (DRL) that integrates bird's-eye view
(BEV) perception for enhanced real-time decision-making. We introduce the
\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction
network that combines BEV-based perception with the Mamba framework for
temporal feature modeling. This integration allows the system to encode vehicle
surroundings and road features in a unified coordinate system and accurately
model long-range dependencies. Building on this, we propose the
\texttt{ME$^3$-BEV} framework, which utilizes the \texttt{Mamba-BEV} model as a
feature input for end-to-end DRL, achieving superior performance in dynamic
urban driving scenarios. We further enhance the interpretability of the model
by visualizing high-dimensional features through semantic segmentation,
providing insight into the learned representations. Extensive experiments on
the CARLA simulator demonstrate that \texttt{ME$^3$-BEV} outperforms existing
models across multiple metrics, including collision rate and trajectory
accuracy, offering a promising solution for real-time autonomous driving.

</details>


### [103] [Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2](https://arxiv.org/abs/2508.06091)
*Stan P Hauke,Przemysław Andrzej Wałęga*

Main category: cs.AI

TL;DR: 本文解决了GNNs与逻辑语言表达能力的开放问题，证明aggregate-combine-readout GNNs的表达力严格超过C2逻辑。


<details>
  <summary>Details</summary>
Motivation: 研究GNNs与逻辑语言的表达能力关系，解决Barceló等人提出的开放问题。

Method: 通过理论证明，比较aggregate-combine-readout GNNs与C2逻辑的表达能力。

Result: 证明aggregate-combine-readout GNNs的表达力严格超过C2逻辑，适用于无向和有向图。

Conclusion: 该结果不仅对GNNs有重要意义，还为无穷逻辑的表达能力提供了新见解。

Abstract: In recent years, there has been growing interest in understanding the
expressive power of graph neural networks (GNNs) by relating them to logical
languages. This research has been been initialised by an influential result of
Barcel\'o et al. (2020), who showed that the graded modal logic (or a guarded
fragment of the logic C2), characterises the logical expressiveness of
aggregate-combine GNNs. As a ``challenging open problem'' they left the
question whether full C2 characterises the logical expressiveness of
aggregate-combine-readout GNNs. This question has remained unresolved despite
several attempts. In this paper, we solve the above open problem by proving
that the logical expressiveness of aggregate-combine-readout GNNs strictly
exceeds that of C2. This result holds over both undirected and directed graphs.
Beyond its implications for GNNs, our work also leads to purely logical
insights on the expressive power of infinitary logics.

</details>


### [104] [PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion](https://arxiv.org/abs/2508.06110)
*Yiran Rex Ma*

Main category: cs.AI

TL;DR: PanelTR框架通过LLM代理科学家实现零样本表格推理，无需数据增强或参数优化，性能优于普通LLM并接近监督模型。


<details>
  <summary>Details</summary>
Motivation: 解决表格推理任务中依赖标注数据或复杂数据增强的问题，提升LLM在零样本场景下的表现。

Method: PanelTR采用结构化科学方法，通过代理科学家进行独立研究、自评和协作评审，利用五种科学家角色实现语义级迁移。

Result: 在四个基准测试中，PanelTR性能优于普通LLM，接近完全监督模型，且无需训练数据。

Conclusion: 结构化科学方法能有效处理复杂任务，具备零样本场景下的灵活语义理解能力。

Abstract: Table reasoning, including tabular QA and fact verification, often depends on
annotated data or complex data augmentation, limiting flexibility and
generalization. LLMs, despite their versatility, often underperform compared to
simple supervised models. To approach these issues, we introduce PanelTR, a
framework utilizing LLM agent scientists for robust table reasoning through a
structured scientific approach. PanelTR's workflow involves agent scientists
conducting individual investigations, engaging in self-review, and
participating in collaborative peer-review discussions. This process, driven by
five scientist personas, enables semantic-level transfer without relying on
data augmentation or parametric optimization. Experiments across four
benchmarks show that PanelTR outperforms vanilla LLMs and rivals fully
supervised models, all while remaining independent of training data. Our
findings indicate that structured scientific methodology can effectively handle
complex tasks beyond table reasoning with flexible semantic understanding in a
zero-shot context.

</details>


### [105] [SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges](https://arxiv.org/abs/2508.06111)
*Dewi S. W. Gould,Bruno Mlodozeniec,Samuel F. Brown*

Main category: cs.AI

TL;DR: SKATE是一种新型评估框架，通过让大语言模型（LLMs）相互生成和解决可验证任务来评估其能力与风险，具有自动化、可扩展和客观的特点。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法需要大量领域专业知识，难以适应快速发展的模型，因此需要一种更灵活、可扩展的评估框架。

Method: SKATE框架将评估视为游戏，模型既作为任务生成者又作为解决者，通过生成可验证任务（如代码输出预测挑战）来相互测试。

Result: 实验表明，较弱模型能可靠区分较强模型，LLMs会生成偏向自身能力的问题，SKATE能自动揭示模型间的细粒度差异。

Conclusion: SKATE为通用、可扩展的评估框架提供了重要进展，能够跟上LLM的发展速度。

Abstract: Evaluating the capabilities and risks of foundation models is paramount, yet
current methods demand extensive domain expertise, hindering their scalability
as these models rapidly evolve. We introduce SKATE: a novel evaluation
framework in which large language models (LLMs) compete by generating and
solving verifiable tasks for one another. Our core insight is to treat
evaluation as a game: models act as both task-setters and solvers, incentivized
to create questions which highlight their own strengths while exposing others'
weaknesses. SKATE offers several key advantages, balancing scalability,
open-endedness, and objectivity. It is fully automated, data-free, and
scalable, requiring no human input or domain expertise. By using verifiable
tasks rather than LLM judges, scoring is objective. Unlike domain-limited
programmatically-generated benchmarks (e.g. chess-playing or spatial
reasoning), having LLMs creatively pose challenges enables open-ended and
scalable evaluation. As a proof of concept, we introduce LLM-set
code-output-prediction (COP) challenges as a verifiable and extensible
framework in which to test our approach. Using a TrueSkill-based ranking
system, we evaluate six frontier LLMs and find that: (1) weaker models can
reliably differentiate and score stronger ones, (2) LLM-based systems are
capable of self-preferencing behavior, generating questions that align with
their own capabilities, and (3) SKATE automatically surfaces fine-grained
capability differences between models. Our findings are an important step
towards general, scalable evaluation frameworks which can keep pace with LLM
progress.

</details>


### [106] [Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem](https://arxiv.org/abs/2508.06129)
*Bachtiar Herdianto,Romain Billot,Flavien Lucas,Marc Sevaux*

Main category: cs.AI

TL;DR: 论文探讨了如何利用机器学习和可解释AI改进车辆路径问题（VRP）的元启发式算法设计，通过敏感性分析和特征重要性分析，提出了一个统一框架。


<details>
  <summary>Details</summary>
Motivation: 传统元启发式算法依赖人工设计，效率有限。机器学习可以捕捉组合优化问题的结构特征，从而改进算法设计。

Method: 使用多种分类器模型进行敏感性分析，预测VRP解的质量，并利用可解释AI分析模型决策过程。

Result: 特征重要性因模型而异，但某些特征始终是强预测因子。研究提出了一个统一框架来排名特征影响。

Conclusion: 特征重要性分析为开发VRP元启发式算法的指导机制提供了基础。

Abstract: The Vehicle Routing Problem (VRP) is a complex optimization problem with
numerous real-world applications, mostly solved using metaheuristic algorithms
due to its $\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely
on human-crafted designs developed through empirical studies. However, recent
research shows that machine learning methods can be used the structural
characteristics of solutions in combinatorial optimization, thereby aiding in
designing more efficient algorithms, particularly for solving VRP. Building on
this advancement, this study extends the previous research by conducting a
sensitivity analysis using multiple classifier models that are capable of
predicting the quality of VRP solutions. Hence, by leveraging explainable AI,
this research is able to extend the understanding of how these models make
decisions. Finally, our findings indicate that while feature importance varies,
certain features consistently emerge as strong predictors. Furthermore, we
propose a unified framework able of ranking feature impact across different
scenarios to illustrate this finding. These insights highlight the potential of
feature importance analysis as a foundation for developing a guidance mechanism
of metaheuristic algorithms for solving the VRP.

</details>


### [107] [Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications](https://arxiv.org/abs/2508.06145)
*Byeonghun Bang,Jongsuk Yoon,Dong-Jin Chang,Seho Park,Yong Oh Lee*

Main category: cs.AI

TL;DR: 研究通过RAG框架增强LLMs在药物禁忌领域的准确性，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在医疗领域的应用，特别是在药物禁忌这一需要高准确性的场景。

Method: 使用GPT-4o-mini和text-embedding-3-small模型，结合Langchain构建混合检索系统，利用DUR数据优化检索和生成。

Result: RAG框架显著提升模型准确率，从0.49-0.57提升至0.87-0.94。

Conclusion: RAG框架能有效减少药物禁忌决策中的不确定性，提供更可靠的信息。

Abstract: The versatility of large language models (LLMs) has been explored across
various sectors, but their application in healthcare poses challenges,
particularly in the domain of pharmaceutical contraindications where accurate
and reliable information is required. This study enhances the capability of
LLMs to address contraindications effectively by implementing a Retrieval
Augmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base
model, and the text-embedding-3-small model for embeddings, our approach
integrates Langchain to orchestrate a hybrid retrieval system with re-ranking.
This system leverages Drug Utilization Review (DUR) data from public databases,
focusing on contraindications for specific age groups, pregnancy, and
concomitant drug use. The dataset includes 300 question-answer pairs across
three categories, with baseline model accuracy ranging from 0.49 to 0.57.
Post-integration of the RAG pipeline, we observed a significant improvement in
model accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications
related to age groups, pregnancy, and concomitant drug use, respectively. The
results indicate that augmenting LLMs with a RAG framework can substantially
reduce uncertainty in prescription and drug intake decisions by providing more
precise and reliable drug contraindication information.

</details>


### [108] [Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution](https://arxiv.org/abs/2508.06225)
*Zailong Tian,Zhuoheng Han,Yanzhe Chen,Haozhe Xu,Xi Yang,richeng xuan,Hongfeng Wang,Lizi Liao*

Main category: cs.AI

TL;DR: 该论文提出从以准确性为中心的评估转向基于置信度的风险感知LLM评估系统，并引入TH-Score量化过自信现象，提出LLM-as-a-Fuser框架以提升可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法过于关注准确性，忽视了置信度校准的重要性，导致实际部署中可靠性不足。

Method: 提出TH-Score量化过自信现象，并设计LLM-as-a-Fuser框架，通过集成方法提升置信度校准。

Result: 实验表明，该方法显著改善了校准效果，实现了自适应、基于置信度的评估，优于现有基线。

Conclusion: 置信度驱动的风险感知评估系统能显著提升LLM作为评估工具的可靠性和实用性。

Abstract: Large Language Models (LLMs) are widely used as automated judges, where
practical value depends on both accuracy and trustworthy, risk-aware judgments.
Existing approaches predominantly focus on accuracy, overlooking the necessity
of well-calibrated confidence, which is vital for adaptive and reliable
evaluation pipelines. In this work, we advocate a shift from accuracy-centric
evaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing
the necessity of well-calibrated confidence for trustworthy and adaptive
evaluation. We systematically identify the **Overconfidence Phenomenon** in
current LLM-as-a-Judges, where predicted confidence significantly overstates
actual correctness, undermining reliability in practical deployment. To
quantify this phenomenon, we introduce **TH-Score**, a novel metric measuring
confidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an
ensemble framework that transforms LLMs into reliable, risk-aware evaluators.
Extensive experiments demonstrate that our approach substantially improves
calibration and enables adaptive, confidence-driven evaluation pipelines,
achieving superior reliability and accuracy compared to existing baselines.

</details>


### [109] [GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines](https://arxiv.org/abs/2508.06226)
*Yumeng Fu,Jiayin Zhu,Lingling Zhang,Bo Zhao,Shaoxuan Ma,Yushun Zhang,Yanrui Wu,Wenjun Wu*

Main category: cs.AI

TL;DR: 论文提出了GeoLaux基准，用于评估多模态大语言模型（MLLMs）在几何问题解决中的长步推理和辅助线构造能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估MLLM几何技能的基准忽略了辅助线构造和细粒度过程评估，不足以评估长步推理能力。

Method: 构建包含2,186个几何问题的GeoLaux基准，设计五维评估策略，评估答案正确性、过程正确性、过程质量、辅助线影响和错误原因。

Result: 实验发现MLLMs在长步推理中性能显著下降，倾向于在证明问题中走捷径，且缺乏辅助线意识。

Conclusion: GeoLaux可作为评估MLLMs长步几何推理能力的基准，并指导能力提升。

Abstract: Geometry problem solving (GPS) requires models to master diagram
comprehension, logical reasoning, knowledge application, numerical computation,
and auxiliary line construction. This presents a significant challenge for
Multimodal Large Language Models (MLLMs). However, existing benchmarks for
evaluating MLLM geometry skills overlook auxiliary line construction and lack
fine-grained process evaluation, making them insufficient for assessing MLLMs'
long-step reasoning abilities. To bridge these gaps, we present the GeoLaux
benchmark, comprising 2,186 geometry problems, incorporating both calculation
and proving questions. Notably, the problems require an average of 6.51
reasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary
line construction. Building on the dataset, we design a novel five-dimensional
evaluation strategy assessing answer correctness, process correctness, process
quality, auxiliary line impact, and error causes. Extensive experiments on 13
leading MLLMs (including thinking models and non-thinking models) yield three
pivotal findings: First, models exhibit substantial performance degradation in
extended reasoning steps (nine models demonstrate over 50% performance drop).
Second, compared to calculation problems, MLLMs tend to take shortcuts when
solving proving problems. Third, models lack auxiliary line awareness, and
enhancing this capability proves particularly beneficial for overall geometry
reasoning improvement. These findings establish GeoLaux as both a benchmark for
evaluating MLLMs' long-step geometric reasoning with auxiliary lines and a
guide for capability advancement. Our dataset and code are included in
supplementary materials and will be released.

</details>


### [110] [Learning Logical Rules using Minimum Message Length](https://arxiv.org/abs/2508.06230)
*Ruben Sharma,Sebastijan Dumančić,Ross D. King,Andrew Cropper*

Main category: cs.AI

TL;DR: 提出了一种贝叶斯归纳逻辑编程方法，通过学习最小消息长度程序来统一概率和逻辑学习，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 统一概率和逻辑学习是AI领域的关键挑战。

Method: 采用贝叶斯归纳逻辑编程，通过先验和似然平衡假设复杂性和数据拟合。

Result: 在游戏和药物设计等领域的实验中表现优于现有方法，数据高效且对样本平衡不敏感。

Conclusion: 该方法在统一概率和逻辑学习方面具有显著优势，适用于多种应用场景。

Abstract: Unifying probabilistic and logical learning is a key challenge in AI. We
introduce a Bayesian inductive logic programming approach that learns minimum
message length programs from noisy data. Our approach balances hypothesis
complexity and data fit through priors, which explicitly favour more general
programs, and a likelihood that favours accurate programs. Our experiments on
several domains, including game playing and drug design, show that our method
significantly outperforms previous methods, notably those that learn minimum
description length programs. Our results also show that our approach is
data-efficient and insensitive to example balance, including the ability to
learn from exclusively positive examples.

</details>


### [111] [Symmetry breaking for inductive logic programming](https://arxiv.org/abs/2508.06263)
*Andrew Cropper,David M. Cerna,Matti Järvisalo*

Main category: cs.AI

TL;DR: 提出一种方法，通过打破假设空间中的对称性来优化归纳逻辑编程的搜索效率。


<details>
  <summary>Details</summary>
Motivation: 归纳逻辑编程需要搜索大量假设空间，且存在许多逻辑等效假设，导致搜索效率低下。

Method: 采用答案集编程实现对称性打破的方法。

Result: 实验表明，该方法在视觉推理和游戏等领域将求解时间从超过一小时缩短至17秒。

Conclusion: 该方法显著提高了归纳逻辑编程的搜索效率。

Abstract: The goal of inductive logic programming is to search for a hypothesis that
generalises training data and background knowledge. The challenge is searching
vast hypothesis spaces, which is exacerbated because many logically equivalent
hypotheses exist. To address this challenge, we introduce a method to break
symmetries in the hypothesis space. We implement our idea in answer set
programming. Our experiments on multiple domains, including visual reasoning
and game playing, show that our approach can reduce solving times from over an
hour to just 17 seconds.

</details>


### [112] [LLM Robustness Leaderboard v1 --Technical report](https://arxiv.org/abs/2508.06296)
*Pierre Peigné - Lefebvre,Quentin Feuillade-Montixi,Tom David,Nicolas Miailhe*

Main category: cs.AI

TL;DR: PRISM Eval开发了BET工具，通过动态对抗优化对41个先进LLM进行测试，攻击成功率达100%。提出细粒度鲁棒性指标，揭示攻击难度差异，并分析漏洞以改进安全性。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLM）的鲁棒性，揭示其漏洞，推动分布式鲁棒性评估的发展。

Method: 使用BET工具进行自动化红队测试，采用动态对抗优化方法，提出细粒度鲁棒性指标和原始级漏洞分析。

Result: 对41个LLM中的37个实现100%攻击成功率，攻击难度差异达300倍以上。

Conclusion: BET工具和细粒度指标为社区提供了实用的鲁棒性评估路径，有助于改进LLM安全性。

Abstract: This technical report accompanies the LLM robustness leaderboard published by
PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior
Elicitation Tool (BET), an AI system performing automated red-teaming through
Dynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)
against 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we
propose a fine-grained robustness metric estimating the average number of
attempts required to elicit harmful behaviors, revealing that attack difficulty
varies by over 300-fold across models despite universal vulnerability. We
introduce primitive-level vulnerability analysis to identify which jailbreaking
techniques are most effective for specific hazard categories. Our collaborative
evaluation with trusted third parties from the AI Safety Network demonstrates
practical pathways for distributed robustness assessment across the community.

</details>


### [113] [A "good regulator theorem" for embodied agents](https://arxiv.org/abs/2508.06326)
*Nathaniel Virgo,Martin Biehl,Manuel Baltieri,Matteo Capucci*

Main category: cs.AI

TL;DR: 论文重新诠释了Conant和Ashby的定理，提出了一种更广义的“信念更新”模型概念，强调观察者在模型定义中的作用。


<details>
  <summary>Details</summary>
Motivation: 探讨Conant和Ashby定理的局限性，并提出一种更普适的理论框架来解释无模型系统的调节行为。

Method: 通过引入“信念更新”的概念，将观察者的视角纳入模型定义，扩展了原有定理的适用范围。

Result: 证明了无论在经典控制理论还是内部状态调节中，系统均可被解释为具有环境模型，解决了原有定理的局限性。

Conclusion: 观察者在模型定义中起关键作用，模型是外部赋予的而非系统固有属性，这一视角为理解调节行为提供了新思路。

Abstract: In a classic paper, Conant and Ashby claimed that "every good regulator of a
system must be a model of that system." Artificial Life has produced many
examples of systems that perform tasks with apparently no model in sight; these
suggest Conant and Ashby's theorem doesn't easily generalise beyond its
restricted setup. Nevertheless, here we show that a similar intuition can be
fleshed out in a different way: whenever an agent is able to perform a
regulation task, it is possible for an observer to interpret it as having
"beliefs" about its environment, which it "updates" in response to sensory
input. This notion of belief updating provides a notion of model that is more
sophisticated than Conant and Ashby's, as well as a theorem that is more
broadly applicable. However, it necessitates a change in perspective, in that
the observer plays an essential role in the theory: models are not a mere
property of the system but are imposed on it from outside. Our theorem holds
regardless of whether the system is regulating its environment in a classic
control theory setup, or whether it's regulating its own internal state; the
model is of its environment either way. The model might be trivial, however,
and this is how the apparent counterexamples are resolved.

</details>


### [114] [AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games](https://arxiv.org/abs/2508.06348)
*Mille Mei Zhen Loo,Gert Luzkov,Paolo Burelli*

Main category: cs.AI

TL;DR: 本文提出了一种基于Transformer的机器学习模型AntiCheatPT_256，用于检测《CS2》中的作弊行为，并公开了标注数据集CS2CD。模型在测试集上表现优异，准确率达89.17%，AUC为93.36%。


<details>
  <summary>Details</summary>
Motivation: 在线游戏作弊问题严重，现有反作弊系统（如VAC）难以在不侵犯用户隐私的情况下应对不断演变的作弊手段。

Method: 使用Transformer模型分析《CS2》的游戏数据，并引入公开数据集CS2CD（包含795场比赛）。通过数据增强解决类别不平衡问题。

Result: 模型在未增强的测试集上达到89.17%的准确率和93.36%的AUC。

Conclusion: AntiCheatPT_256为数据驱动的作弊检测提供了可复现且实用的基准，具有实际应用潜力。

Abstract: Cheating in online video games compromises the integrity of gaming
experiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face
significant challenges in keeping pace with evolving cheating methods without
imposing invasive measures on users' systems. This paper presents
AntiCheatPT\_256, a transformer-based machine learning model designed to detect
cheating behaviour in Counter-Strike 2 using gameplay data. To support this, we
introduce and publicly release CS2CD: A labelled dataset of 795 matches. Using
this dataset, 90,707 context windows were created and subsequently augmented to
address class imbalance. The transformer model, trained on these windows,
achieved an accuracy of 89.17\% and an AUC of 93.36\% on an unaugmented test
set. This approach emphasizes reproducibility and real-world applicability,
offering a robust baseline for future research in data-driven cheat detection.

</details>


### [115] [From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI](https://arxiv.org/abs/2508.06352)
*Christian Meske,Justin Brenne,Erdi Uenal,Sabahat Oelcer,Ayseguel Doganguen*

Main category: cs.AI

TL;DR: 论文提出“解释性AI”作为可解释AI的补充范式，通过生成式AI能力提供上下文推理支持，而非仅关注算法透明性。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法过于抽象且非自适应，难以支持用户理解。

Method: 提出八维概念模型，通过叙事沟通、自适应个性化和渐进披露原则区分解释性AI，并通过快速情境设计方法进行实证验证。

Result: 医疗专业人员更偏好上下文敏感的多模态解释，而非技术透明性。

Conclusion: 需设计以人类理解为中心的AI解释方法，推动跨领域和文化的研究议程。

Abstract: Current explainable AI (XAI) approaches prioritize algorithmic transparency
and present explanations in abstract, non-adaptive formats that often fail to
support meaningful end-user understanding. This paper introduces "Explanatory
AI" as a complementary paradigm that leverages generative AI capabilities to
serve as explanatory partners for human understanding rather than providers of
algorithmic transparency. While XAI reveals algorithmic decision processes for
model validation, Explanatory AI addresses contextual reasoning to support
human decision-making in sociotechnical contexts. We develop a definition and
systematic eight-dimensional conceptual model distinguishing Explanatory AI
through narrative communication, adaptive personalization, and progressive
disclosure principles. Empirical validation through Rapid Contextual Design
methodology with healthcare professionals demonstrates that users consistently
prefer context-sensitive, multimodal explanations over technical transparency.
Our findings reveal the practical urgency for AI systems designed for human
comprehension rather than algorithmic introspection, establishing a
comprehensive research agenda for advancing user-centered AI explanation
approaches across diverse domains and cultural contexts.

</details>


### [116] [Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned](https://arxiv.org/abs/2508.06368)
*Claudia dAmato,Giuseppe Rubini,Francesco Didio,Donato Francioso,Fatima Zahra Amara,Nicola Fanizzi*

Main category: cs.AI

TL;DR: 论文提出两种自动化构建法律知识图谱（KG）的方法，填补法律领域KG的空白，并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 法律决策需要全面的立法背景知识和最新案例信息，但法律领域的KG较少，因此开发针对女性暴力案件的法律KG。

Method: 采用系统化自下而上方法和基于大语言模型的新方法，结合结构化数据提取、本体开发和语义丰富。

Result: 构建了针对女性暴力案件的法律KG，并通过能力问题验证其有效性。

Conclusion: 该KG可提升法律信息的可访问性，支持复杂查询，并为预测性司法机器学习工具提供知识基础。

Abstract: Legal decision-making process requires the availability of comprehensive and
detailed legislative background knowledge and up-to-date information on legal
cases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a
valuable tool to facilitate access to legal information, to be queried and
exploited for the purpose, and to enable advanced reasoning and machine
learning applications. Indeed, legal KGs may act as knowledge intensive
component to be used by pre-dictive machine learning solutions supporting the
decision process of the legal expert. Nevertheless, a few KGs can be found in
the legal domain. To fill this gap, we developed a legal KG targeting legal
cases of violence against women, along with clear adopted methodologies.
Specifically, the paper introduces two complementary approaches for automated
legal KG construction; a systematic bottom-up approach, customized for the
legal domain, and a new solution leveraging Large Language Models. Starting
from legal sentences publicly available from the European Court of Justice, the
solutions integrate structured data extraction, ontology development, and
semantic enrichment to produce KGs tailored for legal cases involving violence
against women. After analyzing and comparing the results of the two approaches,
the developed KGs are validated via suitable competency questions. The obtained
KG may be impactful for multiple purposes: can improve the accessibility to
legal information both to humans and machine, can enable complex queries and
may constitute an important knowledge component to be possibly exploited by
machine learning tools tailored for predictive justice.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [117] [Debiasing Polynomial and Fourier Regression](https://arxiv.org/abs/2508.05920)
*Chris Camaño,Raphael A. Meyer,Kevin Shu*

Main category: cs.DS

TL;DR: 论文提出了一种基于随机矩阵理论的无偏多项式回归方法，用于近似未知函数，具有近最优样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有随机化算法在恢复最优多项式时存在偏差，需要一种无偏且高效的近似方法。

Method: 通过设计随机复矩阵的特征值作为采样点，实现无偏多项式回归。

Result: 提出的方法在实验中优于独立同分布杠杆得分采样，且具有近最优样本复杂度。

Conclusion: 该方法不仅适用于多项式回归，还可用于周期函数的傅里叶级数近似，具有广泛适用性。

Abstract: We study the problem of approximating an unknown function
$f:\mathbb{R}\to\mathbb{R}$ by a degree-$d$ polynomial using as few function
evaluations as possible, where error is measured with respect to a probability
distribution $\mu$. Existing randomized algorithms achieve near-optimal sample
complexities to recover a $ (1+\varepsilon) $-optimal polynomial but produce
biased estimates of the best polynomial approximation, which is undesirable.
  We propose a simple debiasing method based on a connection between polynomial
regression and random matrix theory. Our method involves evaluating
$f(\lambda_1),\ldots,f(\lambda_{d+1})$ where $\lambda_1,\ldots,\lambda_{d+1}$
are the eigenvalues of a suitably designed random complex matrix tailored to
the distribution $\mu$. Our estimator is unbiased, has near-optimal sample
complexity, and experimentally outperforms iid leverage score sampling.
  Additionally, our techniques enable us to debias existing methods for
approximating a periodic function with a truncated Fourier series with
near-optimal sample complexity.

</details>


### [118] [A Structural Linear-Time Algorithm for Computing the Tutte Decomposition](https://arxiv.org/abs/2508.06212)
*Romain Bourneuf,Tim Planken*

Main category: cs.DS

TL;DR: 本文提出了一种基于完全嵌套2-分离的线性时间算法，用于计算图的Tutte分解，并引入了稳定性的新概念。


<details>
  <summary>Details</summary>
Motivation: 研究图的分解结构，尤其是Tutte分解，是图论中的重要问题。本文旨在提供一种更简单、高效的算法，同时探索完全嵌套2-分离的结构性质。

Method: 基于Cunningham和Edmonds的结构表征，提出了一种线性时间算法，首先计算所有完全嵌套的2-分离，然后构建Tutte分解。

Result: 算法成功实现了线性时间复杂度的Tutte分解，并揭示了完全嵌套2-分离的新结构性质。

Conclusion: 本文不仅改进了Tutte分解的计算方法，还为图的结构分析提供了新的理论工具。

Abstract: The block-cut tree decomposes a connected graph along its cutvertices,
displaying its 2-connected components. The Tutte-decomposition extends this
idea to 2-separators in 2-connected graphs, yielding a canonical
tree-decomposition that decomposes the graph into its triconnected components.
In 1973, Hopcroft and Tarjan introduced a linear-time algorithm to compute the
Tutte-decomposition. Cunningham and Edmonds later established a structural
characterization of the Tutte-decomposition via totally-nested 2-separations.
We present a conceptually simple algorithm based on this characterization,
which computes the Tutte-decomposition in linear time. Our algorithm first
computes all totally-nested 2-separations and then builds the
Tutte-decomposition from them.
  Along the way, we derive new structural results on the structure of
totally-nested 2-separations in 2-connected graphs using a novel notion of
stability, which may be of independent interest.

</details>


### [119] [The Beauty of Anisotropic Mesh Refinement: Omnitrees for Efficient Dyadic Discretizations](https://arxiv.org/abs/2508.06316)
*Theresa Pollinger,Masado Ishii,Jens Domke*

Main category: cs.DS

TL;DR: 本文提出了一种称为omnitree的各向异性数据结构，作为octree的扩展，能够更高效地处理各向异性问题，减少存储需求并提高收敛速度。


<details>
  <summary>Details</summary>
Motivation: octree在各向异性问题中效率低下，因为其强制各向同性细化，导致分辨率浪费。本文旨在解决这一问题。

Method: 提出omnitree数据结构，允许仅细化局部最重要的维度，从而减少树的深度和宽度。

Result: 在三维对象表示中，omnitree将平均收敛速度提高1.5倍，存储需求更低，且信息密度更高。

Conclusion: omnitree能显著提升AMR方法的效率，并为高维应用开辟新可能。

Abstract: Structured adaptive mesh refinement (AMR), commonly implemented via quadtrees
and octrees, underpins a wide range of applications including databases,
computer graphics, physics simulations, and machine learning. However, octrees
enforce isotropic refinement in regions of interest, which can be especially
inefficient for problems that are intrinsically anisotropic--much resolution is
spent where little information is gained. This paper presents omnitrees as an
anisotropic generalization of octrees and related data structures. Omnitrees
allow to refine only the locally most important dimensions, providing tree
structures that are less deep than bintrees and less wide than octrees. As a
result, the convergence of the AMR schemes can be increased by up to a factor
of the dimensionality d for very anisotropic problems, quickly offsetting their
modest increase in storage overhead. We validate this finding on the problem of
binary shape representation across 4,166 three-dimensional objects: Omnitrees
increase the mean convergence rate by 1.5x, require less storage to achieve
equivalent error bounds, and maximize the information density of the stored
function faster than octrees. These advantages are projected to be even
stronger for higher-dimensional problems. We provide a first validation by
introducing a time-dependent rotation to create four-dimensional
representations, and discuss the properties of their 4-d octree and omnitree
approximations. Overall, omnitree discretizations can make existing AMR
approaches more efficient, and open up new possibilities for high-dimensional
applications.

</details>


### [120] [A Simple PTAS for Weighted $k$-means and Sensor Coverage](https://arxiv.org/abs/2508.06460)
*Akash Pareek,Supratim Shit*

Main category: cs.DS

TL;DR: 本文提出了一种针对加权k均值问题的简单PTAS算法，无需依赖核心集，扩展了未加权情况的框架，并应用于传感器覆盖问题。


<details>
  <summary>Details</summary>
Motivation: 加权k均值问题在实际中常见，但现有方法多依赖核心集，缺乏简单直接的PTAS算法。

Method: 基于Jaiswal等人的框架，采用加权D²采样技术，扩展至加权情况。

Result: 算法运行时间为n·d·2^O(k²/ε)，输出中心集的聚类成本在最优解的(1+ε)倍内。

Conclusion: 该算法为加权k均值提供了高效解决方案，并显著改进了传感器覆盖问题的近似结果。

Abstract: Clustering is a fundamental technique in data analysis, with the $k$-means
being one of the widely studied objectives due to its simplicity and broad
applicability. In many practical scenarios, data points come with associated
weights that reflect their importance, frequency, or confidence. Given a
weighted point set $P \subset R^d$, where each point $p \in P$ has a positive
weight $w_p$, the goal is to compute a set of $k$ centers $C = \{ c_1, c_2,
\ldots, c_k \} \subset R^d$ that minimizes the weighted clustering cost:
$\Delta_w(P,C) = \sum_{p \in P} w_p \cdot d(p,C)^2$, where $d(p,C)$ denotes the
Euclidean distance from $p$ to its nearest center in $C$. Although most
existing coreset-based algorithms for $k$-means extend naturally to the
weighted setting and provide a PTAS, no prior work has offered a simple,
coreset-free PTAS designed specifically for the weighted $k$-means problem.
  In this paper, we present a simple PTAS for weighted $k$-means that does not
rely on coresets. Building upon the framework of Jaiswal, Kumar, and Sen (2012)
for the unweighted case, we extend the result to the weighted setting by using
the weighted $D^2$-sampling technique. Our algorithm runs in time $n d \cdot
2^{O\left(\frac{k^2}{\epsilon}\right)}$ and outputs a set of $k$ centers whose
total clustering cost is within a $(1 + \epsilon)$-factor of the optimal cost.
As a key application of the weighted $k$-means, we obtain a PTAS for the sensor
coverage problem, which can also be viewed as a continuous locational
optimization problem. For this problem, the best-known result prior to our work
was an $O(\log k)$-approximation by Deshpande (2014), whereas our algorithm
guarantees a $(1 + \epsilon)$-approximation to the optimal coverage cost even
before applying refinement steps like Lloyd desent.

</details>


### [121] [On the Parallel Complexity of Identifying Groups and Quasigroups via Decompositions](https://arxiv.org/abs/2508.06478)
*Dan Johnson,Michael Levet,Petr Vojtěchovský,Brett Widholm*

Main category: cs.DS

TL;DR: 本文研究了有限群和拟群的同构测试计算复杂度，利用其分解特性改进了现有结果。


<details>
  <summary>Details</summary>
Motivation: 探索群和拟群在同构测试中的计算复杂度，特别是通过分解方法优化算法效率。

Method: 利用群的直积分解和拟群的仿射分解，结合Weisfeiler--Leman算法和并行计算技术。

Result: 改进了特定群类的同构测试复杂度，部分问题从TC^1降至L，拟群问题从指数时间降至NC。

Conclusion: 分解方法显著提升了群和拟群同构测试的效率，为相关计算问题提供了新思路。

Abstract: In this paper, we investigate the computational complexity of isomorphism
testing for finite groups and quasigroups, given by their multiplication
tables. We crucially take advantage of their various decompositions to show the
following:
  - We first consider the class $\mathcal{C}$ of groups that admit direct
product decompositions, where each indecompsable factor is $O(1)$-generated,
and either perfect or centerless. We show any group in $\mathcal{C}$ is
identified by the $O(1)$-dimensional count-free Weisfeiler--Leman (WL)
algorithm with $O(\log \log n)$ rounds, and the $O(1)$-dimensional counting WL
algorithm with $O(1)$ rounds. Consequently, the isomorphism problem for
$\mathcal{C}$ is in $\textsf{L}$. The previous upper bound for this class was
$\textsf{TC}^{1}$, using $O(\log n)$ rounds of the $O(1)$-dimensional counting
WL (Grochow and Levet, FCT 2023).
  - We next consider more generally, the class of groups where each
indecomposable factor is $O(1)$-generated. We exhibit an $\textsf{AC}^{3}$
canonical labeling procedure for this class. Here, we accomplish this by
showing that in the multiplication table model, the direct product
decomposition can be computed in $\textsf{AC}^{3}$, parallelizing the work of
Kayal and Nezhmetdinov (ICALP 2009).
  - Isomorphism testing between a central quasigroup $G$ and an arbitrary
quasigroup $H$ is in $\textsf{NC}$. Here, we take advantage of the fact that
central quasigroups admit an affine decomposition in terms of an underlying
Abelian group. Only the trivial bound of $n^{\log(n)+O(1)}$-time was previously
known for isomorphism testing of central quasigroups.

</details>


### [122] [Does block size matter in randomized block Krylov low-rank approximation?](https://arxiv.org/abs/2508.06486)
*Tyler Chen,Ethan N. Epperly,Raphael A. Meyer,Christopher Musco,Akash Rao*

Main category: cs.DS

TL;DR: 论文研究了使用随机块Krylov迭代计算矩阵的秩-$k$近似问题，解决了理论与实践中块大小选择的差距。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，块大小$b=1$或$b=k$时，可以通过$\tilde O(k/\sqrt{\varepsilon})$次矩阵-向量乘积获得$(1 + \varepsilon)$-近似，但$1 < b < k$时的理论性能较差，而实践中块大小$1 \ll b \ll k$表现更优。

Method: 通过证明随机块Krylov迭代在任何块大小$1\le b\le k$下均能以$\tilde O(k/\sqrt{\varepsilon})$次矩阵-向量乘积获得$(1 + \varepsilon)$-近似，解决了理论与实践的差距。

Result: 证明了随机块Krylov迭代在任何块大小下均能以$\tilde O(k/\sqrt{\varepsilon})$次矩阵-向量乘积获得$(1 + \varepsilon)$-近似。

Conclusion: 研究填补了理论与实践的差距，为块Krylov方法提供了统一的理论支持，并可能对其他稀疏线性系统算法有启发。

Abstract: We study the problem of computing a rank-$k$ approximation of a matrix using
randomized block Krylov iteration. Prior work has shown that, for block size $b
= 1$ or $b = k$, a $(1 + \varepsilon)$-factor approximation to the best
rank-$k$ approximation can be obtained after $\tilde O(k/\sqrt{\varepsilon})$
matrix-vector products with the target matrix. On the other hand, when $b$ is
between $1$ and $k$, the best known bound on the number of matrix-vector
products scales with $b(k-b)$, which could be as large as $O(k^2)$.
Nevertheless, in practice, the performance of block Krylov methods is often
optimized by choosing a block size $1 \ll b \ll k$. We resolve this
theory-practice gap by proving that randomized block Krylov iteration produces
a $(1 + \varepsilon)$-factor approximate rank-$k$ approximation using $\tilde
O(k/\sqrt{\varepsilon})$ matrix-vector products for any block size $1\le b\le
k$. Our analysis relies on new bounds for the minimum singular value of a
random block Krylov matrix, which may be of independent interest. Similar
bounds are central to recent breakthroughs on faster algorithms for sparse
linear systems [Peng & Vempala, SODA 2021; Nie, STOC 2022].

</details>
