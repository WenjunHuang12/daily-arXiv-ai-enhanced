<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 16]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.IT](#cs.IT) [Total: 6]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.DS](#cs.DS) [Total: 2]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [XProvence: Zero-Cost Multilingual Context Pruning for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.18886)
*Youssef Mohamed,Mohamed Elhoseiny,Thibault Formal,Nadezhda Chirkova*

Main category: cs.IR

TL;DR: XProvence是一个多语言零成本上下文剪枝模型，用于检索增强生成(RAG)，支持100+语言，在16种语言上训练，通过跨语言迁移实现多语言扩展。


<details>
  <summary>Details</summary>
Motivation: 随着RAG系统在多语言环境中的广泛应用，需要将原本仅支持英语的Provence框架（首次将高效零成本上下文剪枝集成到重排序模型中）扩展到多语言场景。

Method: 探索多种策略将Provence框架泛化到英语之外，在16种语言上训练，通过有效的跨语言迁移支持100+语言，实现多语言零成本上下文剪枝。

Result: 在四个多语言问答基准测试中，XProvence能够以最小到无性能损失的方式剪枝RAG上下文，并优于强基线模型。

Conclusion: XProvence成功将零成本上下文剪枝扩展到多语言环境，为多语言RAG系统提供了高效的上下文管理解决方案，模型已在Hugging Face平台开源。

Abstract: This paper introduces XProvence, a multilingual zero-cost context pruning model for retrieval-augmented generation (RAG), trained on 16 languages and supporting 100+ languages through effective cross-lingual transfer. Motivated by the growing use of RAG systems across diverse languages, we explore several strategies to generalize the Provence framework-which first integrated efficient zero-cost context pruning directly into the re-ranking model-beyond English. Across four multilingual question answering benchmarks, we show how XProvence can prune RAG contexts with minimal-to-no performance degradation and outperforms strong baselines. Our model is available at https://huggingface.co/naver/xprovence-reranker-bgem3-v2.

</details>


### [2] [Recommending Composite Items Using Multi-Level Preference Information: A Joint Interaction Modeling Approach](https://arxiv.org/abs/2601.19005)
*Xuan Bi,Yaqiong Wang,Gediminas Adomavicius,Shawn Curley*

Main category: cs.IR

TL;DR: JIMA是一个联合交互建模方法，使用单一模型整合不同粒度数据，学习原子物品和复合物品的用户偏好以及领域专业知识之间的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 随着推荐系统应用场景变得更加多样和复杂，需要更复杂的推荐技术。特别是在复合物品（如时尚搭配）推荐中，存在多个层次的用户偏好信息，需要有效利用这些不同粒度的数据。

Method: 提出JIMA联合交互建模方法，使用单一模型整合所有不同粒度的数据，通过交互学习低阶（原子物品）和高阶（复合物品）用户偏好以及领域专业知识（如风格匹配）之间的复杂关系。

Result: 通过多个模拟研究和真实数据的离线与在线评估，与先进基线方法比较，结果一致表明所提方法具有优越性能。

Conclusion: JIMA方法能够有效利用不同粒度的数据，学习用户偏好和领域专业知识之间的复杂关系，在复合物品推荐任务中表现出色。

Abstract: With the advancement of machine learning and artificial intelligence technologies, recommender systems have been increasingly used across a vast variety of platforms to efficiently and effectively match users with items. As application contexts become more diverse and complex, there is a growing need for more sophisticated recommendation techniques. One example is the composite item (for example, fashion outfit) recommendation where multiple levels of user preference information might be available and relevant. In this study, we propose JIMA, a joint interaction modeling approach that uses a single model to take advantage of all data from different levels of granularity and incorporate interactions to learn the complex relationships among lower-order (atomic item) and higher-order (composite item) user preferences as well as domain expertise (e.g., on the stylistic fit). We comprehensively evaluate the proposed method and compare it with advanced baselines through multiple simulation studies as well as with real data in both offline and online settings. The results consistently demonstrate the superior performance of the proposed approach.

</details>


### [3] [RobustExplain: Evaluating Robustness of LLM-Based Explanation Agents for Recommendation](https://arxiv.org/abs/2601.19120)
*Guilin Zhang,Kai Zhao,Jeffrey Friedman,Xu Chu*

Main category: cs.IR

TL;DR: 该论文提出了RobustExplain框架，首次系统评估LLM生成推荐解释在真实用户行为噪声下的鲁棒性，发现当前模型仅具有中等鲁棒性，大模型稳定性提升有限。


<details>
  <summary>Details</summary>
Motivation: 现实网络平台中用户交互历史存在意外点击、时间不一致、缺失值和偏好变化等固有噪声，但现有研究主要关注固定输入下的解释流畅性和相关性，对LLM生成解释在噪声下的鲁棒性缺乏系统评估，这影响解释稳定性和用户信任。

Method: 提出RobustExplain评估框架：1)引入五种现实用户行为扰动（在不同严重级别评估）；2)设计多维度鲁棒性指标，包括语义、关键词、结构和长度一致性；3)在四个代表性LLM（7B-70B）上进行实验。

Result: 实验显示当前模型仅具有中等鲁棒性，更大模型稳定性提升有限（最多8%）。建立了首个解释代理的鲁棒性基准，表明鲁棒性是可信赖、代理驱动的推荐系统关键维度。

Conclusion: 该研究首次系统评估LLM生成推荐解释的鲁棒性，建立了原则性任务级评估框架和初始基准，强调鲁棒性对大规模网络推荐系统可信度的重要性，为未来研究提供基础。

Abstract: Large Language Models (LLMs) are increasingly used to generate natural-language explanations in recommender systems, acting as explanation agents that reason over user behavior histories. While prior work has focused on explanation fluency and relevance under fixed inputs, the robustness of LLM-generated explanations to realistic user behavior noise remains largely unexplored. In real-world web platforms, interaction histories are inherently noisy due to accidental clicks, temporal inconsistencies, missing values, and evolving preferences, raising concerns about explanation stability and user trust. We present RobustExplain, the first systematic evaluation framework for measuring the robustness of LLM-generated recommendation explanations. RobustExplain introduces five realistic user behavior perturbations evaluated across multiple severity levels and a multi-dimensional robustness metric capturing semantic, keyword, structural, and length consistency. Our goal is to establish a principled, task-level evaluation framework and initial robustness baselines, rather than to provide a comprehensive leaderboard across all available LLMs. Experiments on four representative LLMs (7B--70B) show that current models exhibit only moderate robustness, with larger models achieving up to 8% higher stability. Our results establish the first robustness benchmarks for explanation agents and highlight robustness as a critical dimension for trustworthy, agent-driven recommender systems at web scale.

</details>


### [4] [LLMs as Orchestrators: Constraint-Compliant Multi-Agent Optimization for Recommendation Systems](https://arxiv.org/abs/2601.19121)
*Guilin Zhang,Kai Zhao,Jeffrey Friedman,Xu Chu*

Main category: cs.IR

TL;DR: DualAgent-Rec：基于LLM协调的双智能体框架，用于约束多目标电商推荐，实现100%约束满足和4-6%帕累托超体积提升


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统在处理硬业务约束（如公平性、覆盖率）时存在不足，传统方法将约束视为软惩罚，导致实际部署中频繁违反约束。需要探索如何利用LLM协调约束优化，确保生产环境中约束的严格满足。

Method: 提出DualAgent-Rec框架：1）开发智能体在硬约束下优先优化准确性；2）探索智能体通过无约束帕累托搜索促进多样性；3）基于LLM的协调器根据优化进度和约束满足情况自适应分配资源；4）自适应epsilon松弛机制保证最终解的可行性。

Result: 在Amazon Reviews 2023数据集上的实验表明：实现100%约束满足率，帕累托超体积提升4-6%，同时保持有竞争力的准确性-多样性权衡。

Conclusion: LLM可以作为有效的编排智能体，构建可部署且符合约束的推荐系统，为实际生产环境中的多目标约束优化提供了可行方案。

Abstract: Recommendation systems must optimize multiple objectives while satisfying hard business constraints such as fairness and coverage. For example, an e-commerce platform may require every recommendation list to include items from multiple sellers and at least one newly listed product; violating such constraints--even once--is unacceptable in production. Prior work on multi-objective recommendation and recent LLM-based recommender agents largely treat constraints as soft penalties or focus on item scoring and interaction, leading to frequent violations in real-world deployments. How to leverage LLMs for coordinating constrained optimization in recommendation systems remains underexplored. We propose DualAgent-Rec, an LLM-coordinated dual-agent framework for constrained multi-objective e-commerce recommendation. The framework separates optimization into an Exploitation Agent that prioritizes accuracy under hard constraints and an Exploration Agent that promotes diversity through unconstrained Pareto search. An LLM-based coordinator adaptively allocates resources between agents based on optimization progress and constraint satisfaction, while an adaptive epsilon-relaxation mechanism guarantees feasibility of final solutions. Experiments on the Amazon Reviews 2023 dataset demonstrate that DualAgent-Rec achieves 100% constraint satisfaction and improves Pareto hypervolume by 4-6% over strong baselines, while maintaining competitive accuracy-diversity trade-offs. These results indicate that LLMs can act as effective orchestration agents for deployable and constraint-compliant recommendation systems.

</details>


### [5] [Accelerating Generative Recommendation via Simple Categorical User Sequence Compression](https://arxiv.org/abs/2601.19158)
*Qijiong Liu,Lu Fan,Zhongzhou Liu,Xiaoyu Dong,Yuankai Luo,Guoyuan An,Nuo Chen,Wei Guo,Yong Liu,Xiao-Ming Wu*

Main category: cs.IR

TL;DR: 提出利用物品类别特征压缩用户历史序列的方法，在保持用户兴趣的同时显著降低计算成本，相比HSTU模型实现6倍计算成本降低和39%准确率提升


<details>
  <summary>Details</summary>
Motivation: 生成式推荐系统在长序列上性能更好，但实时部署面临巨大计算成本挑战，需要压缩长序列历史的方法

Method: 利用物品的类别特征来压缩长期用户历史，通过保留用户兴趣的关键信息来减少序列长度

Result: 在两个大规模数据集上，相比HSTU模型，计算成本降低6倍，在相似序列长度下准确率提升39%

Conclusion: 提出的基于物品类别特征的序列压缩方法有效平衡了计算效率和推荐准确性，为生成式推荐系统的实时部署提供了可行方案

Abstract: Although generative recommenders demonstrate improved performance with longer sequences, their real-time deployment is hindered by substantial computational costs. To address this challenge, we propose a simple yet effective method for compressing long-term user histories by leveraging inherent item categorical features, thereby preserving user interests while enhancing efficiency. Experiments on two large-scale datasets demonstrate that, compared to the influential HSTU model, our approach achieves up to a 6x reduction in computational cost and up to 39% higher accuracy at comparable cost (i.e., similar sequence length).

</details>


### [6] [HELM: A Human-Centered Evaluation Framework for LLM-Powered Recommender Systems](https://arxiv.org/abs/2601.19197)
*Sushant Mehta*

Main category: cs.IR

TL;DR: HELM是一个评估LLM推荐系统的新框架，专注于人类中心维度而非传统准确性指标，通过专家评估揭示GPT-4在解释质量和交互自然度上表现优秀但存在显著流行度偏差。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注传统准确性指标，无法捕捉决定真实用户体验的多方面人类中心品质。LLM推荐系统需要更全面的评估框架来评估其在实际应用中的表现。

Method: 提出HELM框架，系统评估LLM推荐系统在五个人类中心维度：意图对齐、解释质量、交互自然度、信任与透明度、公平性与多样性。使用三个最先进的LLM推荐器（GPT-4、LLaMA-3.1、P5）在三个领域（电影、书籍、餐厅）进行实验，由12位领域专家使用847个推荐场景进行严格评估。

Result: HELM揭示了传统指标无法捕捉的关键质量维度：GPT-4在解释质量（4.21/5.0）和交互自然度（4.35/5.0）上表现优异，但显示出显著的流行度偏差（基尼系数0.73），而传统协同过滤为0.58。

Conclusion: HELM框架为推荐系统社区提供了人类中心评估实践的开源工具包，能够更全面地评估LLM推荐系统的真实用户体验质量。

Abstract: The integration of Large Language Models (LLMs) into recommendation systems has introduced unprecedented capabilities for natural language understanding, explanation generation, and conversational interactions. However, existing evaluation methodologies focus predominantly on traditional accuracy metrics, failing to capture the multifaceted human-centered qualities that determine the real-world user experience. We introduce \framework{} (\textbf{H}uman-centered \textbf{E}valuation for \textbf{L}LM-powered reco\textbf{M}menders), a comprehensive evaluation framework that systematically assesses LLM-powered recommender systems across five human-centered dimensions: \textit{Intent Alignment}, \textit{Explanation Quality}, \textit{Interaction Naturalness}, \textit{Trust \& Transparency}, and \textit{Fairness \& Diversity}. Through extensive experiments involving three state-of-the-art LLM-based recommenders (GPT-4, LLaMA-3.1, and P5) across three domains (movies, books, and restaurants), and rigorous evaluation by 12 domain experts using 847 recommendation scenarios, we demonstrate that \framework{} reveals critical quality dimensions invisible to traditional metrics. Our results show that while GPT-4 achieves superior explanation quality (4.21/5.0) and interaction naturalness (4.35/5.0), it exhibits a significant popularity bias (Gini coefficient 0.73) compared to traditional collaborative filtering (0.58). We release \framework{} as an open-source toolkit to advance human-centered evaluation practices in the recommender systems community.

</details>


### [7] [Propagating Similarity, Mitigating Uncertainty: Similarity Propagation-enhanced Uncertainty for Multimodal Recommendation](https://arxiv.org/abs/2601.19198)
*Xinzhuo Wu,Hongbo Wang,Yuan Lin,Kan Xu,Liang Yang,Hongfei Lin*

Main category: cs.IR

TL;DR: SPUMR是一个新的多模态推荐框架，通过构建模态相似性图和协同相似性图来建模和缓解模态不确定性，并通过不确定性感知的偏好聚合模块自适应融合多模态特征。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐系统面临模态特征（如图像模糊、文本歧义）中的固有噪声和不确定性，且忽视模态特定不确定性，导致特征融合效果不佳。同时，现有方法未能充分利用用户和物品间的丰富相似性模式来优化表示和不确定性估计。

Method: SPUMR首先构建模态相似性图和协同相似性图，从内容和行为角度优化表示；然后通过不确定性感知的偏好聚合模块自适应融合多模态特征，为更可靠的模态分配更大权重。

Result: 在三个基准数据集上的大量实验表明，SPUMR相比现有领先方法取得了显著改进。

Conclusion: SPUMR通过显式建模和缓解不确定性，并利用相似性传播机制，有效提升了多模态推荐系统的性能。

Abstract: Multimodal Recommendation (MMR) systems are crucial for modern platforms but are often hampered by inherent noise and uncertainty in modal features, such as blurry images, diverse visual appearances, or ambiguous text. Existing methods often overlook this modality-specific uncertainty, leading to ineffective feature fusion. Furthermore, they fail to leverage rich similarity patterns among users and items to refine representations and their corresponding uncertainty estimates. To address these challenges, we propose a novel framework, Similarity Propagation-enhanced Uncertainty for Multimodal Recommendation (SPUMR). SPUMR explicitly models and mitigates uncertainty by first constructing the Modality Similarity Graph and the Collaborative Similarity Graph to refine representations from both content and behavioral perspectives. The Uncertainty-aware Preference Aggregation module then adaptively fuses the refined multimodal features, assigning greater weight to more reliable modalities. Extensive experiments on three benchmark datasets demonstrate that SPUMR achieves significant improvements over existing leading methods.

</details>


### [8] [Physics-Informed Neuro-Symbolic Recommender System: A Dual-Physics Approach for Personalized Nutrition](https://arxiv.org/abs/2601.19244)
*Chayan Banerjee*

Main category: cs.IR

TL;DR: 提出了一种结合物理约束的神经符号推荐系统，将营养科学融入电商推荐，确保推荐的商品组合符合用户每日能量消耗和宏量营养素需求


<details>
  <summary>Details</summary>
Motivation: 传统电商推荐系统主要优化用户参与度和购买可能性，忽略了人体健康的刚性生理约束。标准协同过滤算法在结构上无法处理这些硬性限制，经常推荐不符合特定每日能量消耗和宏量营养素平衡需求的商品组合

Method: 采用双层次架构的物理信息神经符号推荐系统：1) 使用句子级编码器构建语义知识图谱，将商业产品与权威营养数据严格对齐；2) 训练阶段使用隐式物理正则化器，应用可微分热力学损失函数，确保学习的潜在嵌入反映营养合理性而非简单流行度；3) 推理阶段使用显式物理优化器，采用模拟退火和弹性数量优化生成严格符合用户蛋白质和热量目标的离散杂货商品组合

Result: 未在摘要中明确说明具体实验结果，但提出了一个能够生成符合营养约束的商品推荐的新框架

Conclusion: 该研究通过将营养科学直接整合到推荐流程中，解决了传统推荐系统忽视健康约束的问题，为电商推荐系统提供了更符合人体生理需求的解决方案

Abstract: Traditional e-commerce recommender systems primarily optimize for user engagement and purchase likelihood, often neglecting the rigid physiological constraints required for human health. Standard collaborative filtering algorithms are structurally blind to these hard limits, frequently suggesting bundles that fail to meet specific total daily energy expenditure and macronutrient balance requirements. To address this disconnect, this paper introduces a Physics-Informed Neuro-Symbolic Recommender System that integrates nutritional science directly into the recommendation pipeline via a dual-layer architecture. The framework begins by constructing a semantic knowledge graph using sentence-level encoders to strictly align commercial products with authoritative nutritional data. During the training phase, an implicit physics regularizer applies a differentiable thermodynamic loss function, ensuring that learned latent embeddings reflect nutritional plausibility rather than simple popularity. Subsequently, during the inference phase, an explicit physics optimizer employs simulated annealing and elastic quantity optimization to generate discrete grocery bundles that strictly adhere to the user's protein and caloric targets.

</details>


### [9] [Talos: Optimizing Top-$K$ Accuracy in Recommender Systems](https://arxiv.org/abs/2601.19276)
*Shengjia Zhang,Weiqin Yang,Jiawei Chen,Peng Wu,Yuegang Sun,Gang Wang,Qihao Shi,Can Wang*

Main category: cs.IR

TL;DR: Talos是一种专门优化Top-K推荐准确度的损失函数，通过分位数技术将复杂的排序依赖操作简化为预测分数与学习阈值之间的比较，并采用采样回归算法高效估计阈值，增强了对分布偏移的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统主要关注Top-K结果的质量，但评估Top-K准确度（如Precision@K、Recall@K）需要确定物品的排序位置，这带来了巨大的计算开销和优化挑战。此外，推荐系统经常面临用户偏好演变或数据偏差导致的分布偏移问题。

Method: 提出Talos损失函数，利用分位数技术将排序依赖操作转换为预测分数与学习阈值之间的简单比较；开发采样回归算法高效准确估计阈值；引入约束项防止分数膨胀保持优化稳定性；设计定制代理函数处理不连续性并增强对分布偏移的鲁棒性。

Result: 通过全面的理论分析和实证实验，证明了Talos在有效性、效率、收敛性和分布鲁棒性方面的优越性能。代码已开源。

Conclusion: Talos通过创新的分位数技术和采样回归算法，有效解决了Top-K推荐准确度优化的计算挑战和分布偏移问题，为推荐系统提供了一种高效且鲁棒的优化方法。

Abstract: Recommender systems (RS) aim to retrieve a small set of items that best match individual user preferences. Naturally, RS place primary emphasis on the quality of the Top-$K$ results rather than performance across the entire item set. However, estimating Top-$K$ accuracy (e.g., Precision@$K$, Recall@$K$) requires determining the ranking positions of items, which imposes substantial computational overhead and poses significant challenges for optimization. In addition, RS often suffer from distribution shifts due to evolving user preferences or data biases, further complicating the task.
  To address these issues, we propose Talos, a loss function that is specifically designed to optimize the Talos recommendation accuracy. Talos leverages a quantile technique that replaces the complex ranking-dependent operations into simpler comparisons between predicted scores and learned score thresholds. We further develop a sampling-based regression algorithm for efficient and accurate threshold estimation, and introduce a constraint term to maintain optimization stability by preventing score inflation. Additionally, we incorporate a tailored surrogate function to address discontinuity and enhance robustness against distribution shifts. Comprehensive theoretical analyzes and empirical experiments are conducted to demonstrate the effectiveness, efficiency, convergence, and distributional robustness of Talos. The code is available at https://github.com/cynthia-shengjia/WWW-2026-Talos.

</details>


### [10] [UniRec: Unified Multimodal Encoding for LLM-Based Recommendations](https://arxiv.org/abs/2601.19423)
*Zijie Lei,Tao Feng,Zhigang Hua,Yan Xie,Guanyu Lin,Shuang Yang,Ge Liu,Jiaxuan You*

Main category: cs.IR

TL;DR: UniRec：用于LLM推荐的多模态统一编码器，通过模态特定编码、三元组表示和分层Q-Former处理文本、图像、分类和数值特征的异质推荐信号，在多个基准测试中优于现有方法15%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多模态推荐主要关注文本和图像，但真实推荐信号包含更多模态（文本、图像、分类特征、数值属性）。这些异质模态带来跨模态和模态内的挑战，特别是数值属性（如价格、评分、时间）具有不同语义含义，以及推荐信号的嵌套结构（用户历史是项目序列，每个项目有多个属性）。

Method: UniRec包含三个关键组件：1）模态特定编码器为异质信号生成一致嵌入；2）三元组表示（属性名、类型、值）分离模式与原始输入并保持语义区分；3）分层Q-Former建模用户交互的嵌套结构同时保持分层组织。

Result: 在多个真实世界基准测试中，UniRec优于最先进的多模态和LLM推荐方法达15%。广泛的消融研究进一步验证了每个组件的贡献。

Conclusion: UniRec通过统一的多模态编码有效处理推荐中的异质信号，解决了跨模态和模态内语义区分以及嵌套结构建模的挑战，为LLM在复杂多模态推荐场景中的应用提供了有效解决方案。

Abstract: Large language models have recently shown promise for multimodal recommendation, particularly with text and image inputs. Yet real-world recommendation signals extend far beyond these modalities. To reflect this, we formalize recommendation features into four modalities: text, images, categorical features, and numerical attributes, and highlight the unique challenges this heterogeneity poses for LLMs in understanding multimodal information. In particular, these challenges arise not only across modalities but also within them, as attributes such as price, rating, and time may all be numeric yet carry distinct semantic meanings. Beyond this intra-modality ambiguity, another major challenge is the nested structure of recommendation signals, where user histories are sequences of items, each associated with multiple attributes. To address these challenges, we propose UniRec, a unified multimodal encoder for LLM-based recommendation. UniRec first employs modality-specific encoders to produce consistent embeddings across heterogeneous signals. It then adopts a triplet representation, comprising attribute name, type, and value, to separate schema from raw inputs and preserve semantic distinctions. Finally, a hierarchical Q-Former models the nested structure of user interactions while maintaining their layered organization. Across multiple real-world benchmarks, UniRec outperforms state-of-the-art multimodal and LLM-based recommenders by up to 15%, and extensive ablation studies further validate the contributions of each component.

</details>


### [11] [Masked Diffusion Generative Recommendation](https://arxiv.org/abs/2601.19501)
*Lingyu Mu,Hao Deng,Haibo Xing,Jinxin Hu,Yu Zhang,Xiaoyi Zeng,Jing Zhang*

Main category: cs.IR

TL;DR: 提出MDGR框架，使用掩码扩散生成推荐，解决传统自回归解码在推荐系统中的三个关键限制


<details>
  <summary>Details</summary>
Motivation: 传统生成式推荐采用自回归解码存在三个问题：1)难以捕捉多维度特征的全局依赖关系；2)固定解码路径假设所有用户关注物品属性的顺序相同；3)推理效率低，难以满足实时要求

Method: 提出MDGR掩码扩散生成推荐框架，从三个角度重塑生成推荐流程：1)采用并行码本为基于扩散的生成推荐提供结构基础；2)训练时沿时间和样本维度自适应构建掩码监督信号；3)推理时采用基于预热的两阶段并行解码策略

Result: 在多个公开和工业规模数据集上，MDGR优于10个最先进的基线方法，提升高达10.78%；在大型在线广告平台部署后，收入增加1.20%

Conclusion: MDGR通过掩码扩散方法有效解决了自回归解码在生成推荐中的局限性，在性能和效率上均有显著提升，具有实际应用价值

Abstract: Generative recommendation (GR) typically first quantizes continuous item embeddings into multi-level semantic IDs (SIDs), and then generates the next item via autoregressive decoding. Although existing methods are already competitive in terms of recommendation performance, directly inheriting the autoregressive decoding paradigm from language models still suffers from three key limitations: (1) autoregressive decoding struggles to jointly capture global dependencies among the multi-dimensional features associated with different positions of SID; (2) using a unified, fixed decoding path for the same item implicitly assumes that all users attend to item attributes in the same order; (3) autoregressive decoding is inefficient at inference time and struggles to meet real-time requirements. To tackle these challenges, we propose MDGR, a Masked Diffusion Generative Recommendation framework that reshapes the GR pipeline from three perspectives: codebook, training, and inference. (1) We adopt a parallel codebook to provide a structural foundation for diffusion-based GR. (2) During training, we adaptively construct masking supervision signals along both the temporal and sample dimensions. (3) During inference, we develop a warm-up-based two-stage parallel decoding strategy for efficient generation of SIDs. Extensive experiments on multiple public and industrial-scale datasets show that MDGR outperforms ten state-of-the-art baselines by up to 10.78%. Furthermore, by deploying MDGR on a large-scale online advertising platform, we achieve a 1.20% increase in revenue, demonstrating its practical value. The code will be released upon acceptance.

</details>


### [12] [Enhancing Academic Paper Recommendations Using Fine-Grained Knowledge Entities and Multifaceted Document Embeddings](https://arxiv.org/abs/2601.19513)
*Haixu Xi,Heng Zhang,Chengzhi Zhang*

Main category: cs.IR

TL;DR: 提出一种新的学术论文推荐方法，通过整合细粒度知识实体、文档标题摘要和引文数据来嵌入多维信息，基于组合论文向量相似度生成推荐，在STM-KG数据集上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前学术论文推荐系统主要基于宽泛的主题或领域相似性进行粗粒度推荐，无法满足学者在特定研究方法、具体研究任务等方面的细粒度需求。随着学术文献爆炸式增长，学者进行文献综述的负担加重，需要更精准的推荐系统来提高研究效率和激发创新思维。

Method: 提出一种新颖的学术论文推荐方法，通过整合三种类型的信息：1）新型细粒度知识实体；2）文档标题和摘要；3）引文数据。将这些多维信息嵌入到向量表示中，然后通过计算组合论文向量之间的相似度来生成推荐。

Result: 在STM-KG数据集（包含十个不同领域科学概念的知识图谱）上进行评估，该方法在top-50推荐中平均精度达到27.3%，相比现有方法提升了6.7%，显著优于基线模型。

Conclusion: 该方法通过整合细粒度知识实体、文档内容和引文信息，能够更好地满足学者在科研过程中多样化和具体的文献需求，为学术论文推荐系统提供了更精准的解决方案。

Abstract: In the era of explosive growth in academic literature, the burden of literature review on scholars are increasing. Proactively recommending academic papers that align with scholars' literature needs in the research process has become one of the crucial pathways to enhance research efficiency and stimulate innovative thinking. Current academic paper recommendation systems primarily focus on broad and coarse-grained suggestions based on general topic or field similarities. While these systems effectively identify related literature, they fall short in addressing scholars' more specific and fine-grained needs, such as locating papers that utilize particular research methods, or tackle distinct research tasks within the same topic. To meet the diverse and specific literature needs of scholars in the research process, this paper proposes a novel academic paper recommendation method. This approach embeds multidimensional information by integrating new types of fine-grained knowledge entities, title and abstract of document, and citation data. Recommendations are then generated by calculating the similarity between combined paper vectors. The proposed recommendation method was evaluated using the STM-KG dataset, a knowledge graph that incorporates scientific concepts derived from papers across ten distinct domains. The experimental results indicate that our method outperforms baseline models, achieving an average precision of 27.3% among the top 50 recommendations. This represents an improvement of 6.7% over existing approaches.

</details>


### [13] [LURE-RAG: Lightweight Utility-driven Reranking for Efficient RAG](https://arxiv.org/abs/2601.19535)
*Manish Chandra,Debasis Ganguly,Iadh Ounis*

Main category: cs.IR

TL;DR: LURE-RAG是一个轻量级效用驱动的重排序框架，通过LambdaMART重排序器增强任意检索器，使用列表排序损失直接优化文档排序，在保持高效的同时达到接近SOTA的性能。


<details>
  <summary>Details</summary>
Motivation: 传统RAG管道依赖相关性检索，但这与效用（即检索到的段落是否真正提高下游任务生成质量）存在偏差。现有效用驱动检索方法存在两个问题：1）资源密集需要查询编码，2）训练时不使用列表排序损失，而文档相对顺序直接影响RAG生成质量。

Method: 提出LURE-RAG框架，通过LambdaMART重排序器增强任意黑盒检索器。使用LLM效用指导的列表排序损失训练重排序器，直接优化检索文档的排序。还提出了其密集变体UR-RAG。

Result: 在两个标准数据集上的实验表明，LURE-RAG达到最先进密集神经基线的97-98%性能，同时在训练和推理中保持高效。其密集变体UR-RAG显著优于现有最佳基线，提升高达3%。

Conclusion: LURE-RAG通过轻量级效用驱动重排序有效解决了传统RAG中检索与效用不匹配的问题，在保持效率的同时实现了接近最优的性能，为RAG系统提供了实用的改进方案。

Abstract: Most conventional Retrieval-Augmented Generation (RAG) pipelines rely on relevance-based retrieval, which often misaligns with utility -- that is, whether the retrieved passages actually improve the quality of the generated text specific to a downstream task such as question answering or query-based summarization. The limitations of existing utility-driven retrieval approaches for RAG are that, firstly, they are resource-intensive typically requiring query encoding, and that secondly, they do not involve listwise ranking loss during training. The latter limitation is particularly critical, as the relative order between documents directly affects generation in RAG. To address this gap, we propose Lightweight Utility-driven Reranking for Efficient RAG (LURE-RAG), a framework that augments any black-box retriever with an efficient LambdaMART-based reranker. Unlike prior methods, LURE-RAG trains the reranker with a listwise ranking loss guided by LLM utility, thereby directly optimizing the ordering of retrieved documents. Experiments on two standard datasets demonstrate that LURE-RAG achieves competitive performance, reaching 97-98% of the state-of-the-art dense neural baseline, while remaining efficient in both training and inference. Moreover, its dense variant, UR-RAG, significantly outperforms the best existing baseline by up to 3%.

</details>


### [14] [Comparing how Large Language Models perform against keyword-based searches for social science research data discovery](https://arxiv.org/abs/2601.19559)
*Mark Green,Maura Halstead,Caroline Jay,Richard Kingston,Alex Singleton,David Topping*

Main category: cs.IR

TL;DR: 本文比较了基于大语言模型的语义搜索与传统关键词搜索在数据发现中的性能，发现语义搜索在结果数量、处理复杂查询和容错性方面表现更优，但两种方法各有侧重，可互补使用。


<details>
  <summary>Details</summary>
Motivation: 评估基于大语言模型的语义搜索工具相对于传统关键词搜索在数据发现任务中的性能差异，探索语义搜索在实际应用中的优势和局限性。

Method: 使用英国研究创新局数据服务的定制语义搜索系统与消费者数据研究中心的关键词搜索进行对比。基于2023年12月至2024年10月CDRC搜索日志中提取的131个最常用搜索词，通过描述性统计、定性检查和定量相似性度量（包括精确数据集重叠、Jaccard相似性和BERT嵌入的余弦相似性）评估返回数据集的差异。

Result: 语义搜索始终返回比关键词搜索更多的结果，在处理基于地点、拼写错误、模糊或复杂查询时表现尤为出色。虽然语义搜索未能捕获所有基于关键词的结果，但返回的数据集在语义上高度相似，尽管精确重叠率较低但余弦相似性得分很高。两种工具的排名策略差异显著，反映了不同的优先级策略。

Conclusion: 基于大语言模型的语义搜索在数据发现方面提供了显著改进，特别是在处理自然语言查询、拼写错误和地理上下文理解方面表现优异，但它并非完全替代传统关键词搜索，而是与之形成互补关系。

Abstract: This paper evaluates the performance of a large language model (LLM) based semantic search tool relative to a traditional keyword-based search for data discovery. Using real-world search behaviour, we compare outputs from a bespoke semantic search system applied to UKRI data services with the Consumer Data Research Centre (CDRC) keyword search. Analysis is based on 131 of the most frequently used search terms extracted from CDRC search logs between December 2023 and October 2024. We assess differences in the volume, overlap, ranking, and relevance of returned datasets using descriptive statistics, qualitative inspection, and quantitative similarity measures, including exact dataset overlap, Jaccard similarity, and cosine similarity derived from BERT embeddings. Results show that the semantic search consistently returns a larger number of results than the keyword search and performs particularly well for place based, misspelled, obscure, or complex queries. While the semantic search does not capture all keyword based results, the datasets returned are overwhelmingly semantically similar, with high cosine similarity scores despite lower exact overlap. Rankings of the most relevant results differ substantially between tools, reflecting contrasting prioritisation strategies. Case studies demonstrate that the LLM based tool is robust to spelling errors, interprets geographic and contextual relevance effectively, and supports natural-language queries that keyword search fails to resolve. Overall, the findings suggest that LLM driven semantic search offers a substantial improvement for data discovery, complementing rather than fully replacing traditional keyword-based approaches.

</details>


### [15] [LLM-Enhanced Reinforcement Learning for Long-Term User Satisfaction in Interactive Recommendation](https://arxiv.org/abs/2601.19585)
*Chongjun Xia,Yanchun Peng,Xianzhi Wang*

Main category: cs.IR

TL;DR: 提出LLM增强的强化学习框架LERL，通过分层设计结合LLM的语义规划能力和RL的细粒度适应性，解决推荐系统中的内容同质化和长期用户满意度问题。


<details>
  <summary>Details</summary>
Motivation: 交互式推荐系统容易因过度拟合短期用户偏好而导致内容同质化和过滤气泡效应。现有方法主要关注静态或一次性设置，忽略了用户兴趣的长期演变。强化学习虽然能优化长期满意度，但受限于稀疏的用户-物品交互和有限的语义规划能力。

Method: 提出LLM增强的强化学习框架LERL，采用分层设计：高层LLM规划器选择语义多样的内容类别，低层RL策略在选定语义空间内推荐个性化物品。这种设计缩小了动作空间，提高了规划效率，并减轻了冗余内容的过度曝光。

Result: 在真实世界数据集上的大量实验表明，与最先进的基线方法相比，LERL显著提高了长期用户满意度。

Conclusion: LERL通过结合LLM的语义规划能力和RL的适应性，有效解决了推荐系统中的内容多样性问题和长期用户满意度优化挑战。

Abstract: Interactive recommender systems can dynamically adapt to user feedback, but often suffer from content homogeneity and filter bubble effects due to overfitting short-term user preferences. While recent efforts aim to improve content diversity, they predominantly operate in static or one-shot settings, neglecting the long-term evolution of user interests. Reinforcement learning provides a principled framework for optimizing long-term user satisfaction by modeling sequential decision-making processes. However, its application in recommendation is hindered by sparse, long-tailed user-item interactions and limited semantic planning capabilities. In this work, we propose LLM-Enhanced Reinforcement Learning (LERL), a novel hierarchical recommendation framework that integrates the semantic planning power of LLM with the fine-grained adaptability of RL. LERL consists of a high-level LLM-based planner that selects semantically diverse content categories, and a low-level RL policy that recommends personalized items within the selected semantic space. This hierarchical design narrows the action space, enhances planning efficiency, and mitigates overexposure to redundant content. Extensive experiments on real-world datasets demonstrate that LERL significantly improves long-term user satisfaction when compared with state-of-the-art baselines. The implementation of LERL is available at https://anonymous.4open.science/r/code3-18D3/.

</details>


### [16] [Differentiable Semantic ID for Generative Recommendation](https://arxiv.org/abs/2601.19711)
*Junchen Fu,Xuri Ge,Alexandros Karatzoglou,Ioannis Arapakis,Suzan Verberne,Joemon M. Jose,Zhaochun Ren*

Main category: cs.IR

TL;DR: DIGER提出了一种可微分的语义ID方法，通过引入Gumbel噪声和不确定性衰减策略，解决生成式推荐中语义ID训练与推荐目标不匹配的问题，防止码本崩溃。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法将语义ID视为预定义且静态的，语义ID仅针对内容重建优化而非推荐准确性，导致索引损失与推荐损失之间的目标不匹配。由于分词器独立训练，推荐损失无法更新语义ID，而直接可微分方法常导致码本崩溃。

Method: 提出DIGER框架，引入Gumbel噪声在早期阶段显式鼓励对码本的探索，缓解码本崩溃并提高码利用率。设计两种不确定性衰减策略，逐步减少Gumbel噪声，实现从早期探索到利用已学语义ID的平滑过渡。

Result: 在多个公共数据集上的广泛实验表明，可微分语义ID带来了一致的性能提升。结果证实了通过可微分语义ID对齐索引和推荐目标的有效性。

Conclusion: 可微分语义索引是生成式推荐中一个有前景的研究方向，DIGER为解决语义ID训练与推荐目标不匹配问题提供了有效方案，通过平衡探索与收敛改善了推荐性能。

Abstract: Generative recommendation provides a novel paradigm in which each item is represented by a discrete semantic ID (SID) learned from rich content. Most existing methods treat SIDs as predefined and train recommenders under static indexing. In practice, SIDs are typically optimized only for content reconstruction rather than recommendation accuracy. This leads to an objective mismatch: the system optimizes an indexing loss to learn the SID and a recommendation loss for interaction prediction, but because the tokenizer is trained independently, the recommendation loss cannot update it. A natural approach is to make semantic indexing differentiable so that recommendation gradients can directly influence SID learning, but this often causes codebook collapse, where only a few codes are used. We attribute this issue to early deterministic assignments that limit codebook exploration, resulting in imbalance and unstable optimization.
  In this paper, we propose DIGER (Differentiable Semantic ID for Generative Recommendation), a first step toward effective differentiable semantic IDs for generative recommendation. DIGER introduces Gumbel noise to explicitly encourage early-stage exploration over codes, mitigating codebook collapse and improving code utilization. To balance exploration and convergence, we further design two uncertainty decay strategies that gradually reduce the Gumbel noise, enabling a smooth transition from early exploration to exploitation of learned SIDs. Extensive experiments on multiple public datasets demonstrate consistent improvements from differentiable semantic IDs. These results confirm the effectiveness of aligning indexing and recommendation objectives through differentiable SIDs and highlight differentiable semantic indexing as a promising research direction.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [17] [Encoder-Free ECG-Language Models](https://arxiv.org/abs/2601.18798)
*William Han,Tony Chen,Chaojing Duan,Xiaoyu Song,Yihang Yao,Yuzhe Yang,Michael A. Rosenberg,Emerson Liu,Ding Zhao*

Main category: cs.MM

TL;DR: ELF是一种无编码器的心电图语言模型，仅使用单个投影层替代传统复杂编码器，在五个数据集上性能匹配或超越现有最先进模型，同时揭示了当前评估实践中模型过度依赖基准伪影和语言先验而非心电图信息的问题。


<details>
  <summary>Details</summary>
Motivation: 当前心电图语言模型大多依赖预训练的心电图编码器和复杂的视觉语言模型架构，增加了模型设计和训练复杂度。受无编码器视觉语言模型的启发，研究者希望简化架构，探索更简洁高效的心电图语言模型设计。

Method: 提出ELF模型，完全去除传统的心电图编码器，仅使用单个投影层将心电图数据映射到语言模型空间，并与大型语言模型联合训练。同时测试了添加架构偏置是否提升性能，发现单个线性投影层仍保持竞争力。

Result: 在五个数据集上，ELF的性能匹配或超越了使用更复杂编码器和训练流程的现有最先进心电图语言模型。研究发现，ELF及其他心电图语言模型往往过度依赖基准伪影和语言先验，而非真正的心电图信息。

Conclusion: 无编码器的心电图语言模型设计是可行的，单个投影层足以实现优秀性能。研究揭示了当前心电图语言模型评估实践和设计中的局限性，即模型可能过度利用数据集伪影而非学习真正的心电图特征，需要更严谨的评估方法。

Abstract: ECG-Language Models (ELMs) extend recent progress in Multimodal Large Language Models (MLLMs) to automated ECG interpretation. However, most ELMs follow Vision-Language Model (VLM) designs and depend on pretrained ECG encoders, adding architectural and training complexity. Inspired by encoder-free VLMs, we introduce ELF, an encoder-free ELM that replaces the ECG encoder with a single projection layer trained jointly with the LLM. Across five datasets, ELF matches or exceeds state-of-the-art ELMs that use far more complex encoders and training pipelines. We also test whether adding architectural biases to ELF improves performance and find that the single linear projection remains competitive. Finally, we show that ELF, and potentially other ELMs, often rely more on benchmark artifacts and language priors than ECG-derived information, highlighting limitations in current evaluation practices and ELM design. All data and code is available at https://github.com/willxxy/ECG-Bench.

</details>


### [18] [Benchmarking Multimodal Large Language Models for Missing Modality Completion in Product Catalogues](https://arxiv.org/abs/2601.19750)
*Junchen Fu,Wenhao Deng,Kaiwen Zheng,Alexandros Karatzoglou,Ioannis Arapakis,Yu Ye,Yongxin Ni,Joemon M. Jose,Xuri Ge*

Main category: cs.MM

TL;DR: 该论文研究了多模态大语言模型在电商场景中补全缺失模态（如图像或文本）的能力，提出了MMPCBench基准，评估了6个先进MLLM模型，发现它们在细粒度对齐方面存在局限，性能随商品类别变化，模型规模与性能无简单相关性，GRPO优化仅对图像到文本任务有效。


<details>
  <summary>Details</summary>
Motivation: 电商平台常因标注错误或元数据不完整而出现缺失模态信息（如图像或文本描述），这影响了商品展示和下游应用如推荐系统。受多模态大语言模型生成能力的启发，研究探索MLLM能否在电商场景中补全缺失模态这一基础但未充分研究的问题。

Method: 提出了缺失模态商品补全基准MMPCBench，包含内容质量补全基准和推荐基准两个子基准。评估了Qwen2.5-VL和Gemma-3系列共6个先进MLLM模型，涵盖9个真实电商类别，专注于图像到文本和文本到图像补全任务。还探索了使用Group Relative Policy Optimization来更好地对齐MLLM与该任务。

Result: 实验结果显示：MLLM能捕捉高层语义，但在细粒度词级和像素/块级对齐方面表现不佳；性能在不同商品类别和模型规模间差异显著；模型大小与性能无简单相关性，与主流基准报告的趋势不同；GRPO改善了图像到文本补全，但对文本到图像补全无效。

Conclusion: 这些发现揭示了当前MLLM在真实世界跨模态生成中的局限性，代表了迈向更有效的缺失模态商品补全的早期步骤。研究为理解MLLM在电商场景中的能力边界提供了重要见解。

Abstract: Missing-modality information on e-commerce platforms, such as absent product images or textual descriptions, often arises from annotation errors or incomplete metadata, impairing both product presentation and downstream applications such as recommendation systems. Motivated by the multimodal generative capabilities of recent Multimodal Large Language Models (MLLMs), this work investigates a fundamental yet underexplored question: can MLLMs generate missing modalities for products in e-commerce scenarios? We propose the Missing Modality Product Completion Benchmark (MMPCBench), which consists of two sub-benchmarks: a Content Quality Completion Benchmark and a Recommendation Benchmark.
  We further evaluate six state-of-the-art MLLMs from the Qwen2.5-VL and Gemma-3 model families across nine real-world e-commerce categories, focusing on image-to-text and text-to-image completion tasks. Experimental results show that while MLLMs can capture high-level semantics, they struggle with fine-grained word-level and pixel- or patch-level alignment. In addition, performance varies substantially across product categories and model scales, and we observe no trivial correlation between model size and performance, in contrast to trends commonly reported in mainstream benchmarks. We also explore Group Relative Policy Optimization (GRPO) to better align MLLMs with this task. GRPO improves image-to-text completion but does not yield gains for text-to-image completion. Overall, these findings expose the limitations of current MLLMs in real-world cross-modal generation and represent an early step toward more effective missing-modality product completion.

</details>


### [19] [Subjective Evaluation of Frame Rate in Bitrate-Constrained Live Streaming](https://arxiv.org/abs/2601.19776)
*Jiaqi He,Zhengfang Duanmu,Kede Ma*

Main category: cs.MM

TL;DR: 提出了高帧率直播流（HFR-LS）数据集，包含384个主观评分的1080p视频，系统性地变化压缩强度和帧率，研究带宽约束下压缩强度与帧率之间的感知权衡。


<details>
  <summary>Details</summary>
Motivation: 直播流中的带宽约束要求视频编码器在压缩强度和帧率之间取得平衡，但这种权衡的感知后果尚未得到充分探索。

Method: 创建HFR-LS数据集，包含384个主观评分的1080p视频，通过系统性地变化压缩强度和帧率进行编码；采用单刺激、隐藏参考的主观研究方法来评估感知质量。

Result: 主观研究表明，帧率对感知质量有显著影响，并且与比特率和源内容存在交互作用。

Conclusion: HFR-LS数据集可用于促进带宽约束下直播流的研究，帧率是影响感知质量的重要因素，需要在压缩强度和帧率之间进行权衡。

Abstract: Bandwidth constraints in live streaming require video codecs to balance compression strength and frame rate, yet the perceptual consequences of this trade-off remain underexplored. We present the high frame rate live streaming (HFR-LS) dataset, comprising 384 subject-rated 1080p videos encoded at multiple target bitrates by systematically varying compression strength and frame rate. A single-stimulus, hidden-reference subjective study shows that frame rate has a noticeable effect on perceived quality, and interacts with both bitrate and source content. The HFR-LS dataset is available at https://github.com/real-hjq/HFR-LS to facilitate research on bitrate-constrained live streaming.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [20] [Accelerating Large-Scale Cheminformatics Using a Byte-Offset Indexing Architecture for Terabyte-Scale Data Integration](https://arxiv.org/abs/2601.18921)
*Malikussaid,Septian Caesar Floresko,Sutiyo*

Main category: cs.DB

TL;DR: 该论文研究了如何整合三大公共化学数据库（PubChem、ChEMBL、eMolecules），通过字节偏移索引技术将算法复杂度从O(N×M)降至O(N+M)，实现740倍的性能提升，成功提取了435,413个经过验证的化合物。


<details>
  <summary>Details</summary>
Motivation: 大规模化学数据库整合是现代化学信息学研究的关键瓶颈，特别是对于需要高质量、多源验证数据集的机器学习应用。现有方法在处理亿级数据时面临可扩展性和数据完整性的挑战。

Method: 采用字节偏移索引架构替代暴力搜索算法，从O(N×M)复杂度降至O(N+M)。使用碰撞安全的完整InChI字符串替代存在哈希碰撞的InChIKey分子标识符，重建数据处理流程。

Result: 将预计100天运行时间的暴力搜索算法优化至3.2小时完成，实现740倍性能提升。系统验证了1.76亿个数据库条目，成功提取435,413个经过验证的化合物，并量化了存储开销与科学严谨性之间的权衡。

Conclusion: 字节偏移索引技术能有效克服亿级数据整合的可扩展性限制，为大规模科学数据整合提供了通用原则，特别是在唯一性约束超出基于哈希的标识符能力时。

Abstract: The integration of large-scale chemical databases represents a critical bottleneck in modern cheminformatics research, particularly for machine learning applications requiring high-quality, multi-source validated datasets. This paper presents a case study of integrating three major public chemical repositories: PubChem (176 million compounds), ChEMBL, and eMolecules, to construct a curated dataset for molecular property prediction. We investigate whether byte-offset indexing can practically overcome brute-force scalability limits while preserving data integrity at hundred-million scale. Our results document the progression from an intractable brute-force search algorithm with projected 100-day runtime to a byte-offset indexing architecture achieving 3.2-hour completion-a 740-fold performance improvement through algorithmic complexity reduction from O(NxM) to O(N+M). Systematic validation of 176 million database entries revealed hash collisions in InChIKey molecular identifiers, necessitating pipeline reconstruction using collision-free full InChI strings. We present performance benchmarks, quantify trade-offs between storage overhead and scientific rigor, and compare our approach with alternative large-scale integration strategies. The resulting system successfully extracted 435,413 validated compounds and demonstrates generalizable principles for large-scale scientific data integration where uniqueness constraints exceed hash-based identifier capabilities.

</details>


### [21] [Educational Database Prototype: the Simplest of All](https://arxiv.org/abs/2601.19165)
*Yi Lyu,Yiyin Shen,Takashi Matsuzawa*

Main category: cs.DB

TL;DR: EduDB是一个用于教学目的的简单数据库原型，旨在帮助学生更好地理解数据库系统内部设计，避免陷入实现细节的角落情况。


<details>
  <summary>Details</summary>
Motivation: 当前数据库课程中，学生在实现特定模块（如B+树）时花费过多精力处理边界情况，而未能获得对数据库系统内部设计的全面理解。

Method: 开发EduDB——一个简洁、清晰、全面的数据库原型，并基于此设计一系列综合性的课程项目，为学生提供实践所学优化技术的平台。

Result: 提出了EduDB教学数据库原型，能够为学生提供数据库系统的全面概览，并支持学生实践课程中学到的各种优化技术。

Conclusion: EduDB作为一个教育工具，能够帮助学生更好地理解数据库系统内部设计，通过综合项目实践提升学习效果，避免陷入实现细节的困境。

Abstract: Database Management System (DBMS) is designed to help store and process large collections of data, and is incredibly flexible to perform various kinds of optimizations as long as it achieves serializability with a high-level interface available. The current undergraduate level DBMS course in UW-Madison (i.e., CS564) involves implementing specific modules of DB architecture, including B+ tree, but students may end up spending numerous amounts of effort on corner cases and not gaining a more comprehensive understanding of the internal design. Thus, we present EduDB, a simple database prototype for educational purposes that provides students a clean, concise, and comprehensive overview of the database system. We also attempt to develop an integrative series of course projects based on EduDB, which offers a platform for students to perform any optimization learned during the semester.

</details>


### [22] [Create Benchmarks for Data Lakes](https://arxiv.org/abs/2601.19176)
*Yi Lyu,Pei-Chieh Lo,Natan Lidukhover*

Main category: cs.DB

TL;DR: 提出一个用于数据湖系统评估的新型基准测试框架，涵盖多种数据类型和工作负载，包括数据检索、聚合、查询和相似性搜索，支持可扩展和可重复的性能比较。


<details>
  <summary>Details</summary>
Motivation: 数据湖作为存储和分析异构数据的解决方案日益流行，但缺乏标准化的综合基准测试工具。现有基准主要针对传统数据仓库和结构化SQL工作负载，无法捕捉数据湖的多样化工作负载和访问模式。

Method: 设计了一个新的基准测试框架，覆盖多种数据类型和工作负载模型（数据检索、聚合、查询、相似性搜索），测量查询执行时间、元数据生成时间、元数据大小等关键性能指标，支持不同规模因子，具有可扩展性和可重复性。

Result: 在CloudLab上进行实验，展示了该基准测试框架如何用于比较商业和开源数据湖平台，提供了客观和比较性的评估能力。

Conclusion: 提出的基准测试框架填补了数据湖系统评估的空白，能够支持现实和多样化场景下的数据湖系统性能比较，为数据湖技术的标准化评估提供了重要工具。

Abstract: Data lakes have emerged as a flexible and scalable solution for storing and analyzing large volumes of heterogeneous data, including structured, semi-structured, and unstructured formats. Despite their growing adoption in both industry and academia, there is a lack of standardized and comprehensive benchmarks for evaluating the performance of data lake systems. Existing benchmarks primarily target traditional data warehouses and focus on structured SQL workloads, making them insufficient for capturing the diverse workloads and access patterns typical of data lakes.
  In this work, we propose a new benchmarking framework for data lakes that aims to provide an objective and comparative evaluation of different data lake implementations. Our benchmark covers multiple data types and workload models, including data retrieval, aggregation, querying, and similarity search, which is a common yet underexplored operation in existing benchmarks. We measure key performance metrics such as query execution time, metadata generation time, and metadata size across different scale factors. The benchmark is designed to be extensible and reproducible, enabling users to generate datasets and evaluate data lake systems under realistic and diverse scenarios. We conduct our experiments on CloudLab and demonstrate how the proposed benchmark can be used to compare both commercial and open-source data lake platforms.

</details>


### [23] [Topology-Aware Subset Repair via Entropy-Guided Density and Graph Decomposition](https://arxiv.org/abs/2601.19671)
*Guoqi Zhao,Xixian Han,Xiaolong Wan*

Main category: cs.DB

TL;DR: 提出基于拓扑感知的近似子集修复框架，通过联合密度-冲突惩罚模型解决传统密度方法在脏数据聚类、计算成本和属性权重方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统子集修复存在多个最小修复方案，密度方法虽然偏好保留高质量数据区域，但受到脏数据聚类造成的密度偏差、高计算成本和均匀属性权重的限制。

Method: 提出拓扑感知近似子集修复框架，包含：1) 两层冲突检测策略；2) EntroCFDensity密度度量（结合信息熵和CFD权重）；3) 冲突度度量与拓扑自适应惩罚机制；4) 冲突图分解为独立子图。开发PPIS启发式算法和MICO混合整数规划方法。

Result: 实验结果表明该方法提高了修复准确性和鲁棒性，同时有效保留了高质量数据。

Conclusion: 提出的拓扑感知框架通过联合密度-冲突惩罚模型，克服了传统密度方法的局限性，在保持高质量数据的同时实现了更准确和鲁棒的修复。

Abstract: Subset repair is an important data cleaning technique that enforces integrity constraints by deleting a minimal number of conflicting tuples, yet multiple minimal repairs often exist. Density-based methods address this ambiguity by favoring repairs that preserve dense, high-quality data regions; however, their effectiveness is limited by density bias from dirty clusters, high computational cost, and uniform attribute weighting. We propose a topology-aware approximate subset repair framework based on a joint density-conflict penalty model. The framework integrates three key components. First, a two-layer conflict detection strategy combines attribute inverted indexes with CFD rule grouping to efficiently identify violations. Second, we introduce EntroCFDensity, a density metric that incorporates information entropy and CFD weights to dynamically adjust attribute importance and reduce homogeneity bias. Third, a conflict degree measure is defined to complement local density, enabling a topology-adaptive penalty mechanism with dynamic weight allocation guided by the coefficient of variation. The conflict graph is further decomposed into independent subgraphs, transforming global repair into tractable local subproblems. Based on this framework, we develop two algorithms: PPIS, a scalable heuristic, and MICO, a mixed-integer programming method with theoretical guarantees. Experimental results show that our approach improves repair accuracy and robustness while effectively preserving high-quality data.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [24] [Belief-Combining Framework for Multi-Trace Reconstruction over Channels with Insertions, Deletions, and Substitutions](https://arxiv.org/abs/2601.18920)
*Aria Nouri*

Main category: cs.IT

TL;DR: 提出一种迭代信念组合方法，通过消息传递传播轨迹推断，以二次复杂度实现与联合最大后验估计相同的重建性能


<details>
  <summary>Details</summary>
Motivation: 从多个受随机插入、删除和替换噪声污染的轨迹中重建源序列时，通常需要联合处理所有轨迹，导致计算复杂度随轨迹数量指数增长

Method: 提出迭代信念组合过程，通过消息传递传播轨迹推断，计算符号级后验概率

Result: 证明该方法收敛后能达到与联合最大后验估计相同的重建性能，同时将复杂度降低到轨迹数量的二次方；使用真实世界短链DNA读取聚类数据集验证了性能等价性

Conclusion: 该方法在保持最优重建性能的同时，显著降低了计算复杂度，适用于DNA序列重建等实际应用

Abstract: Optimal reconstruction of a source sequence from multiple noisy traces corrupted by random insertions, deletions, and substitutions typically requires joint processing of all traces, leading to computational complexity that grows exponentially with the number of traces. In this work, we propose an iterative belief-combining procedure that computes symbol-wise a posteriori probabilities by propagating trace-wise inferences via message passing. We prove that, upon convergence, our method achieves the same reconstruction performance as joint maximum a posteriori estimation, while reducing the complexity to quadratic in the number of traces. This performance equivalence is validated using a real-world dataset of clustered short-strand DNA reads.

</details>


### [25] [How Entanglement Reshapes the Geometry of Quantum Differential Privacy](https://arxiv.org/abs/2601.19126)
*Xi Wang,Parastoo Sadeghi,Guodong Shi*

Main category: cs.IT

TL;DR: 量子纠缠在量子本地差分隐私中引发相变现象：低于熵阈值时隐私泄露与无纠缠输入相同，超过阈值时隐私泄露随熵值降低，纠缠成为隐私增强资源。


<details>
  <summary>Details</summary>
Motivation: 经典相关性通常被视为隐私的威胁，但其量子类比——纠缠在量子隐私中的作用尚未被充分理解。本研究旨在探索量子纠缠如何从根本上塑造量子本地差分隐私。

Method: 考虑具有规定纠缠水平的二分量子系统，每个子系统由本地量子机制处理，仅使用本地操作测量，确保过程中不产生额外纠缠。通过将纠缠约束量子态集参数化为光滑流形，使用黎曼优化分析其非凸几何结构。

Result: 发现纠缠与QLDP之间存在尖锐的相变现象：低于机制依赖的熵阈值时，最优隐私泄露水平与无纠缠输入相同；超过阈值时，隐私泄露水平随熵值降低，严格改善隐私保证，甚至可将非私有机制转变为私有机制。

Conclusion: 纠缠是真正的隐私增强资源，相变现象源于纠缠约束量子态集的固有非凸几何结构。这为设计鲁棒的隐私保护量子协议提供了几何和操作基础。

Abstract: Quantum differential privacy provides a rigorous framework for quantifying privacy guarantees in quantum information processing. While classical correlations are typically regarded as adversarial to privacy, the role of their quantum analogue, entanglement, is not well understood. In this work, we investigate how quantum entanglement fundamentally shapes quantum local differential privacy (QLDP). We consider a bipartite quantum system whose input state has a prescribed level of entanglement, characterized by a lower bound on the entanglement entropy. Each subsystem is then processed by a local quantum mechanism and measured using local operations only, ensuring that no additional entanglement is generated during the process. Our main result reveals a sharp phase-transition phenomenon in the relation between entanglement and QLDP: below a mechanism-dependent entropy threshold, the optimal privacy leakage level mirrors that of unentangled inputs; beyond this threshold, the privacy leakage level decreases with the entropy, which strictly improves privacy guarantees and can even turn some non-private mechanisms into private ones. The phase-transition phenomenon gives rise to a nonlinear dependence of the privacy leakage level on the entanglement entropy, even though the underlying quantum mechanisms and measurements are linear. We show that the transition is governed by the intrinsic non-convex geometry of the set of entanglement-constrained quantum states, which we parametrize as a smooth manifold and analyze via Riemannian optimization. Our findings demonstrate that entanglement serves as a genuine privacy-enhancing resource, offering a geometric and operational foundation for designing robust privacy-preserving quantum protocols.

</details>


### [26] [Information-Theoretic Secure Aggregation over Regular Graphs](https://arxiv.org/abs/2601.19183)
*Xiang Zhang,Zhou Li,Han Yu,Kai Wan,Hua Sun,Mingyue Ji,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出拓扑安全聚合(TSA)框架，研究去中心化网络中邻居用户输入的安全聚合，建立了基于图谱特性的统一线性设计框架，揭示了总密钥需求仅取决于邻居规模d而非网络规模的基本极限。


<details>
  <summary>Details</summary>
Motivation: 大规模去中心化学习（如联邦学习）需要通信效率和强数据安全性，但现有安全聚合研究主要集中于集中式和全连接网络，对于本地连接有限的去中心化网络的安全聚合问题尚未充分探索。

Method: 提出拓扑安全聚合(TSA)框架，开发统一的线性设计框架，通过通信图的谱特性（特别是对角调制邻接矩阵的核）来表征TSA的可实现性。针对几类代表性的d-正则图（环、棱柱、完全拓扑）进行分析。

Result: 建立了最优通信和密钥速率区域：每个用户必须存储至少1个密钥符号，广播至少1个消息符号，所有用户总共必须持有至少d个i.i.d.密钥符号。总密钥需求仅取决于邻居规模d，与网络规模无关。

Conclusion: 揭示了去中心化网络中安全聚合的基本极限：总密钥需求仅由本地连接度决定，与网络总规模无关，为有限本地连接的去中心化网络中的安全聚合提供了理论指导。

Abstract: Large-scale decentralized learning frameworks such as federated learning (FL), require both communication efficiency and strong data security, motivating the study of secure aggregation (SA). While information-theoretic SA is well understood in centralized and fully connected networks, its extension to decentralized networks with limited local connectivity remains largely unexplored. This paper introduces \emph{topological secure aggregation} (TSA), which studies one-shot, information-theoretically secure aggregation of neighboring users' inputs over arbitrary network topologies. We develop a unified linear design framework that characterizes TSA achievability through the spectral properties of the communication graph, specifically the kernel of a diagonally modulated adjacency matrix. For several representative classes of $d$-regular graphs including ring, prism and complete topologies, we establish the optimal communication and secret key rate region. In particular, to securely compute one symbol of the neighborhood sum, each user must (i) store at least one key symbol, (ii) broadcast at least one message symbol, and (iii) collectively, all users must hold at least $d$ i.i.d. key symbols. Notably, this total key requirement depends only on the \emph{neighborhood size} $d$, independent of the network size, revealing a fundamental limit of SA in decentralized networks with limited local connectivity.

</details>


### [27] [Movable-Antenna Empowered Backscatter ISAC: Toward Geometry-Adaptive, Low-Power Networks](https://arxiv.org/abs/2601.19224)
*Haohao Zhang,Bowen Gu,Xianhua Yu,Hao Xie,Liejun Wang,Yongjun Xu,Xiaoming Tao,Haijun Zhang*

Main category: cs.IT

TL;DR: 论文提出利用可移动天线系统解决B-ISAC中双衰落和几何失准问题，通过实时天线重定位主动重构级联信道，实现几何自适应的超低功耗双功能无线系统。


<details>
  <summary>Details</summary>
Motivation: 基于反向散射的集成感知与通信面临级联链路严重双衰落和几何失准敏感的根本问题，这限制了其在实际应用中的性能表现。

Method: 在收发端集成可移动天线系统，通过亚波长级别的天线重定位提供实时可控的空间自由度，在不修改被动标签或消耗额外频谱的情况下主动重构级联信道。

Result: MAS辅助的B-ISAC架构通过比较分析和数值结果展示了系统级增益，在关键物联网应用场景中展现了几何自适应范式的潜力。

Conclusion: 可移动天线系统为B-ISAC提供了解决几何瓶颈的有效方案，指向未来运动感知无线网络的发展方向，实现了超低功耗的双功能无线系统。

Abstract: Backscatter-based integrated sensing and communication (B-ISAC) elevates passive tags into information-bearing scatterers, offering an ultra-low-power path toward dual-function wireless systems. However, this promise is fundamentally undermined by a cascaded backscattering link that suffers from severe double fading and is exquisitely sensitive to geometric misalignment. This article tackles this geometric bottleneck by integrating movable antenna systems (MAS) at the transceiver side. MAS provides real-time, controllable spatial degrees of freedom through sub-wavelength antenna repositioning, enabling active reconfiguration of the cascaded channel without modifying passive tags or consuming additional spectrum. We position this solution within a unified ISAC-backscatter communication-B-ISAC evolution, describe the resulting MAS-assisted B-ISAC architecture and operating principles, and demonstrate its system-level gains through comparative analysis and numerical results. Finally, we showcase the potential of this geometry-adaptive paradigm across key IoT application scenarios, pointing toward future motion-aware wireless networks.

</details>


### [28] [On the Analysis of Platooned Vehicular Networks on Highways](https://arxiv.org/abs/2601.19370)
*Kaushlendra Pandey,Harpreet S. Dhillon,Abhishek K. Gupta*

Main category: cs.IT

TL;DR: 该论文分析了高速公路车辆编队对V2V和V2I通信性能的影响，使用随机几何模型比较了编队与非编队交通场景下的网络性能指标。


<details>
  <summary>Details</summary>
Motivation: 高速公路为车辆编队提供了自然场景，但编队对通信连接性的影响尚未充分探索。有效的编队依赖于可靠的V2V和V2I链路，因此理解高速公路上的连接动态至关重要。

Method: 使用随机几何建模：将RSU建模为一维泊松点过程，非编队场景下VU为一维PPP，编队场景下为一维Matern聚类过程。分析每个RSU的负载分布、覆盖概率、速率覆盖及其元分布。

Result: 通过理论分析和仿真验证，提供了不同交通模式对RSU负载分布、覆盖概率和速率覆盖性能影响的数值见解。

Conclusion: 车辆编队对高速公路通信性能有显著影响，该研究为理解编队场景下的连接动态提供了理论框架和性能评估方法。

Abstract: Vehicular platooning refers to coordinated and close movement of vehicular users (VUs) traveling together along a common route segment, offering strategic benefits such as reduced fuel costs, lower emissions, and improved traffic flow. {Highways offer a natural setting for platooning due to extended travel distances, yet their potential remains underexplored, particularly in terms of communication and connectivity. Given that effective platooning relies on robust vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) links, understanding connectivity dynamics on highways is essential.} In this paper, we analyze the dynamics of vehicular platooning on a highway and its impact on the performance of two forms of vehicular communications -- namely V2V and V2I communication -- compared to independent vehicle movement on a highway. The vehicular networks consists of road-side units (RSUs), modeled as a 1D Poisson point process (PPP), to provide the vehicular connectivity to the VUs. VUs are modeled as 1D PPP under the non-platooned traffic scenario (N-PTS) and as a 1D Matern cluster process (MCP) under the platooned traffic scenario (PTS). We evaluate the distribution on the per-RSU load, representing the number of VUs served, for the typical and tagged RSU. Additionally, we derive coverage probability (CP) and rate coverage (RC), which measures the probability of the signal-to-interference-plus-noise ratio (SINR) and achievable rate above a specified threshold at the typical VU along with their meta distribution (MD), providing a deeper understanding of the reliability and variability of these metrics across different spatial distributions of VUs and RSUs. Finally, we validate our theoretical findings through simulations and provide numerical insights into the impact of different traffic patterns on RSU load distribution, CP, and RC performance.

</details>


### [29] [Joint Power Allocation and Antenna Placement for Pinching-Antenna Systems under User Location Uncertainty](https://arxiv.org/abs/2601.19704)
*Hao Feng,Ming Zeng,Xingwang Li,Wenwu Xie,Nian Xia,Octavia A. Dobre*

Main category: cs.IT

TL;DR: 提出一种针对多用户下行链路pinching天线系统的鲁棒资源分配框架，在存在高斯分布定位不确定性的情况下最大化能效，采用PSO算法优化天线位置。


<details>
  <summary>Details</summary>
Motivation: Pinching天线系统在高频段能维持可靠的视距通信，但其性能严重依赖于有效的资源分配（功率、带宽、天线位置）。现有研究大多假设完美信道状态信息或理想用户定位，而实际中存在定位不确定性，需要更准确的模型来处理现实世界中的定位误差。

Method: 1. 建立高斯分布定位不确定性的模型；2. 在给定天线位置下，推导出满足概率中断约束的解析功率分配策略；3. 采用启发式粒子群优化算法寻找能实现全局能效最优配置的天线位置。

Result: 仿真结果表明，与固定天线基准方案相比，所提方案能显著提升能效和系统可靠性，验证了其在实际高频无线部署中的有效性。

Conclusion: 该研究为存在定位不确定性的pinching天线系统提供了一个有效的鲁棒资源分配框架，通过联合优化功率分配和天线位置，实现了能效和可靠性的双重提升，适用于实际高频无线通信部署。

Abstract: Pinching antenna systems have attracted much attention recently owing to its capability to maintain reliable line-of-sight (LoS) communication in high-frequency bands. By guiding signals through a waveguide and emitting them via a movable pinching antenna, these systems enable dynamic control of signal propagation and spatial adaptability. However, their performance heavily depends on effective resource allocation-encompassing power, bandwidth, and antenna positioning-which becomes challenging under imperfect channel state information (CSI) and user localization uncertainty. Existing studies largely assume perfect CSI or ideal user positioning, while our prior work considered uniform localization errors, an oversimplified assumption. In this paper, we develop a robust resource allocation framework for multiuser downlink pinching antenna systems under Gaussian-distributed localization uncertainty, which more accurately models real-world positioning errors. An energy efficiency (EE) maximization problem is formulated subject to probabilistic outage constraints, and an analytical power allocation strategy is derived under given antenna positions. On this basis, the heuristic particle swarm optimization (PSO) algorithm is employed to identify the antenna position that achieves the global EE configuration. Simulation results illustrate that the proposed scheme greatly enhances both EE and system reliability compared with fixed-antenna benchmark, validating its effectiveness for practical high-frequency wireless deployments.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [30] [Fog of War Chess](https://arxiv.org/abs/2601.18813)
*Matthias Gehnen,Julius Stannat*

Main category: cs.GT

TL;DR: 本文首次对"战争迷雾"国际象棋的残局进行理论分析，研究了王后对王、车对王、双车对王三种局面，发现王后必胜，单车不能必胜，但双车可以必胜。


<details>
  <summary>Details</summary>
Motivation: 战争迷雾国际象棋是一种流行的变体，玩家只能部分了解对手棋子的位置。目前缺乏对这种变体残局的理论分析，特别是基本残局的理论研究。

Method: 对三种残局局面进行理论分析：1) 王和后 vs 王；2) 王和车 vs 王；3) 王和双车 vs 王。通过理论推导和证明来确定必胜条件。

Result: 1) 王和后可以保证必胜孤王；2) 与经典国际象棋不同，王和车不能保证必胜孤王；3) 增加一个车后（王和双车）可以保证必胜。

Conclusion: 战争迷雾国际象棋的残局理论不同于经典国际象棋，信息不完全性改变了必胜条件。王后必胜，单车不能必胜，但双车组合可以必胜，这为理解不完全信息下的棋局提供了理论基础。

Abstract: Fog of War chess is a popular variant of classical chess, in which both players have only partial information about the position of the opponent's pieces. This study provides the first theoretical analysis of endgames in Fog of War chess. In particular, we analyze the setups king and queen versus king, king and rook versus king, and king and two rooks versus king. We show that a king and queen can always guarantee a win against a lone king. In contrast to classical chess, a king and a rook cannot guarantee a win against a lone king. However, adding one more rook guarantees a win.

</details>


### [31] [Differential Voting: Loss Functions For Axiomatically Diverse Aggregation of Heterogeneous Preferences](https://arxiv.org/abs/2601.18824)
*Zhiyu An,Duaa Nakshbandi,Wan Du*

Main category: cs.GT

TL;DR: 论文提出Differential Voting框架，将RLHF中的偏好聚合问题形式化为可微投票规则，使不同的社会选择规则（如Borda、Copeland、Kemeny）可通过可微损失函数实现，从而在优化稳定性和公理保证之间进行权衡。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法（如BTL模型）隐式地将异质人类偏好聚合为单一效用函数，这相当于Borda计数投票，限制了学习奖励的公理性质并模糊了优化中的规范性假设。需要更灵活、可控的偏好聚合框架。

Method: 提出Differential Voting框架，构建实例级别的可微损失函数，其总体最优解对应经典投票规则。开发了基于多数原则（BTL）、Copeland和Kemeny规则的可微代理，分析其校准特性、梯度场和平滑参数消失时的极限行为。

Result: 为每种损失函数建立了与相应社会选择规则的一致性，并刻画了其满足或违反的公理。分析表明损失函数几何设计（如边界敏感性和边界集中度）直接转化为规范性聚合行为。

Conclusion: Differential Voting使偏好聚合成为RLHF中明确且可控的设计选择，能够在公理保证和优化稳定性之间进行原则性权衡。代码已开源。

Abstract: Reinforcement learning from human feedback (RLHF) implicitly aggregates heterogeneous human preferences into a single utility function, even though the underlying utilities of the participants are in practice diverse. Hence, RLHF can be viewed as a form of voting, where the aggregation mechanism is defined by the loss function. Although Arrow's Impossibility Theorem suggests that different mechanisms satisfy different sets of desirable axioms, most existing methods rely on a single aggregation principle, typically the Bradley-Terry-Luce (BTL) model, which corresponds to Borda count voting. This restricts the axiomatic properties of the learned reward and obscures the normative assumptions embedded in optimization. In this work, we introduce Differential Voting, a unifying framework that constructs instance-wise, differentiable loss functions whose population-level optima provably correspond to distinct classical voting rules. We develop differentiable surrogates for majority-based aggregation (BTL), Copeland, and Kemeny rules, and formally analyze their calibration properties, gradient fields, and limiting behavior as smoothing parameters vanish. For each loss, we establish consistency with the corresponding social choice rule and characterize the axioms it satisfies or violates. Our analysis shows how design choices in loss geometry-such as margin sensitivity and boundary concentration-directly translate into normative aggregation behavior. Differential Voting makes preference aggregation an explicit and controllable design choice in RLHF, enabling principled trade-offs between axiomatic guarantees and optimization stability. Code to reproduce our experiments is open-sourced.

</details>


### [32] [Ad Insertion in LLM-Generated Responses](https://arxiv.org/abs/2601.19435)
*Shengwei Xu,Zhaohua Chen,Xiaotie Deng,Zhiyi Huang,Grant Schoenebeck*

Main category: cs.GT

TL;DR: 提出一个用于LLM广告的实用框架，通过解耦广告插入与响应生成、以及基于"流派"的解耦竞价，解决传统搜索广告在对话场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统搜索广告依赖静态关键词，无法捕捉对话流中短暂、上下文相关的用户意图。LLM广告需要同时满足上下文连贯性、计算效率、伦理和监管标准等多重约束，现有方法难以全面满足这些要求。

Method: 采用两种解耦策略：1) 广告插入与响应生成解耦，确保安全和明确披露；2) 使用"流派"（高层语义聚类）作为代理，将竞价与具体用户查询解耦。应用VCG拍卖机制，并引入"LLM-as-a-Judge"指标评估上下文连贯性。

Result: 基于流派的VCG拍卖机制实现了近似主导策略激励相容性、个体理性以及近似最优社会福利，同时保持高计算效率。LLM-as-a-Judge指标与人类评分强相关（Spearman's ρ≈0.66），优于80%的人类评估者。

Conclusion: 该框架通过解耦策略有效解决了LLM广告的多重约束问题，在保持上下文连贯性、计算效率和伦理标准的同时，实现了可持续的商业模式。

Abstract: Sustainable monetization of Large Language Models (LLMs) remains a critical open challenge. Traditional search advertising, which relies on static keywords, fails to capture the fleeting, context-dependent user intents--the specific information, goods, or services a user seeks--embedded in conversational flows. Beyond the standard goal of social welfare maximization, effective LLM advertising imposes additional requirements on contextual coherence (ensuring ads align semantically with transient user intents) and computational efficiency (avoiding user interaction latency), as well as adherence to ethical and regulatory standards, including preserving privacy and ensuring explicit ad disclosure. Although various recent solutions have explored bidding on token-level and query-level, both categories of approaches generally fail to holistically satisfy this multifaceted set of constraints.
  We propose a practical framework that resolves these tensions through two decoupling strategies. First, we decouple ad insertion from response generation to ensure safety and explicit disclosure. Second, we decouple bidding from specific user queries by using ``genres'' (high-level semantic clusters) as a proxy. This allows advertisers to bid on stable categories rather than sensitive real-time response, reducing computational burden and privacy risks. We demonstrate that applying the VCG auction mechanism to this genre-based framework yields approximately dominant strategy incentive compatibility (DSIC) and individual rationality (IR), as well as approximately optimal social welfare, while maintaining high computational efficiency. Finally, we introduce an "LLM-as-a-Judge" metric to estimate contextual coherence. Our experiments show that this metric correlates strongly with human ratings (Spearman's $ρ\approx 0.66$), outperforming 80% of individual human evaluators.

</details>


### [33] [Single-Winner Voting on Matchings](https://arxiv.org/abs/2601.19653)
*Niclas Boehmer,Jessica Dierking*

Main category: cs.GT

TL;DR: 论文研究图匹配选举问题，选民对整个匹配有偏好而非作为图的一部分，候选空间是指数级的所有可行匹配，作者通过匹配结构探索计算可行性边界


<details>
  <summary>Details</summary>
Motivation: 传统匹配问题中参与者是图的一部分，而本文研究选民对整个匹配有偏好的选举问题。候选匹配空间指数级增长使得标准算法不可行，需要探索是否可以通过匹配结构恢复计算可行性。

Method: 研究匹配选举的计算复杂性，分析社会福利最大化、帕累托最优结果构建与验证、孔多塞胜者存在性与验证等问题，在一个仿射和两个基于批准的效用模型下进行

Result: 获得混合的算法和不可行性结果，揭示了可处理与不可处理情况之间的清晰边界，复杂性变化源于效用模型或解决方案概念的微妙变化

Conclusion: 通过匹配结构可以部分恢复计算可行性，但存在明显的复杂性边界，效用模型和解决方案概念的细微变化会导致计算复杂性的显著变化

Abstract: We introduce a single-winner perspective on voting on matchings, in which voters have preferences over possible matchings in a graph, and the goal is to select a single collectively desirable matching. Unlike in classical matching problems, voters in our model are not part of the graph; instead, they have preferences over the entire matching. In the resulting election, the candidate space consists of all feasible matchings, whose exponential size renders standard algorithms for identifying socially desirable outcomes computationally infeasible. We study whether the computational tractability of finding such outcomes can be regained by exploiting the matching structure of the candidate space. Specifically, we provide a complete complexity landscape for questions concerning the maximization of social welfare, the construction and verification of Pareto optimal outcomes, and the existence and verification of Condorcet winners under one affine and two approval-based utility models. Our results consist of a mix of algorithmic and intractability results, revealing sharp boundaries between tractable and intractable cases, with complexity jumps arising from subtle changes in the utility model or solution concept.

</details>


### [34] [Robustness of Approval-Based Multiwinner Voting Rules](https://arxiv.org/abs/2601.19706)
*Piotr Faliszewski,Grzegorz Gawron,Bartosz Kusek*

Main category: cs.GT

TL;DR: 研究基于批准的多赢家投票规则对投票微小扰动的鲁棒性，分析委员会如何因单个批准操作的增删改而改变，并计算改变获胜委员会所需的最小操作数及其计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 投票系统在实际应用中可能受到各种微小扰动的影响，需要评估基于批准的多赢家投票规则对这些扰动的鲁棒性，以确保选举结果的稳定性和可靠性。

Method: 通过分析添加/删除/交换单个批准操作对委员会结果的影响，研究改变获胜委员会所需的最小操作数，并分析相关决策问题的计算复杂度。同时考虑计数变体问题，计算随机扰动后选举结果改变的概率。

Result: 论文提供了关于基于批准的多赢家投票规则鲁棒性的理论分析，确定了改变获胜委员会所需的最小操作数，并分析了相关决策问题的计算复杂度特性。

Conclusion: 基于批准的多赢家投票规则对微小扰动具有一定的敏感性，研究结果为评估投票系统的鲁棒性提供了理论框架和计算方法，有助于设计更稳定的选举机制。

Abstract: We investigate how robust approval-based multiwinner voting rules are to small perturbations in the votes. In particular, we consider the extent to which a committee can change after we add/remove/swap one approval, and we consider the computational complexity of deciding how many such operations are necessary to change the set of winning committees. We also consider the counting variants of our problems, which can be interpreted as computing the probability that the result of an election changes after a given number of random perturbations of the given election.

</details>


### [35] [How Similar Are Two Elections?](https://arxiv.org/abs/2601.19716)
*Piotr Faliszewski,Piotr Skowron,Arkadii Slinko,Krzysztof Sornat,Stanisław Szufa,Nimrod Talmon*

Main category: cs.GT

TL;DR: 本文提出并研究了序数选举之间的同构距离，这些距离在候选人和选民重命名下保持不变，并证明了选举同构测试的多项式时间可解性，但两种自然同构距离是NP完全且难以近似的，同时给出了几种参数化的FPT算法。


<details>
  <summary>Details</summary>
Motivation: 研究选举之间的相似性度量对于选举分析、投票系统比较和选举操纵检测具有重要意义。现有距离度量通常对候选人和选民的重命名敏感，需要开发在重命名下不变的度量方法，以捕捉选举结构本质上的相似性。

Method: 提出同构距离的概念：给定偏好顺序之间的距离d，通过统一候选人名称并找到投票之间的匹配来扩展到选举距离d-ID，使得匹配投票之间的d距离之和最小。研究了选举同构测试的计算复杂度，并分析了两种自然同构距离的计算特性。

Result: 1. 选举同构测试可以在多项式时间内完成；2. 两种非常自然的同构距离是NP完全的且难以近似；3. 针对几种自然参数化提出了FPT（固定参数可解）算法。

Conclusion: 虽然选举同构测试是多项式时间可解的，但计算同构距离在一般情况下是困难的。通过参数化算法设计，可以在特定条件下高效计算这些距离，为选举相似性分析提供了实用的计算工具。

Abstract: We introduce and study isomorphic distances between ordinal
  elections (with the same numbers of candidates and voters). The main
  feature of these distances is that they are invariant to renaming
  the candidates and voters, and two elections are at distance zero if
  and only if they are isomorphic. Specifically, we consider
  isomorphic extensions of distances between preference orders: Given
  such a distance d, we extend it to distance d-ID between
  elections by unifying candidate names and finding a matching between
  the votes, so that the sum of the d-distances between the matched
  votes is as small as possible.
  We show that testing isomorphism of two elections can be done in
  polynomial time so, in principle, such distances can be tractable.
  Yet, we show that two very natural isomorphic distances are
  NP-complete and hard to approximate. We attempt to rectify the
  situation by showing FPT algorithms for several natural
  parameterizations.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [36] [Analysis of Shuffling Beyond Pure Local Differential Privacy](https://arxiv.org/abs/2601.19154)
*Shun Takagi,Seng Pei Liew*

Main category: cs.DS

TL;DR: 本文提出了一种新的渐近分析方法，用于精确量化洗牌机制对本地差分隐私的放大效应，通过引入"洗牌指数"这一标量参数，为洗牌DP提供了紧致的上下界分析。


<details>
  <summary>Details</summary>
Motivation: 现有洗牌隐私分析主要关注本地DP参数ε₀，给出的通用上界往往较宽松，且未能准确刻画基本机制（如高斯机制）在洗牌后的隐私放大效果。需要更精确的分析方法来理解洗牌如何放大隐私。

Method: 重新审视Balle等人的隐私毯界（毯散度），在温和的正则性假设下，对广泛的本地随机化器进行渐近分析。引入"洗牌指数"χ作为关键参数，通过渐近分析推导洗牌机制(ε_n,δ_n)-DP保证的紧致上下界。同时开发了基于FFT的算法，用于有限n时的毯散度计算。

Result: 发现毯散度的主导项仅通过单个标量参数χ（洗牌指数）依赖于本地机制。获得了洗牌机制DP保证的紧致上下界带。对于k≥3的k-RR族，上下界渐近一致；对于广义高斯机制，虽然条件可能不满足，但得到的界带仍然紧致。FFT算法提供了相对误差严格可控、运行时间接近线性的数值分析工具。

Conclusion: 该研究为洗牌DP提供了精确的渐近理论和实用的数值分析工具，通过洗牌指数χ这一简洁参数刻画了洗牌隐私放大的本质特征，显著改进了现有分析方法的精度和实用性。

Abstract: Shuffling is a powerful way to amplify privacy of a local randomizer in private distributed data analysis, but existing analyses mostly treat the local differential privacy (DP) parameter $\varepsilon_0$ as the only knob and give generic upper bounds that can be loose and do not even characterize how shuffling amplifies privacy for basic mechanisms such as the Gaussian mechanism. We revisit the privacy blanket bound of Balle et al. (the blanket divergence) and develop an asymptotic analysis that applies to a broad class of local randomizers under mild regularity assumptions, without requiring pure local DP. Our key finding is that the leading term of the blanket divergence depends on the local mechanism only through a single scalar parameter $χ$, which we call the shuffle index. By applying this asymptotic analysis to both upper and lower bounds, we obtain a tight band for $δ_n$ in the shuffled mechanism's $(\varepsilon_n,δ_n)$-DP guarantee. Moreover, we derive a simple structural necessary and sufficient condition on the local randomizer under which the blanket-divergence-based upper and lower bounds coincide asymptotically. $k$-RR families with $k\ge3$ satisfy this condition, while for generalized Gaussian mechanisms the condition may not hold but the resulting band remains tight. Finally, we complement the asymptotic theory with an FFT-based algorithm for computing the blanket divergence at finite $n$, which offers rigorously controlled relative error and near-linear running time in $n$, providing a practical numerical analysis for shuffle DP.

</details>


### [37] [Preprocessing Uncertain Data into Supersequences for Sorting and Gaps](https://arxiv.org/abs/2601.19453)
*Maarten Löffler,Benjamin Raichel*

Main category: cs.DS

TL;DR: 提出使用超序列作为处理不确定数据的辅助结构，应用于排序和计算数值集合的最小/最大间隔问题


<details>
  <summary>Details</summary>
Motivation: 现有不确定数据处理框架需要为特定问题设计专门的辅助结构，作者希望提出更通用、更简单的解决方案

Method: 使用区间超序列作为辅助结构，将预处理阶段与重构阶段解耦，分别解决两个独立的算法问题

Result: 超序列方法比之前专门设计的辅助结构更简单，能够获得已知和新结果，并识别出一个关键的开放问题

Conclusion: 超序列作为辅助结构简化了不确定数据处理框架，实现了预处理与重构的更强解耦，具有独立研究价值

Abstract: In the preprocessing framework for dealing with uncertain data, one is given a set of regions that one is allowed to preprocess to create some auxiliary structure such that when a realization of these regions is given, consisting of one point per region, this auxiliary structure can be used to reconstruct some desired output structure more efficiently than would have been possible without preprocessing. The framework has been successfully applied to several, mostly geometric, computational problems.
  In this note, we propose using a supersequence of input items as the auxiliary structure, and explore its potential on the problems of sorting and computing the smallest or largest gap in a set of numbers. That is, our uncertainty regions are intervals on the real line, and in the preprocessing phase we output a supersequence of the intervals such that the sorted order / smallest gap / largest gap of any realization is a subsequence of this sequence.
  We argue that supersequences are simpler than specialized auxiliary structures developed in previous work. An advantage of using supersequences as the auxiliary structures is that it allows us to decouple the preprocessing phase from the reconstruction phase in a stronger sense than was possible in previous work, resulting in two separate algorithmic problems for which different solutions may be combined to obtain known and new results. We identify one key open problem which we believe is of independent interest.

</details>
