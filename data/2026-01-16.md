<div id=toc></div>

# Table of Contents

- [cs.DS](#cs.DS) [Total: 3]
- [cs.IT](#cs.IT) [Total: 41]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.DB](#cs.DB) [Total: 5]


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [1] [Improved Algorithms for Fair Matroid Submodular Maximization](https://arxiv.org/abs/2601.09860)
*Sepideh Mahabadi,Sherry Sarkar,Jakub Tarnawski*

Main category: cs.DS

TL;DR: 提出了一种近似算法，能在公平性约束下最大化单调次模函数，相比之前损失因子2的方法，新算法能任意接近完全公平性（仅损失(1-ε)因子）。


<details>
  <summary>Details</summary>
Motivation: 在次模最大化问题中，随着算法在包含敏感属性（如性别、种族）的决策中应用日益广泛，确保公平性以避免偏见和歧视变得至关重要。现有最佳算法只能满足松弛版本的公平约束（损失因子2），特别是当ℓ=1时没有可证明的保证，这对应完美匹配约束的关键特例。

Method: 提出了一种新算法，对于任意常数ε>0，能够给出公平单调次模最大化的常数因子近似，在期望中仅损失(1-ε)因子的下界公平约束。算法实现了公平性与优化目标之间的新权衡。

Result: 在包括聚类、推荐和覆盖任务在内的标准真实世界数据集套件上进行了实证评估，证明了方法的实际有效性。算法能够任意接近完全公平性，相比之前损失因子2的方法有显著改进。

Conclusion: 这项工作在公平次模最大化领域取得了重要进展，通过新算法实现了公平性约束与优化目标之间更好的平衡，为实际应用中的公平决策提供了更有效的解决方案。

Abstract: Submodular maximization subject to matroid constraints is a central problem with many applications in machine learning. As algorithms are increasingly used in decision-making over datapoints with sensitive attributes such as gender or race, it is becoming crucial to enforce fairness to avoid bias and discrimination. Recent work has addressed the challenge of developing efficient approximation algorithms for fair matroid submodular maximization. However, the best algorithms known so far are only guaranteed to satisfy a relaxed version of the fairness constraints that loses a factor 2, i.e., the problem may ask for $\ell$ elements with a given attribute, but the algorithm is only guaranteed to find $\lfloor \ell/2 \rfloor$. In particular, there is no provable guarantee when $\ell=1$, which corresponds to a key special case of perfect matching constraints.
  In this work, we achieve a new trade-off via an algorithm that gets arbitrarily close to full fairness. Namely, for any constant $\varepsilon>0$, we give a constant-factor approximation to fair monotone matroid submodular maximization that in expectation loses only a factor $(1-\varepsilon)$ in the lower-bound fairness constraint. Our empirical evaluation on a standard suite of real-world datasets -- including clustering, recommendation, and coverage tasks -- demonstrates the practical effectiveness of our methods.

</details>


### [2] [Scalable Algorithms for Approximate DNF Model Counting](https://arxiv.org/abs/2601.10511)
*Paul Burkhardt,David G. Harris,Kevin T Schmitt*

Main category: cs.DS

TL;DR: 提出一种新的DNF公式模型计数蒙特卡洛方法，采用自适应停止规则和短路公式评估，在理论和实验上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DNF公式的模型计数在概率推理和网络可靠性等应用中至关重要，但由于精确计算是计算难解的，需要高效的近似算法。现有方法包括Karp-Luby-Madras经典算法、哈希方法和神经网络启发式方法，但仍有改进空间。

Method: 开发了一种新的蒙特卡洛方法，结合自适应停止规则和短路公式评估技术。该方法能够自动确定采样次数，并在满足精度要求时提前停止，同时通过短路评估减少计算开销。

Result: 理论证明该方法达到PAC学习界限，且渐近效率优于先前方法。实验表明，该方法比现有算法快几个数量级，能够扩展到数百万变量的大规模问题。

Conclusion: 提出的自适应蒙特卡洛方法在DNF模型计数问题上显著优于现有技术，为概率推理等应用提供了更高效的解决方案。

Abstract: Model counting of Disjunctive Normal Form (DNF) formulas is a critical problem in applications such as probabilistic inference and network reliability. For example, it is often used for query evaluation in probabilistic databases. Due to the computational intractability of exact DNF counting, there has been a line of research into a variety of approximation algorithms. These include Monte Carlo approaches such as the classical algorithms of Karp, Luby, and Madras (1989), as well as methods based on hashing (Soos et al. 2023), and heuristic approximations based on Neural Nets (Abboud, Ceylan, and Lukasiewicz 2020).
  We develop a new Monte Carlo approach with an adaptive stopping rule and short-circuit formula evaluation. We prove it achieves Probably Approximately Correct (PAC) learning bounds and is asymptotically more efficient than the previous methods. We also show experimentally that it out-performs prior algorithms by orders of magnitude, and can scale to much larger problems with millions of variables.

</details>


### [3] [UFO Trees: Practical and Provably-Efficient Parallel Batch-Dynamic Trees](https://arxiv.org/abs/2601.10706)
*Quinten De Man,Atharva Sharma,Kishen N Gowda,Laxman Dhulipala*

Main category: cs.DS

TL;DR: UFO trees：一种新的并行批量动态树数据结构，支持广泛查询功能，工作高效的并行批量动态更新，在顺序运行时与link-cut树竞争，并在低直径树上实现亚对数时间操作。


<details>
  <summary>Details</summary>
Motivation: 尽管link-cut树已有40年历史且仍是顺序最快的动态树数据结构，但它不支持并行批量动态更新，且查询功能有限。需要一种既能支持广泛查询，又能高效并行处理批量更新，同时保持顺序性能竞争力的新数据结构。

Method: 设计UFO树（并行批量动态树数据结构），支持多种查询功能，实现工作高效的并行批量动态更新。通过理论分析证明其在低直径树上的亚对数时间性能，并开发优化的C++实现进行实验验证。

Result: 实验评估了UFO树与10种其他动态树实现（包括几种新实现），在合成和真实世界不同直径和大小的树上进行广泛基准测试。结果显示，在顺序和并行设置下，UFO树是支持广泛查询的最快动态树数据结构，空间占用低，可扩展到十亿规模输入。

Conclusion: UFO树在支持广泛查询功能的同时，实现了工作高效的并行批量动态更新，并在顺序和并行性能上都表现出色。其低空间占用和可扩展性使其成为实践中实现更复杂动态图算法的有前景构建模块。

Abstract: The dynamic trees problem is to maintain a tree under edge updates while supporting queries like connectivity queries or path queries. Despite the first data structure for this fundamental problem -- the link-cut tree -- being invented 40 years ago, our experiments reveal that they are still the fastest sequential data structure for the problem. However, link-cut trees cannot support parallel batch-dynamic updates and have limitations on the kinds of queries they support.
  In this paper, we design a new parallel batch-dynamic trees data structure called UFO trees that simultaneously supports a wide range of query functionality, supports work-efficient parallel batch-dynamic updates, and is competitive with link-cut trees when run sequentially. We prove that a key reason for the strong practical performance of both link-cut trees and UFO trees is that they can perform updates and queries in sub-logarithmic time for low-diameter trees. We perform an experimental study of our optimized C++ implementations of UFO trees with ten other dynamic tree implementations, several of which are new, in a broad benchmark of both synthetic and real-world trees of varying diameter and size. Our results show that, in both sequential and parallel settings, UFO trees are the fastest dynamic tree data structure that supports a wide range of queries. Our new implementation of UFO trees has low space usage and easily scales to billion-size inputs, making it a promising building block for implementing more complex dynamic graph algorithms in practice.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [4] [High signal-to-noise ratio asymptotics of entropy-constrained Gaussian channel capacity](https://arxiv.org/abs/2601.09864)
*Adway Girish,Shlomo Shamai,Emre Telatar*

Main category: cs.IT

TL;DR: 研究高斯信道在渐近高信噪比下的输入熵约束容量问题，发现最优分布是离散高斯分布，且熵与容量差随SNR指数衰减。


<details>
  <summary>Details</summary>
Motivation: 研究高斯信道在输入熵约束下的容量问题，特别是在高信噪比渐近区域，探索最优输入分布的特性。

Method: 采用渐近分析方法，在高信噪比极限下分析高斯信道容量，推导最优输入分布形式。

Result: 证明当信噪比趋于无穷时，容量达到分布是支撑在缩放整数格上的离散高斯分布，且输入熵与容量之间的差距随信噪比指数衰减到零。

Conclusion: 在高信噪比渐近区域，高斯信道的最优输入分布具有离散结构，且熵容量差呈指数衰减特性，为相关通信系统设计提供了理论指导。

Abstract: We study the input-entropy-constrained Gaussian channel capacity problem in the asymptotic high signal-to-noise ratio (SNR) regime. We show that the capacity-achieving distribution as SNR goes to infinity is given by a discrete Gaussian distribution supported on a scaled integer lattice. Further, we show that the gap between the input entropy and the capacity decreases to zero exponentially in SNR, and characterize this exponent.

</details>


### [5] [One-Cold Poisson Channel: A Simple Continuous-Time Channel with Zero Dispersion](https://arxiv.org/abs/2601.09894)
*Cheuk Ting Li*

Main category: cs.IT

TL;DR: 论文提出了一种新型通信信道——单冷泊松信道（OCPC），其中发射机每次选择衰减一个频带。完美OCPC具有容量1、零信道色散和退化的信息谱分布，是唯一已知具有闭式最优非渐近错误概率的非平凡无记忆信道。


<details>
  <summary>Details</summary>
Motivation: 研究动机是寻找一种极其简单且具有闭式最优性能的通信信道模型，可用于光学通信中的可调谐带阻滤波器应用，并作为信息的基本单位（替代比特），因为比特不是无限可分的。

Method: 引入单冷泊松信道模型，发射机在多个频带中选择一个进行衰减。研究完美OCPC（频带数量无限）的特性和一般OCPC的非渐近编码与信道仿真结果。

Result: 完美OCPC具有容量1、零信道色散，信息谱分布为在1处的退化分布。这是唯一已知具有闭式最优非渐近错误概率的非平凡无记忆信道。完美反馈OCPC可推广前缀码。

Conclusion: OCPC是一种极其简单的连续时间无记忆信道，可作为信息的基本货币单位（替代比特），具有无限可分性。其简单性和闭式最优性能使其在理论分析和实际应用中都具有重要价值。

Abstract: We introduce the one-cold Poisson channel (OCPC), where the transmitter chooses one of several frequency bands to attenuate at a time. In particular, the perfect OCPC, where the number of bands is unlimited, is an extremely simple continuous-time memoryless channel. It has a capacity 1, zero channel dispersion, and an information spectrum being the degenerate distribution at 1. It is the only known nontrivial (discrete or continuous-time) memoryless channel with a closed-form formula for its optimal non-asymptotic error probability, making it the simplest channel in this sense. A potential application is optical communication with a tunable band rejection filter. Due to its simplicity, we may use it as a basic currency of information that is infinitely divisible, as an alternative to bits which are not infinitely divisible. OCPC with perfect feedback gives a generalization of prefix codes. We also study non-asymptotic coding and channel simulation results for the general OCPC.

</details>


### [6] [Learning-Augmented Perfectly Secure Collaborative Matrix Multiplication](https://arxiv.org/abs/2601.09916)
*Zixuan He,Mohammad Reza Deylam Salehi,Derya Malak,Photios A. Stavrou*

Main category: cs.IT

TL;DR: 提出了一种完美安全的矩阵乘法协议，用于多方计算A⊤B，具有信息论隐私保证和最优恢复阈值，并引入了学习增强扩展以提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 在多方计算中实现矩阵乘法时，需要在保证完美安全性和隐私性的同时，满足本地存储约束并提高计算效率，特别是处理大规模矩阵时。

Method: 使用稀疏掩码多项式编码子矩阵，结合系数对齐和Beaver式随机性确保完美保密；引入学习增强扩展，集成基于张量分解的本地块乘法，支持经典和学习的低秩方法。

Result: 协议在阈值下有界半诚实合谋攻击下保证正确性和信息论隐私，任何低于安全阈值的合谋方只能观察到均匀随机份额；恢复阈值达到最优，匹配现有信息论极限；学习增强版本在保持隐私和恢复保证的同时，随着矩阵维度增长可提供高达80%的计算效率提升。

Conclusion: 该工作提出了一个完美安全的矩阵乘法协议框架，实现了最优恢复阈值和信息论隐私，并通过学习增强扩展显著提升了大规模矩阵计算的可扩展性，为安全多方计算中的矩阵运算提供了高效解决方案。

Abstract: This paper presents a perfectly secure matrix multiplication (PSMM) protocol for multiparty computation (MPC) of $\mathrm{A}^{\top}\mathrm{B}$ over finite fields. The proposed scheme guarantees correctness and information-theoretic privacy against threshold-bounded, semi-honest colluding agents, under explicit local storage constraints. Our scheme encodes submatrices as evaluations of sparse masking polynomials and combines coefficient alignment with Beaver-style randomness to ensure perfect secrecy. We demonstrate that any colluding set of parties below the security threshold observes uniformly random shares, and that the recovery threshold is optimal, matching existing information-theoretic limits. Building on this framework, we introduce a learning-augmented extension that integrates tensor-decomposition-based local block multiplication, capturing both classical and learned low-rank methods. We demonstrate that the proposed learning-based PSMM preserves privacy and recovery guarantees for MPC, while providing scalable computational efficiency gains (up to $80\%$) as the matrix dimensions grow.

</details>


### [7] [Reconstructing Reed-Solomon Codes from Multiple Noisy Channel Outputs](https://arxiv.org/abs/2601.09947)
*Shubhransh Singhvi,Han Mao Kiah,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 研究RS码在q元DMS替代信道下的高效序列重构问题，提出基于Koetter-Vardy软判决解码的算法，推导出明确的速率阈值


<details>
  <summary>Details</summary>
Motivation: Levenshtein于2001年提出的序列重构问题考虑发送方传输码字，接收方观察到K个独立噪声版本。本研究旨在解决当每个输出被q元离散无记忆对称替代信道污染时的高效重构问题

Method: 针对Reed-Solomon码，将Koetter-Vardy软判决解码算法适配为高效重构算法

Result: 对于足够大的分组长度和字母表大小，推导出仅依赖于(p, K)的显式速率阈值，当码率R低于该阈值时，能以任意小错误概率重构传输码字

Conclusion: 成功开发了RS码在DMS替代信道下的高效序列重构算法，并建立了理论性能保证

Abstract: The sequence reconstruction problem, introduced by Levenshtein in 2001, considers a communication setting in which a sender transmits a codeword and the receiver observes K independent noisy versions of this codeword. In this work, we study the problem of efficient reconstruction when each of the $K$ outputs is corrupted by a $q$-ary discrete memoryless symmetric (DMS) substitution channel with substitution probability $p$. Focusing on Reed-Solomon (RS) codes, we adapt the Koetter-Vardy soft-decision decoding algorithm to obtain an efficient reconstruction algorithm. For sufficiently large blocklength and alphabet size, we derive an explicit rate threshold, depending only on $(p, K)$, such that the transmitted codeword can be reconstructed with arbitrarily small probability of error whenever the code rate $R$ lies below this threshold.

</details>


### [8] [Private Information Retrieval for Graph-based Replication with Minimal Subpacketization](https://arxiv.org/abs/2601.09957)
*Vayur Shanbhag,Prasad Krishnan*

Main category: cs.IT

TL;DR: 设计新的最小子分组化方案，用于基于图复制数据库的信息论私有信息检索，针对星形图和一般图实现了单位子分组化（最小化），在某些图类中达到比现有方案更高的速率。


<details>
  <summary>Details</summary>
Motivation: 在基于图复制的私有信息检索系统中，需要在保持高检索速率的同时降低子分组化程度，因为子分组化限制了协议执行时的文件大小。现有方案在子分组化方面存在不足，需要设计更优的方案。

Method: 提出了两种新方案：1) 针对星形图的特殊方案；2) 针对一般图的方案，通过独立集分解图结构。还扩展到多重图情况，针对完全多重图实现了更高速率。

Result: 实现了单位子分组化（最小化），星形图方案在一般星形图上比现有低子分组化方案有更好的速率。一般图方案虽然在完全图上速率低于先前方案，但在某些特定图类中能达到更高速率。多重图扩展在完全多重图上实现了比先前方案更高的速率。

Conclusion: 成功设计了最小子分组化的私有信息检索方案，针对不同图结构优化了速率性能，为基于图复制的PIR系统提供了更实用的解决方案。

Abstract: We design new minimal-subpacketization schemes for information-theoretic private information retrieval on graph-based replicated databases. In graph-based replication, the system consists of $K$ files replicated across $N$ servers according to a graph with $N$ vertices and $K$ edges. The client wants to retrieve one desired file, while keeping the index of the desired file private from each server via a query-response protocol. We seek PIR protocols that have (a) high rate, which is the ratio of the file-size to the total download cost, and (b) low subpacketization, which acts as a constraint on the size of the files for executing the protocol. We report two new schemes which have unit-subpacketization (which is minimal): (i) for a special class of graphs known as star graphs, and (ii) for general graphs. Our star-graph scheme has a better rate than previously known schemes with low subpacketization for general star graphs. Our scheme for general graphs uses a decomposition of the graph via independent sets. This scheme achieves a rate lower than prior schemes for the complete graph, however it can achieve higher rates than known for some specific graph classes. An extension of our scheme to the case of multigraphs achieves a higher rate than previous schemes for the complete multi-graph.

</details>


### [9] [On the Leaky Private Information Retrieval with Side Information](https://arxiv.org/abs/2601.09960)
*Yingying Huangfu,Tian Bai*

Main category: cs.IT

TL;DR: 本文研究带侧信息的泄露隐私私有信息检索问题，通过放宽完美隐私要求来提升通信效率，建立统一的概率框架量化隐私泄露，并刻画了下载成本与隐私泄露的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有PIR-SI研究主要关注完美隐私保护，但在实际应用中完全隐私可能导致通信效率低下。本文旨在探索在可控信息泄露条件下，如何利用侧信息提升检索效率，填补隐私泄露与侧信息结合的研究空白。

Method: 提出统一的概率框架构建L-PIR-SI方案，使用参数ε量化隐私泄露（符合差分隐私标准），分析不同隐私保护级别下的下载成本，建立泄露、侧信息和检索效率之间的权衡模型。

Result: 刻画了L-PIR-SI的可实现下载成本，证明结果可推广到多个PIR经典结论：当ε→0时恢复PIR-SI容量，无侧信息时退化为已知的leaky-PIR边界，首次揭示了泄露、侧信息和检索效率之间的权衡关系。

Conclusion: 本文首次系统研究带侧信息的泄露隐私PIR问题，建立了统一的量化框架，为实际应用中隐私保护与通信效率的平衡提供了理论指导，扩展了PIR研究领域。

Abstract: This paper investigates the problem of leaky-private Private Information Retrieval with Side Information (L-PIR-SI), which relaxes the requirement of perfect privacy to achieve improved communication efficiency in the presence of side information. While the capacities of PIR-SI under both $W$-privacy and $(W,S)$-privacy have been partially explored, the impact of controlled information leakage in these settings remains unaddressed. We propose a unified probabilistic framework to construct L-PIR-SI schemes where the privacy leakage is quantified by a parameter $\varepsilon$, consistent with differential privacy standards. We characterize the achievable download costs and show that our results generalize several landmark results in the PIR literature: they recover the capacity of PIR-SI when $\varepsilon \to 0$, and reduce to the known bounds for leaky-PIR when side information is absent. This work provides the first look at the trade-offs between leakage, side information, and retrieval efficiency.

</details>


### [10] [Fundamental Limits of Coded Polynomial Aggregation](https://arxiv.org/abs/2601.10028)
*Xi Zhong,Jörg Kliewer,Mingyue Ji*

Main category: cs.IT

TL;DR: 将编码多项式聚合（CPA）扩展到含掉队者的分布式计算系统，提出基于预定义非掉队者模式的CPA框架，建立了精确恢复的充要条件，并证明了交集大小阈值的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统基于个体解码的多项式编码计算需要较多工作节点响应，而CPA可以直接恢复加权聚合而无需单独解码每个项。本文旨在将CPA扩展到含掉队者的分布式系统，减少所需工作节点响应数量。

Method: 提出含掉队者感知的CPA框架，采用预定义非掉队者模式，仅要求对给定可接受非掉队者集合进行精确恢复。通过分析非掉队者模式的交集结构，建立精确恢复的充要条件，并识别交集大小阈值。

Result: 证明精确恢复所需工作节点响应少于基于个体解码的多项式编码计算；建立了交集结构对可行性的根本性表征；识别了保证精确恢复的交集大小阈值；当可接受非掉队者集合足够多时，该阈值成为充要条件；提供了可行的CPA方案构造方法。

Conclusion: 含掉队者感知的CPA框架能够显著减少分布式计算中的工作节点响应需求，通过交集大小阈值可以精确判断可行性，仿真验证了阈值在实践中的紧致性，为分布式计算系统设计提供了理论指导。

Abstract: Coded polynomial aggregation (CPA) enables the master to directly recover a weighted aggregation of polynomial evaluations without individually decoding each term, thereby reducing the number of required worker responses. In this paper, we extend CPA to straggler-aware distributed computing systems and introduce a straggler-aware CPA framework with pre-specified non-straggler patterns, where exact recovery is required only for a given collection of admissible non-straggler sets. Our main result shows that exact recovery of the desired aggregation is achievable with fewer worker responses than required by polynomial coded computing based on individual decoding, and that feasibility is fundamentally characterized by the intersection structure of the non-straggler patterns. In particular, we establish necessary and sufficient conditions for exact recovery in straggler-aware CPA and identify an intersection-size threshold that is sufficient to guarantee exact recovery. We further prove that this threshold becomes both necessary and sufficient when the number of admissible non-straggler sets is sufficiently large. We also provide an explicit construction of feasible CPA schemes whenever the intersection size exceeds the derived threshold. Finally, simulations reveal a sharp feasibility transition at the predicted threshold, providing empirical evidence that the bound is tight in practice.

</details>


### [11] [Optimal Proximity Gap for Folded Reed--Solomon Codes via Subspace Designs](https://arxiv.org/abs/2601.10047)
*Fernando Granha Jeronimo,Lenny Liu,Pranav Rajpal*

Main category: cs.IT

TL;DR: 该论文研究了折叠里德-所罗门码的邻近间隙性质，证明了其在最优容量范围内存在类似RS码的(δ,ε)-邻近间隙。


<details>
  <summary>Details</summary>
Motivation: Ben-Sasson等人的开创性工作表明仿射子空间集合对于RS码存在(δ,ε)-邻近间隙，但只适用于Johnson界范围。FRS码能达到最优列表解码半径δ（容量区域），因此需要研究FRS码是否在最优容量范围内也存在类似的邻近间隙性质。

Method: 采用类似Ben-Sasson等人的技术框架，但针对FRS码的特性进行扩展。该框架自然适用于合适的子空间设计码，利用FRS码的列表解码算法和结构特性。

Result: 论文肯定地回答了研究问题：FRS码在最优容量范围内确实存在(δ,ε)-邻近间隙性质。这一结果扩展了RS码的邻近间隙理论到更优的解码性能范围。

Conclusion: FRS码不仅具有最优列表解码能力，还继承了RS码的邻近间隙性质，这为理解FRS码与随机线性码的相似性提供了理论基础，并扩展了子空间设计码的分析框架。

Abstract: A collection of sets satisfies a $(δ,\varepsilon)$-proximity gap with respect to some property if for every set in the collection, either (i) all members of the set are $δ$-close to the property in (relative) Hamming distance, or (ii) only a small $\varepsilon$-fraction of members are $δ$-close to the property.
  In a seminal work, Ben-Sasson \textit{et al.}\ showed that the collection of affine subspaces exhibits a $(δ,\varepsilon)$-proximity gap with respect to the property of being Reed--Solomon (RS) codewords with $δ$ up to the so-called Johnson bound for list decoding. Their technique relies on the Guruswami--Sudan list decoding algorithm for RS codes, which is guaranteed to work in the Johnson bound regime.
  Folded Reed--Solomon (FRS) codes are known to achieve the optimal list decoding radius $δ$, a regime known as capacity. Moreover, a rich line of list decoding algorithms was developed for FRS codes. It is then natural to ask if FRS codes can be shown to exhibit an analogous $(δ,\varepsilon)$-proximity gap, but up to the so-called optimal capacity regime. We answer this question in the affirmative (and the framework naturally applies more generally to suitable subspace-design codes).
  An additional motivation to understand proximity gaps for FRS codes is the recent results [BCDZ'25] showing that they exhibit properties similar to random linear codes, which were previously shown to be related to properties of RS codes with random evaluation points in [LMS'25], as well as codes over constant-size alphabet based on AEL [JS'25].

</details>


### [12] [Function Correcting Codes for Maximally-Unbalanced Boolean Functions](https://arxiv.org/abs/2601.10135)
*Rajlaxmi Pandey,Shiven Bajpai,Anjana A Mahesh,B. Sundar Rajan*

Main category: cs.IT

TL;DR: 研究针对最大不平衡布尔函数的最优单错误纠正函数纠正码，分析其距离矩阵结构对AWGN信道中误码性能的影响。


<details>
  <summary>Details</summary>
Motivation: 函数纠正码允许在噪声信道中可靠计算函数而不需要完全恢复消息，但不同结构的FCC在错误性能上可能存在显著差异，需要系统分析。

Method: 通过关联码字距离矩阵分析最优SEFCC结构，识别不同FCC类别，在AWGN信道上使用软判决和硬判决解码评估代表性FCC的错误性能。

Result: 不同距离矩阵结构的FCC在数据BER和函数错误行为上表现出显著差异，且码结构的影响强烈依赖于解码策略。

Conclusion: FCC的结构特性对其错误性能有重要影响，解码策略选择应与码结构特性相匹配，这为设计高效函数计算系统提供了指导。

Abstract: Function-Correcting Codes (FCCs) enable reliable computation of a function of a $k$-bit message over noisy channels without requiring full message recovery. In this work, we study optimal single-error correcting FCCs (SEFCCs) for maximally-unbalanced Boolean functions, where $k$ denotes the message length and $t$ denotes the error-correction capability. We analyze the structure of optimal SEFCC constructions through their associated codeword distance matrices and identify distinct FCC classes based on this structure. We then examine the impact of these structural differences on error performance by evaluating representative FCCs over the additive white Gaussian noise (AWGN) channel using both soft-decision and hard-decision decoding. The results show that FCCs with different distance-matrix structures can exhibit markedly different Data BER and function error behavior, and that the influence of code structure depends strongly on the decoding strategy.

</details>


### [13] [On Existence of Girth-8 QC-LDPC Code with Large Column Weight: Combining Mirror-sequence with Classification Modulo Ten](https://arxiv.org/abs/2601.10170)
*Guohua Zhang,Xiangya Liu,Jianhua Zhang,Yi Fang*

Main category: cs.IT

TL;DR: 本文在GCD框架下，通过引入镜像序列和新的行重组方案，代数构造了列重为7和8、长度极短、围长为8的QC-LDPC码，将连续循环尺寸的下界提高了约20%。


<details>
  <summary>Details</summary>
Motivation: 具有大围长的准循环LDPC码在信道编码、压缩感知和分布式存储等多个领域至关重要。主要挑战是如何用代数方法（而非搜索方法）构造出具有最短可能长度（或最小循环尺寸）的此类码。

Method: 在先前提出的最大公约数(GCD)框架下，引入镜像序列概念并采用新的行重组方案，以代数方式构造列重为7和8、任意行重、围长为8的QC-LDPC码。

Result: 对于列重7和8，连续循环尺寸的下界相比现有基准均提高了约20%。新构造还能提供比新下界小约25%的循环尺寸。

Conclusion: 通过创新的代数方法，成功构造了列重更高、长度更短的QC-LDPC码，显著改进了循环尺寸的下界，为实际应用提供了更优的码构造方案。

Abstract: Quasi-cyclic (QC) LDPC codes with large girths play a crucial role in several research and application fields, including channel coding, compressed sensing and distributed storage systems. A major challenge in respect of the code construction is how to obtain such codes with the shortest possible length (or equivalently, the smallest possible circulant size) using algebraic methods instead of search methods. The greatest-common-divisor (GCD) framework we previously proposed has algebraically constructed QC-LDPC codes with column weights of 5 and 6, very short lengths, and a girth of 8. By introducing the concept of a mirror sequence and adopting a new row-regrouping scheme, QC-LDPC codes with column weights of 7 and 8, very short lengths, and a girth of 8 are proposed for arbitrary row weights in this article via an algebraic manner under the GCD framework. Thanks to these novel algebraic methods, the lower bounds (for column weights 7 and 8) on consecutive circulant sizes are both improved by asymptotically about 20%, compared with the existing benchmarks. Furthermore, these new constructions can also offer circulant sizes asymptotically about 25% smaller than the novel bounds.

</details>


### [14] [A Low-Complexity Architecture for Multi-access Coded Caching Systems with Arbitrary User-cache Access Topology](https://arxiv.org/abs/2601.10175)
*Ting Yang,Minquan Cheng,Xinping Yi,Robert Caiming Qiu,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出基于图神经网络的通用学习框架，解决任意用户-缓存访问拓扑下的多接入编码缓存问题，在保持接近最优传输负载的同时大幅降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有MACC模型依赖高度结构化的连接拓扑，无法处理任意用户-缓存访问关系。需要设计适用于任意拓扑的通用低复杂度传输方案，解决传统图着色算法在大规模图中的计算瓶颈。

Method: 1) 建立基于冲突图的通用框架，将传输设计转化为图着色问题；2) 扩展任意拓扑下的索引编码下界；3) 提出基于图神经网络的学习框架，高效构建近最优编码多播传输；4) 设计低复杂度贪婪近似算法。

Result: 学习方案在传输负载上接近DSatur算法和理论下界，同时显著降低计算时间。方案具有良好的泛化能力，适用于不同访问拓扑和用户数量。

Conclusion: 提出的图神经网络框架为任意拓扑MACC问题提供了高效解决方案，在保持接近最优性能的同时解决了大规模系统中的计算复杂度问题，具有实际应用价值。

Abstract: This paper studies the multi-access coded caching (MACC) problem under arbitrary user-cache access topologies, extending existing models that rely on highly structured and combinatorially designed connectivity. We consider a MACC system consisting of a single server, multiple cache nodes, and multiple user nodes. Each user can access an arbitrary subset of cache nodes to retrieve cached content. The objective is to design a general and low-complexity delivery scheme under fixed cache placement for arbitrary access topologies. We propose a universal graph-based framework for modeling the MACC delivery problem, where decoding conflicts among requested packets are captured by a conflict graph and the delivery design is reduced to a graph coloring problem. In this formulation, a lower transmission load corresponds to using fewer colors. The classical greedy coloring algorithm DSatur achieves a transmission load close to the index-coding converse bound, providing a tight benchmark, but its computational complexity becomes prohibitive for large-scale graphs. To overcome this limitation, we develop a learning-based framework using graph neural networks that efficiently constructs near-optimal coded multicast transmissions and generalizes across diverse access topologies and varying numbers of users. In addition, we extend the index-coding converse bound for uncoded cache placement to arbitrary access topologies and propose a low-complexity greedy approximation. Numerical results demonstrate that the proposed learning-based scheme achieves transmission loads close to those of DSatur and the converse bound while significantly reducing computational time.

</details>


### [15] [Error-Correcting Codes for the Sum Channel](https://arxiv.org/abs/2601.10256)
*Lyan Abboud,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 提出一种新的信道模型——和信道，用于分布式存储和DNA数据存储，构建了能纠正两个删除的编码方案，冗余度接近最优。


<details>
  <summary>Details</summary>
Motivation: 受分布式存储和DNA数据存储应用启发，需要处理二进制矩阵数据，其中最后一行是前ℓ行的奇偶校验和，需要设计能纠正删除和替换错误的编码方案。

Method: 引入和信道模型，构建了能纠正两个删除的编码，冗余度为2⌈log₂log₂n⌉ + O(ℓ²)；当ℓ=2时，证明冗余度上界为⌈log₂log₂n⌉ + O(1)；还提出了纠正单个替换的编码，冗余度为⌈log₂(ℓ+1)⌉。

Result: 两删除纠正编码的冗余度接近最优（相差因子2以内）；单替换纠正编码的冗余度在最优值1比特以内；当ℓ=2时，证明了冗余度上界，显示所提方案接近最优。

Conclusion: 和信道模型在分布式存储和DNA存储中有应用价值，提出的编码方案在冗余度方面接近理论最优，为相关存储系统提供了有效的错误纠正方案。

Abstract: We introduce the sum channel, a new channel model motivated by applications in distributed storage and DNA data storage. In the error-free case, it takes as input an $\ell$-row binary matrix and outputs an $(\ell+1)$-row matrix whose first $\ell$ rows equal the input and whose last row is their parity (sum) row. We construct a two-deletion-correcting code with redundancy $2\lceil\log_2\log_2 n\rceil + O(\ell^2)$ for $\ell$-row inputs. When $\ell=2$, we establish an upper bound of $\lceil\log_2\log_2 n\rceil + O(1)$, implying that our redundancy is optimal up to a factor of 2. We also present a code correcting a single substitution with $\lceil \log_2(\ell+1)\rceil$ redundant bits and prove that it is within one bit of optimality.

</details>


### [16] [Transmission Mask Analysis for Range-Doppler Sensing in Half-Duplex ISAC](https://arxiv.org/abs/2601.10259)
*Dikai Liu,Yifeng Xiong,Marco Lops,Fan Liu,Jianhua Zhang*

Main category: cs.IT

TL;DR: 分析MASM在ISAC中的周期性传输掩码，推导出闭式期望距离-多普勒响应，证明距离旁瓣的多普勒不变性，并发现不同动态环境下掩码设计的最优性


<details>
  <summary>Details</summary>
Motivation: 研究半双工集成感知与通信中掩码调制（MASM）的周期性传输掩码，旨在理解其在距离-多普勒域的性能特性，为ISAC系统设计提供理论指导

Method: 分析周期性传输掩码，推导闭式期望距离-多普勒响应，研究距离旁瓣和主瓣特性，使用循环差集（特别是Singer CDS）进行优化设计

Result: 距离旁瓣具有多普勒不变性，将距离旁瓣最优性扩展到二维设置；对于距离主瓣，周期性掩码产生稀疏多普勒旁瓣：在中等动态环境下CDS是最小最大最优的，在高度动态环境下多普勒旁瓣能量是掩码自相关的凹函数

Conclusion: 周期性掩码设计在不同动态环境下表现出不同的最优特性，揭示了主瓣波动与多普勒旁瓣性能之间的不可避免的权衡关系，为ISAC系统掩码设计提供了理论基础

Abstract: In this paper, we analyze the periodic transmission masks for MASked Modulation (MASM) in half-duplex integrated sensing and communication (ISAC), and derive their closed-form expected range-Doppler response $\mathbb{E}\{r(k,l,ν)\}$. We show that range sidelobes ($k\neq l$) are Doppler-invariant, extending the range-sidelobe optimality to the 2-D setting. For the range mainlobe ($k=l$), periodic masking yields sparse Doppler sidelobes: Cyclic difference sets (CDSs) (in particular Singer CDSs) are minimax-optimal in a moderately dynamic regime, while in a highly dynamic regime the Doppler-sidelobe energy is a concave function of the mask autocorrelation, revealing an inevitable tradeoff with mainlobe fluctuation.

</details>


### [17] [Algebraic Properties of PAC Codes](https://arxiv.org/abs/2601.10262)
*Vlad-Florin Dragoi,Mohammad Rowshan*

Main category: cs.IT

TL;DR: 本文通过代数表示分析极化调整卷积码，定义了广义多项式极化码这一大类，包括PAC码和反向PAC码，并推导了其结构特性如对偶性、最小距离等


<details>
  <summary>Details</summary>
Motivation: 利用极化和Reed-Muller码的代数表示来深入分析极化调整卷积码的结构特性，扩展对PAC码及其变体的理论理解

Method: 采用代数方法定义广义多项式极化码类，包括PAC码和反向PAC码，然后推导其结构性质如对偶性、最小距离、最小重量码字数等

Result: 建立了广义多项式极化码的结构特性理论框架，包括对偶关系、最小距离性质，以及关于最小重量码字数量和单项式子码维度的结构限制

Conclusion: 通过代数方法成功分析了极化调整卷积码的结构特性，为PAC码及其变体提供了系统的理论分析框架，揭示了这类码的基本结构限制

Abstract: We analyze polarization-adjusted convolutional codes using the algebraic representation of polar and Reed-Muller codes. We define a large class of codes, called generalized polynomial polar codes which include PAC codes and Reverse PAC codes. We derive structural properties of generalized polynomial polar codes, such as duality, minimum distance. We also deduce some structural limits in terms of number of minimum weight codewords, and dimension of monomial sub-code.

</details>


### [18] [On the Capacity of Noisy Frequency-based Channels](https://arxiv.org/abs/2601.10329)
*Yuval Gerzon,Ilan Shomorony,Nir Weinberger*

Main category: cs.IT

TL;DR: 该论文研究了基于频率的信道在噪声下的容量，针对DNA数据存储中的短分子机制，通过随机采样和噪声识别建立信道模型，推导了容量上下界并量化了识别噪声带来的信息损失。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于DNA数据存储中的短分子机制，其中信息编码在项目类型的频率而非顺序中。现有研究已解决了无噪声频率信道的容量问题，但识别噪声的影响尚未完全表征，需要建立噪声频率信道的容量分析框架。

Method: 方法包括：1）通过随机降解和数据处理不等式推导容量上界；2）采用多项式采样过程的泊松化，分析带符号间干扰的向量泊松信道，建立容量下界；3）改进Feinstein界中信息密度的集中不等式，显式表征识别噪声导致的互信息加性损失。

Result: 建立了噪声频率信道的容量上下界，显式量化了识别噪声带来的信息损失。将结果应用于DNA存储信道，量化了短分子机制下可靠存储比特总数缩放中的损失。

Conclusion: 该研究完整表征了噪声频率信道的容量特性，为DNA数据存储中的短分子机制提供了理论分析框架，明确了识别噪声对存储容量的具体影响。

Abstract: We investigate the capacity of noisy frequency-based channels, motivated by DNA data storage in the short-molecule regime, where information is encoded in the frequency of items types rather than their order. The channel output is a histogram formed by random sampling of items, followed by noisy item identification. While the capacity of the noiseless frequency-based channel has been previously addressed, the effect of identification noise has not been fully characterized. We present a converse bound on the channel capacity that follows from stochastic degradation and the data processing inequality. We then establish an achievable bound, which is based on a Poissonization of the multinomial sampling process, and an analysis of the resulting vector Poisson channel with inter-symbol interference. This analysis refines concentration inequalities for the information density used in Feinstein bound, and explicitly characterizes an additive loss in the mutual information due to identification noise. We apply our results to a DNA storage channel in the short-molecule regime, and quantify the resulting loss in the scaling of the total number of reliably stored bits.

</details>


### [19] [Convertible Codes for Data and Device Heterogeneity](https://arxiv.org/abs/2601.10341)
*Anina Gruica,Benjamin Jany,Stanislav Kruglik*

Main category: cs.IT

TL;DR: 本文研究分布式存储系统中的可转换编码，解决数据异构性和设备异构性问题，推导线性编码转换的读写成本下界，并针对Reed-Muller编码构建显式转换方案。


<details>
  <summary>Details</summary>
Motivation: 分布式存储系统面临两个关键挑战：数据异构性（由非均匀访问需求引起）和设备异构性（由节点可靠性随时间变化引起）。现有研究通常单独处理这两个问题，缺乏统一的解决方案。

Method: 1. 研究可转换编码，支持在合并机制中以最小成本将一种编码转换为另一种编码；2. 推导适用于任意线性编码的线性编码转换读写成本一般下界；3. 针对能高效处理数据异构性的Reed-Muller编码，构建显式转换程序。

Result: 1. 建立了线性编码转换的读写成本理论下界；2. 首次为Reed-Muller编码构建了显式转换方案，能够同时处理数据异构性和设备异构性；3. 实现了两种异构性在分布式数据存储中的统一处理。

Conclusion: 本文通过可转换编码理论框架，首次实现了对分布式存储系统中数据异构性和设备异构性的统一处理，为Reed-Muller编码提供了高效的显式转换方案，具有重要的理论和实践意义。

Abstract: Distributed storage systems must handle both data heterogeneity, arising from non-uniform access demands, and device heterogeneity, caused by time-varying node reliability. In this paper, we study convertible codes, which enable the transformation of one code into another with minimum cost in the merge regime, addressing the latter. We derive general lower bounds on the read and write costs of linear code conversion, applicable to arbitrary linear codes. We then focus on Reed-Muller codes, which efficiently handle data heterogeneity, addressing the former issue, and construct explicit conversion procedures that, for the first time, combine both forms of heterogeneity for distributed data storage.

</details>


### [20] [A New Construction Structure on MISO Coded Caching with Linear Subpacketization: Half-Sum Disjoint Packing](https://arxiv.org/abs/2601.10353)
*Bowen Zheng,Minquan Cheng,Kai Wan,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出基于L-half-sum disjoint packing结构的MISO编码缓存方案，在保持线性子分组(F=K)的同时实现高sum-DoF，显著降低子分组复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有MISO编码缓存方案在达到最大sum-DoF时，子分组数F随用户数K呈指数增长，导致实现复杂度高。需要设计在保持高sum-DoF的同时具有低子分组复杂度的方案。

Method: 基于拉丁方框架，将F=K的MAPDA设计转化为L-half-sum disjoint packing结构的构造问题。通过构建L-HSDP来生成新的MISO编码缓存方案。

Result: 提出的L-HSDP方案相比指数子分组方案显著降低了子分组复杂度，仅轻微牺牲sum-DoF；相比现有线性子分组方案，同时实现了更高的sum-DoF和更低的子分组数。

Conclusion: 通过L-HSDP结构成功设计了具有线性子分组(F=K)的MISO编码缓存方案，在子分组复杂度和sum-DoF之间取得了良好平衡，为实际系统实现提供了可行性。

Abstract: In the $(L,K,M,N)$ cache-aided multiple-input single-output (MISO) broadcast channel (BC) system, the server is equipped with $L$ antennas and communicates with $K$ single-antenna users through a wireless broadcast channel where the server has a library containing $N$ files, and each user is equipped with a cache of size $M$ files. Under the constraints of uncoded placement and one-shot linear delivery strategies, many schemes achieve the maximum sum Degree-of-Freedom (sum-DoF). However, for general parameters $L$, $M$, and $N$, their subpacketizations increase exponentially with the number of users. We aim to design a MISO coded caching scheme that achieves a large sum-DoF with low subpacketization $F$. An interesting combinatorial structure, called the multiple-antenna placement delivery array (MAPDA), can be used to generate MISO coded caching schemes under these two strategies; moreover, all existing schemes with these strategies can be represented by the corresponding MAPDAs. In this paper, we study the case with $F=K$ (i.e., $F$ grows linearly with $K$) by investigating MAPDAs. Specifically, based on the framework of Latin squares, we transform the design of MAPDA with $F=K$ into the construction of a combinatorial structure called the $L$-half-sum disjoint packing (HSDP). It is worth noting that a $1$-HSDP is exactly the concept of NHSDP, which is used to generate the shared-link coded caching scheme with $F=K$. By constructing $L$-HSDPs, we obtain a class of new schemes with $F=K$. Finally, theoretical and numerical analyses show that our $L$-HSDP schemes significantly reduce subpacketization compared to existing schemes with exponential subpacketization, while only slightly sacrificing sum-DoF, and achieve both a higher sum-DoF and lower subpacketization than the existing schemes with linear subpacketization.

</details>


### [21] [Generalized Weight Structure of Polar Codes: Selected Template Polynomials](https://arxiv.org/abs/2601.10362)
*Mohammad Rowshan,Vlad-Florin Dragoi*

Main category: cs.IT

TL;DR: 该论文提出了一个代数框架来计算极码（视为递减单项式码）的汉明重量，推导了关键结构模板的闭式表达式，并结合LTA群作用得到显式的多重性公式，用于表征和枚举码字。


<details>
  <summary>Details</summary>
Motivation: 极码可以视为递减单项式码，具有由下三角仿射（LTA）群支配的丰富代数结构。需要开发一个通用框架来计算由单项式之和生成的码字的汉明重量，并系统地分析低重量和中重量谱。

Method: 开发了一个通用框架：1）计算由单项式之和生成的码字的汉明重量；2）将这些重量表示为规范的二元形式；3）推导生成低重量和中重量谱的关键结构模板（不相交和、嵌套块、互补翻转）的闭式表达式；4）结合LTA群作用得到显式的多重性公式。

Result: 获得了统一的代数方法来表征和枚举码字，能够显式计算汉明重量，并为关键结构模板提供了闭式表达式，从而系统地分析了极码的重量谱。

Conclusion: 通过将极码视为递减单项式码并利用其LTA群结构，建立了一个统一的代数框架来计算汉明重量、推导结构模板并获得多重性公式，为极码的重量谱分析提供了系统的方法。

Abstract: Polar codes can be viewed as decreasing monomial codes, revealing a rich algebraic structure governed by the lower-triangular affine (LTA) group. We develop a general framework to compute the Hamming weight of codewords generated by sums of monomials, express these weights in a canonical dyadic form, and derive closed expressions for key structural templates (disjoint sums, nested blocks, complementary flips) that generate the low and intermediate weight spectrum. Combining these templates with the LTA group action, we obtain explicit multiplicity formulas, yielding a unified algebraic method to characterize and enumerate codewords.

</details>


### [22] [A Hybrid Reliability--Weight Framework for Construction of Polar Codes](https://arxiv.org/abs/2601.10376)
*Mohammad Rowshan,Vlad-Florin Dragoi*

Main category: cs.IT

TL;DR: 提出一种结合可靠性和权重的混合比特信道排序方法，通过最小化截断的SC/ML联合界代理来优化极化码构造，在短中长度下改善最小距离和重数。


<details>
  <summary>Details</summary>
Motivation: 传统极化码构造基于可靠性排序，能保证容量可达但短中长度下可能产生较差的最小权重谱。需要结合代数分析得到的比特信道对最小权重码字重数的贡献，设计更优的构造方法。

Method: 定义每个比特的成本函数，距离项来自最小权重码字的轨道枚举，并用Bhattacharyya型因子缩放。混合构造在递减单项式码类中最小化截断的SC/ML联合界代理。将混合度量与SCL解码中的错误事件通过剪枝/ML分解联系起来。

Result: 混合设计作为可靠性构造的局部扰动，其渐近影响随码长趋近无穷而消失。数值结果显示在BPSK-AWGN信道上，混合构造在最小距离、重数和联合界近似方面优于纯可靠性构造。

Conclusion: 混合比特信道排序方法有效平衡了可靠性和权重优化，在短中长度极化码设计中实现了更好的性能权衡，同时保持了渐近容量可达特性。

Abstract: Polar codes are usually constructed by ranking synthetic bit-channels according to reliability, which guarantees capacity-achieving behavior but can yield poor low-weight spectra at short and moderate lengths. Recent algebraic results express the contribution of individual bit-channels to the multiplicities of minimum and near-minimum weight codewords in closed form. In this work we combine these insights into a mixed (reliability--weight) bit-channel ordering. We define a per-bit cost whose distance term is derived from orbit enumeration of minimum-weight codewords and scaled by a Bhattacharyya-type factor, and show that the resulting mixed construction minimises a truncated SC/ML union-bound surrogate within a class of decreasing monomial codes. We relate the mixed metric to error events in SCL decoding via a pruning/ML decomposition, and prove that mixed designs act as local perturbations of reliability-based constructions whose asymptotic impact vanishes as code-length approaches infinity. Numerical results for short and moderate lengths on BPSK-AWGN, implemented via Gaussian approximation and closed-form weight contributions, illustrate the trade-off between pure reliability-based and mixed constructions in terms of minimum distance, multiplicity, and union-bound approximations. All proofs are deferred to the appendices.

</details>


### [23] [Codebook Design for Limited Feedback in Near-Field XL-MIMO Systems](https://arxiv.org/abs/2601.10391)
*Liujia Yao,Changsheng You,Zixuan Huang,Chao Zhou,Zhaohui Yang,Xiaoyang Li*

Main category: cs.IT

TL;DR: 提出针对XL-MIMO FDD系统的用户分布感知码本设计，通过联合优化角度-距离采样和比特分配提升反馈效率


<details>
  <summary>Details</summary>
Motivation: 现有XL-MIMO码本设计（如极域码本）未充分考虑实际用户分布，导致反馈开销过大。需要设计更高效的反馈码本以适应实际部署场景

Method: 1) 针对均匀分布用户场景，建立和速率最大化问题，联合优化角度-距离采样和比特分配；2) 使用Voronoi分割证明均匀角度采样最优；3) 推导接收功率下界，提出几何距离采样作为高质量次优解；4) 扩展到非均匀用户分布，采用交替采样方法；5) 理论分析反馈比特分配趋势

Result: 1) 均匀角度采样被证明为最优；2) 几何距离采样能最大化接收功率下界；3) 理论表明随着阵列增大，比特分配应偏向距离采样；4) 数值结果验证了所提码本在各种系统设置下的优越速率性能和鲁棒性，相比基准方案（包括广泛使用的极域码本）获得显著增益

Conclusion: 提出的用户分布感知码本设计能有效提升XL-MIMO FDD系统的反馈效率，通过联合优化角度-距离采样和比特分配，在实际部署中实现显著的性能增益

Abstract: In this paper, we study efficient codebook design for limited feedback in extremely large-scale multiple-input-multiple-output (XL-MIMO) frequency division duplexing (FDD) systems. It is worth noting that existing codebook designs for XL-MIMO, such as polar-domain codebook, have not well taken into account user (location) distribution in practice, thereby incurring excessive feedback overhead. To address this issue, we propose in this paper a novel and efficient feedback codebook tailored to user distribution. To this end, we first consider a typical scenario where users are uniformly distributed within a specific polar-region, based on which a sum-rate maximization problem is formulated to jointly optimize angle-range samples and bit allocation among angle/range feedback. This problem is challenging to solve due to the lack of a closed-form expression for the received power in terms of angle and range samples. By leveraging a Voronoi partitioning approach, we show that uniform angle sampling is optimal for received power maximization. For more challenging range sampling design, we obtain a tight lower-bound on the received power and show that geometric sampling, where the ratio between adjacent samples is constant, can maximize the lower bound and thus serves as a high-quality suboptimal solution. We then extend the proposed framework to accommodate more general non-uniform user distribution via an alternating sampling method. Furthermore, theoretical analysis reveals that as the array size increases, the optimal allocation of feedback bits increasingly favors range samples at the expense of angle samples. Finally, numerical results validate the superior rate performance and robustness of the proposed codebook design under various system setups, achieving significant gains over benchmark schemes, including the widely used polar-domain codebook.

</details>


### [24] [Multiaccess Coded Caching with Heterogeneous Retrieval Costs](https://arxiv.org/abs/2601.10394)
*Wenbo Huang,Minquan Cheng,Kai Wan,Xiaojun Li,Robert Caiming Qiu,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出一种基于叠加编码的成本感知多接入编码缓存框架，通过优化缓存放置来最小化包含缓存访问成本和广播成本的总系统成本。


<details>
  <summary>Details</summary>
Motivation: 现有MACC系统假设用户从连接的缓存节点检索内容没有通信成本，但实际中用户从不同缓存节点检索内容成本不同，服务器向用户传输内容也有成本。需要设计成本感知的系统来最小化总成本。

Method: 提出基于叠加编码的新型编码缓存框架，将Cheng等人的MACC方案分层；推导成本感知优化问题来优化缓存放置；利用最优解的稀疏性提出复杂度降低的结构感知算法。

Result: 仿真结果表明，在异构检索成本场景下，所提方案始终优于Cheng等人的方案。

Conclusion: 提出的成本感知MACC框架能有效降低系统总成本，特别是在异构检索成本场景下表现优异。

Abstract: The multiaccess coded caching (MACC) system, as formulated by Hachem {\it et al.}, consists of a central server with a library of $N$ files, connected to $K$ cache-less users via an error-free shared link, and $K$ cache nodes, each equipped with cache memory of size $M$ files. Each user can access $L$ neighboring cache nodes under a cyclic wrap-around topology. Most existing studies operate under the strong assumption that users can retrieve content from their connected cache nodes at no communication cost. In practice, each user retrieves content from its $L$ different connected cache nodes at varying costs. Additionally, the server also incurs certain costs to transmit the content to the users. In this paper, we focus on a cost-aware MACC system and aim to minimize the total system cost, which includes cache-access costs and broadcast costs. Firstly, we propose a novel coded caching framework based on superposition coding, where the MACC schemes of Cheng \textit{et al.} are layered. Then, a cost-aware optimization problem is derived that optimizes cache placement and minimizes system cost. By identifying a sparsity property of the optimal solution, we propose a structure-aware algorithm with reduced complexity. Simulation results demonstrate that our proposed scheme consistently outperforms the scheme of Cheng {\it et al.} in scenarios with heterogeneous retrieval costs.

</details>


### [25] [Placement Delivery Array for Cache-Aided MIMO Systems](https://arxiv.org/abs/2601.10422)
*Yifei Huang,Kai Wan,Minquan Cheng,Jinyan Wang,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出MIMO-PDA统一结构，实现最大sum-DoF和低子分组化，构建两种方案：线性子分组化（严格约束）和指数级子分组化（宽松约束）


<details>
  <summary>Details</summary>
Motivation: 在缓存辅助的MIMO网络中，需要同时实现最大sum-DoF和低子分组化，现有方案难以平衡这两个目标

Method: 引入MIMO-PDA统一组合结构，分析其性质推导sum-DoF上界，构建两种MIMO-PDA方案：线性子分组化方案（严格参数约束）和有序指数子分组化方案（宽松约束）

Result: 推导出sum-DoF上界min{KG, Gt+G⌈L/G⌉}，与最优DoF一致；第二种方案在保持最大sum-DoF的同时，相比现有方案指数级降低子分组化

Conclusion: MIMO-PDA框架有效平衡了sum-DoF和子分组化的权衡，提出的两种构造方案在不同约束条件下都能实现最优性能

Abstract: We consider a $(G,L,K,M,N)$ cache-aided multiple-input multiple-output (MIMO) network, where a server equipped with $L$ antennas and a library of $N$ equal-size files communicates with $K$ users, each equipped with $G$ antennas and a cache of size $M$ files, over a wireless interference channel. Each user requests an arbitrary file from the library. The goal is to design coded caching schemes that simultaneously achieve the maximum sum degrees of freedom (sum-DoF) and low subpacketization. In this paper, we first introduce a unified combinatorial structure, termed the MIMO placement delivery array (MIMO-PDA), which characterizes uncoded placement and one-shot zero-forcing delivery. By analyzing the combinatorial properties of MIMO-PDAs, we derive a sum-DoF upper bound of $\min\{KG, Gt+G\lceil L/G \rceil\}$, where $t=KM/N$, which coincides with the optimal DoF characterization in prior work by Tehrani \emph{et al.}. Based on this upper bound, we present two novel constructions of MIMO-PDAs that achieve the maximum sum-DoF. The first construction achieves linear subpacketization under stringent parameter constraints, while the second achieves ordered exponential subpacketization under substantially milder constraints. Theoretical analysis and numerical comparisons demonstrate that the second construction exponentially reduces subpacketization compared to existing schemes while preserving the maximum sum-DoF.

</details>


### [26] [Energy-Efficient Probabilistic Semantic Communication Over Visible Light Networks With Rate Splitting](https://arxiv.org/abs/2601.10452)
*Zhouxiang Zhao,Zhaohui Yang,Mingzhe Chen,Chen Zhu,Xin Tong,Zhaoyang Zhang*

Main category: cs.IT

TL;DR: 该论文研究了在资源受限的可见光通信系统中，基于概率语义通信的能量效率最大化问题，通过联合优化波束成形、直流偏置、速率分配和语义压缩比来提高系统性能。


<details>
  <summary>Details</summary>
Motivation: 可见光通信作为未来无线通信的关键技术，具有独特的物理层优势，但其与语义通信等高层技术的结合尚未充分探索。在资源受限的VLC系统中，语义压缩会带来额外的计算开销，同时需要保持知识库同步，这带来了能量效率优化的挑战。

Method: 提出基于概率语义通信的VLC系统模型，使用概率图表示知识库，采用速率分割多址接入同时传输知识和信息数据。开发了基于逐次凸逼近和Dinkelbach方法的交替优化算法，联合优化发射波束成形、直流偏置、公共速率分配和语义压缩比。

Result: 仿真结果表明所提出的方法有效提高了系统的能量效率，通过联合优化通信和计算成本，实现了在资源受限VLC系统中的高效语义通信。

Conclusion: 该研究成功解决了VLC系统中概率语义通信的能量效率优化问题，提出的联合优化框架和算法为未来可见光通信与语义通信的融合提供了有效解决方案。

Abstract: Visible light communication (VLC) is emerging as a key technology for future wireless communication systems due to its unique physical-layer advantages over traditional radio-frequency (RF)-based systems. However, its integration with higher-layer techniques, such as semantic communication, remains underexplored. This paper investigates the energy efficiency maximization problem in a resource-constrained VLC-based probabilistic semantic communication (PSCom) system. In the considered model, light-emitting diode (LED) transmitters perform semantic compression to reduce data size, which incurs additional computation overhead. The compressed semantic information is transmitted to the users for semantic inference using a shared knowledge base that requires periodic updates to ensure synchronization. In the PSCom system, the knowledge base is represented by probabilistic graphs. To enable simultaneous transmission of both knowledge and information data, rate splitting multiple access (RSMA) is employed. The optimization problem focuses on maximizing energy efficiency by jointly optimizing transmit beamforming, direct current (DC) bias, common rate allocation, and semantic compression ratio, while accounting for both communication and computation costs. To solve this problem, an alternating optimization algorithm based on successive convex approximation (SCA) and Dinkelbach method is developed. Simulation results demonstrate the effectiveness of the proposed approach.

</details>


### [27] [Joint Source-Channel Coding for ISAC: Distortion Tradeoffs and Separation Theorems](https://arxiv.org/abs/2601.10470)
*Gefei Peng,Youlong Wu*

Main category: cs.IT

TL;DR: 本文建立了ISAC系统中信道容量、通信失真、感知失真和估计成本之间的权衡关系，证明了分离源信道编码在此场景下能达到联合最优性。


<details>
  <summary>Details</summary>
Motivation: 集成感知与通信（ISAC）系统因其能同时实现高效通信和环境感知而受到广泛关注。该领域的核心目标是刻画感知与通信之间的性能权衡关系。

Method: 采用联合源信道编码（JSCC）框架，系统包含带有信道状态估计器和联合源信道编码器的发射机、状态相关无记忆信道、以及带有联合源信道解码器的接收机。从信息论角度分析各性能指标间的权衡关系。

Result: 建立了信道容量、通信失真、感知失真和估计成本之间的权衡关系，证明了分离源信道编码在此设置下能达到联合最优性，并通过二进制设置的示例验证了理论结果。

Conclusion: 本文从信息论角度为ISAC系统建立了性能权衡的理论框架，证明了分离编码的最优性，为ISAC系统的设计提供了理论基础。

Abstract: Integrated Sensing and Communication (ISAC) systems have garnered significant attention due to their capability to simultaneously achieve efficient communication and environmental sensing. A core objective in this field is characterizing the performance tradeoff between sensing and communication. In this paper, we consider a joint source-channel coding (JSCC) framework for the ISAC system that consists of a transmitter with a channel state estimator and a joint source-channel encoder, a state-dependent memoryless channel, and a receiver with a joint source-channel decoder. From an information-theoretic perspective, we establish the tradeoff relationships among channel capacity, distortions in both communication and sensing processes, and the estimation cost. We prove that the separate source and channel coding can achieve joint optimality in this setting. An illustrative example of a binary setting is also provided to validate our theoretical results.

</details>


### [28] [A Construction Framework of Coded Caching Scheme for Multi-Access MIMO Systems via Knapsack Problem](https://arxiv.org/abs/2601.10484)
*Siying Luo,Youlong Wu,Mingming Zhang,Minquan Cheng,Dianhua Wu*

Main category: cs.IT

TL;DR: 提出一种用于组合拓扑多接入MISO网络的编码缓存方案，通过0-1背包问题设计多天线放置交付阵列，实现高和自由度与低子分组复杂度的平衡


<details>
  <summary>Details</summary>
Motivation: 在具有组合拓扑的多接入MISO网络中，现有方案难以同时实现高和自由度与低子分组复杂度，需要设计更高效的缓存方案

Method: 将多天线放置交付阵列设计转化为0-1背包问题，优化缓存放置和交付策略，将复杂组合结构转化为可处理的优化框架

Result: 方案在组合拓扑网络中比现有方案获得更高和自由度，子分组复杂度与现有线性方案相当，特定条件下可达理论最大和自由度并进一步降低子分组

Conclusion: 提出的基于优化框架的编码缓存方案能有效平衡和自由度与子分组复杂度，为组合拓扑多接入MISO网络提供了高效解决方案

Abstract: This paper investigates the coded caching problem in a multi-access multiple-input single-output (MAMISO) network with the combinatorial topology. The considered system consists of a server containing $N$ files, $Λ$ cache nodes, and $K$ cache-less users, where each user can access a unique subset of $r$ cache nodes. The server is equipped with $L$ transmit antennas. Our objective is to design a caching scheme that simultaneously achieves a high sum Degree of Freedom (sum-DoF) and low subpacketization complexity. To address this challenge, we formulate the design of multi-antenna placement delivery arrays (MAPDA) as a $0$--$1$ knapsack problem to maximize the achievable DoF, thereby transforming the complex combinatorial caching structure into a tractable optimization framework that yields efficient cache placement and flexible delivery strategies. Theoretical and numerical analyses demonstrate that: for networks with combinatorial topologies, the proposed scheme achieves a higher sum-DoF than existing schemes. Under identical cache size constraints, the subpacketization level remains comparable to existing linear subpacketization schemes. Moreover, under specific system conditions, the proposed scheme attains the theoretical maximum sum-DoF of $\min\{L+KM/N, K\}$ while achieving further reductions subpacketization. For particular combinatorial structures, we further derive optimized constructions that achieve even higher sum-DoF with lower subpacketization. ```

</details>


### [29] [Coded Caching for Combinatorial Multi-Access Hotplug Networks from $t$-Designs](https://arxiv.org/abs/2601.10503)
*Dhruv Pratap Singh,Anjana A. Mahesh,B. Sundar Rajan*

Main category: cs.IT

TL;DR: 该论文研究组合多接入网络中的热插拔编码缓存，提出基于t设计的编码缓存方案，在特定内存区间优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有热插拔编码缓存模型限制用户只能访问单个缓存，而实际网络中用户可能访问多个缓存，且交付阶段只有部分缓存在线。需要扩展模型以支持组合多接入场景。

Method: 首先将热插拔放置交付数组(HpPDA)框架扩展到组合多接入设置，然后提出基于t设计的编码缓存方案。通过参数设计确保活跃用户能访问足够多的编码子文件来解码请求文件，并消除冗余组播传输。

Result: 提出的t方案实现了灵活子分组化的一系列速率-内存权衡，数值比较显示在特定内存区间优于现有热插拔编码缓存方案。

Conclusion: 该研究成功将热插拔编码缓存扩展到组合多接入网络，提出的t设计方案在保持灵活子分组化的同时改进了性能，为实际网络中的缓存设计提供了新思路。

Abstract: We study hotplug coded caching in combinatorial multi-access networks, which generalizes existing hotplug coded caching models by allowing users to access multiple caches, while only a subset of caches is online during the delivery phase. We first generalize the Hotplug Placement Delivery Array (HpPDA) framework to the combinatorial multi-access setting. Based on this generalized framework, we propose a t-design-based coded caching scheme for combinatorial multi-access networks. We characterize a class of design parameters under which every active user has access to a sufficient number of coded subfiles to decode its requested file, and show that appropriate parameter choices allow for the elimination of redundant multicast transmissions. As a result, the proposed scheme achieves a family of rate memory trade offs with flexible subpacketization. We present numerical comparisons illustrating that the proposed t-scheme outperforms existing hotplug coded caching schemes in certain memory regimes.

</details>


### [30] [A New Construction Structure on Coded Caching with Linear Subpacketization: Non-Half-Sum Latin Rectangle](https://arxiv.org/abs/2601.10505)
*Yongcheng Yang,Minquan Cheng,Kai Wan,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出了一种基于非半和拉丁矩形(NHSLR)的编码缓存方案，将线性子分组化从F=K扩展到F=O(K)，在保持线性子分组化的同时进一步降低了传输负载。


<details>
  <summary>Details</summary>
Motivation: 现有编码缓存方案在子分组化和传输负载之间存在权衡：指数或多项式子分组化方案性能好但复杂度高，而线性子分组化方案(F=K)虽然复杂度低但传输负载较高。需要设计既能保持线性子分组化又能降低传输负载的方案。

Method: 提出了一种新的组合结构——非半和拉丁矩形(NHSLR)，扩展了Cheng等人提出的非半和不相交打包(NHSDP)框架。NHSLR允许从F=K扩展到F=O(K)，通过构造NHSLR获得了一类新的编码缓存方案。

Result: 提出的方案实现了线性可扩展的子分组化(F=O(K))，同时相比NHSDP方案进一步降低了传输负载。理论分析和数值结果表明，该方案不仅比现有线性子分组化方案传输负载更低，而且接近某些指数子分组化方案的性能。

Conclusion: NHSLR结构为编码缓存方案设计提供了新的框架，在保持低复杂度的同时改善了性能，在子分组化和传输负载之间取得了更好的平衡。

Abstract: Coded caching is recognized as an effective method for alleviating network congestion during peak periods by leveraging local caching and coded multicasting gains. The key challenge in designing coded caching schemes lies in simultaneously achieving low subpacketization and low transmission load. Most existing schemes require exponential or polynomial subpacketization levels, while some linear subpacketization schemes often result in excessive transmission load. Recently, Cheng et al. proposed a construction framework for linear coded caching schemes called Non-Half-Sum Disjoint Packing (NHSDP), where the subpacketization equals the number of users $K$. This paper introduces a novel combinatorial structure, termed the Non-Half-Sum Latin Rectangle (NHSLR), which extends the framework of linear coded caching schemes from $F=K$ (i.e., the construction via NHSDP) to a broader scenario with $F=\mathcal{O}(K)$. By constructing NHSLR, we have obtained a new class of coded caching schemes that achieves linearly scalable subpacketization, while further reducing the transmission load compared with the NHSDP scheme. Theoretical and numerical analyses demonstrate that the proposed schemes not only achieves lower transmission load than existing linear subpacketization schemes but also approaches the performance of certain exponential subpacketization schemes.

</details>


### [31] [A New Construction Structure on Multi-access Coded Caching with Linear Subpacketization: Cyclic Multi-Access Non-Half-Sum Disjoint Packing](https://arxiv.org/abs/2601.10510)
*Mengyuan Li,Minquan Cheng,Kai Wan,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出了一种基于CMA-NHSDP结构的多接入编码缓存方案，实现了线性子分组化(F=K)的同时保持了较低的传输负载。


<details>
  <summary>Details</summary>
Motivation: 现有多接入编码缓存方案存在子分组化水平随用户数指数增长的问题，而线性或多项式子分组化的方案传输负载较高。需要设计一种既能保持线性子分组化又能降低传输负载的方案。

Method: 将NHSDP结构扩展到多接入系统，提出CMA-NHSDP组合结构，基于该结构构造新的多接入编码缓存方案。

Result: 新方案在线性子分组化(F=K)下，传输负载低于现有线性子分组化方案，在某些情况下甚至优于指数子分组化方案。

Conclusion: CMA-NHSDP结构成功解决了多接入编码缓存中线性子分组化与低传输负载之间的权衡问题，为实际系统部署提供了更实用的方案。

Abstract: We consider the $(K,L,M,N)$ multi-access coded caching system introduced by Hachem et al., which consists of a central server with $N$ files and $K$ cache nodes, each of memory size $M$, where each user can access $L$ cache nodes in a cyclic wrap-around fashion. At present, several existing schemes achieve competitive transmission performance, but their subpacketization levels grow exponentially with the number of users. In contrast, schemes with linear or polynomial subpacketization always incur higher transmission loads. We aim to design a multi-access coded caching scheme with linear subpacketization $F$ while maintaining low transmission load. Recently, Cheng et al. proposed a construction framework for coded caching schemes with linear subpacketization (i.e., $F=K$) called non-half-sum disjoint packing (NHSDP). Inspired by this structure, we introduce a novel combinatorial structure named cyclic multi-access non-half-sum disjoint packing (CMA-NHSDP) by extending NHSDP to MACC system. By constructing CMA-NHSDP, we obtain a new class of multi-access coded caching schemes. Theoretical and numerical analyses show that our scheme achieves lower transmission loads than some existing schemes with linear subpacketization. Moreover, the proposed schemes achieves lower transmission load compared to existing schemes with exponential subpacketization in some case.

</details>


### [32] [On the suboptimality of linear codes for binary distributed hypothesis testing](https://arxiv.org/abs/2601.10526)
*Adway Girish,Robinson D. H. Cung,Emre Telatar*

Main category: cs.IT

TL;DR: 研究二元分布式假设检验问题，两个代理观察相关二元向量，以相同速率向中央决策者传输压缩信息。分析线性压缩方案，证明截断在两种情况下是最佳线性方案：(1)测试相同幅度但符号相反的相关系数，(2)测试独立性或非独立性。数值证据支持截断对于测试任何符号相反的相关系数都是最佳线性编码的猜想。对于测试独立性，计算经典随机编码指数，显示截断（及任何线性编码）严格次优。


<details>
  <summary>Details</summary>
Motivation: 研究分布式假设检验中的压缩方案性能，特别关注线性压缩方案在二元相关向量测试中的最优性。探索在通信速率受限情况下，简单线性方案（如截断）是否能够达到最佳性能，以及线性方案与经典随机编码方案的性能比较。

Method: 研究二元分布式假设检验问题，两个代理观察相关二元向量，以相同速率向中央决策者传输压缩信息。分析线性压缩方案，特别关注截断方案。通过理论分析证明截断在特定情况下的最优性，并通过数值证据支持更广泛的猜想。计算经典随机编码指数用于性能比较。

Result: 证明截断是两种情况下的最佳线性方案：(1)测试相同幅度但符号相反的相关系数，(2)测试独立性或非独立性。数值证据支持截断对于测试任何符号相反的相关系数都是最佳线性编码的猜想。对于测试独立性，计算显示截断（及任何线性编码）严格劣于经典随机编码方案。

Conclusion: 截断作为简单线性压缩方案，在特定二元分布式假设检验场景中表现出最优性，但在测试独立性时严格劣于随机编码。这表明线性方案在某些情况下足够好，但在其他情况下需要更复杂的编码策略以达到最优性能。

Abstract: We study a binary distributed hypothesis testing problem where two agents observe correlated binary vectors and communicate compressed information at the same rate to a central decision maker. In particular, we study linear compression schemes and show that simple truncation is the best linear scheme in two cases: (1) testing opposite signs of the same magnitude of correlation, and (2) testing for or against independence. We conjecture, supported by numerical evidence, that truncation is the best linear code for testing any correlations of opposite signs. Further, for testing against independence, we also compute classical random coding exponents and show that truncation, and consequently any linear code, is strictly suboptimal.

</details>


### [33] [Network Integrated Sensing and Communication](https://arxiv.org/abs/2601.10538)
*Edward Andrews,Lawrence Ong,Duy T. Ngo,Yao Liu,Min Li*

Main category: cs.IT

TL;DR: 该论文研究了网络级ISAC系统，分析了中继网络中通信路由与感知覆盖之间的基本权衡关系，为一维路径网络提供了完整的感知-吞吐量区域解析表征，并为一般网络拓扑揭示了分段线性的帕累托边界。


<details>
  <summary>Details</summary>
Motivation: 现有ISAC研究主要集中在链路级设计，而大规模部署需要理解网络级性能。论文旨在研究网络ISAC模型中通信路由与感知覆盖之间的基本权衡关系，为未来6G异构网络设计提供关键见解。

Method: 提出了一种新颖的优化框架，捕捉多节点路由与感知覆盖之间的相互作用。针对一维路径网络提供了完整的感知-吞吐量区域解析表征，并将分析扩展到一般网络拓扑，建立了分段线性的感知-吞吐量帕累托边界。

Result: 揭示了一维路径网络中感知覆盖与通信路由之间的基本权衡关系，为一般网络拓扑提供了分段线性的帕累托边界，并对每个分段给出了物理解释，为6G异构网络设计提供了关键见解。

Conclusion: 网络级ISAC系统存在通信路由与感知覆盖之间的基本权衡关系，该研究为未来6G异构网络的设计提供了理论基础和设计指导，有助于实现高效的大规模ISAC部署。

Abstract: Integrated sensing and communication (ISAC) is a cornerstone technology for 6G networks, offering unified support for high-rate communication and high-accuracy sensing. While existing literature extensively covers link-level designs, the transition toward large-scale deployment necessitates a fundamental understanding of network-level performance. This paper investigates a network ISAC model where a source node communicates with a destination via a relay network, while intermediate nodes concurrently perform cooperative sensing over specific spatial regions. We formulate a novel optimization framework that captures the interplay between multi-node routing and sensing coverage. For a one-dimensional path network, we provide an analytical characterization of the complete sensing-throughput region. Extending this to general network topologies, we establish that the sensing-throughput Pareto boundary is piecewise linear and provide physical interpretations for each segment. Our results reveal the fundamental trade-offs between sensing coverage and communication routing, offering key insights for the design of future 6G heterogeneous networks.

</details>


### [34] [Error-Correcting Codes for Two Bursts of t1-Deletion-t2-Insertion with Low Computational Complexity](https://arxiv.org/abs/2601.10540)
*Yajuan Liu,Tolga M. Duman*

Main category: cs.IT

TL;DR: 该论文研究了能纠正多个突发(t₁,t₂)-DI错误的纠错码，建立了不同错误类型间的等价关系，推导了码率上下界，并提出了低复杂度构造方法。


<details>
  <summary>Details</summary>
Motivation: 在DNA数据存储和文档同步等实际场景中，经常出现同时包含插入、删除和替换的突发错误，需要开发能纠正此类错误的信道编码。

Method: 1) 建立了三种错误类型的等价关系；2) 推导了两个突发(t₁,t₂)-DI错误码的码率上下界；3) 提出了两个突发(t₁,t₂)-DI错误码的构造方法。

Result: 提出的编码构造相比基于综合征压缩技术的方法，显著降低了计算复杂度，同时建立了错误类型的理论等价关系。

Conclusion: 该研究为处理多个突发插入-删除错误的纠错码提供了理论框架和实用构造，在DNA存储和文档同步等应用中有重要价值。

Abstract: Burst errors involving simultaneous insertions, deletions, and substitutions occur in practical scenarios, including DNA data storage and document synchronization, motivating developments of channel codes that can correct such errors. In this paper, we address the problem of constructing error-correcting codes (ECCs) capable of handling multiple bursts of $t_1$-deletion-$t_2$-insertion ($(t_1,t_2)$-DI) errors, where each burst consists of $t_1$ deletions followed by $t_2$ insertions in a binary sequence. We make three key contributions: Firstly, we establish the fundamental equivalence of (1) two bursts of $(t_1,t_2)$-DI ECCs, (2) two bursts of $(t_2,t_1)$-DI ECCs, and (3) one burst each of $(t_1,t_2)$-DI and $(t_2,t_1)$-DI ECCs. Then, we derive lower and upper bounds on the code size of two bursts of $(t_1,t_2)$-DI ECCs, which can naturally be extended to the case of multiple bursts. Finally, we present constructions of two bursts of $(t_1,t_2)$-DI ECCs. Compared to the codes obtained by the syndrome compression technique, the resulting codes achieve significantly lower computational complexity.

</details>


### [35] [Sparse Signal Recovery from Random Measurements](https://arxiv.org/abs/2601.10569)
*Siu-Wing Cheng,Man Ting Wong*

Main category: cs.IT

TL;DR: 提出一种无需优化或线性系统求解的压缩感知信号重构方法，使用对数数量随机测量矩阵，时间复杂度为O(kn log n)，适用于稀疏信号恢复。


<details>
  <summary>Details</summary>
Motivation: 传统压缩感知方法需要解决优化问题或线性系统，计算复杂度高。本文旨在开发一种更简单、更高效的方法来恢复稀疏信号，避免复杂的数值计算。

Method: 使用Θ(log n)个随机测量矩阵，每个矩阵大小为k×n，其中k = Θ(s log n)，s是信号的非零元素数量。通过简单的算法直接确定信号值，无需优化求解。

Result: 方法时间复杂度为O(kn log n)，能够有效恢复稀疏信号。在二进制信号上实验表明，与基于优化的方法相比具有竞争力。

Conclusion: 提出了一种新颖的压缩感知信号恢复方法，避免了传统优化问题的求解，计算效率高，特别适用于稀疏信号恢复任务。

Abstract: Given the compressed sensing measurements of an unknown vector $z \in \mathbb{R}^n$ using random matrices, we present a simple method to determine $z$ without solving any optimization problem or linear system. Our method uses $Θ(\log n)$ random sensing matrices in $\mathbb{R}^{k \times n}$ and runs in $O(kn\log n)$ time, where $k = Θ(s\log n)$ and $s$ is the number of nonzero coordinates in $z$. We adapt our method to determine the support set of $z$ and experimentally compare with some optimization-based methods on binary signals.

</details>


### [36] [Fundamental Limits of Multi-User Distributed Computing of Linearly Separable Functions](https://arxiv.org/abs/2601.10603)
*K. K. Krishnan Namboodiri,Elizabath Peter,Derya Malak,Petros Elia*

Main category: cs.IT

TL;DR: 本文研究了多用户分布式计算线性可分函数的基本极限，提出了任务分配和传输的联合设计方案，在实数和有限域中分别建立了最优性能界。


<details>
  <summary>Details</summary>
Motivation: 分布式计算中通信与计算之间存在根本权衡：每个服务器计算能力有限（最多计算M个子函数），通信能力也有限（最多向Δ个用户发送线性组合）。需要设计能降低通信成本的分布式计算方案。

Method: 针对给定的K、L、M、Δ参数，提出联合设计任务分配和传输的分布式计算方案。在实数域中使用新颖的反向证明方法，在有限域中使用基于计数论证的反向证明方法。

Result: 所提方案在各种条件下实现了实数域中的最优性能，并在有限域中表征了方案的性能界限。

Conclusion: 本文建立了多用户分布式计算线性可分函数的基本极限，提出的联合设计方案在通信与计算权衡方面达到了理论最优，为分布式计算系统设计提供了理论基础。

Abstract: This work establishes the fundamental limits of the classical problem of multi-user distributed computing of linearly separable functions. In particular, we consider a distributed computing setting involving $L$ users, each requesting a linearly separable function over $K$ basis subfunctions from a master node, who is assisted by $N$ distributed servers. At the core of this problem lies a fundamental tradeoff between communication and computation: each server can compute up to $M$ subfunctions, and each server can communicate linear combinations of their locally computed subfunctions outputs to at most $Δ$ users. The objective is to design a distributed computing scheme that reduces the communication cost (total amount of data from servers to users), and towards this, for any given $K$, $L$, $M$, and $Δ$, we propose a distributed computing scheme that jointly designs the task assignment and transmissions, and shows that the scheme achieves optimal performance in the real field under various conditions using a novel converse. We also characterize the performance of the scheme in the finite field using another converse based on counting arguments.

</details>


### [37] [Basis-Spline Assisted Coded Computing: Strategies and Error Bounds](https://arxiv.org/abs/2601.10616)
*Rimpi Borah,J. Harshan,V. Lalitha*

Main category: cs.IT

TL;DR: 提出基于三次B样条插值的编码计算框架，用于处理非多项式函数的分布式计算，相比现有Berrut方法在存在大量掉队者时提供更好的精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有Berrut近似编码计算方法在处理非多项式函数时，由于Berrut插值具有全局支撑特性，当掉队者数量较多时精度会显著下降，需要更稳定、精确的解决方案。

Method: 提出基于三次B样条插值的编码计算框架，利用B样条的局部支撑和平滑特性，在主机节点重建服务器端函数评估，增强稳定性和精度。

Result: 理论分析推导了关于服务器数量和掉队者数量的近似误差界限，比较分析显示该框架在各种非多项式函数上显著优于基于Berrut的方法。

Conclusion: 基于B样条的编码计算框架为解决非多项式函数分布式计算中的精度问题提供了有效方案，特别适用于存在大量掉队者的场景。

Abstract: Coded computing has become a key framework for reliable distributed computation over decentralized networks, effectively mitigating the impact of stragglers. Although there exists a wide range of coded computing methods to handle both polynomial and non-polynomial functions, computing methods for the latter class have received traction due its inherent challenges in reconstructing non-polynomial functions using a finite number of evaluations. Among them, the state-of-the-art method is Berrut Approximated coded computing, wherein Berrut interpolants, are used for approximating the non-polynomial function. However, since Berrut interpolants have global support characteristics, such methods are known to offer degraded accuracy when the number of stragglers is large. To address this challenge, we propose a coded computing framework based on cubic B-spline interpolation. In our approach, server-side function evaluations are reconstructed at the master node using B-splines, exploiting their local support and smoothness properties to enhance stability and accuracy. We provide a systematic methodology for integrating B-spline interpolation into coded computing and derive theoretical bounds on approximation error in terms of the number of servers and stragglers. Comparative analysis demonstrates that our framework significantly outperforms Berrut-based methods for various non-polynomial functions.

</details>


### [38] [Converse Bounds for Sun-Jafar-type Weak Private Information Retrieval](https://arxiv.org/abs/2601.10643)
*Chandan Anand,Jayesh Seshadri,Prasad Krishnan,Gowtham R. Kurri*

Main category: cs.IT

TL;DR: 本文证明了Chandan等人提出的弱私有信息检索(WPIR)方案在特定条件下的类最优性，并展示了当阈值约束不成立时存在更高速率的反例。


<details>
  <summary>Details</summary>
Motivation: Chandan等人最近提出了针对无合谋和T-合谋场景的WPIR方案，并给出了速率-隐私权衡的表达式，但这些权衡的类最优性未知。本文旨在研究这些方案是否达到类最优。

Method: 通过理论证明和构造反例的方法：1) 证明Chandan等人的Sun-Jafar型方案在无合谋复制存储设置下的最优性；2) 证明Banawan-Ulukus型MDS-WPIR和Sun-Jafar型T-合谋WPIR方案在系统参数满足阈值约束下的类最优性；3) 当阈值约束不成立时，构造反例展示存在更高速率的方案。

Result: 1) 证明了Chandan等人的Sun-Jafar型方案在无合谋复制存储设置下是最优的；2) 证明了在阈值约束下，Banawan-Ulukus型MDS-WPIR和Sun-Jafar型T-合谋WPIR方案达到类最优；3) 当阈值约束不成立时，构造了反例，表明存在比之前报道的速率更高的方案。

Conclusion: 本文确定了Chandan等人WPIR方案的类最优性条件：在无合谋复制存储设置下完全最优，在MDS编码和T-合谋设置下需满足阈值约束。当阈值约束不满足时，存在性能更优的方案，这为进一步研究WPIR方案的极限性能提供了方向。

Abstract: Building on the well-established capacity-achieving schemes of Sun-Jafar (for replicated storage) and the closely related scheme of Banawan-Ulukus (for MDS-coded setting), a recent work by Chandan et al. proposed new classes of weak private information retrieval (WPIR) schemes for the collusion-free (replication and MDS-coded) setting, as well as for the $T$-colluding scenario. In their work, Chandan et al. characterized the expressions for the rate-privacy trade-offs for these classes of WPIR schemes, under the mutual information leakage and maximal leakage metrics. Explicit achievable trade-offs for the same were also presented, which were shown to be competitive or better than prior WPIR schemes. However, the class-wise optimality of the reported trade-offs were unknown. In this work, we show that the explicit rate-privacy trade-offs reported for the Sun-Jafar-type schemes by Chandan et al. are optimal for the non-colluding and replicated setting. Furthermore, we prove the class-wise optimality for Banawan-Ulukus-type MDS-WPIR and Sun-Jafar-type $T$-colluding WPIR schemes, under threshold-constraints on the system parameters. When these threshold-constraints do not hold, we present counter-examples which show that even higher rates than those reported before can be achieved.

</details>


### [39] [One-Shot Broadcast Joint Source-Channel Coding with Codebook Diversity](https://arxiv.org/abs/2601.10648)
*Joseph Rowan,Buu Phan,Ashish Khisti*

Main category: cs.IT

TL;DR: 研究单次联合信源信道编码，广播到K个解码器，至少一个解码器恢复信源即可成功。发现使用不相交码本可获得码本分集增益，不同于信道分集增益。提出混合编码方案，分组平衡码本与信道分集，在二进制对称信道中优于完全共享或完全不相交码本策略。


<details>
  <summary>Details</summary>
Motivation: 研究广播场景下单次联合信源信道编码问题，其中信源被编码一次并通过独立信道广播到多个解码器。成功标准是至少有一个解码器能在最大失真约束下恢复信源。需要探索如何利用多个解码器的优势来提高恢复概率。

Method: 1) 使用不相交码本为每个解码器编码，获得码本分集增益；2) 提出混合编码方案，将解码器分组以优化平衡码本分集和信道分集；3) 通过改进的泊松匹配引理推导一阶和二阶可达界；4) 在二进制对称信道上进行数值验证。

Result: 1) 发现不相交码本策略可获得码本分集增益，不同于传统信道分集增益；2) 混合编码方案在二进制对称信道上表现优于完全共享码本或完全不相交码本策略；3) 通过泊松匹配引理推导了理论可达界。

Conclusion: 在单次联合信源信道编码广播场景中，码本分集是重要增益来源。混合编码方案能有效平衡码本分集和信道分集，在实际信道中表现更优。这为多解码器广播系统设计提供了新思路。

Abstract: We study a one-shot joint source-channel coding setting where the source is encoded once and broadcast to $K$ decoders through independent channels. Success is predicated on at least one decoder recovering the source within a maximum distortion constraint. We find that in the one-shot regime, utilizing disjoint codebooks at each decoder yields a codebook diversity gain, distinct from the channel diversity gain that may be expected when several decoders observe independent realizations of the channel's output but share the same codebook. Coding schemes are introduced that leverage this phenomenon, where first- and second-order achievability bounds are derived via an adaptation of the Poisson matching lemma (Li and Anantharam, 2021) which allows for multiple decoders using disjoint codebooks. We further propose a hybrid coding scheme that partitions decoders into groups to optimally balance codebook and channel diversity. Numerical results on the binary symmetric channel demonstrate that the hybrid approach outperforms strategies where the decoders' codebooks are either fully shared or disjoint.

</details>


### [40] [Breaking the Storage-Bandwidth Tradeoff in Distributed Storage with Quantum Entanglement](https://arxiv.org/abs/2601.10676)
*Lei Hu,Mohamed Nomeir,Alptug Aytekin,Sennur Ulukus*

Main category: cs.IT

TL;DR: 量子分布式存储系统通过量子信道传输经典信息，利用量子纠缠显著改善存储-带宽权衡，在特定条件下可同时最小化存储和修复带宽。


<details>
  <summary>Details</summary>
Motivation: 研究量子资源在分布式存储系统中的应用，探索量子通信如何突破经典存储系统的性能限制，特别是在存储和修复带宽之间的权衡关系。

Method: 在(n,k,d)分布式存储系统中，允许帮助节点通过量子信道向新节点传输经典信息，新节点通过对接收的量子态进行测量来生成存储。分析量子纠缠对存储-带宽权衡的影响。

Result: 量子纠缠显著改善了存储-带宽权衡，特别是在最小存储再生点。当d≥2k-2时，存在一个操作点可以同时最小化存储和修复带宽，这在经典系统中是不可能的。

Conclusion: 量子通信为分布式存储系统带来了根本性的新机制，打破了经典系统中的权衡限制，展示了量子资源在存储系统中的巨大潜力。

Abstract: This work investigates the use of quantum resources in distributed storage systems. Consider an $(n,k,d)$ distributed storage system in which a file is stored across $n$ nodes such that any $k$ nodes suffice to reconstruct the file. When a node fails, any $d$ helper nodes transmit information to a newcomer to rebuild the system. In contrast to the classical repair, where helper nodes transmit classical bits, we allow them to send classical information over quantum channels to the newcomer. The newcomer then generates its storage by performing appropriate measurements on the received quantum states. In this setting, we fully characterize the fundamental tradeoff between storage and repair bandwidth (total communication cost). Compared to classical systems, the optimal storage--bandwidth tradeoff can be significantly improved with the enhancement of quantum entanglement shared only among the surviving nodes, particularly at the minimum-storage regenerating point. Remarkably, we show that when $d \geq 2k-2$, there exists an operating point at which \textit{both storage and repair bandwidth are simultaneously minimized}. This phenomenon breaks the tradeoff in the classical setting and reveals a fundamentally new regime enabled by quantum communication.

</details>


### [41] [Synchronizing Probabilities in Model-Driven Lossless Compression](https://arxiv.org/abs/2601.10678)
*Aviv Adler,Jennifer Tang*

Main category: cs.IT

TL;DR: PMATIC是一种概率匹配区间编码算法，能够容忍预测模型中的有界不匹配，解决了深度学习模型在无损压缩中因硬件/软件差异导致的解码失败问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在无损数据压缩中能有效估计符号概率，但压缩器和解压器必须具有完全匹配的预测。由于硬件、软件或计算顺序的微小非确定性差异，会导致级联解码失败，这是模型驱动压缩中的关键问题。

Method: 提出了概率匹配区间编码（PMATIC），这是一种模型无关的算法，能够容忍有界预测不匹配且开销较低。PMATIC使用预测概率，可作为模型驱动压缩工具中算术编码器的直接替代品。

Result: 理论证明了PMATIC的正确性和性能边界，并在文本数据上验证了结果。当与先进的预测模型配对时，PMATIC对预测不匹配具有鲁棒性，同时压缩率优于标准现代压缩工具。

Conclusion: PMATIC解决了模型驱动压缩中的预测不匹配问题，提供了一种鲁棒的压缩方案，能够在保持高压缩率的同时容忍预测模型中的微小差异。

Abstract: It is well-known in the field of lossless data compression that probabilistic next-symbol prediction can be used to compress sequences of symbols. Deep neural networks are able to capture rich dependencies in data, offering a powerful means of estimating these probabilities and hence an avenue towards more effective compression algorithms. However, both compressor and decompressor must have exactly matching predictions; even small non-deterministic differences (which often happen with learned models due to hardware, software, or computation order) can lead to cascading decoding failures. In this paper, we formalize the problem of prediction mismatch in model-driven compression, and introduce Probability Matching Interval Coding (PMATIC), a model-agnostic algorithm that tolerates bounded prediction mismatch with low overhead. PMATIC works with the predicted probabilities, making it compatible as a drop-in replacement for the arithmetic encoder in model-driven compression tools. We show theoretical correctness and performance bounds for PMATIC, and validate these results on text data. These results confirm that, when paired an advanced prediction model, PMATIC is robust to prediction mismatch while achieving compression rates that out-perform standard modern compression tools.

</details>


### [42] [Implementation of Oblivious Transfer over Binary-Input AWGN Channels by Polar Codes](https://arxiv.org/abs/2601.10682)
*Pin-Hsun Lin,Hadi Aghaee,Christian Deppe,Eduard A. Jorswieck,Holger Boche*

Main category: cs.IT

TL;DR: 基于极化码的二进制输入加性高斯白噪声信道上的二选一不经意传输协议，利用极化变换的自同构实现完美接收者隐私和渐近发送者隐私。


<details>
  <summary>Details</summary>
Motivation: 设计在二进制输入加性高斯白噪声信道上实现二选一不经意传输的协议，需要同时保证接收者隐私（接收者的选择比特不被泄露）和发送者隐私（发送者的两个消息中未被选择的不被泄露），并在有限码长下提供性能保证。

Method: 使用极化码，通过极化变换的自同构连接两个解码器视图，公开从相应的自同构群中随机选择编码器。在选定的坏比特信道上故意注入随机性，通过信道极化结合隐私放大获得发送者隐私，并推导出松弛的可靠性准则。

Result: 实现了在任何有限码长下的完美接收者隐私（因为公开编码器分布与接收者的选择比特无关），通过信道极化和隐私放大获得了渐近的发送者隐私。将极化变换的自同构表征为比特信道索引的比特级置换，并利用这种结构推导和优化了可实现的有限码长OT速率。

Conclusion: 该工作提出了一种基于极化码的二选一不经意传输协议，在二进制输入加性高斯白噪声信道上同时保证了接收者隐私和发送者隐私，并提供了有限码长性能分析和速率优化方法，为实际OT系统设计提供了理论框架。

Abstract: We develop a one-out-of-two-oblivious transfer protocol over the binary-input additive white Gaussian noise channel using polar codes. The scheme uses two decoder views linked by automorphisms of the polar transform and publicly draws the encoder at random from the corresponding automorphism group. This yields perfect receiver privacy at any finite blocklength, since the public encoder distribution is independent of the receiver's choice bit. Sender privacy is obtained asymptotically via channel polarization combined with privacy amplification. Because the construction deliberately injects randomness on selected bad bit-channels, we derive a relaxed reliability criterion and evaluate finite-blocklength performance. Finally, we characterize the polar-transform automorphisms as bit-level permutations of bit-channel indices, and exploit this structure to derive and optimize an achievable finite-blocklength OT rate.

</details>


### [43] [Improved Constructions of Reed-Solomon Codes with Optimal Repair Bandwidth](https://arxiv.org/abs/2601.10685)
*Jing Qiu,Weijun Fang,Shu-Tao Xia,Fang-Wei Fu*

Main category: cs.IT

TL;DR: 本文改进了RS-MSR码的构造，消除了素数模s同余1的限制，显著降低了子分组化程度并扩展了可行参数范围。


<details>
  <summary>Details</summary>
Motivation: 现有的RS-MSR码构造要求素数满足p_i ≡ 1 (mod s)的限制，这导致子分组化程度较高且参数选择受限，需要更灵活的构造方法。

Method: 提出改进的RS-MSR码构造，消除了素数模s同余1的限制条件，通过新的数学方法实现最小存储再生特性。

Result: 新构造将子分组化程度降低了φ(s)^n倍，显著扩展了RS-MSR码的可行参数范围，同时保持了MSR特性。

Conclusion: 本文成功构建了更高效的RS-MSR码，突破了原有构造的限制，为分布式存储系统提供了更灵活、更实用的编码方案。

Abstract: Maximum-distance-separable (MDS) codes are widely used in distributed storage, yet naive repair of a single erasure in an $[n,k]$ MDS code downloads the entire contents of $k$ nodes. Minimum Storage Regenerating (MSR) codes (Dimakis et al., 2010) minimize repair bandwidth by contacting $d>k$ helpers and downloading only a fraction of data from each. Guruswami and Wootters first proposed a linear repair scheme for Reed-Solomon (RS) codes, showing that they can be repaired with lower bandwidth than the naive approach. The existence of RS codes achieving the MSR point (RS-MSR codes) nevertheless remained open until the breakthrough construction of Tamo, Barg, and Ye, which yields RS-MSR codes with subpacketization $\ell = s \prod_{i=1}^n p_i$, where $p_i$ are distinct primes satisfying $p_i \equiv 1 \pmod{s}$ and $s=d+1-k$.
  In this paper, we present an improved construction of RS-MSR codes by eliminating the congruence condition $p_i \equiv 1 \pmod{s}$. Consequently, our construction reduces the subpacketization by a multiplicative factor of $φ(s)^n$ ( $φ(\cdot)$ is Euler's totient function) and broadens the range of feasible parameters for RS-MSR codes.

</details>


### [44] [Perfect Secret Key Generation for a class of Hypergraphical Sources](https://arxiv.org/abs/2601.10697)
*Manuj Mukherjee,Sagnik Chatterjee,Alhad Sethi*

Main category: cs.IT

TL;DR: 本文扩展了PIN模型到超图，提出了两种完美密钥生成方案：一种针对完全t-均匀超图，另一种针对3-均匀超图，部分方案达到容量上限。


<details>
  <summary>Details</summary>
Motivation: Nitinawarat和Narayan提出的PIN模型基于图的生成树打包率实现完美密钥生成。本文旨在将这一框架扩展到超图模型，利用超图的组合性质设计类似的完美密钥生成方案。

Method: 1. 针对完全t-均匀超图：利用星超图打包完全t-均匀超图，设计每个星图生成binom(m-2,t-2)比特完美密钥的方案
2. 针对3-均匀超图：首先设计投影为环的3-均匀星超图的2比特完美密钥生成方案，然后通过星图打包和图哈密顿打包扩展到一般3-均匀超图

Result: 1. 完全t-均匀超图的方案达到容量上限
2. 3-均匀超图的方案对某些超图类别达到容量上限
3. 成功将PIN模型的密钥生成框架从图扩展到超图

Conclusion: 本文成功将完美密钥生成从图模型扩展到超图模型，提出了两种基于超图组合性质的方案，部分方案达到容量最优，为超图环境下的安全通信提供了理论基础。

Abstract: Nitinawarat and Narayan proposed a perfect secret key generation scheme for the so-called \emph{pairwise independent network (PIN) model} by exploiting the combinatorial properties of the underlying graph, namely the spanning tree packing rate. This work considers a generalization of the PIN model where the underlying graph is replaced with a hypergraph, and makes progress towards designing similar perfect secret key generation schemes by exploiting the combinatorial properties of the hypergraph.
  Our contributions are two-fold. We first provide a capacity achieving scheme for a complete $t$-uniform hypergraph on $m$ vertices by leveraging a packing of the complete $t$-uniform hypergraphs by what we refer to as star hypergraphs, and designing a scheme that gives $\binom{m-2}{t-2}$ bits of perfect secret key per star graph. Our second contribution is a 2-bit perfect secret key generation scheme for 3-uniform star hypergraphs whose projections are cycles. This scheme is then extended to a perfect secret key generation scheme for generic 3-uniform hypergraphs by exploiting star graph packing of 3-uniform hypergraphs and Hamiltonian packings of graphs. The scheme is then shown to be capacity achieving for certain classes of hypergraphs.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [45] [EditEmoTalk: Controllable Speech-Driven 3D Facial Animation with Continuous Expression Editing](https://arxiv.org/abs/2601.10000)
*Diqiong Jiang,Kai Zhu,Dan Song,Jian Chang,Chenglizhao Chen,Zhenyu Wu*

Main category: cs.MM

TL;DR: EditEmoTalk：基于连续情感编辑的语音驱动3D面部动画框架，通过边界感知语义嵌入实现平滑情感操控，同时保持准确的唇形同步。


<details>
  <summary>Details</summary>
Motivation: 现有语音驱动3D面部动画方法虽然能实现高质量的唇形同步，但通常依赖离散情感类别，限制了连续和细粒度的情感控制能力。

Method: 提出边界感知语义嵌入，学习情感决策边界法线方向，构建连续表情流形；引入情感一致性损失，通过映射网络确保生成动作动态与目标情感嵌入的语义对齐。

Result: 实验表明EditEmoTalk在保持准确唇形同步的同时，实现了优越的可控性、表现力和泛化能力。

Conclusion: EditEmoTalk框架成功解决了连续情感编辑问题，为语音驱动3D面部动画提供了更精细的情感控制能力，代码和预训练模型将开源。

Abstract: Speech-driven 3D facial animation aims to generate realistic and expressive facial motions directly from audio. While recent methods achieve high-quality lip synchronization, they often rely on discrete emotion categories, limiting continuous and fine-grained emotional control. We present EditEmoTalk, a controllable speech-driven 3D facial animation framework with continuous emotion editing. The key idea is a boundary-aware semantic embedding that learns the normal directions of inter-emotion decision boundaries, enabling a continuous expression manifold for smooth emotion manipulation. Moreover, we introduce an emotional consistency loss that enforces semantic alignment between the generated motion dynamics and the target emotion embedding through a mapping network, ensuring faithful emotional expression. Extensive experiments demonstrate that EditEmoTalk achieves superior controllability, expressiveness, and generalization while maintaining accurate lip synchronization. Code and pretrained models will be released.

</details>


### [46] [Subjective evaluation of UHD video coded using VVC with LCEVC and ML-VVC](https://arxiv.org/abs/2601.10448)
*Naeem Ramzan,Muhammad Tufail Khan*

Main category: cs.MM

TL;DR: LCEVC作为VVC基础层的增强层进行主观质量评估，比较了LCEVC增强、VVC上采样和多层VVC三种方案在10%和50%增强层比特率下的性能


<details>
  <summary>Details</summary>
Motivation: 评估多层视频编码配置中LCEVC作为VVC增强层的性能，为视频编码标准选择提供主观质量参考

Method: 采用MPEG多层视频编码评估方法，使用LCEVC Test Model 8.1，比较HD VVC基础层+LCEVC增强与VVC上采样、多层VVC两种参考方案，在10%和50%增强层比特率下进行Degradation Category Rating主观评估

Result: 报告了包含95%置信区间的平均意见分数，能够比较不同编码方法和操作点在测试范围内的感知质量差异

Conclusion: LCEVC作为VVC增强层在多层视频编码配置中具有应用潜力，主观评估结果可为编码方案选择提供依据

Abstract: This paper presents the results of a subjective quality assessment of a multilayer video coding configuration in which Low Complexity Enhancement Video Coding (LCEVC) is applied as an enhancement layer on top of a Versatile Video Coding (VVC) base layer. The evaluation follows the same test methodology and conditions previously defined for MPEG multilayer video coding assessments, with the LCEVC enhancement layer encoded using version 8.1 of the LCEVC Test Model (LTM). The test compares reconstructed UHD output generated from an HD VVC base layer with LCEVC enhancement against two reference cases: upsampled VVC base layer decoding and multilayer VVC (ML-VVC). Two operating points are considered, corresponding to enhancement layers representing approximately 10% and 50% of the total bitrate. Subjective assessment was conducted using the Degradation Category Rating (DCR) methodology with twenty five participants, across a dataset comprising fifteen SDR and HDR sequences. The reported results include Mean Opinion Scores (MOS) with associated 95% confidence intervals, enabling comparison of perceptual quality across coding approaches and operating points within the defined test scope.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [47] [A Control Theoretic Approach to Decentralized AI Economy Stabilization via Dynamic Buyback-and-Burn Mechanisms](https://arxiv.org/abs/2601.09961)
*Zehua Cheng,Wei Dai,Zhipeng Wang,Rui Sun,Nick Wen,Jiahao Sun*

Main category: cs.GT

TL;DR: 提出基于PID控制器的动态回购机制DCBM，通过控制理论框架稳定去中心化AI网络的代币经济，相比静态模型减少66%价格波动和降低运营商流失率。


<details>
  <summary>Details</summary>
Motivation: 去中心化AI网络的长期生存能力受到其原生经济层极端波动性的严重威胁。当前基于静态或阈值回购的代币经济模型无法处理复杂系统动态，且往往具有顺周期性，在市场下跌时加剧不稳定性。

Method: 提出动态控制回购机制(DCBM)，这是一个基于控制理论的框架，使用带有严格偿付能力约束的比例-积分-微分(PID)控制器，将代币经济作为动态系统进行调节。通过基于代理的模拟和跳跃扩散过程进行验证。

Result: DCBM显著优于静态基准模型：在高度波动环境下，代币价格波动减少约66%，运营商流失率从19.5%降至8.1%。

Conclusion: 将代币经济从静态规则转变为连续、结构约束的控制循环，是确保去中心化智能网络安全和可持续发展的必要条件。

Abstract: The democratization of artificial intelligence through decentralized networks represents a paradigm shift in computational provisioning, yet the long-term viability of these ecosystems is critically endangered by the extreme volatility of their native economic layers. Current tokenomic models, which predominantly rely on static or threshold-based buyback heuristics, are ill-equipped to handle complex system dynamics and often function pro-cyclically, exacerbating instability during market downturns. To bridge this gap, we propose the Dynamic-Control Buyback Mechanism (DCBM), a formalized control-theoretic framework that utilizes a Proportional-Integral-Derivative (PID) controller with strict solvency constraints to regulate the token economy as a dynamical system. Extensive agent-based simulations utilizing Jump-Diffusion processes demonstrate that DCBM fundamentally outperforms static baselines, reducing token price volatility by approximately 66% and lowering operator churn from 19.5% to 8.1% in high-volatility regimes. These findings establish that converting tokenomics from static rules into continuous, structurally constrained control loops is a necessary condition for secure and sustainable decentralized intelligence networks.

</details>


### [48] [Inverse Learning in $2\times2$ Games: From Synthetic Interactions to Traffic Simulation](https://arxiv.org/abs/2601.10367)
*Daniela Aguirre Salazar,Firas Moatemri,Tatiana Tatarenko*

Main category: cs.GT

TL;DR: 论文提出了两种逆博弈理论学习方法：针对2×2博弈的闭式相关均衡最大似然估计器(CE-ML)和基于随机响应过程的Logit最佳响应最大似然估计器(LBR-ML)，用于从有限行为数据中理解多智能体系统的协调与竞争。


<details>
  <summary>Details</summary>
Motivation: 理解智能体如何从有限行为数据中进行协调或竞争，对于交通、机器人等多智能体系统中的战略交互建模至关重要。现有方法需要在静态均衡一致性和动态行为真实性之间取得平衡。

Method: 提出了两种互补的逆博弈理论学习方法：1) 专门针对2×2博弈的闭式相关均衡最大似然估计器(CE-ML)；2) 通过随机响应过程捕捉长期适应动态的Logit最佳响应最大似然估计器(LBR-ML)。

Result: 在合成的"chicken-dare"游戏和SUMO模拟的交通交互场景中评估了两种方法，比较了参数恢复和分布拟合效果。结果显示模型在可解释性、计算可处理性和行为表达能力之间存在明显权衡。

Conclusion: 两种方法在静态均衡一致性和动态行为真实性之间形成了互补，为理解多智能体战略交互提供了不同的建模视角，揭示了逆博弈学习中不同建模目标之间的权衡关系。

Abstract: Understanding how agents coordinate or compete from limited behavioral data is central to modeling strategic interactions in traffic, robotics, and other multi-agent systems. In this work, we investigate the following complementary formulations of inverse game-theoretic learning: (i) a Closed-form Correlated Equilibrium Maximum-Likelihood estimator (CE-ML) specialized for $2\times2$ games; and (ii) a Logit Best Response Maximum-Likelihood estimator (LBR-ML) that captures long-run adaptation dynamics via stochastic response processes. Together, these approaches span the spectrum between static equilibrium consistency and dynamic behavioral realism. We evaluate them on synthetic "chicken-dare" games and traffic-interaction scenarios simulated in SUMO, comparing parameter recovery and distributional fit. Results reveal clear trade-offs between interpretability, computational tractability, and behavioral expressiveness across models.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [49] [STCRank: Spatio-temporal Collaborative Ranking for Interactive Recommender System at Kuaishou E-shop](https://arxiv.org/abs/2601.10027)
*Boyang Xia,Ruilin Bao,Hanjun Jiang,Jun Wang,Wenwu Ou*

Main category: cs.IR

TL;DR: 快手电商提出STCRank框架，通过时空协同排序解决全屏UI和沉浸式下滑功能带来的多目标冲突和时序贪婪陷阱问题，实现购买和日活双增长。


<details>
  <summary>Details</summary>
Motivation: 快手电商在主页推荐系统之外部署了交互式推荐系统，但全屏UI和沉浸式下滑功能带来两个挑战：1）转化、浏览、下滑等多目标之间存在显式干扰（重叠或冲突）；2）排序系统在序列推荐槽位转换中容易陷入时序贪婪陷阱。

Method: 提出时空协同排序（STCRank）框架，包含两个模块：1）多目标协同（MOC）模块，通过缓解目标重叠和冲突来推动帕累托前沿；2）多槽位协同（MSC）模块，通过双阶段前瞻排序机制实现整体序列槽位的全局最优。

Result: 大量实验证明该方法带来了购买和日活的双重增长。该系统已于2025年6月在快手电商部署上线。

Conclusion: STCRank框架有效解决了全屏电商推荐系统中的多目标冲突和时序贪婪问题，实现了空间（单槽位内）和时间（多槽位间）的协同优化，提升了推荐系统的整体性能。

Abstract: As a popular e-commerce platform, Kuaishou E-shop provides precise personalized product recommendations to tens of millions of users every day. To better respond real-time user feedback, we have deployed an interactive recommender system (IRS) alongside our core homepage recommender system. This IRS is triggered by user click on homepage, and generates a series of highly relevant recommendations based on the clicked item to meet focused browsing demands. Different from traditional e-commerce RecSys, the full-screen UI and immersive swiping down functionality present two distinct challenges for regular ranking system. First, there exists explicit interference (overlap or conflicts) between ranking objectives, i.e., conversion, view and swipe down. This is because there are intrinsic behavioral co-occurrences under the premise of immersive browsing and swiping down functionality. Second, the ranking system is prone to temporal greedy traps in sequential recommendation slot transitions, which is caused by full-screen UI design. To alleviate these challenges, we propose a novel Spatio-temporal collaborative ranking (STCRank) framework to achieve collaboration between multi-objectives within one slot (spatial) and between multiple sequential recommondation slots. In multi-objective collaboration (MOC) module, we push Pareto frontier by mitigating the objective overlaps and conflicts. In multi-slot collaboration (MSC) module, we achieve global optima on overall sequential slots by dual-stage look-ahead ranking mechanism. Extensive experiments demonstrate our proposed method brings about purchase and DAU co-growth. The proposed system has been already deployed at Kuaishou E-shop since 2025.6.

</details>


### [50] [Development of Ontological Knowledge Bases by Leveraging Large Language Models](https://arxiv.org/abs/2601.10436)
*Le Ngoc Luyen,Marie-Hélène Abel,Philippe Gouspillou*

Main category: cs.IR

TL;DR: 本文提出了一种利用大语言模型（LLMs）优化本体知识库开发的迭代方法，通过车辆销售领域的用户上下文配置文件本体案例研究，展示了该方法能显著加速本体构建、提高一致性、减少偏见并增强透明度。


<details>
  <summary>Details</summary>
Motivation: 传统本体知识库（OKBs）的手动开发存在可扩展性、一致性和适应性方面的挑战。生成式AI特别是大语言模型（LLMs）的发展为自动化和增强OKB开发提供了有前景的解决方案。

Method: 提出了一种结构化的迭代方法，利用LLMs优化知识获取、自动化本体工件生成，并支持持续改进循环。通过车辆销售领域的用户上下文配置文件本体案例研究来演示该方法。

Result: 该方法显著加速了本体构建过程，提高了本体一致性，有效缓解了偏见，并增强了本体工程过程的透明度。

Conclusion: 将LLMs集成到本体开发中具有变革性潜力，显著提高了知识管理系统的可扩展性、集成能力和整体效率。

Abstract: Ontological Knowledge Bases (OKBs) play a vital role in structuring domain-specific knowledge and serve as a foundation for effective knowledge management systems. However, their traditional manual development poses significant challenges related to scalability, consistency, and adaptability. Recent advancements in Generative AI, particularly Large Language Models (LLMs), offer promising solutions for automating and enhancing OKB development. This paper introduces a structured, iterative methodology leveraging LLMs to optimize knowledge acquisition, automate ontology artifact generation, and enable continuous refinement cycles. We demonstrate this approach through a detailed case study focused on developing a user context profile ontology within the vehicle sales domain. Key contributions include significantly accelerated ontology construction processes, improved ontological consistency, effective bias mitigation, and enhanced transparency in the ontology engineering process. Our findings highlight the transformative potential of integrating LLMs into ontology development, notably improving scalability, integration capabilities, and overall efficiency in knowledge management systems.

</details>


### [51] [iTIMO: An LLM-empowered Synthesis Dataset for Travel Itinerary Modification](https://arxiv.org/abs/2601.10609)
*Zhuoxuan Huang,Yunshan Ma,Hongyu Zhang,Hua Ma,Zhu Sun*

Main category: cs.IR

TL;DR: 论文提出了iTIMO数据集，专门用于行程修改任务研究，通过意图驱动的扰动方法生成需要修改的行程数据，并设计了混合评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注固定行程规划，而行程修改是旅行中的常见需求，但缺乏专门的数据集和研究。缺乏"需要修改"的行程数据是阻碍该任务研究的关键瓶颈。

Method: 1. 正式定义行程修改任务；2. 提出生成需要修改行程数据的通用流程，将其构建为意图驱动的扰动任务；3. 使用大语言模型通过REPLACE、ADD、DELETE三种原子编辑操作扰动真实行程；4. 扰动基于三种意图：流行度、空间距离和类别多样性；5. 设计混合评估指标确保扰动有效性。

Result: 创建了iTIMO数据集，在数据集上进行了全面实验，揭示了当前大语言模型的局限性，并为未来研究提供了有价值的方向。

Conclusion: 论文填补了行程修改任务的研究空白，提出的iTIMO数据集和方法为相关研究提供了基础，实验结果为未来改进指明了方向。

Abstract: Addressing itinerary modification is crucial for enhancing the travel experience as it is a frequent requirement during traveling. However, existing research mainly focuses on fixed itinerary planning, leaving modification underexplored. To bridge this gap, we formally define the itinerary modification task and introduce iTIMO, a dataset specifically tailored for this purpose. We identify the lack of {\itshape need-to-modify} itinerary data as the critical bottleneck hindering research on this task and propose a general pipeline to overcome it. This pipeline frames the generation of such data as an intent-driven perturbation task. It instructs large language models to perturb real world itineraries using three atomic editing operations: REPLACE, ADD, and DELETE. Each perturbation is grounded in three intents, including disruptions of popularity, spatial distance, and category diversity. Furthermore, a hybrid evaluation metric is designed to ensure perturbation effectiveness. We conduct comprehensive experiments on iTIMO, revealing the limitations of current LLMs and lead to several valuable directions for future research. Dataset and corresponding code are available at https://github.com/zelo2/iTIMO.

</details>


### [52] [RoutIR: Fast Serving of Retrieval Pipelines for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.10644)
*Eugene Yang,Andrew Yates,Dawn Lawrie,James Mayfield,Trevor Adriaanse*

Main category: cs.IR

TL;DR: RoutIR是一个Python包，为检索模型提供HTTP API，支持动态RAG系统中的在线检索服务


<details>
  <summary>Details</summary>
Motivation: 当前学术IR平台基于Cranfield范式，只能批量离线处理已知查询，无法支持需要在线服务的动态RAG系统（如多轮检索、反馈循环、自组织代理等）

Method: 开发RoutIR Python包，通过简单的JSON配置文件包装任意检索方法（包括一阶段检索、重排序、查询扩展、结果融合），提供HTTP API支持动态构建和查询检索管道

Result: RoutIR支持异步查询批处理和默认缓存，已集成多种先进检索方法，且通过实现Engine抽象类易于扩展，已在GitHub开源

Conclusion: RoutIR填补了学术检索模型与动态RAG应用之间的鸿沟，为构建在线检索服务提供了灵活高效的解决方案

Abstract: Retrieval models are key components of Retrieval-Augmented Generation (RAG) systems, which generate search queries, process the documents returned, and generate a response. RAG systems are often dynamic and may involve multiple rounds of retrieval. While many state-of-the-art retrieval methods are available through academic IR platforms, these platforms are typically designed for the Cranfield paradigm in which all queries are known up front and can be batch processed offline. This simplification accelerates research but leaves state-of-the-art retrieval models unable to support downstream applications that require online services, such as arbitrary dynamic RAG pipelines that involve looping, feedback, or even self-organizing agents. In this work, we introduce RoutIR, a Python package that provides a simple and efficient HTTP API that wraps arbitrary retrieval methods, including first stage retrieval, reranking, query expansion, and result fusion. By providing a minimal JSON configuration file specifying the retrieval models to serve, RoutIR can be used to construct and query retrieval pipelines on-the-fly using any permutation of available models (e.g., fusing the results of several first-stage retrieval methods followed by reranking). The API automatically performs asynchronous query batching and caches results by default. While many state-of-the-art retrieval methods are already supported by the package, RoutIR is also easily expandable by implementing the Engine abstract class. The package is open-sourced and publicly available on GitHub: http://github.com/hltcoe/routir.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [53] [Multiverse: Transactional Memory with Dynamic Multiversioning](https://arxiv.org/abs/2601.09735)
*Gaetano Coccimiglio,Trevor Brown,Srivatsan Ravi*

Main category: cs.DB

TL;DR: Multiverse是一个结合了无版本和多版本事务内存优势的新型STM，能够在支持长运行读取的同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 传统STM在处理长运行读取（访问大量频繁更新地址）时存在局限性，多版本方法虽然能支持这类工作负载但代价昂贵，会降低不需要版本化的事务性能。

Method: 提出Multiverse STM，支持版本化和无版本化事务并发执行，确保无版本事务达到最先进STM的性能，同时支持快速版本化事务以满足长运行读取需求。

Result: 实验表明，对于没有长运行读取的常见工作负载，Multiverse性能相当或更好；对于有长运行读取和频繁更新的工作负载，Multiverse显著优于现有STM，吞吐量在某些情况下比其他STM快几个数量级。

Conclusion: Multiverse成功结合了无版本和多版本STM的优势，为支持长运行读取的工作负载提供了高效解决方案，同时保持了常规工作负载的高性能。

Abstract: Software transactional memory (STM) allows programmers to easily implement concurrent data structures. STMs simplify atomicity. Recent STMs can achieve good performance for some workloads but they have some limitations. In particular, STMs typically cannot support long-running reads which access a large number of addresses that are frequently updated. Multiversioning is a common approach used to support this type of workload. However, multiversioning is often expensive and can reduce the performance of transactions where versioning is not necessary. In this work we present Multiverse, a new STM that combines the best of both unversioned TM and multiversioning. Multiverse features versioned and unversioned transactions which can execute concurrently. A main goal of Multiverse is to ensure that unversioned transactions achieve performance comparable to the state of the art unversioned STM while still supporting fast versioned transactions needed to enable long running reads. We implement Multiverse and compare it against several STMs. Our experiments demonstrate that Multiverse achieves comparable or better performance for common case workloads where there are no long running reads. For workloads with long running reads and frequent updates Multiverse significantly outperforms existing STMS. In several cases for these workloads the throughput of Multiverse is several orders of magnitude faster than other STMs.

</details>


### [54] [The "I" in FAIR: Translating from Interoperability in Principle to Interoperation in Practice](https://arxiv.org/abs/2601.10008)
*Evan Morris,Gaurav Vaidya,Phil Owen,Jason Reilly,Karamarie Fecho,Patrick Wang,Yaphet Kebede,E. Kathleen Carter,Chris Bizon*

Main category: cs.DB

TL;DR: 开发了Babel和ORION两个工具来解决FAIR数据原则在实际互操作中的挑战，通过标识符映射和数据模型转换实现知识库的真正互操作。


<details>
  <summary>Details</summary>
Motivation: 虽然许多资源使用FAIR兼容的词汇表和良好注释，但由于标识符模式和数据模型的差异，这些资源在实际中仍然难以有效互操作。

Method: 开发了两个工具：Babel通过创建标识符映射集合来解决多标识符模式问题；ORION通过将知识库转换为社区管理的通用数据模型来解决多数据模型问题。

Result: 成功创建了完全互操作的知识库库，可通过https://robokop.renci.org下载使用，展示了Babel和ORION支持数据互操作的能力。

Conclusion: Babel和ORION工具能够有效弥合FAIR原则的理论互操作与实际互操作之间的差距，为科学数据生态系统提供实用的互操作解决方案。

Abstract: The FAIR (Findable, Accessible, Interoperable, and Reusable) data principles [1] promote the interoperability of scientific data by encouraging the use of persistent identifiers, standardized vocabularies, and formal metadata structures. Many resources are created using vocabularies that are FAIR-compliant and well-annotated, yet the collective ecosystem of these resources often fails to interoperate effectively in practice. This continued challenge is mainly due to variation in identifier schemas and data models used in these resources. We have created two tools to bridge the chasm between interoperability in principle and interoperation in practice. Babel solves the problem of multiple identifier schemes by producing a curated set of identifier mappings to create cliques of equivalent identifiers that are exposed through high-performance APIs. ORION solves the problems of multiple data models by ingesting knowledge bases and transforming them into a common, community-managed data model. Here, we describe Babel and ORION and demonstrate their ability to support data interoperation. A library of fully interoperable knowledge bases created through the application of Babel and ORION is available for download and use at https://robokop.renci.org.

</details>


### [55] [Redundancy-Driven Top-$k$ Functional Dependency Discovery](https://arxiv.org/abs/2601.10130)
*Xiaolong Wan,Xixian Han*

Main category: cs.DB

TL;DR: 提出SDP算法，通过冗余计数排名发现top-k函数依赖，使用单调上界剪枝搜索空间，显著提升大规模高维数据上的FD发现效率。


<details>
  <summary>Details</summary>
Motivation: 传统FD发现算法存在两个主要问题：1) 计算成本过高，复杂度随元组数平方增长、随属性数指数增长；2) 结果集过大，难以识别有用的依赖关系。

Method: 提出SDP算法，基于冗余计数排名发现top-k函数依赖，利用冗余计数的单调上界进行剪枝搜索。采用三种优化：按分区基数排序属性、使用分区基数矩阵收紧边界、全局调度器优先探索有希望的分支。

Result: 在40多个数据集上的实验表明，SDP比穷举方法快得多且内存使用更少，能够高效发现top-k函数依赖。

Conclusion: SDP通过选择性发现和剪枝策略，有效解决了大规模高维数据上函数依赖发现的效率和可管理性问题，冗余计数排名直接关联存储开销和更新异常。

Abstract: Functional dependencies (FDs) are basic constraints in relational databases and are used for many data management tasks. Most FD discovery algorithms find all valid dependencies, but this causes two problems. First, the computational cost is prohibitive: computational complexity grows quadratically with the number of tuples and exponentially with the number of attributes, making discovery slow on large-scale and high-dimensional data. Second, the result set can be huge, making it hard to identify useful dependencies. We propose SDP (Selective-Discovery-and-Prune), which discovers the top-$k$ FDs ranked by redundancy count. Redundancy count measures how much duplicated information an FD explains and connects directly to storage overhead and update anomalies. SDP uses an upper bound on redundancy to prune the search space. It is proved that this upper bound is monotone: adding attributes refines partitions and thus decreases the bound. Once the bound falls below the top-$k$ threshold, the entire branch can be skipped. We improve SDP with three optimizations: ordering attributes by partition cardinality, using pairwise statistics in a Partition Cardinality Matrix to tighten bounds, and a global scheduler to explore promising branches first. Experiments on over 40 datasets show that SDP is much faster and uses less memory than exhaustive methods.

</details>


### [56] [Improving Database Performance by Application-side Transaction Merging](https://arxiv.org/abs/2601.10596)
*Xueyuan Ren,Frank Li,Yang Wang*

Main category: cs.DB

TL;DR: 提出TransactionMerger中间件，通过合并结构相似的SQL语句和事务来提升应用端事务处理性能，在TPC-C和Spree应用中分别实现最高2.65倍和3.52倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 传统数据库优化主要关注数据库内部，而应用端的事务处理性能优化机会被忽视。结构相似的事务和语句存在合并潜力，可以减少冗余操作和争用，从而提升整体性能。

Method: 1) 基于SQL语义合并相似语句；2) 消除冗余读取；3) 通过预计算聚合效果合并跨事务的争用语句。开发TransactionMerger中间件收集和合并不同客户端的事务，并提供静态分析工具识别合并机会而不违反隔离性。

Result: 在TPC-C基准测试中吞吐量提升最高达2.65倍，在实际应用Spree中吞吐量提升达3.52倍，验证了事务合并方法的有效性。

Conclusion: 应用端事务合并是提升事务处理性能的有效途径，TransactionMerger中间件和静态分析工具能够在不违反隔离性的前提下识别和利用合并机会，显著提升系统吞吐量。

Abstract: This paper explores a new opportunity to improve the performance of transaction processing at the application side by merging structurely similar statements or transactions. Concretely, we re-write transactions to 1) merge similar statements using specific SQL semantics; 2) eliminate redundant reads; and 3) merge contending statements across transactions by pre-computing their aggregated effect. Following this idea, we present the design of TransactionMerger, a middleware to collect and merge transactions across different clients. We further present a static analysis tool to identify the merging opportunity without violating isolation as well as our experience of re-writing transactions in TPC-C and Spree, a popular real-world application. Our evaluation shows that such transaction merging can improve TPC-C throughput by up to 2.65X and Spree throughput by 3.52X.

</details>


### [57] [Translating database mathematical schemes into relational database software applications with MatBase](https://arxiv.org/abs/2601.10604)
*Christian Mancas,Diana Christina Mancas*

Main category: cs.DB

TL;DR: 提出将数学数据模型转换为关系模式的算法，证明其高效、可靠、完整且最优，并应用于家谱树建模示例


<details>
  <summary>Details</summary>
Motivation: 需要将数学数据模型（Elementary Mathematical Data Model）转换为关系数据库模式，以便在MatBase智能数据库管理系统中使用，同时保持非关系约束的完整性

Method: 开发伪代码算法，将数学数据模型方案转换为关系模式及相应的非关系约束集，证明算法的性能特性，并以家谱树建模为例进行应用

Result: 算法被证明非常快速、可靠、完整且最优，成功应用于家谱树建模，提供了SQL和VBA代码示例来强制执行非关系约束

Conclusion: 该算法有效地将数学数据模型转换为关系数据库模式，为MatBase系统提供了实用的转换工具，并通过代码示例展示了约束执行方法

Abstract: We present a pseudocode algorithm for translating our (Elementary) Mathematical Data Model schemes into relational ones and associated sets of non-relational constraints, used by MatBase, our intelligent database management system prototype. We prove that this algorithm is very fast, solid, complete, and optimal. We apply it to a mathematical scheme modeling the genealogical trees subuniverse. We also provide examples of SQL and VBA code for enforcing some of its non-relational constraints, as well as guidelines to develop code for enforcing such constraints.

</details>
