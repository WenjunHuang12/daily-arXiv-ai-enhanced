<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 17]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.IT](#cs.IT) [Total: 4]
- [cs.LG](#cs.LG) [Total: 92]
- [cs.AI](#cs.AI) [Total: 10]
- [cs.DB](#cs.DB) [Total: 3]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering](https://arxiv.org/abs/2508.14052)
*Chanyeol Choi,Jihoon Kwon,Alejandro Lopez-Lira,Chaewoon Kim,Minjae Kim,Juneha Hwang,Jaeseon Ha,Hojun Choi,Suyeol Yun,Yongjin Kim,Yongjae Lee*

Main category: cs.IR

TL;DR: FinAgentBench是首个金融领域多步推理检索的大规模基准测试，包含3429个专家标注的S&P-100公司案例，评估LLM代理在金融文档检索中的两步推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统检索方法在金融领域准确性不足，需要同时捕获语义相似性和进行细粒度推理。虽然LLM为多步推理检索提供了新机会，但金融领域缺乏相应的评估基准。

Method: 构建包含3429个专家标注样本的基准测试，评估LLM代理的两个推理步骤：(1)识别最相关文档类型；(2)在选定文档中定位关键段落。采用分离式评估框架解决上下文限制问题。

Result: 评估了多个最先进模型，并证明针对性微调能显著提升代理检索性能。基准为研究金融领域复杂任务中检索导向的LLM行为提供了量化基础。

Conclusion: FinAgentBench填补了金融领域多步推理检索评估的空白，为研究复杂领域特定任务中的LLM检索行为奠定了基础，未来计划扩展到S&P 500及更广泛范围。

Abstract: Accurate information retrieval (IR) is critical in the financial domain,
where investors must identify relevant information from large collections of
documents. Traditional IR methods-whether sparse or dense-often fall short in
retrieval accuracy, as it requires not only capturing semantic similarity but
also performing fine-grained reasoning over document structure and
domain-specific knowledge. Recent advances in large language models (LLMs) have
opened up new opportunities for retrieval with multi-step reasoning, where the
model ranks passages through iterative reasoning about which information is
most relevant to a given query. However, there exists no benchmark to evaluate
such capabilities in the financial domain. To address this gap, we introduce
FinAgentBench, the first large-scale benchmark for evaluating retrieval with
multi-step reasoning in finance -- a setting we term agentic retrieval. The
benchmark consists of 3,429 expert-annotated examples on S&P-100 listed firms
and assesses whether LLM agents can (1) identify the most relevant document
type among candidates, and (2) pinpoint the key passage within the selected
document. Our evaluation framework explicitly separates these two reasoning
steps to address context limitations. This design enables to provide a
quantitative basis for understanding retrieval-centric LLM behavior in finance.
We evaluate a suite of state-of-the-art models and further demonstrated how
targeted fine-tuning can significantly improve agentic retrieval performance.
Our benchmark provides a foundation for studying retrieval-centric LLM behavior
in complex, domain-specific tasks for finance. We will release the dataset
publicly upon acceptance of the paper and plan to expand and share dataset for
the full S&P 500 and beyond.

</details>


### [2] [Dual-Phase Playtime-guided Recommendation: Interest Intensity Exploration and Multimodal Random Walks](https://arxiv.org/abs/2508.14058)
*Jingmao Zhang,Zhiting Zhao,Yunqi Lin,Jianghong Ma,Tianjun Wei,Haijun Zhang,Xiaofeng Zhang*

Main category: cs.IR

TL;DR: DP2Rec是一个双阶段游戏推荐模型，通过利用游戏时长和多模态信息，同时优化推荐准确性和多样性


<details>
  <summary>Details</summary>
Motivation: 游戏行业快速发展需要可扩展的推荐系统，现有模型未能充分利用游戏时长这一独特行为信号，也忽视了多模态信息提升多样性的潜力

Method: 提出双阶段模型：1）游戏时长引导的兴趣强度探索模块，通过双beta建模区分强弱偏好；2）游戏时长引导的多模态随机游走模块，结合兴趣相似性和语义相似性模拟玩家探索

Result: 在真实游戏数据集上的实验表明，DP2Rec在推荐准确性和多样性方面均优于现有方法

Conclusion: DP2Rec通过有效利用游戏时长和多模态信息，成功实现了准确性和多样性的联合优化，为游戏推荐系统提供了新的解决方案

Abstract: The explosive growth of the video game industry has created an urgent need
for recommendation systems that can scale with expanding catalogs and maintain
user engagement. While prior work has explored accuracy and diversity in
recommendations, existing models underutilize playtime, a rich behavioral
signal unique to gaming platforms, and overlook the potential of multimodal
information to enhance diversity. In this paper, we propose DP2Rec, a novel
Dual-Phase Playtime-guided Recommendation model designed to jointly optimize
accuracy and diversity. First, we introduce a playtime-guided interest
intensity exploration module that separates strong and weak preferences via
dual-beta modeling, enabling fine-grained user profiling and more accurate
recommendations. Second, we present a playtime-guided multimodal random walks
module that simulates player exploration using transitions guided by both
playtime-derived interest similarity and multimodal semantic similarity. This
mechanism preserves core preferences while promoting cross-category discovery
through latent semantic associations and adaptive category balancing. Extensive
experiments on a real-world game dataset show that DP2Rec outperforms existing
methods in both recommendation accuracy and diversity.

</details>


### [3] [Graph Neural Network for Product Recommendation on the Amazon Co-purchase Graph](https://arxiv.org/abs/2508.14059)
*Mengyang Cao,Frank F. Yang,Yi Jin,Yijun Yan*

Main category: cs.IR

TL;DR: 本研究评估了四种GNN架构在亚马逊产品共购网络上的链接预测性能，分析了不同模型在性能、可扩展性、训练复杂度和泛化能力方面的权衡


<details>
  <summary>Details</summary>
Motivation: 在海量数据中识别相关信息是现代推荐系统的挑战，图神经网络通过图结构学习利用结构和语义关系显示出巨大潜力

Method: 在亚马逊产品共购网络上评估LightGCN、GraphSAGE、GAT和PinSAGE四种GNN架构的链接预测能力

Result: 展示了每种模型在实际推荐场景部署中的性能特征和实用权衡

Conclusion: 研究结果为在实际推荐系统中部署GNN提供了不同架构的性能特征和实用权衡分析

Abstract: Identifying relevant information among massive volumes of data is a challenge
for modern recommendation systems. Graph Neural Networks (GNNs) have
demonstrated significant potential by utilizing structural and semantic
relationships through graph-based learning. This study assessed the abilities
of four GNN architectures, LightGCN, GraphSAGE, GAT, and PinSAGE, on the Amazon
Product Co-purchase Network under link prediction settings. We examined
practical trade-offs between architectures, model performance, scalability,
training complexity and generalization. The outcomes demonstrated each model's
performance characteristics for deploying GNN in real-world recommendation
scenarios.

</details>


### [4] [GPT-2 as a Compression Preprocessor: Improving Gzip for Structured Text Domains](https://arxiv.org/abs/2508.14061)
*Anurag Kumar Ojha*

Main category: cs.IR

TL;DR: 提出基于GPT-2的预处理管道，用于处理gzip难以压缩的领域特定文件（如JSON、XML、HTML、日志文件），通过语义重复检测提升压缩效率


<details>
  <summary>Details</summary>
Motivation: 传统压缩算法如gzip基于二进制模式重复，但领域特定文件虽有语义重复却缺乏语法重复，导致压缩效果不佳，需要专门解决方案

Method: 使用GPT-2作为预处理器，将领域特定文件转换为gzip友好的格式，然后由gzip进行常规压缩，形成端到端处理管道

Result: 实验显示在真实和合成数据上取得显著改进，防御日志压缩率提升0.34%，HTML文件压缩率提升5.8%

Conclusion: GPT-based预处理方法能有效解决领域特定文件的压缩难题，为结构化但语法不重复的数据提供了高效的压缩解决方案

Abstract: In the modern era, large volumes of data are being produced continuously,
especially in domain-specific fields such as medical records and clinical
files, defence logs and HTML-based web traffic. Data with such volume and
complexity needs to be compressed before storing and transmitting efficiently.
Data compression has gained significant attention from modern researchers,
resulting in the development of fast and efficient compression algorithms such
as Gzip. However, since gzip works on the principle of repetition of binary
patterns, one of the limitations of gzip is that domain-specific formats like
JSON, XML, HTML, and log files, while structured, may have semantic repetition
but not syntactic repetition, which gzip finds difficult to compress. In this
article, we propose a GPT-based preprocessor for such domain-specific files. We
propose a pipeline made up of GPT-2 taking domain-specific files as input,
which pattern-based compressors like gzip find difficult to work on. The
preprocessor results are output in a file that is designed for compressors like
gzip. After preprocessing, the gzip works on the other end of the pipeline and
compresses the data as usual. We used different types of both real-world and
synthetically generated data, such as logs and HTML files, for the experiment
of the proposed model. We found promising results and an improvement of the
Defence logs by 0.34 per cent and HTML files by 5.8 per cent.

</details>


### [5] [A Multi-Agent Approach to Neurological Clinical Reasoning](https://arxiv.org/abs/2508.14063)
*Moran Sorka,Alon Gorenshtein,Dvir Aran,Shahar Shelly*

Main category: cs.IR

TL;DR: 该研究系统评估了大型语言模型在神经学推理任务上的表现，开发了基于神经学认证考试的基准测试，发现多智能体框架能显著提升复杂医学推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医学领域显示出潜力，但处理专业神经学推理的能力需要系统评估，特别是在复杂临床推理方面。

Method: 使用305个以色列神经学认证考试问题构建基准测试，按三个复杂度维度分类。评估了10个LLM的基础模型、检索增强生成(RAG)和新型多智能体系统。多智能体框架将神经学推理分解为问题分析、知识检索、答案合成和验证等专门认知功能。

Result: OpenAI-o1基础模型表现最佳(90.9%准确率)，专业医学模型表现较差(Meditron-70B为52.9%)。RAG效果有限，而多智能体框架带来显著提升：LLaMA 3.3-70B从69.5%提升至89.2%，在复杂度3级问题上获得大幅增益，并将不一致的亚专业表现转化为统一优秀。

Conclusion: 结构化多智能体方法能显著增强复杂医学推理能力，模拟专门认知过程的方法为解决具有挑战性的临床场景中的AI辅助提供了有前景的方向。

Abstract: Large language models (LLMs) have shown promise in medical domains, but their
ability to handle specialized neurological reasoning requires systematic
evaluation. We developed a comprehensive benchmark using 305 questions from
Israeli Board Certification Exams in Neurology, classified along three
complexity dimensions: factual knowledge depth, clinical concept integration,
and reasoning complexity. We evaluated ten LLMs using base models,
retrieval-augmented generation (RAG), and a novel multi-agent system. Results
showed significant performance variation. OpenAI-o1 achieved the highest base
performance (90.9% accuracy), while specialized medical models performed poorly
(52.9% for Meditron-70B). RAG provided modest benefits but limited
effectiveness on complex reasoning questions. In contrast, our multi-agent
framework, decomposing neurological reasoning into specialized cognitive
functions including question analysis, knowledge retrieval, answer synthesis,
and validation, achieved dramatic improvements, especially for mid-range
models. The LLaMA 3.3-70B-based agentic system reached 89.2% accuracy versus
69.5% for its base model, with substantial gains on level 3 complexity
questions. The multi-agent approach transformed inconsistent subspecialty
performance into uniform excellence, addressing neurological reasoning
challenges that persisted with RAG enhancement. We validated our approach using
an independent dataset of 155 neurological cases from MedQA. Results confirm
that structured multi-agent approaches designed to emulate specialized
cognitive processes significantly enhance complex medical reasoning, offering
promising directions for AI assistance in challenging clinical contexts.

</details>


### [6] [An automatic patent literature retrieval system based on LLM-RAG](https://arxiv.org/abs/2508.14064)
*Yao Ding,Yuqing Wu,Ziyang Ding*

Main category: cs.IR

TL;DR: 本研究提出了一个集成大型语言模型(LLM)和检索增强生成(RAG)技术的自动化专利检索框架，在Google专利数据集上实现了80.5%的语义匹配准确率和92.1%的召回率，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的关键词和基于规则的专利检索方法难以处理复杂查询意图和跨技术领域的语义关联，导致检索结果不完整且相关性低。

Method: 系统包含三个组件：1)专利数据标准化预处理模块；2)基于LLM生成嵌入的高效向量检索引擎；3)结合外部文档检索和上下文感知响应生成的RAG增强查询模块。

Result: 在包含数百万全球专利记录的Google Patents数据集上，gpt-3.5-turbo-0125-RAG配置实现了80.5%的语义匹配准确率和92.1%的召回率，比基线LLM方法提高了28个百分点。

Conclusion: LLM-RAG集成在智能专利检索中具有显著效果，为下一代AI驱动的知识产权分析平台奠定了基础，在跨领域分类和语义聚类任务中表现出强大的泛化能力。

Abstract: With the acceleration of technological innovation efficient retrieval and
classification of patent literature have become essential for intellectual
property management and enterprise RD Traditional keyword and rulebased
retrieval methods often fail to address complex query intents or capture
semantic associations across technical domains resulting in incomplete and
lowrelevance results This study presents an automated patent retrieval
framework integrating Large Language Models LLMs with RetrievalAugmented
Generation RAG technology The system comprises three components: 1) a
preprocessing module for patent data standardization, 2) a highefficiency
vector retrieval engine leveraging LLMgenerated embeddings, and 3) a
RAGenhanced query module that combines external document retrieval with
contextaware response generation Evaluations were conducted on the Google
Patents dataset 20062024 containing millions of global patent records with
metadata such as filing date domain and status The proposed gpt35turbo0125RAG
configuration achieved 805 semantic matching accuracy and 92.1% recall
surpassing baseline LLM methods by 28 percentage points The framework also
demonstrated strong generalization in crossdomain classification and semantic
clustering tasks These results validate the effectiveness of LLMRAG integration
for intelligent patent retrieval providing a foundation for nextgeneration
AIdriven intellectual property analysis platforms

</details>


### [7] [Personalized Contest Recommendation in Fantasy Sports](https://arxiv.org/abs/2508.14065)
*Madiraju Srilakshmi,Kartavya Kothari,Kamlesh Marathe,Vedavyas Chigurupati,Hitesh Kapoor*

Main category: cs.IR

TL;DR: 本文提出了一个基于Wide and Deep Interaction Ranker (WiDIR)的可扩展竞赛推荐系统，用于每日梦幻体育中的竞赛个性化推荐，在实际平台部署中显著提升了召回率和关键业务指标。


<details>
  <summary>Details</summary>
Motivation: 梦幻体育平台上有大量不同费用、名额和奖池分布的竞赛，玩家偏好各异，需要通过竞赛个性化来匹配玩家与合适的竞赛。

Method: 使用Wide and Deep Interaction Ranker (WiDIR)作为核心推荐模型，构建可扩展的竞赛推荐系统。

Result: 在拥有数百万日竞赛和玩家的梦幻体育平台上线实验显示，相比其他候选模型，在召回率和关键业务指标上有显著提升。

Conclusion: WiDIR模型能够有效实现梦幻体育竞赛的个性化推荐，为大规模平台提供可扩展的推荐解决方案。

Abstract: In daily fantasy sports, players enter into "contests" where they compete
against each other by building teams of athletes that score fantasy points
based on what actually occurs in a real-life sports match. For any given sports
match, there are a multitude of contests available to players, with substantial
variation across 3 main dimensions: entry fee, number of spots, and the prize
pool distribution. As player preferences are also quite heterogeneous, contest
personalization is an important tool to match players with contests. This paper
presents a scalable contest recommendation system, powered by a Wide and Deep
Interaction Ranker (WiDIR) at its core. We productionized this system at our
company, one of the large fantasy sports platforms with millions of daily
contests and millions of players, where online experiments show a marked
improvement over other candidate models in terms of recall and other critical
business metrics.

</details>


### [8] [Retrieval-Augmented Generation in Industry: An Interview Study on Use Cases, Requirements, Challenges, and Evaluation](https://arxiv.org/abs/2508.14066)
*Lorenz Brehme,Benedikt Dornauer,Thomas Ströhle,Maximilian Ehrhart,Ruth Breu*

Main category: cs.IR

TL;DR: 该研究通过13位业界专家访谈调查了RAG技术在工业界的应用状况，发现当前主要集中在领域特定QA任务，系统仍处于原型阶段，数据预处理是主要挑战，评估主要依靠人工而非自动化方法。


<details>
  <summary>Details</summary>
Motivation: 虽然RAG技术在AI领域得到广泛关注并快速发展，但工业界实际应用方面的研究仍很缺乏。本研究旨在探索RAG在实际业界环境中的采用状况和应用挑战。

Method: 采用半结构化访谈研究方法，访谈13位业界实践者，分析他们在实际业务中应用RAG的经验。

Result: 研究发现：(1)工业应用主要集中在领域特定QA任务；(2)系统要求主要关注数据保护、安全和质量；(3)数据预处理是主要挑战；(4)系统评估主要依靠人工方法。

Conclusion: 当前RAG在工业界的应用仍处于初级阶段，需要更多关注数据预处理、系统评估自动化以及伦理偏见等被忽视的问题，以促进RAG技术在业界的成熟应用。

Abstract: Retrieval-Augmented Generation (RAG) is a well-established and rapidly
evolving field within AI that enhances the outputs of large language models by
integrating relevant information retrieved from external knowledge sources.
While industry adoption of RAG is now beginning, there is a significant lack of
research on its practical application in industrial contexts. To address this
gap, we conducted a semistructured interview study with 13 industry
practitioners to explore the current state of RAG adoption in real-world
settings. Our study investigates how companies apply RAG in practice, providing
(1) an overview of industry use cases, (2) a consolidated list of system
requirements, (3) key challenges and lessons learned from practical
experiences, and (4) an analysis of current industry evaluation methods. Our
main findings show that current RAG applications are mostly limited to
domain-specific QA tasks, with systems still in prototype stages; industry
requirements focus primarily on data protection, security, and quality, while
issues such as ethics, bias, and scalability receive less attention; data
preprocessing remains a key challenge, and system evaluation is predominantly
conducted by humans rather than automated methods.

</details>


### [9] [RewardRank: Optimizing True Learning-to-Rank Utility](https://arxiv.org/abs/2508.14180)
*Gaurav Bhatt,Kiran Koshy Thekumparampil,Tanmay Gangwani,Tesi Xiao,Leonid Sigal*

Main category: cs.IR

TL;DR: RewardRank框架通过反事实奖励学习建模用户行为，使用深度效用模型估计排列效用，并通过可微分排序优化策略，在大型基准测试中显著优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统排序系统使用简化的代理损失函数，无法捕捉真实用户行为中的复杂偏差（如位置偏差、品牌偏好等），导致模型与用户实际效用不匹配

Method: 1) 训练深度效用模型估计整个排列的用户参与度 2) 通过可微分软排列操作优化排序策略以最大化预测效用 3) 引入KD-Eval和LLM-Eval两种自动化评估协议

Result: 在Baidu-ULTR和Amazon KDD Cup等大规模基准测试中，该方法 consistently outperforms强基线方法

Conclusion: 建模用户行为动态对于效用优化的排序至关重要，RewardRank框架有效解决了传统方法无法捕捉复杂用户偏好的问题

Abstract: Traditional ranking systems rely on proxy loss functions that assume
simplistic user behavior, such as users preferring a rank list where items are
sorted by hand-crafted relevance. However, real-world user interactions are
influenced by complex behavioral biases, including position bias, brand
affinity, decoy effects, and similarity aversion, which these objectives fail
to capture. As a result, models trained on such losses often misalign with
actual user utility, such as the probability of any click or purchase across
the ranked list. In this work, we propose a data-driven framework for modeling
user behavior through counterfactual reward learning. Our method, RewardRank,
first trains a deep utility model to estimate user engagement for entire item
permutations using logged data. Then, a ranking policy is optimized to maximize
predicted utility via differentiable soft permutation operators, enabling
end-to-end training over the space of factual and counterfactual rankings. To
address the challenge of evaluation without ground-truth for unseen
permutations, we introduce two automated protocols: (i) $\textit{KD-Eval}$,
using a position-aware oracle for counterfactual reward estimation, and (ii)
$\textit{LLM-Eval}$, which simulates user preferences via large language
models. Experiments on large-scale benchmarks, including Baidu-ULTR and the
Amazon KDD Cup datasets, demonstrate that our approach consistently outperforms
strong baselines, highlighting the effectiveness of modeling user behavior
dynamics for utility-optimized ranking. Our code is available at:
https://github.com/GauravBh1010tt/RewardRank

</details>


### [10] [You Only Evaluate Once: A Tree-based Rerank Method at Meituan](https://arxiv.org/abs/2508.14420)
*Shuli Wang,Yinqiu Huang,Changhao Li,Yuan Zhou,Yonggang Liu,Yongqiang Zhang,Yinhua Zhu,Haitao Wang,Xingxing Wang*

Main category: cs.IR

TL;DR: YOLOR提出了一种单阶段重排序方法，通过移除通用搜索单元(GSU)并保留精确搜索单元(ESU)来解决传统两阶段方法中的不一致性问题，实现了同时兼顾效果和效率。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段重排序方法存在严重的不一致性问题，即GSU经常错过ESU中的高价值列表，需要在效果和效率之间进行权衡。

Method: YOLOR包含两个核心模块：1)基于树的上下文提取模块(TCEM)分层聚合多尺度上下文特征实现列表级效果；2)上下文缓存模块(CCM)实现候选排列间特征高效重用实现排列级效率。

Result: 在公共和工业数据集上的广泛实验验证了YOLOR的性能，并已成功部署在美团外卖平台上。

Conclusion: YOLOR通过单阶段设计有效解决了重排序中的不一致性问题，在保持高效的同时实现了更好的推荐效果。

Abstract: Reranking plays a crucial role in modern recommender systems by capturing the
mutual influences within the list. Due to the inherent challenges of
combinatorial search spaces, most methods adopt a two-stage search paradigm: a
simple General Search Unit (GSU) efficiently reduces the candidate space, and
an Exact Search Unit (ESU) effectively selects the optimal sequence. These
methods essentially involve making trade-offs between effectiveness and
efficiency, while suffering from a severe \textbf{inconsistency problem}, that
is, the GSU often misses high-value lists from ESU. To address this problem, we
propose YOLOR, a one-stage reranking method that removes the GSU while
retaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context
Extraction Module (TCEM) that hierarchically aggregates multi-scale contextual
features to achieve "list-level effectiveness", and (2) a Context Cache Module
(CCM) that enables efficient feature reuse across candidate permutations to
achieve "permutation-level efficiency". Extensive experiments across public and
industry datasets validate YOLOR's performance, and we have successfully
deployed YOLOR on the Meituan food delivery platform.

</details>


### [11] [Diverse Negative Sampling for Implicit Collaborative Filtering](https://arxiv.org/abs/2508.14468)
*Yueqing Xuan,Kacper Sokol,Mark Sanderson,Jeffrey Chan*

Main category: cs.IR

TL;DR: 提出DivNS方法，通过多样性负采样解决传统方法负样本同质化问题，提升推荐模型表达能力和泛化性能


<details>
  <summary>Details</summary>
Motivation: 传统负采样策略从密集区域过采样负样本，导致负训练数据同质化，限制了模型表达能力

Method: DivNS方法包含三个步骤：1) 寻找困难负样本并构建用户特定缓存；2) 多样性增强采样器从缓存中选择多样化负样本；3) 合成负样本生成器结合多样性和困难负样本形成更有效的训练数据

Result: 在四个公开数据集上的大量实验证明DivNS能有效提升推荐质量，同时保持计算效率

Conclusion: DivNS通过生成既信息丰富又多样化的合成负样本，使推荐器能够学习更广泛的物品空间，提高泛化能力

Abstract: Implicit collaborative filtering recommenders are usually trained to learn
user positive preferences. Negative sampling, which selects informative
negative items to form negative training data, plays a crucial role in this
process. Since items are often clustered in the latent space, existing negative
sampling strategies normally oversample negative items from the dense regions.
This leads to homogeneous negative data and limited model expressiveness. In
this paper, we propose Diverse Negative Sampling (DivNS), a novel approach that
explicitly accounts for diversity in negative training data during the negative
sampling process. DivNS first finds hard negative items with large preference
scores and constructs user-specific caches that store unused but highly
informative negative samples. Then, its diversity-augmented sampler selects a
diverse subset of negative items from the cache while ensuring dissimilarity
from the user's hard negatives. Finally, a synthetic negatives generator
combines the selected diverse negatives with hard negatives to form more
effective training data. The resulting synthetic negatives are both informative
and diverse, enabling recommenders to learn a broader item space and improve
their generalisability. Extensive experiments on four public datasets
demonstrate the effectiveness of DivNS in improving recommendation quality
while maintaining computational efficiency.

</details>


### [12] [Distribution-Guided Auto-Encoder for User Multimodal Interest Cross Fusion](https://arxiv.org/abs/2508.14485)
*Moyu Zhang,Yongxiang Tang,Yujun Jin,Jinxin Hu,Yu Zhang*

Main category: cs.IR

TL;DR: DMAE模型通过分布引导的多模态兴趣自编码器，在行为级别实现用户多模态兴趣的交叉融合，解决了传统ID推荐方法的数据稀疏问题和现有多模态推荐方法忽略用户行为序列上下文影响的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于ID的推荐方法面临数据稀疏性问题，现有多模态推荐方法主要采用早期融合方式，专注于文本和图像特征的结合，但忽视了用户行为序列的上下文影响，无法基于行为模式动态调整多模态兴趣表示。

Method: 提出分布引导的多模态兴趣自编码器(DMAE)，在行为级别实现用户多模态兴趣的交叉融合，能够动态适应用户行为模式来调整多模态兴趣表示。

Result: 大量实验证明了DMAE方法的优越性，在推荐准确性方面表现突出。

Conclusion: DMAE通过行为级别的多模态兴趣融合，有效解决了传统推荐方法的数据稀疏问题，并能够更好地捕捉用户的多模态兴趣，显著提升了推荐性能。

Abstract: Traditional recommendation methods rely on correlating the embedding vectors
of item IDs to capture implicit collaborative filtering signals to model the
user's interest in the target item. Consequently, traditional ID-based methods
often encounter data sparsity problems stemming from the sparse nature of ID
features. To alleviate the problem of item ID sparsity, recommendation models
incorporate multimodal item information to enhance recommendation accuracy.
However, existing multimodal recommendation methods typically employ early
fusion approaches, which focus primarily on combining text and image features,
while neglecting the contextual influence of user behavior sequences. This
oversight prevents dynamic adaptation of multimodal interest representations
based on behavioral patterns, consequently restricting the model's capacity to
effectively capture user multimodal interests. Therefore, this paper proposes
the Distribution-Guided Multimodal-Interest Auto-Encoder (DMAE), which achieves
the cross fusion of user multimodal interest at the behavioral
level.Ultimately, extensive experiments demonstrate the superiority of DMAE.

</details>


### [13] [Global-Distribution Aware Scenario-Specific Variational Representation Learning Framework](https://arxiv.org/abs/2508.14493)
*Moyu Zhang,Yujun Jin,Jinxin Hu,Yu Zhang*

Main category: cs.IR

TL;DR: 提出了GSVR框架，通过变分推理和全局知识引导，为多场景推荐学习鲁棒的场景特定表示，解决数据稀疏性问题


<details>
  <summary>Details</summary>
Motivation: 当前多场景推荐方法使用共享底层表示，无法充分捕捉场景独特性，且跨场景数据稀疏阻碍了场景特定表示的学习

Method: 使用概率模型生成用户和物品在每个场景中的场景特定分布，通过变分推理估计，并引入全局知识感知的多项分布作为先验知识来规范后验分布学习

Result: 大量实验结果证实GSVR能有效帮助现有多场景推荐方法学习更鲁棒的表示

Conclusion: GSVR框架能够通过学习场景特定的变分表示，有效解决多场景推荐中的数据稀疏问题，提升推荐性能

Abstract: With the emergence of e-commerce, the recommendations provided by commercial
platforms must adapt to diverse scenarios to accommodate users' varying
shopping preferences. Current methods typically use a unified framework to
offer personalized recommendations for different scenarios. However, they often
employ shared bottom representations, which partially hinders the model's
capacity to capture scenario uniqueness. Ideally, users and items should
exhibit specific characteristics in different scenarios, prompting the need to
learn scenario-specific representations to differentiate scenarios. Yet,
variations in user and item interactions across scenarios lead to data sparsity
issues, impeding the acquisition of scenario-specific representations. To learn
robust scenario-specific representations, we introduce a Global-Distribution
Aware Scenario-Specific Variational Representation Learning Framework (GSVR)
that can be directly applied to existing multi-scenario methods. Specifically,
considering the uncertainty stemming from limited samples, our approach employs
a probabilistic model to generate scenario-specific distributions for each user
and item in each scenario, estimated through variational inference (VI).
Additionally, we introduce the global knowledge-aware multinomial distributions
as prior knowledge to regulate the learning of the posterior user and item
distributions, ensuring similarities among distributions for users with akin
interests and items with similar side information. This mitigates the risk of
users or items with fewer records being overwhelmed in sparse scenarios.
Extensive experimental results affirm the efficacy of GSVR in assisting
existing multi-scenario recommendation methods in learning more robust
representations.

</details>


### [14] [DGenCTR: Towards a Universal Generative Paradigm for Click-Through Rate Prediction via Discrete Diffusion](https://arxiv.org/abs/2508.14500)
*Moyu Zhang,Yun Chen,Yujun Jin,Jinxin Hu,Yu Zhang*

Main category: cs.IR

TL;DR: 提出DGenCTR框架，使用离散扩散生成模型进行CTR预测，通过两阶段训练（生成预训练+CTR微调）解决传统判别模型在标签稀缺场景的局限性


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐系统主要关注序列生成，不适合CTR预测任务。CTR模型严重依赖用户-物品交叉特征，丢弃这些特征会显著降低性能。希望利用生成模型理解数据分布的能力，缓解传统判别模型在标签稀缺空间的限制

Method: 提出两阶段离散扩散生成CTR框架：1）扩散生成预训练阶段 2）CTR目标监督微调阶段。不同于序列生成方法，采用样本级生成范式专门为CTR任务设计

Result: 通过大量离线实验和在线A/B测试，验证了框架的有效性

Conclusion: DGenCTR框架成功将生成模型应用于CTR预测任务，解决了传统方法的局限性，在标签稀缺场景下表现出色

Abstract: Recent advances in generative models have inspired the field of recommender
systems to explore generative approaches, but most existing research focuses on
sequence generation, a paradigm ill-suited for click-through rate (CTR)
prediction. CTR models critically depend on a large number of cross-features
between the target item and the user to estimate the probability of clicking on
the item, and discarding these cross-features will significantly impair model
performance. Therefore, to harness the ability of generative models to
understand data distributions and thereby alleviate the constraints of
traditional discriminative models in label-scarce space, diverging from the
item-generation paradigm of sequence generation methods, we propose a novel
sample-level generation paradigm specifically designed for the CTR task: a
two-stage Discrete Diffusion-Based Generative CTR training framework (DGenCTR).
This two-stage framework comprises a diffusion-based generative pre-training
stage and a CTR-targeted supervised fine-tuning stage for CTR. Finally,
extensive offline experiments and online A/B testing conclusively validate the
effectiveness of our framework.

</details>


### [15] [MISS: Multi-Modal Tree Indexing and Searching with Lifelong Sequential Behavior for Retrieval Recommendation](https://arxiv.org/abs/2508.14515)
*Chengcheng Guo,Junda She,Kuo Cai,Shiyao Wang,Qigen Hu,Qiang Luo,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: MISS模型通过多模态索引树和终身序列建模模块，在树基检索模型中整合多模态信息和用户终身行为序列，解决检索阶段难以利用终身行为和大规模候选项的问题


<details>
  <summary>Details</summary>
Motivation: 现有检索方法主要依赖交互信息，忽视了宝贵的多模态信息；同时检索阶段难以利用用户终身行为序列，因为候选项数量庞大

Method: 提出多模态索引树使用多模态嵌入精确表示物品相似度；设计协同通用搜索单元(Co-GSU)和多模态通用搜索单元(MM-GSU)从多角度捕捉用户兴趣

Result: 构建了能够精确表示物品相似度的多模态索引结构，并实现了对用户终身序列中多样化兴趣的精确捕捉

Conclusion: MISS是首个在先进树基检索模型中整合多模态信息和终身序列建模的探索性工作，为大规模推荐系统的检索阶段提供了新的解决方案

Abstract: Large-scale industrial recommendation systems typically employ a two-stage
paradigm of retrieval and ranking to handle huge amounts of information. Recent
research focuses on improving the performance of retrieval model. A promising
way is to introduce extensive information about users and items. On one hand,
lifelong sequential behavior is valuable. Existing lifelong behavior modeling
methods in ranking stage focus on the interaction of lifelong behavior and
candidate items from retrieval stage. In retrieval stage, it is difficult to
utilize lifelong behavior because of a large corpus of candidate items. On the
other hand, existing retrieval methods mostly relay on interaction information,
potentially disregarding valuable multi-modal information. To solve these
problems, we represent the pioneering exploration of leveraging multi-modal
information and lifelong sequence model within the advanced tree-based
retrieval model. We propose Multi-modal Indexing and Searching with lifelong
Sequence (MISS), which contains a multi-modal index tree and a multi-modal
lifelong sequence modeling module. Specifically, for better index structure, we
propose multi-modal index tree, which is built using the multi-modal embedding
to precisely represent item similarity. To precisely capture diverse user
interests in user lifelong sequence, we propose collaborative general search
unit (Co-GSU) and multi-modal general search unit (MM-GSU) for
multi-perspective interests searching.

</details>


### [16] [OneLoc: Geo-Aware Generative Recommender Systems for Local Life Service](https://arxiv.org/abs/2508.14646)
*Zhipeng Wei,Kuo Cai,Junda She,Jie Chen,Minghao Chen,Yang Zeng,Qiang Luo,Wencong Zeng,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: 快手本地生活服务场景中提出的端到端生成式推荐模型OneLoc，通过地理感知语义ID、自注意力机制和邻居感知提示，结合强化学习平衡多目标，实现GMV和订单量显著提升


<details>
  <summary>Details</summary>
Motivation: 本地生活服务推荐需要同时考虑用户兴趣和实时地理位置，现有端到端生成推荐模型无法直接应用，存在地理信息利用和多目标平衡两大挑战

Method: 提出OneLoc模型：1）地理感知语义ID融合视频和地理信息；2）地理感知自注意力编码器利用位置相似性和用户实时位置；3）邻居感知提示捕获用户周边上下文信息；使用强化学习设计地理奖励和GMV奖励函数平衡多目标

Result: 模型已部署在快手App本地生活服务，每日服务4亿活跃用户，GMV提升21.016%，订单数量提升17.891%

Conclusion: OneLoc成功解决了本地生活服务推荐中的地理信息利用和多目标平衡问题，证明了端到端生成推荐模型在该场景的有效性

Abstract: Local life service is a vital scenario in Kuaishou App, where video
recommendation is intrinsically linked with store's location information. Thus,
recommendation in our scenario is challenging because we should take into
account user's interest and real-time location at the same time. In the face of
such complex scenarios, end-to-end generative recommendation has emerged as a
new paradigm, such as OneRec in the short video scenario, OneSug in the search
scenario, and EGA in the advertising scenario. However, in local life service,
an end-to-end generative recommendation model has not yet been developed as
there are some key challenges to be solved. The first challenge is how to make
full use of geographic information. The second challenge is how to balance
multiple objectives, including user interests, the distance between user and
stores, and some other business objectives. To address the challenges, we
propose OneLoc. Specifically, we leverage geographic information from different
perspectives: (1) geo-aware semantic ID incorporates both video and geographic
information for tokenization, (2) geo-aware self-attention in the encoder
leverages both video location similarity and user's real-time location, and (3)
neighbor-aware prompt captures rich context information surrounding users for
generation. To balance multiple objectives, we use reinforcement learning and
propose two reward functions, i.e., geographic reward and GMV reward. With the
above design, OneLoc achieves outstanding offline and online performance. In
fact, OneLoc has been deployed in local life service of Kuaishou App. It serves
400 million active users daily, achieving 21.016% and 17.891% improvements in
terms of gross merchandise value (GMV) and orders numbers.

</details>


### [17] [Benefiting from Negative yet Informative Feedback by Contrasting Opposing Sequential Patterns](https://arxiv.org/abs/2508.14786)
*Veronika Ivanova,Evgeny Frolov,Alexey Vasilev*

Main category: cs.IR

TL;DR: 提出了一种使用正负反馈的双Transformer编码器序列推荐模型，通过复合损失函数整合两种反馈信号，提高推荐准确性和用户满意度


<details>
  <summary>Details</summary>
Motivation: 传统序列推荐模型主要关注正向交互，忽略了负向反馈在提升用户满意度和准确识别用户兴趣方面的重要价值

Method: 使用两个独立的Transformer编码器分别处理正向和负向交互序列，采用包含正负交叉熵和对比损失的复合损失函数来建模对立模式

Result: 相比最先进的序列推荐方法，该方法在提高真正例指标的同时减少了错误推荐的负向项目数量

Conclusion: 整合正负反馈信号能够显著提升序列推荐系统的性能，负向反馈为准确识别用户兴趣提供了有价值的信号

Abstract: We consider the task of learning from both positive and negative feedback in
a sequential recommendation scenario, as both types of feedback are often
present in user interactions. Meanwhile, conventional sequential learning
models usually focus on considering and predicting positive interactions,
ignoring that reducing items with negative feedback in recommendations improves
user satisfaction with the service. Moreover, the negative feedback can
potentially provide a useful signal for more accurate identification of true
user interests. In this work, we propose to train two transformer encoders on
separate positive and negative interaction sequences. We incorporate both types
of feedback into the training objective of the sequential recommender using a
composite loss function that includes positive and negative cross-entropy as
well as a cleverly crafted contrastive term, that helps better modeling
opposing patterns. We demonstrate the effectiveness of this approach in terms
of increasing true-positive metrics compared to state-of-the-art sequential
recommendation methods while reducing the number of wrongly promoted negative
items.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [18] [Algorithms for Stable Roommate with Externalities](https://arxiv.org/abs/2508.14194)
*Jing Leng,Sanjukta Roy*

Main category: cs.GT

TL;DR: 研究室友匹配问题中4人稳定和2人稳定分配的效率与策略证明性，提出串行独裁算法实现4PS的帕累托最优和策略证明，分析TTC算法的局限性，并识别多项式时间内可找到2PS分配的特殊偏好结构


<details>
  <summary>Details</summary>
Motivation: 在室友匹配模型中，当代理人对房间和室友都有偏好时，研究不同类型稳定分配（4PS和2PS）的效率特性和策略证明性，填补现有研究的空白

Method: 设计基于串行独裁的算法实现4PS分配，分析TTC算法的变体，识别特殊偏好结构使得2PS分配可在多项式时间内找到

Result: 串行独裁算法能产生帕累托最优且策略证明的4PS分配，但无法实现2PS；TTC变体无法同时满足策略证明和帕累托最优；在某些偏好结构下可多项式时间找到2PS分配

Conclusion: 4PS分配可通过简单算法实现理想性质，而2PS分配在一般情况下计算困难，但在特定偏好结构下可高效求解

Abstract: In the roommate matching model, given a set of 2n agents and n rooms, we find
an assignment of a pair of agents to a room. Although the roommate matching
problem is well studied, the study of the model when agents have preference
over both rooms and roommates was recently initiated by Chan et al. [11]. We
study two types of stable roommate assignments, namely, 4-person stable (4PS)
and 2-person stable (2PS) in conjunction with efficiency and
strategy-proofness. We design a simple serial dictatorship based algorithm for
finding a 4PS assignment that is Pareto optimal and strategy-proof. However,
the serial dictatorship algorithm is far from being 2PS. Next, we study top
trading cycle (TTC) based algorithms. We show that variations of TTC cannot be
strategy-proof or PO. Finally, as Chan et al. (2016) showed that deciding the
existence of 2PS assignment is NP-complete, we identify preference structures
where a 2PS assignment can be found in polynomial time.

</details>


### [19] [Explainable Information Design](https://arxiv.org/abs/2508.14196)
*Yiling Chen,Tao Lin,Wei Tang,Jamie Tucker-Foltz*

Main category: cs.GT

TL;DR: 本文研究可解释信息设计，限制使用K-划分信号方案（确定性单调划分），证明可解释性代价为1/2，即划分方案性能不低于任意方案的50%，并提供了计算近似最优可解释方案的算法。


<details>
  <summary>Details</summary>
Motivation: 传统信息设计中的最优信号方案常涉及不可解释的随机化或复杂划分，难以审计和沟通。需要研究可解释的信息设计方案，使用简单划分结构。

Method: 提出K-划分信号方案（确定性单调划分），开发技术工具将任意最优信号方案转换为划分方案并获得1/2效用，分析可解释性代价并设计算法。

Result: 证明最坏情况下可解释性代价为1/2；精确优化是NP难问题；对Lipschitz效用函数有多项式时间近似算法；对分段常数效用函数有匹配1/2界限的高效算法。

Conclusion: 划分信号方案提供了良好的可解释性与性能平衡（至少50%性能），并存在高效计算近似最优方案的方法，为实际应用提供了可行解决方案。

Abstract: The optimal signaling schemes in information design (Bayesian persuasion)
problems often involve non-explainable randomization or disconnected partitions
of state space, which are too intricate to be audited or communicated. We
propose explainable information design in the context of information design
with a continuous state space, restricting the information designer to use
$K$-partitional signaling schemes defined by deterministic and monotone
partitions of the state space, where a unique signal is sent for all states in
each part. We first prove that the price of explainability (PoE) -- the ratio
between the performances of the optimal explainable signaling scheme and
unrestricted signaling scheme -- is exactly $1/2$ in the worst case, meaning
that partitional signaling schemes are never worse than arbitrary signaling
schemes by a factor of 2.
  We then study the complexity of computing optimal explainable signaling
schemes. We show that the exact optimization problem is NP-hard in general. But
for Lipschitz utility functions, an $\varepsilon$-approximately optimal
explainable signaling scheme can be computed in polynomial time. And for
piecewise constant utility functions, we provide an efficient algorithm to find
an explainable signaling scheme that provides a $1/2$ approximation to the
optimal unrestricted signaling scheme, which matches the worst-case PoE bound.
  A technical tool we develop is a conversion from any optimal signaling scheme
(which satisfies a bi-pooling property) to a partitional signaling scheme that
achieves $1/2$ fraction of the expected utility of the former. We use this tool
in the proofs of both our PoE result and algorithmic result.

</details>


### [20] [Properties of Egalitarian Sequences of Committees: Theory and Experiments](https://arxiv.org/abs/2508.14439)
*Paula Böhm,Robert Bredereck,Till Fluschnik*

Main category: cs.GT

TL;DR: 本文研究选举τ个平等主义委员会序列的任务，提出了多个选举规则和性质，分析了计算复杂度，并通过实验数据比较了这些规则。


<details>
  <summary>Details</summary>
Motivation: 研究在τ个层级上选举委员会序列的问题，旨在找到能够最大化社会福利的平等主义选举方法。

Method: 提出了多个选举平等主义委员会序列的规则，定义了相关性质，分析了计算复杂度，并使用文献中的选举数据进行实验比较。

Result: 确定了各选举规则的计算复杂度，对规则进行了性质分类，并通过实证分析比较了不同规则的性能表现。

Conclusion: 为多层级委员会选举提供了有效的平等主义选举规则，建立了理论框架并通过实验验证了规则的实用性和性质特征。

Abstract: We study the task of electing egalitarian sequences of $\tau$ committees
given a set of agents with additive utilities for candidates available on each
of $\tau$ levels. We introduce several rules for electing an egalitarian
committee sequence as well as properties for such rules. We settle the
computational complexity of finding a winning sequence for our rules and
classify them against our properties. Additionally, we transform sequential
election data from existing election data from the literature. Using this data
set, we compare our rules empirically and test them experimentally against our
properties.

</details>


### [21] [Learning in Repeated Multi-Objective Stackelberg Games with Payoff Manipulation](https://arxiv.org/abs/2508.14705)
*Phurinut Srisawad,Juergen Branke,Long Tran-Thanh*

Main category: cs.GT

TL;DR: 研究重复多目标Stackelberg博弈中的收益操纵问题，领导者通过策略性影响跟随者的确定性最佳响应来优化自身收益，无需知道跟随者的效用函数。


<details>
  <summary>Details</summary>
Motivation: 在多目标Stackelberg博弈中，领导者需要在不了解跟随者多目标偏好权重的情况下，通过交互学习并策略性地操纵跟随者的响应来最大化自身长期收益。

Method: 提出基于期望效用(EU)和长期期望效用(longEU)的操纵策略，领导者在选择行动和提供激励时权衡短期收益与长期影响，通过交互学习跟随者的线性效用函数权重参数。

Result: 理论证明在无限重复交互下longEU收敛到最优操纵，实验结果表明该方法在基准环境中提高了领导者的累积效用，同时促进了互利结果。

Conclusion: 该方法能够在不需要明确协商或先验知识的情况下，有效解决多目标Stackelberg博弈中的收益操纵问题，实现领导者的长期收益最大化。

Abstract: We study payoff manipulation in repeated multi-objective Stackelberg games,
where a leader may strategically influence a follower's deterministic best
response, e.g., by offering a share of their own payoff. We assume that the
follower's utility function, representing preferences over multiple objectives,
is unknown but linear, and its weight parameter must be inferred through
interaction. This introduces a sequential decision-making challenge for the
leader, who must balance preference elicitation with immediate utility
maximisation. We formalise this problem and propose manipulation policies based
on expected utility (EU) and long-term expected utility (longEU), which guide
the leader in selecting actions and offering incentives that trade off
short-term gains with long-term impact. We prove that under infinite repeated
interactions, longEU converges to the optimal manipulation. Empirical results
across benchmark environments demonstrate that our approach improves cumulative
leader utility while promoting mutually beneficial outcomes, all without
requiring explicit negotiation or prior knowledge of the follower's utility
function.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [22] [FakeHunter: Multimodal Step-by-Step Reasoning for Explainable Video Forensics](https://arxiv.org/abs/2508.14581)
*Chen Chen,Runze Li,Zejun Zhang,Pukun Zhao,Fanqing Zhou,Longxiang Wang,Haojian Huang*

Main category: cs.MM

TL;DR: FakeHunter是一个多模态深度伪造检测框架，结合记忆引导检索、思维链推理和工具增强验证，提供准确且可解释的视频取证分析。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测方法往往缺乏可解释性，难以提供详细的篡改定位和解释。需要开发一个既能准确检测又能提供结构化解释的系统。

Method: 使用CLIP编码视觉内容，CLAP编码音频，生成联合音频-视觉嵌入，从FAISS索引的记忆库中检索相似的真实样本进行上下文锚定。通过观察-思考-行动的思维链推理迭代分析证据，在置信度低时自动调用专业工具进行细粒度验证。

Result: 在X-AVFake基准测试上达到34.75%的准确率，比基础模型提升16.87个百分点。记忆检索贡献7.75个百分点的增益，工具检查将低置信度案例提升至46.50%。处理10分钟视频在单GPU上需8分钟，四GPU上需2分钟。

Conclusion: FakeHunter通过多模态融合、记忆检索和工具增强的协同作用，实现了准确且可解释的深度伪造检测，展示了实际部署的可行性。

Abstract: FakeHunter is a multimodal deepfake detection framework that combines
memory-guided retrieval, chain-of-thought (Observation-Thought-Action)
reasoning, and tool-augmented verification to provide accurate and
interpretable video forensics. FakeHunter encodes visual content using CLIP and
audio using CLAP, generating joint audio-visual embeddings that retrieve
semantically similar real exemplars from a FAISS-indexed memory bank for
contextual grounding. Guided by the retrieved context, the system iteratively
reasons over evidence to localize manipulations and explain them. When
confidence is low, it automatically invokes specialized tools-such as zoom-in
image forensics or mel-spectrogram inspection-for fine-grained verification.
Built on Qwen2.5-Omni-7B, FakeHunter produces structured JSON verdicts that
specify what was modified, where it occurs, and why it is judged fake. We also
introduce X-AVFake, a benchmark comprising 5.7k+ manipulated and real videos
(950+ min) annotated with manipulation type, region/entity, violated reasoning
category, and free-form justification. On X-AVFake, FakeHunter achieves an
accuracy of 34.75%, outperforming the vanilla Qwen2.5-Omni-7B by 16.87
percentage points and MiniCPM-2.6 by 25.56 percentage points. Ablation studies
reveal that memory retrieval contributes a 7.75 percentage point gain, and
tool-based inspection improves low-confidence cases to 46.50%. Despite its
multi-stage design, the pipeline processes a 10-minute clip in 8 minutes on a
single NVIDIA A800 (0.8x real-time) or 2 minutes on four GPUs (0.2x),
demonstrating practical deployability.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [23] [Optimal Subspace Embeddings: Resolving Nelson-Nguyen Conjecture Up to Sub-Polylogarithmic Factors](https://arxiv.org/abs/2508.14234)
*Shabarish Chenakkod,Michał Dereziński,Xiaoyu Dong*

Main category: cs.DS

TL;DR: 本文证明了Nelson和Nguyen关于最优维度与稀疏度的猜想，构建了具有次多项式对数因子的随机矩阵，实现了高效的子空间嵌入。


<details>
  <summary>Details</summary>
Motivation: 解决Nelson和Nguyen在FOCS 2013提出的关于最优维度与稀疏度的猜想，为线性回归任务提供更快的矩阵乘法时间缩减。

Method: 使用称为迭代解耦的矩阵集中技术，通过现有随机矩阵普适性工具精细调整高阶迹矩边界。

Result: 成功证明了猜想，构建了具有最优维度~O(d/ε²)和每列稀疏度~O(log(d)/ε)的随机矩阵Π，实现了高概率的(1±ε)近似保距性。

Conclusion: 该工作不仅解决了重要猜想，还引入了新的分析技术，为线性回归等任务提供了新的最快子当前矩阵乘法时间缩减方法。

Abstract: We give a proof of the conjecture of Nelson and Nguyen [FOCS 2013] on the
optimal dimension and sparsity of oblivious subspace embeddings, up to
sub-polylogarithmic factors: For any $n\geq d$ and $\epsilon\geq d^{-O(1)}$,
there is a random $\tilde O(d/\epsilon^2)\times n$ matrix $\Pi$ with $\tilde
O(\log(d)/\epsilon)$ non-zeros per column such that for any
$A\in\mathbb{R}^{n\times d}$, with high probability,
$(1-\epsilon)\|Ax\|\leq\|\Pi Ax\|\leq(1+\epsilon)\|Ax\|$ for all
$x\in\mathbb{R}^d$, where $\tilde O(\cdot)$ hides only sub-polylogarithmic
factors in $d$. Our result in particular implies a new fastest sub-current
matrix multiplication time reduction of size $\tilde O(d/\epsilon^2)$ for a
broad class of $n\times d$ linear regression tasks.
  A key novelty in our analysis is a matrix concentration technique we call
iterative decoupling, which we use to fine-tune the higher-order trace moment
bounds attainable via existing random matrix universality tools [Brailovskaya
and van Handel, GAFA 2024].

</details>


### [24] [Nearly Tight Bounds for the Online Sorting Problem](https://arxiv.org/abs/2508.14287)
*Yossi Azar,Debmalya Panigrahi,Or Vardi*

Main category: cs.DS

TL;DR: 本文解决了在线排序问题的指数级差距，提出了接近最优的空间与竞争比权衡算法，对于m=(1+ε)n空间给出O(log²n/ε)竞争比算法，对于m=γn空间给出O(log²n/γ)竞争比算法


<details>
  <summary>Details</summary>
Motivation: 先前Aamand等人的工作在m=(1+ε)n空间下给出了指数级竞争比算法，同时存在指数级的上界与下界差距，需要填补这一理论空白

Method: 提出了确定性算法，针对不同空间约束条件设计相应的竞争比保证，算法可扩展到未知数值范围的情况

Result: 成功填补了指数级差距，对于m=(1+ε)n空间达到O(log²n/ε)竞争比，对于m=γn空间达到O(log²n/γ)竞争比，接近理论下界

Conclusion: 本文几乎完全解决了在线排序问题，提供了接近最优的空间-竞争比权衡方案，将理论差距从指数级缩小到多项式级别

Abstract: In the online sorting problem, a sequence of $n$ numbers in $[0, 1]$
(including $\{0,1\}$) have to be inserted in an array of size $m \ge n$ so as
to minimize the sum of absolute differences between pairs of numbers occupying
consecutive non-empty cells. Previously, Aamand {\em et al.} (SODA 2023) gave a
deterministic $2^{\sqrt{\log n} \sqrt{\log \log n + \log
(1/\varepsilon)}}$-competitive algorithm when $m = (1+\varepsilon) n$ for any
$\varepsilon \ge \Omega(\log n/n)$. They also showed a lower bound: with $m =
\gamma n$ space, the competitive ratio of any deterministic algorithm is at
least $\frac{1}{\gamma}\cdot\Omega(\log n / \log \log n)$. This left an
exponential gap between the upper and lower bounds for the problem.
  In this paper, we bridge this exponential gap and almost completely resolve
the online sorting problem. First, we give a deterministic $O(\log^2 n /
\varepsilon)$-competitive algorithm with $m = (1+\varepsilon) n$, for any
$\varepsilon \ge \Omega(\log n / n)$. Next, for $m = \gamma n$ where $\gamma =
[O(1), O(\log^2 n)]$, we give a deterministic $O(\log^2 n /
\gamma)$-competitive algorithm. In particular, this implies an
$O(1)$-competitive algorithm with $O(n \log^2 n)$ space, which is within an
$O(\log n\cdot \log \log n)$ factor of the lower bound of $\Omega(n \log n /
\log \log n)$. Combined, the two results imply a close to optimal tradeoff
between space and competitive ratio for the entire range of interest:
specifically, an upper bound of $O(\log^2 n)$ on the product of the competitive
ratio and $\gamma$ while the lower bound on this product is $\Omega(\log n /
\log\log n)$. We also show that these results can be extended to the case when
the range of the numbers is not known in advance, for an additional $O(\log n)$
factor in the competitive ratio.

</details>


### [25] [Sublinear-Time Approximation for Graph Frequency Vectors in Hyperfinite Graphs](https://arxiv.org/abs/2508.14324)
*Gregory Moroie*

Main category: cs.DS

TL;DR: 本文提出了在超有限性假设下，用亚线性时间近似有界度图的k-disc分布的算法，通过分离切割误差和采样误差，实现了ε精度的ℓ1距离近似。


<details>
  <summary>Details</summary>
Motivation: 解决在有界度超有限图中，高效近似k-disc分布的问题，为图分析和机器学习中的子线性时间算法提供理论支持。

Method: 基于Hassidim等人的划分预言机框架，采样N=Θ(ε⁻²)个顶点，查询局部划分预言机，构建摘要图H，通过控制切割误差(由超有限参数φ控制)和采样误差(由精度参数ε控制)来保证近似精度。

Result: 算法在poly(d,k,ε⁻¹)时间内构建大小为poly(dᵏ,1/ε)的摘要图H，其k-disc频率向量与原图的ℓ1距离误差不超过ε，且具有高概率保证。

Conclusion: 该方法清晰揭示了运行时间和摘要大小对参数d、k和ε的依赖关系，为图数据的子线性时间分析提供了有效的理论工具。

Abstract: In this work, we address the problem of approximating the $k$-disc
distribution (``frequency vector") of a bounded-degree graph in sublinear-time
under the assumption of hyperfiniteness. We revisit the partition-oracle
framework of Hassidim, Kelner, Nguyen, and Onak \cite{hassidim2009local}, and
provide a concise, self-contained analysis that explicitly separates the two
sources of error: (i) the cut error, controlled by hyperfiniteness parameter
$\phi$, which incurs at most $\varepsilon/2$ in $\ell_1$-distance by removing
at most $\phi |V|$ edges; and (ii) the sampling error, controlled by the
accuracy parameter $\varepsilon$, bounded by $\varepsilon/2$ via
$N=\Theta(\varepsilon^{-2})$ random vertex queries and a Chernoff and union
bound argument. Combining these yields an overall $\ell_1$-error of
$\varepsilon$ with high probability. Algorithmically, we show that by sampling
$N=\lceil C\varepsilon^{-2} \rceil$ vertices and querying the local partition
oracle, one can in time $poly(d,k,\varepsilon^{-1})$ construct a summary graph
$H$ of size $|H|=poly(d^k,1/\varepsilon)$ whose $k$-disc frequency vector
approximates that of the original graph within $\varepsilon$ in
$\ell_1$-distance. Our approach clarifies the dependence of both runtime and
summary-size on the parameter $d$,$k$, and $\varepsilon$.

</details>


### [26] [Improved Online Sorting](https://arxiv.org/abs/2508.14361)
*Jubayer Nirjhor,Nicole Wein*

Main category: cs.DS

TL;DR: 提出了一个确定性在线排序算法，将n个实数在线放置到(1+ε)n大小的数组中，最终获得准多对数成本(ε⁻¹log n)^O(log log n)


<details>
  <summary>Details</summary>
Motivation: 研究在线排序问题，其中数字按在线方式到达，算法必须立即将每个数字放入数组中，目标是使相邻数字绝对差之和最小化

Method: 开发了一种确定性算法来处理在线排序问题，算法在每一步立即将新到达的数字放入数组的合适位置

Result: 获得了准多对数成本(ε⁻¹log n)^O(log log n)，相比之前工作的指数成本2^O(√(log n·log log n + log ε⁻¹))有显著改进

Conclusion: 该确定性算法显著改进了在线排序问题的成本上界，达到了准多对数级别的性能

Abstract: We study the online sorting problem, where $n$ real numbers arrive in an
online fashion, and the algorithm must immediately place each number into an
array of size $(1+\varepsilon) n$ before seeing the next number. After all $n$
numbers are placed into the array, the cost is defined as the sum over the
absolute differences of all $n-1$ pairs of adjacent numbers in the array,
ignoring empty array cells. Aamand, Abrahamsen, Beretta, and Kleist introduced
the problem and obtained a deterministic algorithm with cost
$2^{O\left(\sqrt{\log n \cdot\log\log n +\log \varepsilon^{-1}}\right)}$, and a
lower bound of $\Omega(\log n / \log\log n)$ for deterministic algorithms. We
obtain a deterministic algorithm with quasi-polylogarithmic cost
$\left(\varepsilon^{-1}\log n\right)^{O\left(\log \log n\right)}$.
  Concurrent and independent work by Azar, Panigrahi, and Vardi achieves
polylogarithmic cost $O(\varepsilon^{-1}\log^2 n)$.

</details>


### [27] [Compact representation of maximal palindromes](https://arxiv.org/abs/2508.14384)
*Takuya Mieno*

Main category: cs.DS

TL;DR: 提出了一种新的O(n)位表示法来表示字符串中的所有最大回文，支持O(1)时间检索任意位置中心的最大回文长度，并应用于构建O(n)位数据结构以在O(log n)时间内计算任意子串中的最长回文。


<details>
  <summary>Details</summary>
Motivation: 回文计算在形式语言理论和生物信息学中有重要应用。虽然最大回文可以用O(n log n)位空间隐式表示，但需要更紧凑的空间表示来支持空间高效的解决方案开发。

Method: 提出了一种新颖的O(n)位表示法来表示所有最大回文，基于Manacher算法和最大回文概念，能够实现常数时间检索。

Result: 成功构建了O(n)位的数据结构，能够在O(log n)时间内计算任意给定子串中的最长回文，显著提高了空间效率。

Conclusion: 该紧凑表示法为涉及回文结构的问题提供了空间高效的解决方案基础，有望加速更多空间优化算法的发展。

Abstract: Palindromes are strings that read the same forward and backward. The
computation of palindromic structures within strings is a fundamental problem
in string algorithms, being motivated by potential applications in formal
language theory and bioinformatics. Although the number of palindromic factors
in a string of length $n$ can be quadratic, they can be implicitly represented
in $O(n \log n)$ bits of space by storing the lengths of all maximal
palindromes in an integer array, which can be computed in $O(n)$ time
[Manacher, 1975]. In this paper, we propose a novel $O(n)$-bit representation
of all maximal palindromes in a string, which enables $O(1)$-time retrieval of
the length of the maximal palindrome centered at any given position. Since
Manacher's algorithm and the notion of maximal palindromes are widely utilized
for solving numerous problems involving palindromic structures, our compact
representation will accelerate the development of more space-efficient
solutions. Indeed, as the first application of our compact representation of
maximal palindromes, we present a data structure of size $O(n)$ bits that can
compute the longest palindrome appearing in any given factor of the string in
$O(\log n)$ time.

</details>


### [28] [Incremental-Decremental Maximization](https://arxiv.org/abs/2508.14516)
*Yann Disser,Max Klimm,Annette Lutz,Lea Strubberg*

Main category: cs.DS

TL;DR: 提出了一个增量-减量最大化框架，用于基础设施逐步改造或更新，通过随机和确定性算法找到元素转换顺序，在转换过程中保持较大效用值。


<details>
  <summary>Details</summary>
Motivation: 捕捉基础设施逐步改造或更新的过程，解决在转换过程中既要维护现有功能又要实现新功能的挑战。

Method: 提出随机和确定性算法，通过一次转换一个元素的方式，在有界曲率和/或通用子模性比率的效用函数下找到最优转换顺序。

Result: 算法能够为子模函数和总替代函数等提供竞争性解决方案，但增量-减量最大化比纯增量最大化更具挑战性。

Conclusion: 该框架成功解决了基础设施渐进式改造的优化问题，但证明了增量-减量最大化比传统增量最大化问题更加复杂困难。

Abstract: We introduce a framework for incremental-decremental maximization that
captures the gradual transformation or renewal of infrastructures. In our
model, an initial solution is transformed one element at a time and the utility
of an intermediate solution is given by the sum of the utilities of the
transformed and untransformed parts. We propose a simple randomized and a
deterministic algorithm that both find an order in which to transform the
elements while maintaining a large utility during all stages of transformation,
relative to an optimum solution for the current stage. More specifically, our
algorithms yield competitive solutions for utility functions of bounded
curvature and/or generic submodularity ratio, and, in particular, for
submodular functions, and gross substitute functions. Our results exhibit that
incremental-decremental maximization is substantially more difficult than
incremental maximization.

</details>


### [29] [A $(4/3+\varepsilon)$-Approximation for Preemptive Scheduling with Batch Setup Times](https://arxiv.org/abs/2508.14528)
*Max A. Deppert,David Fischer,Klaus Jansen*

Main category: cs.DS

TL;DR: 本文提出了一个(4/3+ε)-近似算法，用于解决带预中断和设置时间的并行机调度问题，运行时间为O(n²log(1/ε))，改进了之前的3/2近似比。


<details>
  <summary>Details</summary>
Motivation: 解决NP-hard的并行机调度问题，其中作业分为多个类别，机器在不同类别作业间切换时需要设置时间，目标是最小化最大完成时间。

Method: 将实例划分为"简单"和"困难"两部分，对简单部分计算4/3T近似解，通过结构分析证明困难部分也存在4/3T近似解，最终设计出(4/3+ε)-近似算法。

Result: 算法在O(n²log(1/ε))时间内获得(4/3+ε)-近似解，当ε<1/6时优于之前最好的3/2近似比。

Conclusion: 该研究为带设置时间的并行机调度问题提供了更好的近似算法，通过创新的实例划分和结构分析技术实现了性能提升。

Abstract: We consider the $\mathcal{NP}$-hard problem $\mathrm{P} \mathbf{\vert}
\mathrm{pmtn, setup=s_i} \mathbf{\vert} \mathrm{C_{\max}}$, the problem of
scheduling $n$ jobs, which are divided into $c$ classes, on $m$ identical
parallel machines while allowing preemption. For each class $i$ of the $c$
classes, we are given a setup time $s_i$ that is required to be scheduled
whenever a machine switches from processing a job of one class to a job from
another class. The goal is to find a schedule that minimizes the makespan.
  We give a $(4/3+\varepsilon)$-approximate algorithm with run time in
$\mathcal{O}(n^2 \log(1/\varepsilon))$. For any $\varepsilon < 1/6$, this
improves upon the previously best known approximation ratio of $3/2$ for this
problem.
  Our main technical contributions are as follows. We first partition any
instance into an "easy" and a "hard" part, such that a $4/3 T$-approximation
for the former is easy to compute for some given makespan $T$. We then proceed
to show our main structural result, namely that there always exists a $4/3
T$-approximation for any instance that has a solution with makespan $T$, where
the hard part has some easy to compute properties. Finally, we obtain an
algorithm that computes a $(4/3+\varepsilon)$-approximation in time n
$\mathcal{O}(n^2 \log(1/\varepsilon))$ for general instances by computing
solutions with the previously shown structural properties.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [30] [Multi-Source Peak Age of Information Optimization in Mobile Edge Computing Systems](https://arxiv.org/abs/2508.14328)
*Jianhang Zhu,Jie Gong*

Main category: cs.IT

TL;DR: 本文研究了多源单服务器系统中计算密集型状态数据的信息新鲜度优化问题，提出了联合调度和采样的传输-计算平衡方法，证明了随机调度器的最优性，并开发了交替优化算法。


<details>
  <summary>Details</summary>
Motivation: 随着实时监控系统对信息新鲜度的需求增长，Age of Information (AoI) 成为重要度量指标。对于计算密集型状态数据，信息在处理完成后才能被揭示，这需要同时考虑传输时间和处理时间的联合优化。

Method: 研究了多源单服务器系统，采用随机生成模型，考虑随机传输时间和处理时间。证明了随机调度器在非抢占和抢占服务器设置下的最优性，并分析了最优采样器的结构特征。提出了交替优化算法来联合优化调度频率和采样阈值/函数。

Result: 数值实验表明，所提出的算法在广泛设置下能够达到最优性能，验证了理论分析的正确性和算法的有效性。

Conclusion: 在多源系统中，随机调度是最优策略，最优采样器结构与单源系统一致。传输-计算平衡的联合优化方法能有效提升信息新鲜度，交替优化算法具有良好的性能表现。

Abstract: Age of Information (AoI) is emerging as a novel metric for measuring
information freshness in real-time monitoring systems. For
computation-intensive status data, the information is not revealed until being
processed. We consider a status update problem in a multi-source single-server
system where the sources are scheduled to generate and transmit status data
which are received and processed at the edge server. Generate-at-will sources
with both random transmission time and process time are considered, introducing
the joint optimization of source scheduling and status sampling on the basis of
transmission-computation balancing. We show that a random scheduler is optimal
for both non-preemptive and preemptive server settings, and the optimal sampler
depends on the scheduling result and its structure remains consistent with the
single-source system, i.e., threshold-based sampler for non-preemptive case and
transmission-aware deterministic sampler for preemptive case. Then, the problem
can be transformed to jointly optimizing the scheduling frequencies and the
sampling thresholds/functions, which is non-convex. We proposed an alternation
optimization algorithm to solve it. Numerical experiments show that the
proposed algorithm can achieve the optimal in a wide range of settings.

</details>


### [31] [Reconstruction Codes for Deletions and Insertions: Connection, Distinction, and Construction](https://arxiv.org/abs/2508.14386)
*Yubo Sun,Gennian Ge*

Main category: cs.IT

TL;DR: 本文研究q元序列的重构码，重点关注t-删除球和t-插入球的重构问题。建立了删除和插入重构码之间的基本联系，证明了插入重构码也是删除重构码。对于N=O(n^{t-1})且t≥2的情况，发现了删除和插入重构码之间的显著差异，并推翻了之前的一个猜想。


<details>
  <summary>Details</summary>
Motivation: 研究重构码在序列重建中的应用，特别是在面对删除和插入错误时的性能差异。目标是确定或建立(n,q,N;B)-重构码的最小冗余度界限，这对于数据存储和传输中的错误纠正具有重要意义。

Method: 通过建立删除和插入重构码之间的理论联系，分析不同参数下的冗余度界限。使用数学证明和构造方法，特别针对t=2的情况构建了具体的重构码，并建立了相应的冗余度上界。

Result: 证明了插入重构码也是删除重构码，得到ρ(n,q,N;D_t)≤ρ(n,q,N;I_t)。对于删除操作，当N=O(n^{t-1})时冗余度为O(1)；对于插入操作，冗余度为log log n + O(1)。针对t=2的情况，为N∈{2,3,4,5}构建了重构码并建立了相应的冗余度上界。

Conclusion: 本文揭示了删除和插入重构码之间的基本关系和非对称性，推翻了之前的猜想，并为特定参数下的重构码提供了具体的构造和冗余度界限，推动了重构码理论的发展。

Abstract: Let $\mathcal{B}(\cdot)$ be an error ball function. A set of $q$-ary
sequences of length $n$ is referred to as an
\emph{$(n,q,N;\mathcal{B})$-reconstruction code} if each sequence
$\boldsymbol{x}$ within this set can be uniquely reconstructed from any $N$
distinct elements within its error ball $\mathcal{B}(\boldsymbol{x})$. The main
objective in this area is to determine or establish bounds for the minimum
redundancy of $(n,q,N;\mathcal{B})$-reconstruction codes, denoted by
$\rho(n,q,N;\mathcal{B})$. In this paper, we investigate reconstruction codes
where the error ball is either the \emph{$t$-deletion ball}
$\mathcal{D}_t(\cdot)$ or the \emph{$t$-insertion ball} $\mathcal{I}_t(\cdot)$.
Firstly, we establish a fundamental connection between reconstruction codes for
deletions and insertions. For any positive integers $n,t,q,N$, any
$(n,q,N;\mathcal{I}_t)$-reconstruction code is also an
$(n,q,N;\mathcal{D}_t)$-reconstruction code. This leads to the inequality
$\rho(n,q,N;\mathcal{D}_t)\leq \rho(n,q,N;\mathcal{I}_t)$. Then, we identify a
significant distinction between reconstruction codes for deletions and
insertions when $N=O(n^{t-1})$ and $t\geq 2$. For deletions, we prove that
$\rho(n,q,\tfrac{2(q-1)^{t-1}}{q^{t-1}(t-1)!}n^{t-1}+O(n^{t-2});\mathcal{D}_t)=O(1)$,
which disproves a conjecture posed in \cite{Chrisnata-22-IT}. For insertions,
we show that
$\rho(n,q,\tfrac{(q-1)^{t-1}}{(t-1)!}n^{t-1}+O(n^{t-2});\mathcal{I}_t)=\log\log
n + O(1)$, which extends a key result from \cite{Ye-23-IT}. Finally, we
construct $(n,q,N;\mathcal{B})$-reconstruction codes, where $\mathcal{B}\in
\{\mathcal{D}_2,\mathcal{I}_2\}$, for $N \in \{2,3, 4, 5\}$ and establish
respective upper bounds of $3\log n+O(\log\log n)$, $3\log n+O(1)$, $2\log
n+O(\log\log n)$ and $\log n+O(\log\log n)$ on the minimum redundancy
$\rho(n,q,N;\mathcal{B})$. This generalizes results previously established in
\cite{Sun-23-IT}.

</details>


### [32] [DeepTelecom: A Digital-Twin Deep Learning Dataset for Channel and MIMO Applications](https://arxiv.org/abs/2508.14507)
*Bohao Wang,Zehua Jiang,Zhenyu Yang,Chongwen Huang,Yongliang Shen,Siming Jiang,Chen Zhu,Zhaohui Yang,Richeng Jin,Zhaoyang Zhang,Sami Muhaidat,Merouane Debbah*

Main category: cs.IT

TL;DR: DeepTelecom是一个3D数字孪生信道数据集，通过LLM辅助构建高细节场景，利用GPU加速的射线追踪技术生成大规模、高保真、多模态的无线信道数据，为无线AI研究提供统一基准和训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有无线AI数据集生成缓慢、建模保真度有限、场景覆盖狭窄，无法满足AI驱动无线创新的需求。

Method: 使用大语言模型辅助构建LoD3级室内外场景，基于Sionna射线追踪引擎模拟全无线电波传播效应，利用GPU加速生成射线路径轨迹、实时信号强度热图、多视图图像、信道张量和多尺度衰落轨迹等多模态数据。

Result: 创建了能够高效流式传输大规模、高保真、多模态信道数据的DeepTelecom数据集，支持实时视频流和同步多视图输出。

Conclusion: DeepTelecom为无线AI研究提供了统一的基准测试平台，并为基础模型将大模型智能与未来通信系统紧密融合提供了丰富的领域训练数据。

Abstract: Domain-specific datasets are the foundation for unleashing artificial
intelligence (AI)-driven wireless innovation. Yet existing wireless AI corpora
are slow to produce, offer limited modeling fidelity, and cover only narrow
scenario types. To address the challenges, we create DeepTelecom, a
three-dimension (3D) digital-twin channel dataset. Specifically, a large
language model (LLM)-assisted pipeline first builds the third level of details
(LoD3) outdoor and indoor scenes with segmentable material-parameterizable
surfaces. Then, DeepTelecom simulates full radio-wave propagation effects based
on Sionna's ray-tracing engine. Leveraging GPU acceleration, DeepTelecom
streams ray-path trajectories and real-time signal-strength heat maps, compiles
them into high-frame-rate videos, and simultaneously outputs synchronized
multi-view images, channel tensors, and multi-scale fading traces. By
efficiently streaming large-scale, high-fidelity, and multimodal channel data,
DeepTelecom not only furnishes a unified benchmark for wireless AI research but
also supplies the domain-rich training substrate that enables foundation models
to tightly fuse large model intelligence with future communication systems.

</details>


### [33] [Minimizing Task-Oriented Age of Information for Remote Monitoring with Pre-Identification](https://arxiv.org/abs/2508.14575)
*Shuying Gan,Xijun Wang,Chao Xu,Xiang Chen*

Main category: cs.IT

TL;DR: 提出了任务导向信息年龄(TAoI)新指标来衡量信息内容与系统任务的相关性，通过SMDP建模和阈值策略优化无线监控系统中的图像传输。


<details>
  <summary>Details</summary>
Motivation: 新兴智能应用需要任务导向通信范式，但缺乏全面、通用且实用的度量标准来释放该范式的潜力。

Method: 将动态传输问题建模为半马尔可夫决策过程(SMDP)，转化为等效马尔可夫决策过程(MDP)，提出基于TAoI阈值的低复杂度相对值迭代算法。

Result: 分析表明最优策略是基于TAoI的阈值策略，实验验证了最优传输策略相比两种基准方法的优越性能。

Conclusion: TAoI是有效的任务导向通信度量指标，阈值策略在性能和复杂度之间取得了良好平衡，单阈值策略虽然性能略有下降但收敛更快。

Abstract: The emergence of new intelligent applications has fostered the development of
a task-oriented communication paradigm, where a comprehensive, universal, and
practical metric is crucial for unleashing the potential of this paradigm. To
this end, we introduce an innovative metric, the Task-oriented Age of
Information (TAoI), to measure whether the content of information is relevant
to the system task, thereby assisting the system in efficiently completing
designated tasks. We apply TAoI to a wireless monitoring system tasked with
identifying targets and transmitting their images for subsequent analysis. To
minimize TAoI and determine the optimal transmission policy, we formulate the
dynamic transmission problem as a Semi-Markov Decision Process (SMDP) and
transform it into an equivalent Markov Decision Process (MDP). Our analysis
demonstrates that the optimal policy is threshold-based with respect to TAoI.
Building on this, we propose a low-complexity relative value iteration
algorithm tailored to this threshold structure to derive the optimal
transmission policy. Additionally, we introduce a simpler single-threshold
policy, which, despite a slight performance degradation, offers faster
convergence. Comprehensive experiments and simulations validate the superior
performance of our optimal transmission policy compared to two established
baseline approaches.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [34] [Deep Learning for School Dropout Detection: A Comparison of Tabular and Graph-Based Models for Predicting At-Risk Students](https://arxiv.org/abs/2508.14057)
*Pablo G. Almeida,Guilherme A. L. Silva,Valéria Santos,Gladston Moreira,Pedro Silva,Eduardo Luz*

Main category: cs.LG

TL;DR: 本研究探讨了将表格学生数据转换为图结构是否能提升辍学预测准确率，发现特定的GNN配置（GraphSAGE结合PCA-KMeans聚类）优于传统表格模型，但其他配置效果不稳定。


<details>
  <summary>Details</summary>
Motivation: 学生辍学是全球教育系统的重大挑战，传统机器学习模型在表格数据上表现良好，但图神经网络（GNNs）可能通过捕捉学生数据中的复杂关系提供优势。

Method: 将表格学生数据转换为图结构（主要使用聚类技术），比较GNNs（自定义GCN和GraphSAGE）与表格模型（随机森林、XGBoost、TabNet）的性能，探索基于不同聚类算法（K-Means、HDBSCAN）和降维技术（PCA、UMAP）的图构建策略。

Result: 特定的GNN配置（GraphSAGE在PCA-KMeans聚类生成的图上）表现最佳，宏观F1分数提高约7个百分点，准确率提高近2个百分点，优于最强的表格基线（XGBoost）。但其他GNN配置和图构建方法未能一致超越表格模型。

Conclusion: GNNs在辍学预测中具有潜力，但图生成策略和GNN架构选择至关重要，突显了在该领域优化转换表格数据用于图学习的挑战。

Abstract: Student dropout is a significant challenge in educational systems worldwide,
leading to substantial social and economic costs. Predicting students at risk
of dropout allows for timely interventions. While traditional Machine Learning
(ML) models operating on tabular data have shown promise, Graph Neural Networks
(GNNs) offer a potential advantage by capturing complex relationships inherent
in student data if structured as graphs. This paper investigates whether
transforming tabular student data into graph structures, primarily using
clustering techniques, enhances dropout prediction accuracy. We compare the
performance of GNNs (a custom Graph Convolutional Network (GCN) and GraphSAGE)
on these generated graphs against established tabular models (Random Forest
(RF), XGBoost, and TabNet) using a real-world student dataset. Our experiments
explore various graph construction strategies based on different clustering
algorithms (K-Means, HDBSCAN) and dimensionality reduction techniques
(Principal Component Analysis (PCA), Uniform Manifold Approximation and
Projection (UMAP)). Our findings demonstrate that a specific GNN configuration,
GraphSAGE on a graph derived from PCA-KMeans clustering, achieved superior
performance, notably improving the macro F1-score by approximately 7 percentage
points and accuracy by nearly 2 percentage points over the strongest tabular
baseline (XGBoost). However, other GNN configurations and graph construction
methods did not consistently surpass tabular models, emphasizing the critical
role of the graph generation strategy and GNN architecture selection. This
highlights both the potential of GNNs and the challenges in optimally
transforming tabular data for graph-based learning in this domain.

</details>


### [35] [Load Forecasting on A Highly Sparse Electrical Load Dataset Using Gaussian Interpolation](https://arxiv.org/abs/2508.14069)
*Chinmoy Biswas,Nafis Faisal,Vivek Chowdhury,Abrar Al-Shadid Abir,Sabir Mahmud,Mithon Rahman,Shaikh Anowarul Fattah,Hafiz Imtiaz*

Main category: cs.LG

TL;DR: 该研究证明对于约62%稀疏的电力负荷数据，在宽平稳假设下使用高斯插值处理缺失值后，LSTM神经网络在负荷预测中表现最佳


<details>
  <summary>Details</summary>
Motivation: 现实数据集中的稀疏性（缺失值或零值）给数据操作带来重大挑战，特别是在电力负荷预测中，需要找到合适的插值方法来处理高度稀疏的数据

Method: 对小时级电力负荷数据进行统计分析，假设数据为宽平稳(WSS)，使用高斯插值处理缺失值，然后训练多种机器学习和深度学习模型进行比较

Result: 实证表明高斯插值是处理负荷预测问题的合适选择，其中基于LSTM的神经网络模型在各类经典和神经网络模型中表现最佳

Conclusion: 对于宽平稳的稀疏电力负荷数据，高斯插值结合LSTM神经网络能够有效解决负荷预测问题

Abstract: Sparsity, defined as the presence of missing or zero values in a dataset,
often poses a major challenge while operating on real-life datasets. Sparsity
in features or target data of the training dataset can be handled using various
interpolation methods, such as linear or polynomial interpolation, spline,
moving average, or can be simply imputed. Interpolation methods usually perform
well with Strict Sense Stationary (SSS) data. In this study, we show that an
approximately 62\% sparse dataset with hourly load data of a power plant can be
utilized for load forecasting assuming the data is Wide Sense Stationary (WSS),
if augmented with Gaussian interpolation. More specifically, we perform
statistical analysis on the data, and train multiple machine learning and deep
learning models on the dataset. By comparing the performance of these models,
we empirically demonstrate that Gaussian interpolation is a suitable option for
dealing with load forecasting problems. Additionally, we demonstrate that Long
Short-term Memory (LSTM)-based neural network model offers the best performance
among a diverse set of classical and neural network-based models.

</details>


### [36] [Edge-Selector Model Applied for Local Search Neighborhood for Solving Vehicle Routing Problems](https://arxiv.org/abs/2508.14071)
*Bachtiar Herdianto,Romain Billot,Flavien Lucas,Marc Sevaux,Daniele Vigo*

Main category: cs.LG

TL;DR: 提出了一种混合机器学习和元启发式的方法来解决车辆路径问题，通过边缘选择器模型在局部搜索中识别禁止移动，从而指导搜索过程。


<details>
  <summary>Details</summary>
Motivation: 车辆路径问题(VRP)是组合优化中的重要问题，传统元启发式方法在搜索过程中可能效率不高，需要智能机制来指导搜索方向。

Method: 使用两种学习机制：基于梯度提升树和前馈神经网络的表格二元分类器，以及利用图结构的图神经网络(GNN)来预测解决方案边缘，识别禁止移动。

Result: 该方法展现出良好的可扩展性和泛化能力，在不同基准元启发式、各种问题规模和变体（包括CVRP和CVRPTW）上都取得了性能提升，在最多30,000个客户节点的基准数据集上验证了改进效果。

Conclusion: 混合机器学习和元启发式的机制能够有效指导局部搜索过程，在车辆路径问题上实现了显著的性能改进，具有实际应用价值。

Abstract: This research proposes a hybrid Machine Learning and metaheuristic mechanism
that is designed to solve Vehicle Routing Problems (VRPs). The main of our
method is an edge solution selector model, which classifies solution edges to
identify prohibited moves during the local search, hence guiding the search
process within metaheuristic baselines. Two learning-based mechanisms are used
to develop the edge selector: a simple tabular binary classifier and a Graph
Neural Network (GNN). The tabular classifier employs Gradient Boosting Trees
and Feedforward Neural Network as the baseline algorithms. Adjustments to the
decision threshold are also applied to handle the class imbalance in the
problem instance. An alternative mechanism employs the GNN to utilize graph
structure for direct solution edge prediction, with the objective of guiding
local search by predicting prohibited moves. These hybrid mechanisms are then
applied in state-fo-the-art metaheuristic baselines. Our method demonstrates
both scalability and generalizability, achieving performance improvements
across different baseline metaheuristics, various problem sizes and variants,
including the Capacitated Vehicle Routing Problem (CVRP) and CVRP with Time
Windows (CVRPTW). Experimental evaluations on benchmark datasets up to 30,000
customer nodes, supported by pair-wise statistical analysis, verify the
observed improvements.

</details>


### [37] [Multi-Objective Bayesian Optimization with Independent Tanimoto Kernel Gaussian Processes for Diverse Pareto Front Exploration](https://arxiv.org/abs/2508.14072)
*Anabel Yong*

Main category: cs.LG

TL;DR: GP-MOBO是一种新颖的多目标贝叶斯优化算法，通过整合高效精确高斯过程处理分子指纹全维度，在分子优化中超越传统方法，获得更高质量SMILES分子和更广泛的化学空间探索。


<details>
  <summary>Details</summary>
Motivation: 解决传统多目标贝叶斯优化方法在处理高维稀疏分子指纹时的计算效率问题，提升分子优化性能。

Method: 整合快速精确高斯过程包，无需大量计算资源即可高效处理分子指纹全维度，开发GP-MOBO多目标贝叶斯优化算法。

Result: 在DockSTRING数据集上，GP-MOBO在20次贝叶斯优化迭代中获得更高的几何平均值，在所有测试场景中更接近帕累托前沿，识别出更高质量和有效的SMILES分子。

Conclusion: GP-MOBO以最小计算开销有效解决复杂多目标优化挑战，在分子优化领域实现了性能突破。

Abstract: We present GP-MOBO, a novel multi-objective Bayesian Optimization algorithm
that advances the state-of-the-art in molecular optimization. Our approach
integrates a fast minimal package for Exact Gaussian Processes (GPs) capable of
efficiently handling the full dimensionality of sparse molecular fingerprints
without the need for extensive computational resources. GP-MOBO consistently
outperforms traditional methods like GP-BO by fully leveraging fingerprint
dimensionality, leading to the identification of higher-quality and valid
SMILES. Moreover, our model achieves a broader exploration of the chemical
search space, as demonstrated by its superior proximity to the Pareto front in
all tested scenarios. Empirical results from the DockSTRING dataset reveal that
GP-MOBO yields higher geometric mean values across 20 Bayesian optimization
iterations, underscoring its effectiveness and efficiency in addressing complex
multi-objective optimization challenges with minimal computational overhead.

</details>


### [38] [Context Steering: A New Paradigm for Compression-based Embeddings by Synthesizing Relevant Information Features](https://arxiv.org/abs/2508.14780)
*Guillermo Sarasa Durán,Ana Granados Fontecha,Francisco de Borja Rodríguez Ortíz*

Main category: cs.LG

TL;DR: 提出了一种名为"上下文引导"的新方法，通过主动指导特征形成过程来改进压缩距离在聚类和分类任务中的表现，而不是被动接受数据中出现的结构。


<details>
  <summary>Details</summary>
Motivation: 压缩距离虽然能够灵活地测量相似性，但因其特征是从数据中衍生而非预先定义，往往难以与特定任务对齐，特别是在复杂的聚类或分类场景中。

Method: 通过系统分析每个对象在聚类框架内如何影响关系上下文，主动"引导"特征形成过程，生成定制化的嵌入空间来分离和放大类别区分信息。使用NCD和NRC距离结合层次聚类进行验证。

Result: 在从文本到真实世界音频的异构数据集上的实验验证了上下文引导方法的鲁棒性和通用性，为压缩距离的应用提供了有效的替代方案。

Conclusion: 该方法标志着压缩距离应用的根本转变：从仅仅发现内在数据结构转向主动塑造针对特定目标定制的特征空间。

Abstract: Compression-based distances (CD) offer a flexible and domain-agnostic means
of measuring similarity by identifying implicit information through
redundancies between data objects. However, as similarity features are derived
from the data, rather than defined as an input, it often proves difficult to
align with the task at hand, particularly in complex clustering or
classification settings. To address this issue, we introduce "context
steering," a novel methodology that actively guides the feature-shaping
process. Instead of passively accepting the emergent data structure (typically
a hierarchy derived from clustering CDs), our approach "steers" the process by
systematically analyzing how each object influences the relational context
within a clustering framework. This process generates a custom-tailored
embedding that isolates and amplifies class-distinctive information. We
validate the capabilities of this strategy using Normalized Compression
Distance (NCD) and Relative Compression Distance (NRC) with common hierarchical
clustering, providing an effective alternative to common transductive methods.
Experimental results across heterogeneous datasets-from text to real-world
audio-validate the robustness and generality of context steering, marking a
fundamental shift in their application: from merely discovering inherent data
structures to actively shaping a feature space tailored to a specific
objective.

</details>


### [39] [MCLPD:Multi-view Contrastive Learning for EEG-based PD Detection Across Datasets](https://arxiv.org/abs/2508.14073)
*Qian Zhanga,Ruilin Zhang,Jun Xiao,Yifan Liu,Zhe Wang*

Main category: cs.LG

TL;DR: 提出MCLPD半监督学习框架，通过多视角对比预训练和轻量级监督微调，显著提升帕金森病跨数据集检测性能，仅需1-5%标注数据即可达到优异效果


<details>
  <summary>Details</summary>
Motivation: EEG数据标注成本高导致数据集规模有限且存在显著差异，传统模型在跨数据集检测场景中泛化能力不足

Method: 结合多视角对比预训练（时间域和频率域双重增强）和轻量级监督微调，使用未标注UNM数据集进行自监督预训练，仅用少量UI和UC数据集标注数据进行微调

Result: 仅用1%标注数据时在UI和UC数据集上分别达到0.91和0.81的F1分数，使用5%标注数据时分别提升至0.97和0.87，显著优于现有方法

Conclusion: MCLPD框架有效提升了跨数据集泛化能力，大幅降低了对标注数据的依赖，证明了该方法的有效性

Abstract: Electroencephalography has been validated as an effective technique for
detecting Parkinson's disease,particularly in its early stages.However,the high
cost of EEG data annotation often results in limited dataset size and
considerable discrepancies across datasets,including differences in acquisition
protocols and subject demographics,significantly hinder the robustness and
generalizability of models in cross-dataset detection scenarios.To address such
challenges,this paper proposes a semi-supervised learning framework named
MCLPD,which integrates multi-view contrastive pre-training with lightweight
supervised fine-tuning to enhance cross-dataset PD detection performance.During
pre-training,MCLPD uses self-supervised learning on the unlabeled UNM
dataset.To build contrastive pairs,it applies dual augmentations in both time
and frequency domains,which enrich the data and naturally fuse time-frequency
information.In the fine-tuning phase,only a small proportion of labeled data
from another two datasets (UI and UC)is used for supervised
optimization.Experimental results show that MCLPD achieves F1 scores of 0.91 on
UI and 0.81 on UC using only 1%of labeled data,which further improve to 0.97
and 0.87,respectively,when 5%of labeled data is used.Compared to existing
methods,MCLPD substantially improves cross-dataset generalization while
reducing the dependency on labeled data,demonstrating the effectiveness of the
proposed framework.

</details>


### [40] [GEPD:GAN-Enhanced Generalizable Model for EEG-Based Detection of Parkinson's Disease](https://arxiv.org/abs/2508.14074)
*Qian Zhang,Ruilin Zhang,Biaokai Zhu,Xun Han,Jun Xiao,Yifan Liu,Zhe Wang*

Main category: cs.LG

TL;DR: 提出GEPD模型，使用GAN生成融合EEG数据和质量评估，结合多CNN分类网络，在跨数据集帕金森病检测中达到84.3%准确率


<details>
  <summary>Details</summary>
Motivation: 现有帕金森病EEG检测方法在单个数据集表现良好，但跨数据集时因方法差异和小样本量导致泛化性差，需要开发通用模型

Method: 设计生成网络创建融合EEG数据并控制分布相似性，构建EEG信号质量评估模型确保生成质量，使用多CNN组合分类网络捕获时频特征

Result: 在跨数据集设置下达到84.3%准确率和84.0% F1分数，性能与最先进模型相当

Conclusion: GEPD模型展示了良好的泛化能力，为神经系统疾病的诊断和监测提供了有效的智能方法

Abstract: Electroencephalography has been established as an effective method for
detecting Parkinson's disease, typically diagnosed early.Current Parkinson's
disease detection methods have shown significant success within individual
datasets, however, the variability in detection methods across different EEG
datasets and the small size of each dataset pose challenges for training a
generalizable model for cross-dataset scenarios. To address these issues, this
paper proposes a GAN-enhanced generalizable model, named GEPD, specifically for
EEG-based cross-dataset classification of Parkinson's disease.First, we design
a generative network that creates fusion EEG data by controlling the
distribution similarity between generated data and real data.In addition, an
EEG signal quality assessment model is designed to ensure the quality of
generated data great.Second, we design a classification network that utilizes a
combination of multiple convolutional neural networks to effectively capture
the time-frequency characteristics of EEG signals, while maintaining a
generalizable structure and ensuring easy convergence.This work is dedicated to
utilizing intelligent methods to study pathological manifestations, aiming to
facilitate the diagnosis and monitoring of neurological diseases.The evaluation
results demonstrate that our model performs comparably to state-of-the-art
models in cross-dataset settings, achieving an accuracy of 84.3% and an
F1-score of 84.0%, showcasing the generalizability of the proposed model.

</details>


### [41] [Explainable Graph Spectral Clustering For Text Embeddings](https://arxiv.org/abs/2508.14075)
*Mieczysław A. Kłopotek,Sławomir T. Wierzchoń,Bartłomiej Starosta,Piotr Borkowski,Dariusz Czerski,Eryk Laskowski*

Main category: cs.LG

TL;DR: 本文扩展了图谱聚类在文档分析中的可解释性方法，从基于词向量空间余弦相似度的文档相似性计算，推广到其他文档嵌入方法（特别是GloVe嵌入）


<details>
  <summary>Details</summary>
Motivation: 在之前基于词向量空间余弦相似度的图谱聚类可解释性研究基础上，希望将该方法推广到更广泛的文档嵌入表示方法

Method: 采用GloVe等嵌入方法来表示文档，并在此基础上进行图谱聚类分析，扩展原有的可解释性框架

Result: 成功将图谱聚类的可解释性方法从特定的余弦相似度计算推广到更通用的文档嵌入表示方法

Conclusion: 该方法具有很好的通用性，能够适应不同的文档嵌入技术，为文档聚类结果的可解释性分析提供了更灵活的工具

Abstract: In a previous paper, we proposed an introduction to the explainability of
Graph Spectral Clustering results for textual documents, given that document
similarity is computed as cosine similarity in term vector space.
  In this paper, we generalize this idea by considering other embeddings of
documents, in particular, based on the GloVe embedding idea.

</details>


### [42] [PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning](https://arxiv.org/abs/2508.14076)
*Mengdi Li,Guanqiao Chen,Xufeng Zhao,Haochen Wen,Shu Yang,Di Wang*

Main category: cs.LG

TL;DR: PersRM-R1是一个基于推理的奖励建模框架，通过少量个人示例就能识别和表示个人偏好因素，在准确性和泛化性方面优于同类模型。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型难以捕捉细微的、用户特定的偏好，特别是在数据有限和跨领域的情况下，需要更有效的个性化LLM对齐方法。

Method: 结合合成数据生成和两阶段训练流程（监督微调+强化微调），专门设计用于从少量个人示例中识别个人因素。

Result: 实验结果表明PersRM-R1在准确性和泛化性方面优于同类规模模型，并能与更大模型性能相匹配。

Conclusion: 该框架为更有效的个性化LLM铺平了道路，解决了有限数据可用性和鲁棒泛化要求的挑战。

Abstract: Reward models (RMs), which are central to existing post-training methods, aim
to align LLM outputs with human values by providing feedback signals during
fine-tuning. However, existing RMs struggle to capture nuanced, user-specific
preferences, especially under limited data and across diverse domains. Thus, we
introduce PersRM-R1, the first reasoning-based reward modeling framework
specifically designed to identify and represent personal factors from only one
or a few personal exemplars. To address challenges including limited data
availability and the requirement for robust generalization, our approach
combines synthetic data generation with a two-stage training pipeline
consisting of supervised fine-tuning followed by reinforcement fine-tuning.
Experimental results demonstrate that PersRM-R1 outperforms existing models of
similar size and matches the performance of much larger models in both accuracy
and generalizability, paving the way for more effective personalized LLMs.

</details>


### [43] [Label Smoothing is a Pragmatic Information Bottleneck](https://arxiv.org/abs/2508.14077)
*Sota Kudo*

Main category: cs.LG

TL;DR: 本研究通过信息瓶颈理论重新审视标签平滑，证明在模型灵活性足够且无标签冲突的情况下，标签平滑获得的模型输出探索了信息瓶颈的最优解，可视为信息瓶颈的实用实现方法。


<details>
  <summary>Details</summary>
Motivation: 重新审视标签平滑技术，从信息瓶颈理论的角度为其提供理论解释和证明，探索其作为信息瓶颈实用方法的潜力。

Method: 基于信息瓶颈理论框架，在模型灵活性足够且无标签冲突的假设下，通过理论分析和实验验证标签平滑与信息瓶颈最优解的关系。

Result: 理论证明和实验表明，标签平滑获得的模型输出确实探索了信息瓶颈的最优解，且表现出对无关信息因素不敏感的特性。

Conclusion: 标签平滑可以解释为信息瓶颈的一种实用实现方法，具有简单易用的优势，同时具备信息瓶颈方法对无关信息的鲁棒性特性。

Abstract: This study revisits label smoothing via a form of information bottleneck.
Under the assumption of sufficient model flexibility and no conflicting labels
for the same input, we theoretically and experimentally demonstrate that the
model output obtained through label smoothing explores the optimal solution of
the information bottleneck. Based on this, label smoothing can be interpreted
as a practical approach to the information bottleneck, enabling simple
implementation. As an information bottleneck method, we experimentally show
that label smoothing also exhibits the property of being insensitive to factors
that do not contain information about the target, or to factors that provide no
additional information about it when conditioned on another variable.

</details>


### [44] [Out-of-Sample Hydrocarbon Production Forecasting: Time Series Machine Learning using Productivity Index-Driven Features and Inductive Conformal Prediction](https://arxiv.org/abs/2508.14078)
*Mohamed Hassan Abdalla Idris,Jakub Marek Cebula,Jebraeel Gholinezhad,Shamsul Masum,Hongjie Ma*

Main category: cs.LG

TL;DR: 提出结合油藏工程知识（PI特征选择）和机器学习（LSTM等算法）的新框架，通过ICP进行不确定性量化，显著提升了油气产量预测的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统油气产量预测中样本外预测鲁棒性不足的问题，特别是在多变量时间序列分析中需要更可靠的不确定性量化方法。

Method: 整合生产力指数（PI）驱动的特征选择和归纳共形预测（ICP）框架，使用LSTM、BiLSTM、GRU和XGBoost等算法进行多变量时间序列预测。

Result: LSTM模型表现最佳，在PF14井测试集和样本外预测中分别获得最低MAE（19.468和29.638），ICP框架成功提供了有效的95%预测区间。

Conclusion: 将领域专业知识与先进机器学习技术结合，特别是PI特征选择和ICP不确定性量化，能够显著提高油气产量预测的可靠性。

Abstract: This research introduces a new ML framework designed to enhance the
robustness of out-of-sample hydrocarbon production forecasting, specifically
addressing multivariate time series analysis. The proposed methodology
integrates Productivity Index (PI)-driven feature selection, a concept derived
from reservoir engineering, with Inductive Conformal Prediction (ICP) for
rigorous uncertainty quantification. Utilizing historical data from the Volve
(wells PF14, PF12) and Norne (well E1H) oil fields, this study investigates the
efficacy of various predictive algorithms-namely Long Short-Term Memory (LSTM),
Bidirectional LSTM (BiLSTM), Gated Recurrent Unit (GRU), and eXtreme Gradient
Boosting (XGBoost) - in forecasting historical oil production rates (OPR_H).
All the models achieved "out-of-sample" production forecasts for an upcoming
future timeframe. Model performance was comprehensively evaluated using
traditional error metrics (e.g., MAE) supplemented by Forecast Bias and
Prediction Direction Accuracy (PDA) to assess bias and trend-capturing
capabilities. The PI-based feature selection effectively reduced input
dimensionality compared to conventional numerical simulation workflows. The
uncertainty quantification was addressed using the ICP framework, a
distribution-free approach that guarantees valid prediction intervals (e.g.,
95% coverage) without reliance on distributional assumptions, offering a
distinct advantage over traditional confidence intervals, particularly for
complex, non-normal data. Results demonstrated the superior performance of the
LSTM model, achieving the lowest MAE on test (19.468) and genuine out-of-sample
forecast data (29.638) for well PF14, with subsequent validation on Norne well
E1H. These findings highlight the significant potential of combining
domain-specific knowledge with advanced ML techniques to improve the
reliability of hydrocarbon production forecasts.

</details>


### [45] [A Guide to Robust Generalization: The Impact of Architecture, Pre-training, and Optimization Strategy](https://arxiv.org/abs/2508.14079)
*Maxime Heuillet,Rishika Bhagwatkar,Jonas Ngnawé,Yann Pequignot,Alexandre Larouche,Christian Gagné,Irina Rish,Ola Ahmad,Audrey Durand*

Main category: cs.LG

TL;DR: 本文通过大规模实证研究发现，在大数据集上监督预训练的卷积神经网络在鲁棒微调中表现最佳，挑战了当前流行的基于注意力架构和鲁棒预训练表示的趋势。


<details>
  <summary>Details</summary>
Motivation: 深度学习的鲁棒微调设计选择（如模型架构、预训练表示、优化策略等）如何影响泛化能力仍是一个开放问题，需要系统性的实证研究来指导实践。

Method: 在6个数据集、40种预训练架构、2种专用损失函数和3种适应协议下进行了1,440种训练配置和7,200次鲁棒性测量，涵盖了五种扰动类型的大规模基准测试。

Result: 发现监督预训练的卷积神经网络在大多数情况下表现最好，而基于注意力的架构和鲁棒预训练表示并不总是最优选择。

Conclusion: 研究结果既证实又挑战了先前的设计假设，为鲁棒微调提供了实用指导，并指出了有前景的研究方向。

Abstract: Deep learning models operating in the image domain are vulnerable to small
input perturbations. For years, robustness to such perturbations was pursued by
training models from scratch (i.e., with random initializations) using
specialized loss objectives. Recently, robust fine-tuning has emerged as a more
efficient alternative: instead of training from scratch, pretrained models are
adapted to maximize predictive performance and robustness. To conduct robust
fine-tuning, practitioners design an optimization strategy that includes the
model update protocol (e.g., full or partial) and the specialized loss
objective. Additional design choices include the architecture type and size,
and the pretrained representation. These design choices affect robust
generalization, which is the model's ability to maintain performance when
exposed to new and unseen perturbations at test time. Understanding how these
design choices influence generalization remains an open question with
significant practical implications. In response, we present an empirical study
spanning 6 datasets, 40 pretrained architectures, 2 specialized losses, and 3
adaptation protocols, yielding 1,440 training configurations and 7,200
robustness measurements across five perturbation types. To our knowledge, this
is the most diverse and comprehensive benchmark of robust fine-tuning to date.
While attention-based architectures and robust pretrained representations are
increasingly popular, we find that convolutional neural networks pretrained in
a supervised manner on large datasets often perform best. Our analysis both
confirms and challenges prior design assumptions, highlighting promising
research directions and offering practical guidance.

</details>


### [46] [KnowDR-REC: A Benchmark for Referring Expression Comprehension with Real-World Knowledge](https://arxiv.org/abs/2508.14080)
*Guanghao Jin,Jingpei Wu,Tianpei Guo,Yiyi Niu,Weidong Zhou,Guoyang Liu*

Main category: cs.LG

TL;DR: 提出了KnowDR-REC新基准，用于评估多模态大语言模型在知识驱动的视觉定位任务中的推理能力，包含真实知识基础、负样本构造和新评估指标。


<details>
  <summary>Details</summary>
Motivation: 传统REC基准仅依赖图像内线索或缺乏细粒度标注，无法充分评估MLLMs的推理能力，需要新的基准来测试知识驱动的多模态推理。

Method: 构建基于真实知识的KnowDR-REC数据集，包含细粒度多模态推理任务、精心构造的负样本（通过表达式编辑）和三个新的评估指标来分析模型内部推理过程。

Result: 评估了16个最先进的多模态模型，发现现有MLLMs在知识驱动的视觉定位任务上表现不佳，存在文本理解与视觉定位的解耦现象，模型易受记忆的捷径相关性影响。

Conclusion: 该基准将推动开发更鲁棒、可解释和知识密集的视觉定位框架，促进更可靠的多模态系统在复杂现实场景中的应用发展。

Abstract: Referring Expression Comprehension (REC) is a popular multimodal task that
aims to accurately detect target objects within a single image based on a given
textual expression. However, due to the limitations of earlier models,
traditional REC benchmarks either rely solely on intra-image cues or lack
sufficiently fine-grained instance annotations, making them inadequate for
evaluating the reasoning capabilities of Multi-modal Large Language Models
(MLLMs). To address this gap, we propose a new benchmark, KnowDR-REC,
characterized by three key features: Firstly, it is built upon real-world
knowledge, requiring fine-grained multimodal reasoning across text and image.
Secondly, the dataset includes elaborately constructed negative samples via
fine-grained expression editing, designed to evaluate a model's robustness and
anti-hallucination ability. Lastly, we introduce three novel evaluation metrics
to systematically explore the model's internal reasoning process. We evaluate
16 state-of-the-art multimodal models on KnowDR-REC, with experimental results
showing that existing MLLMs still struggle with knowledge-driven visual
grounding tasks. Furthermore, we observe a decoupling between textual
understanding and visual grounding in MLLMs, where many models are
significantly influenced by memorized shortcut correlations, which severely
affect their behavior on our benchmark and hinder genuine multimodal reasoning.
We anticipate that the proposed benchmark will inspire future research towards
developing more robust, interpretable, and knowledge-intensive visual grounding
frameworks, driving the development of more reliable and robust multimodal
systems for complex real-world scenarios.

</details>


### [47] [Toward Lifelong Learning in Equilibrium Propagation: Sleep-like and Awake Rehearsal for Enhanced Stability](https://arxiv.org/abs/2508.14081)
*Yoshimasa Kubo,Jean Erik Delanois,Maxim Bazhenov*

Main category: cs.LG

TL;DR: 该论文提出了一种睡眠式回放巩固(SRC)算法，用于解决使用平衡传播(EP)训练的循环神经网络在持续学习中的灾难性遗忘问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 基于平衡传播训练的循环神经网络在持续学习中面临灾难性遗忘问题，而人类大脑通过睡眠期间的记忆回放巩固机制能够有效保留新旧知识，这启发了研究团队开发类似的算法。

Method: 提出了睡眠式回放巩固(SRC)算法，在每次新任务训练后实施回放巩固，并与清醒回放(rehearsal)技术结合使用。

Result: SRC显著提高了EP训练的多层循环神经网络在持续学习中的抗遗忘能力，在多个数据集上表现优于或等同于使用BPTT训练的模型，特别是在Fashion MNIST、Kuzushiji-MNIST、CIFAR10和ImageNet数据集上超越了BPTT模型。

Conclusion: 研究表明睡眠式回放技术适用于循环神经网络，为将人类学习行为整合到人工神经网络中提供了潜在可能性。

Abstract: Recurrent neural networks (RNNs) trained using Equilibrium Propagation (EP),
a biologically plausible training algorithm, have demonstrated strong
performance in various tasks such as image classification and reinforcement
learning. However, these networks face a critical challenge in continuous
learning: catastrophic forgetting, where previously acquired knowledge is
overwritten when new tasks are learned. This limitation contrasts with the
human brain's ability to retain and integrate both old and new knowledge, aided
by processes like memory consolidation during sleep through the replay of
learned information. To address this challenge in RNNs, here we propose a
sleep-like replay consolidation (SRC) algorithm for EP-trained RNNs. We found
that SRC significantly improves RNN's resilience to catastrophic forgetting in
continuous learning scenarios. In class-incremental learning with SRC
implemented after each new task training, the EP-trained multilayer RNN model
(MRNN-EP) performed significantly better compared to feedforward networks
incorporating several well-established regularization techniques. The MRNN-EP
performed on par with MRNN trained using Backpropagation Through Time (BPTT)
when both were equipped with SRC on MNIST data and surpassed BPTT-based models
on the Fashion MNIST, Kuzushiji-MNIST, CIFAR10, and ImageNet datasets.
Combining SRC with rehearsal, also known as "awake replay", further boosted the
network's ability to retain long-term knowledge while continuing to learn new
tasks. Our study reveals the applicability of sleep-like replay techniques to
RNNs and highlights the potential for integrating human-like learning behaviors
into artificial neural networks (ANNs).

</details>


### [48] [Toward Generalist Semi-supervised Regression via Decoupled Representation Distillation](https://arxiv.org/abs/2508.14082)
*Ye Su,Hezhe Qiao,Wei Huang,Lin Chen*

Main category: cs.LG

TL;DR: 提出DRILL框架，将半监督回归任务转化为离散分布估计任务，通过解耦分布对齐提升模型泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有半监督回归方法过度依赖伪标签质量，直接回归难以学习标签分布且容易过拟合

Method: 将回归任务转化为离散分布估计，使用解耦分布对齐(DDA)技术对齐师生模型的分布

Result: 在多个领域数据集上的实验表明DRILL具有强泛化能力并优于竞争方法

Conclusion: DRILL框架通过离散化处理和分布对齐有效解决了半监督回归中的过拟合和伪标签质量问题

Abstract: Semi-supervised regression (SSR), which aims to predict continuous scores of
samples while reducing reliance on a large amount of labeled data, has recently
received considerable attention across various applications, including computer
vision, natural language processing, and audio and medical analysis. Existing
semi-supervised methods typically apply consistency regularization on the
general regression task by generating pseudo-labels. However, these methods
heavily rely on the quality of pseudo-labels, and direct regression fails to
learn the label distribution and can easily lead to overfitting. To address
these challenges, we introduce an end-to-end Decoupled Representation
distillation framework (DRILL) which is specially designed for the
semi-supervised regression task where we transform the general regression task
into a Discrete Distribution Estimation (DDE) task over multiple buckets to
better capture the underlying label distribution and mitigate the risk of
overfitting associated with direct regression. Then we employ the Decoupled
Distribution Alignment (DDA) to align the target bucket and non-target bucket
between teacher and student on the distribution of buckets, encouraging the
student to learn more robust and generalized knowledge from the teacher.
Extensive experiments conducted on datasets from diverse domains demonstrate
that the proposed DRILL has strong generalization and outperforms the competing
methods.

</details>


### [49] [GeoMAE: Masking Representation Learning for Spatio-Temporal Graph Forecasting with Missing Values](https://arxiv.org/abs/2508.14083)
*Songyu Ke,Chenyu Wu,Yuxuan Liang,Xiuwen Yi,Yanping Sun,Junbo Zhang,Yu Zheng*

Main category: cs.LG

TL;DR: 提出了一种基于对比自学习的时空数据框架(CST)，用于从低质量数据中准确推断POI人群流量，通过自监督图表示学习和对比学习技术解决标注数据稀缺和复杂时空依赖的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于城市传感技术的限制，大多数数据源的质量不足以监测每个POI的人群流量，且面临标注数据稀缺、POI间复杂时空依赖、以及精确人群流量与GPS报告间多重相关性三大挑战。

Method: 将人群流量推断问题重构为自监督属性图表示学习任务，构建基于POI和距离的空间邻接图，采用对比学习技术利用大量未标注时空数据，使用交换预测方法从相似实例预测目标子图表示，最后用精确人群流量数据进行微调。

Result: 在两个真实世界数据集上的实验表明，基于大量噪声数据预训练的模型始终优于从头开始训练的模型。

Conclusion: 所提出的对比自学习框架能够有效利用未标注数据解决POI人群流量推断问题，在数据质量受限的情况下仍能获得准确结果。

Abstract: Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal
for effective traffic management, public service, and urban planning. Despite
this importance, due to the limitations of urban sensing techniques, the data
quality from most sources is inadequate for monitoring crowd flow at each POI.
This renders the inference of accurate crowd flow from low-quality data a
critical and challenging task. The complexity is heightened by three key
factors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{The
intricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriad
correlations between precise crowd flow and GPS reports}.
  To address these challenges, we recast the crowd flow inference problem as a
self-supervised attributed graph representation learning task and introduce a
novel \underline{C}ontrastive \underline{S}elf-learning framework for
\underline{S}patio-\underline{T}emporal data (\model). Our approach initiates
with the construction of a spatial adjacency graph founded on the POIs and
their respective distances. We then employ a contrastive learning technique to
exploit large volumes of unlabeled spatio-temporal data. We adopt a swapped
prediction approach to anticipate the representation of the target subgraph
from similar instances. Following the pre-training phase, the model is
fine-tuned with accurate crowd flow data. Our experiments, conducted on two
real-world datasets, demonstrate that the \model pre-trained on extensive noisy
data consistently outperforms models trained from scratch.

</details>


### [50] [Parameter-Aware Ensemble SINDy for Interpretable Symbolic SGS Closure](https://arxiv.org/abs/2508.14085)
*Hanseul Kang,Shervin Karimkashi,Ville Vuorinen*

Main category: cs.LG

TL;DR: 提出了一个可扩展的参数感知稀疏回归框架，用于从多参数模拟数据中发现可解释的偏微分方程和亚网格尺度闭合模型。该框架基于SINDy方法，通过四项创新解决了关键限制。


<details>
  <summary>Details</summary>
Motivation: 现有的SINDy方法在处理多参数数据和物理参数变化时存在局限性，需要开发能够统一处理参数变化、保持单位一致性、支持批量处理并提供鲁棒模型识别的新框架。

Method: 1) 符号参数化使物理参数在统一回归中变化；2) 维度相似性过滤器确保单位一致性并减少候选库；3) 内存高效的Gram矩阵累积支持批处理；4) 集成共识和系数稳定性分析实现鲁棒模型识别。

Result: 在一维基准测试中可靠地恢复了控制方程。在过滤Burgers数据集中发现了SGS闭合模型τ_SGS = 0.1603·Δ²(∂ū/∂x)²，对应Smagorinsky常数约0.4004，R²达到0.886，预测精度优于经典闭合模型。

Conclusion: 该框架能够从数据中自主发现Smagorinsky型闭合结构，无需先验理论假设，为现有湍流建模方法提供了补充途径，推动了数据驱动闭合发现领域的发展。

Abstract: We present a scalable, parameter-aware sparse regression framework for
discovering interpretable partial differential equations and subgrid-scale
closures from multi-parameter simulation data. Building on SINDy (Sparse
Identification of Nonlinear Dynamics), our approach addresses key limitations
through four innovations: symbolic parameterisation enabling physical
parameters to vary within unified regression; Dimensional Similarity Filter
enforcing unit-consistency whilst reducing candidate libraries;
memory-efficient Gram-matrix accumulation enabling batch processing; and
ensemble consensus with coefficient stability analysis for robust model
identification.
  Validation on canonical one-dimensional benchmarks demonstrates reliable
recovery of governing equations across parameter ranges. Applied to filtered
Burgers datasets, the framework discovers an SGS closure $\tau_{\mathrm{SGS}} =
0.1603\cdot\Delta^2\left(\frac{\partial \bar{u}}{\partial x}\right)^2$,
corresponding to a Smagorinsky constant of approximately 0.4004. This
represents autonomous discovery of Smagorinsky-type closure structure from data
without prior theoretical assumptions.
  The discovered model achieves $R^2 = 0.886$ across filter scales and
demonstrates improved prediction accuracy compared to classical closures. The
framework's ability to identify physically meaningful SGS forms and calibrate
coefficients offers a complementary approach to existing turbulence modelling
methods, contributing to the growing field of data-driven closure discovery.

</details>


### [51] [EEGDM: EEG Representation Learning via Generative Diffusion Model](https://arxiv.org/abs/2508.14086)
*Jia Hong Puah,Sim Kuan Goh,Ziwei Zhang,Zixuan Ye,Chow Khuen Chan,Kheng Seang Lim,Si Lei Fong,Kok Sin Woon*

Main category: cs.LG

TL;DR: 提出基于生成扩散模型的EEG表示学习框架EEGDM，使用结构化状态空间模型进行扩散预训练，在保持轻量化的同时超越现有方法性能


<details>
  <summary>Details</summary>
Motivation: 现有EEG基础模型计算成本高但性能提升有限，需要更高效的表示学习方法来解决EEG信号标注有限和变异性高的问题

Method: 开发结构化状态空间扩散预训练模型(SSMDP)捕捉EEG时序动态，使用去噪扩散概率模型训练，通过潜在融合变换器(LFT)进行下游分类

Result: 在Temple大学EEG事件语料库上验证，性能优于现有方法同时模型轻量19倍

Conclusion: EEGDM为当前基础模型提供了有前景的轻量化替代方案

Abstract: While electroencephalogram (EEG) has been a crucial tool for monitoring the
brain and diagnosing neurological disorders (e.g., epilepsy), learning
meaningful representations from raw EEG signals remains challenging due to
limited annotations and high signal variability. Recently, EEG foundation
models (FMs) have shown promising potential by adopting transformer
architectures and self-supervised pre-training methods from large language
models (e.g., masked prediction) to learn representations from diverse EEG
data, followed by fine-tuning on specific EEG tasks. Nonetheless, these large
models often incurred high computational costs during both training and
inference, with only marginal performance improvements as model size increases.
In this work, we proposed EEG representation learning framework building upon
Generative Diffusion Model (EEGDM). Specifically, we developed structured
state-space model for diffusion pretraining (SSMDP) to better capture the
temporal dynamics of EEG signals and trained the architecture using a Denoising
Diffusion Probabilistic Model. The resulting latent EEG representations were
then used for downstream classification tasks via our proposed latent fusion
transformer (LFT). To evaluate our method, we used the multi-event Temple
University EEG Event Corpus and compared EEGDM with current state-of-the-art
approaches, including EEG FMs. Empirical results showed that our method
outperformed existing methods while being approximately 19x more lightweight.
These findings suggested that EEGDM offered a promising alternative to current
FMs. Our code is available at: https://github.com/jhpuah/EEGDM.

</details>


### [52] [FM4NPP: A Scaling Foundation Model for Nuclear and Particle Physics](https://arxiv.org/abs/2508.14087)
*David Park,Shuhang Li,Yi Huang,Xihaier Luo,Haiwang Yu,Yeonju Go,Christopher Pinkenburg,Yuewei Lin,Shinjae Yoo,Joseph Osborn,Jin Huang,Yihui Ren*

Main category: cs.LG

TL;DR: 该研究开发了一个用于粒子物理实验的科学基础模型，通过自监督学习方法处理探测器数据，在多个下游任务中表现出优异的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的自监督学习范式启发了科学基础模型的发展，但粒子物理探测器数据的稀疏性和空间分布特性与自然语言差异巨大，需要专门的方法来处理。

Method: 提出了新的自监督训练方法处理探测器数据，构建了包含1100万粒子碰撞事件的数据集，使用冻结权重和任务特定适配器的架构，模型参数达到1.88亿。

Result: 该基础模型在所有下游任务中均优于基线模型，表现出强大的数据高效适应能力，提取的表征具有任务无关性但可通过线性映射专门化。

Conclusion: 证明了粒子物理基础模型的可扩展性和跨任务泛化能力，为科学领域的基础模型发展提供了重要参考。

Abstract: Large language models have revolutionized artificial intelligence by enabling
large, generalizable models trained through self-supervision. This paradigm has
inspired the development of scientific foundation models (FMs). However,
applying this capability to experimental particle physics is challenging due to
the sparse, spatially distributed nature of detector data, which differs
dramatically from natural language. This work addresses if an FM for particle
physics can scale and generalize across diverse tasks. We introduce a new
dataset with more than 11 million particle collision events and a suite of
downstream tasks and labeled data for evaluation. We propose a novel
self-supervised training method for detector data and demonstrate its neural
scalability with models that feature up to 188 million parameters. With frozen
weights and task-specific adapters, this FM consistently outperforms baseline
models across all downstream tasks. The performance also exhibits robust
data-efficient adaptation. Further analysis reveals that the representations
extracted by the FM are task-agnostic but can be specialized via a single
linear mapping for different downstream tasks.

</details>


### [53] [CoBAD: Modeling Collective Behaviors for Human Mobility Anomaly Detection](https://arxiv.org/abs/2508.14088)
*Haomin Wen,Shurui Cao,Leman Akoglu*

Main category: cs.LG

TL;DR: CoBAD是一个用于检测人类移动集体异常的新模型，通过两阶段注意力机制和预训练方法，能够有效识别集体行为中的异常模式，在多个指标上显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法主要关注个体移动模式，而集体异常检测需要建模个体间的时空依赖关系，这是一个未被充分探索的挑战领域。

Method: 提出CoBAD模型，使用集体事件序列和共现事件图表示问题，采用两阶段注意力机制建模个体移动模式和个体间交互，通过掩码事件和链接重建任务进行预训练。

Result: 在大规模移动数据集上的实验表明，CoBAD在AUCROC指标上提升13%-18%，在AUCPR指标上提升19%-70%，显著优于现有基线方法。

Conclusion: CoBAD成功解决了集体异常检测问题，能够检测意外共现异常和缺失异常两种类型，为公共安全和城市规划等应用提供了有效工具。

Abstract: Detecting anomalies in human mobility is essential for applications such as
public safety and urban planning. While traditional anomaly detection methods
primarily focus on individual movement patterns (e.g., a child should stay at
home at night), collective anomaly detection aims to identify irregularities in
collective mobility behaviors across individuals (e.g., a child is at home
alone while the parents are elsewhere) and remains an underexplored challenge.
Unlike individual anomalies, collective anomalies require modeling
spatiotemporal dependencies between individuals, introducing additional
complexity. To address this gap, we propose CoBAD, a novel model designed to
capture Collective Behaviors for human mobility Anomaly Detection. We first
formulate the problem as unsupervised learning over Collective Event Sequences
(CES) with a co-occurrence event graph, where CES represents the event
sequences of related individuals. CoBAD then employs a two-stage attention
mechanism to model both the individual mobility patterns and the interactions
across multiple individuals. Pre-trained on large-scale collective behavior
data through masked event and link reconstruction tasks, CoBAD is able to
detect two types of collective anomalies: unexpected co-occurrence anomalies
and absence anomalies, the latter of which has been largely overlooked in prior
work. Extensive experiments on large-scale mobility datasets demonstrate that
CoBAD significantly outperforms existing anomaly detection baselines, achieving
an improvement of 13%-18% in AUCROC and 19%-70% in AUCPR. All source code is
available at https://github.com/wenhaomin/CoBAD.

</details>


### [54] [Logical Expressivity and Explanations for Monotonic GNNs with Scoring Functions](https://arxiv.org/abs/2508.14091)
*Matthew Morris,David J. Tena Cucala,Bernardo Cuenca Grau*

Main category: cs.LG

TL;DR: 本文提出了一种从使用评分函数的图神经网络中提取可解释Datalog规则的方法，通过使GNN和评分函数单调化来保证规则提取的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的从GNN中提取规则的方法只适用于低表达力的图编码/解码方法，无法处理使用评分函数进行链接预测的更通用方法，缺乏对这类GNN预测的可解释性分析。

Method: 通过使GNN和评分函数单调化，利用单调性提取可靠的解释规则，并为特定类别的单调GNN与评分函数定义等价的Datalog程序生成过程。

Result: 实验表明，在链接预测基准测试中，单调化的GNN和评分函数在实践中表现良好，并能产生大量可靠的规则。

Conclusion: 该方法成功地将规则提取技术扩展到使用评分函数的GNN模型，为这类模型的预测提供了可解释性保证，同时保持了良好的预测性能。

Abstract: Graph neural networks (GNNs) are often used for the task of link prediction:
predicting missing binary facts in knowledge graphs (KGs). To address the lack
of explainability of GNNs on KGs, recent works extract Datalog rules from GNNs
with provable correspondence guarantees. The extracted rules can be used to
explain the GNN's predictions; furthermore, they can help characterise the
expressive power of various GNN models. However, these works address only a
form of link prediction based on a restricted, low-expressivity graph
encoding/decoding method. In this paper, we consider a more general and popular
approach for link prediction where a scoring function is used to decode the GNN
output into fact predictions. We show how GNNs and scoring functions can be
adapted to be monotonic, use the monotonicity to extract sound rules for
explaining predictions, and leverage existing results about the kind of rules
that scoring functions can capture. We also define procedures for obtaining
equivalent Datalog programs for certain classes of monotonic GNNs with scoring
functions. Our experiments show that, on link prediction benchmarks, monotonic
GNNs and scoring functions perform well in practice and yield many sound rules.

</details>


### [55] [Physics-Informed Reward Machines](https://arxiv.org/abs/2508.14093)
*Daniel Ajeleye,Ashutosh Trivedi,Majid Zamani*

Main category: cs.LG

TL;DR: 本文提出了物理信息奖励机(pRMs)，通过符号化方式表达复杂学习目标和奖励结构，结合反事实经验生成和奖励塑形技术，显著提高了强化学习的效率和可编程性。


<details>
  <summary>Details</summary>
Motivation: 传统奖励机(RMs)虽然能处理非马尔可夫奖励，但在表达复杂物理环境的学习目标方面仍有局限。需要一种更强大的符号化机器来更好地分离环境知识和未知信息，提高学习效率。

Method: 提出物理信息奖励机(pRMs)框架，开发了能够利用pRMs的强化学习算法，通过反事实经验生成和奖励塑形技术来加速学习过程。

Result: 实验结果表明，在有限和连续物理环境中，pRMs技术显著加速了训练阶段的奖励获取，提高了多个控制任务的学习效率。

Conclusion: 物理信息奖励机(pRMs)为强化学习提供了更可编程、表达力更强且高效的学习框架，通过结构化奖励规范和采样优化技术有效降低了样本复杂度。

Abstract: Reward machines (RMs) provide a structured way to specify non-Markovian
rewards in reinforcement learning (RL), thereby improving both expressiveness
and programmability. Viewed more broadly, they separate what is known about the
environment, captured by the reward mechanism, from what remains unknown and
must be discovered through sampling. This separation supports techniques such
as counterfactual experience generation and reward shaping, which reduce sample
complexity and speed up learning. We introduce physics-informed reward machines
(pRMs), a symbolic machine designed to express complex learning objectives and
reward structures for RL agents, thereby enabling more programmable,
expressive, and efficient learning. We present RL algorithms capable of
exploiting pRMs via counterfactual experiences and reward shaping. Our
experimental results show that these techniques accelerate reward acquisition
during the training phases of RL. We demonstrate the expressiveness and
effectiveness of pRMs through experiments in both finite and continuous
physical environments, illustrating that incorporating pRMs significantly
improves learning efficiency across several control tasks.

</details>


### [56] [Hard Examples Are All You Need: Maximizing GRPO Post-Training Under Annotation Budgets](https://arxiv.org/abs/2508.14094)
*Benjamin Pikus,Pratyush Ranjan Tiwari,Burton Ye*

Main category: cs.LG

TL;DR: 在固定预算下，选择最难的数据进行GRPO微调可获得最大性能提升（达47%），而简单数据提升最小，因为困难样本提供更多学习机会


<details>
  <summary>Details</summary>
Motivation: 资源受限的语言模型微调中，如何在固定采集预算下选择数据难度（简单、中等、困难或随机）以获得最佳性能提升

Method: 使用Group Relative Policy Optimization (GRPO)在不同模型规模和家族上进行微调，通过基础模型的多样本评估获取难度估计，比较四种子集选择策略

Result: 训练最难样本获得最大性能提升（高达47%），训练简单样本获得最小提升，困难样本在GRPO训练中提供更多可学习机会

Conclusion: 预算受限的后训练中应优先选择困难样本，使用GRPO在推理任务上可获得显著性能提升

Abstract: Collecting high-quality training examples for language model fine-tuning is
expensive, with practical budgets limiting the amount of data that can be
procured. We investigate a critical question for resource-constrained
alignment: under a fixed acquisition budget, should practitioners prioritize
examples that are easy, medium, hard, or of random difficulty? We study Group
Relative Policy Optimization (GRPO) fine-tuning across different model sizes
and families, comparing four subset selection policies chosen from the same
unlabeled pool using base-model difficulty estimates obtained via multi-sample
evaluation. Our experiments reveal that training on the hardest examples yields
the largest performance gains, up to 47%, while training on easy examples yield
the smallest gains. Analysis reveals that this effect arises from harder
examples providing more learnable opportunities during GRPO training. These
findings provide practical guidance for budget-constrained post-training:
prioritizing hard examples yields substantial performance gains on reasoning
tasks when using GRPO.

</details>


### [57] [Implicit Hypergraph Neural Network](https://arxiv.org/abs/2508.14101)
*Akash Choudhuri,Yongjian Zhong,Bijaya Adhikari*

Main category: cs.LG

TL;DR: 提出了隐式超图神经网络(IHNN)，通过隐式微分和联合学习节点与超边的固定点表示，解决了传统超图神经网络在捕获长程依赖时性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 传统超图神经网络由于消息传递轮次有限，只能捕获局部信息而忽略了长程高阶依赖关系，盲目增加消息传递轮次又会降低性能。

Method: 提出IHNN框架，通过隐式微分和投影梯度下降方法，联合学习节点和超边的固定点表示，以端到端方式有效捕获长程依赖。

Result: 在真实世界超图节点分类任务上的大量实验表明，IHNN在大多数设置下优于现有最佳方法，建立了超图学习的新state-of-the-art。

Conclusion: IHNN通过隐式学习方法有效解决了超图神经网络的长程依赖捕获问题，为超图学习提供了新的有效框架。

Abstract: Hypergraphs offer a generalized framework for capturing high-order
relationships between entities and have been widely applied in various domains,
including healthcare, social networks, and bioinformatics. Hypergraph neural
networks, which rely on message-passing between nodes over hyperedges to learn
latent representations, have emerged as the method of choice for predictive
tasks in many of these domains. These approaches typically perform only a small
number of message-passing rounds to learn the representations, which they then
utilize for predictions. The small number of message-passing rounds comes at a
cost, as the representations only capture local information and forego
long-range high-order dependencies. However, as we demonstrate, blindly
increasing the message-passing rounds to capture long-range dependency also
degrades the performance of hyper-graph neural networks.
  Recent works have demonstrated that implicit graph neural networks capture
long-range dependencies in standard graphs while maintaining performance.
Despite their popularity, prior work has not studied long-range dependency
issues on hypergraph neural networks. Here, we first demonstrate that existing
hypergraph neural networks lose predictive power when aggregating more
information to capture long-range dependency. We then propose Implicit
Hypergraph Neural Network (IHNN), a novel framework that jointly learns
fixed-point representations for both nodes and hyperedges in an end-to-end
manner to alleviate this issue. Leveraging implicit differentiation, we
introduce a tractable projected gradient descent approach to train the model
efficiently. Extensive experiments on real-world hypergraphs for node
classification demonstrate that IHNN outperforms the closest prior works in
most settings, establishing a new state-of-the-art in hypergraph learning.

</details>


### [58] [Beyond Fixed Morphologies: Learning Graph Policies with Trust Region Compensation in Variable Action Spaces](https://arxiv.org/abs/2508.14102)
*Thomas Gallien*

Main category: cs.LG

TL;DR: 本文分析了信任区域优化方法（TRPO和PPO）在不同动作空间维度下的理论行为，特别关注形态变化对优化景观的影响，并通过Swimmer环境进行实证评估。


<details>
  <summary>Details</summary>
Motivation: 随着对可扩展和可重用控制策略需求的增长，形态泛化能力变得重要。图基策略架构能有效编码结构差异，但信任区域方法在变化动作空间维度下的行为尚不明确。

Method: 对TRPO和PPO进行理论分析，研究KL散度和策略裁剪惩罚约束下动作空间维度变化的影响，并在Gymnasium Swimmer环境中进行实证评估。

Result: 理论分析揭示了动作空间维度变化对优化景观的具体影响机制，实证评估在受控形态变化设置下验证了理论发现。

Conclusion: 该研究为理解信任区域方法在形态变化场景下的行为提供了理论框架和实证证据，对开发具有形态泛化能力的强化学习算法具有重要意义。

Abstract: Trust region-based optimization methods have become foundational
reinforcement learning algorithms that offer stability and strong empirical
performance in continuous control tasks. Growing interest in scalable and
reusable control policies translate also in a demand for morphological
generalization, the ability of control policies to cope with different
kinematic structures. Graph-based policy architectures provide a natural and
effective mechanism to encode such structural differences. However, while these
architectures accommodate variable morphologies, the behavior of trust region
methods under varying action space dimensionality remains poorly understood. To
this end, we conduct a theoretical analysis of trust region-based policy
optimization methods, focusing on both Trust Region Policy Optimization (TRPO)
and its widely used first-order approximation, Proximal Policy Optimization
(PPO). The goal is to demonstrate how varying action space dimensionality
influence the optimization landscape, particularly under the constraints
imposed by KL-divergence or policy clipping penalties. Complementing the
theoretical insights, an empirical evaluation under morphological variation is
carried out using the Gymnasium Swimmer environment. This benchmark offers a
systematically controlled setting for varying the kinematic structure without
altering the underlying task, making it particularly well-suited to study
morphological generalization.

</details>


### [59] [From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery](https://arxiv.org/abs/2508.14111)
*Jiaqi Wei,Yuejin Yang,Xiang Zhang,Yuhan Chen,Xiang Zhuang,Zhangyang Gao,Dongzhan Zhou,Guangshuai Wang,Zhiqiang Gao,Juntai Cao,Zijie Qiu,Xuming He,Qiang Zhang,Chenyu You,Shuangjia Zheng,Ning Ding,Wanli Ouyang,Nanqing Dong,Yu Cheng,Siqi Sun,Lei Bai,Bowen Zhou*

Main category: cs.LG

TL;DR: 本文提出了"代理科学"作为AI for Science的新范式，将AI系统从辅助工具提升为具有完整科学自主性的研究伙伴，通过统一框架综述了跨学科自主科学发现的研究进展。


<details>
  <summary>Details</summary>
Motivation: 人工智能正从专业计算工具发展为自主科研伙伴，需要建立系统框架来理解AI如何实现从部分协助到完整科学自主性的演进，统一当前碎片化的研究视角。

Method: 采用领域导向的综述方法，构建统一框架连接基础能力、核心流程和领域实现，从过程导向、自主性导向和机制导向三个维度分析自主科学发现。

Result: 提出了五类核心科学自主能力，建立了四阶段动态发现工作流模型，系统综述了生命科学、化学、材料科学和物理等领域的应用案例。

Conclusion: 确立了代理科学作为结构化范式，为推进AI驱动研究提供了系统框架，识别了关键挑战和未来机遇，推动了自主科学发现领域的发展。

Abstract: Artificial intelligence (AI) is reshaping scientific discovery, evolving from
specialized computational tools into autonomous research partners. We position
Agentic Science as a pivotal stage within the broader AI for Science paradigm,
where AI systems progress from partial assistance to full scientific agency.
Enabled by large language models (LLMs), multimodal systems, and integrated
research platforms, agentic AI shows capabilities in hypothesis generation,
experimental design, execution, analysis, and iterative refinement -- behaviors
once regarded as uniquely human. This survey provides a domain-oriented review
of autonomous scientific discovery across life sciences, chemistry, materials
science, and physics. We unify three previously fragmented perspectives --
process-oriented, autonomy-oriented, and mechanism-oriented -- through a
comprehensive framework that connects foundational capabilities, core
processes, and domain-specific realizations. Building on this framework, we (i)
trace the evolution of AI for Science, (ii) identify five core capabilities
underpinning scientific agency, (iii) model discovery as a dynamic four-stage
workflow, (iv) review applications across the above domains, and (v) synthesize
key challenges and future opportunities. This work establishes a
domain-oriented synthesis of autonomous scientific discovery and positions
Agentic Science as a structured paradigm for advancing AI-driven research.

</details>


### [60] [A Cost-Effective Framework for Predicting Parking Availability Using Geospatial Data and Machine Learning](https://arxiv.org/abs/2508.14125)
*Madyan Bagosher,Tala Mustafa,Mohammad Alsmirat,Amal Al-Ali,Isam Mashhour Al Jawarneh*

Main category: cs.LG

TL;DR: 基于多源数据的智能停车预测框架，通过机器学习模型预测停车占住情况，随机森林回归模型表现最佳


<details>
  <summary>Details</summary>
Motivation: 城市化进程中停车管理挑战加剧，特别是大学校园内学生需要快速找到空余停车位，而传统感知系统成本高且安装复杂

Method: 整合地图、移动性和气象数据，通过空间关联操作分析停车行为，采用线性回归、SVR、随机森林回归和LSTM等模型进行预测，使用网格搜索进行超参数调优

Result: 随机森林回归模型表现最佳，RMSE为0.142，R²为0.582，LSTM模型在更长时间段和更多数据情况下可能更优

Conclusion: 无需传统感知器的多源数据整合方法可以有效预测停车占住情况，为智能停车管理提供了成本效益更高的解决方案

Abstract: As urban populations continue to grow, cities face numerous challenges in
managing parking and determining occupancy. This issue is particularly
pronounced in university campuses, where students need to find vacant parking
spots quickly and conveniently during class timings. The limited availability
of parking spaces on campuses underscores the necessity of implementing
efficient systems to allocate vacant parking spots effectively. We propose a
smart framework that integrates multiple data sources, including street maps,
mobility, and meteorological data, through a spatial join operation to capture
parking behavior and vehicle movement patterns over the span of 3 consecutive
days with an hourly duration between 7AM till 3PM. The system will not require
any sensing tools to be installed in the street or in the parking area to
provide its services since all the data needed will be collected using location
services. The framework will use the expected parking entrance and time to
specify a suitable parking area. Several forecasting models, namely, Linear
Regression, Support Vector Regression (SVR), Random Forest Regression (RFR),
and Long Short-Term Memory (LSTM), are evaluated. Hyperparameter tuning was
employed using grid search, and model performance is assessed using Root Mean
Squared Error (RMSE), Mean Absolute Error (MAE) and Coefficient of
Determination (R2). Random Forest Regression achieved the lowest RMSE of 0.142
and highest R2 of 0.582. However, given the time-series nature of the task, an
LSTM model may perform better with additional data and longer timesteps.

</details>


### [61] [Comparison of derivative-free and gradient-based minimization for multi-objective compositional design of shape memory alloys](https://arxiv.org/abs/2508.14127)
*S. Josyula,Y. Noiman,E. J. Payton,T. Giovannelli*

Main category: cs.LG

TL;DR: 使用机器学习模型作为代理预测器，结合数值优化方法寻找满足马氏体起始温度要求且成本最低的形状记忆合金成分。神经网络模型配合梯度优化器比树模型配合无导数优化器表现更稳定、效果更好。


<details>
  <summary>Details</summary>
Motivation: 设计既满足性能目标又保持经济性和可持续性的形状记忆合金是一个复杂挑战，需要优化合金成分以实现目标马氏体起始温度同时最小化成本。

Method: 使用实验表征合金数据集和物理信息特征训练两种机器学习模型（树集成模型和神经网络），分别配合无导数优化器（COBYLA）和梯度优化器（TRUST-CONSTR）进行合金成分优化搜索。

Result: 两种模型预测精度相似，但神经网络配合梯度优化器能更稳定地找到更好的解决方案，而COBYLA经常收敛到次优结果，特别是在初始猜测远离目标时。

Conclusion: 研究展示了结合物理信息数据、机器学习模型和优化算法探索新型形状记忆合金成分的实用方法，虽然数据集规模较小，但使用实验数据提高了预测可靠性，该方法可扩展到其他材料设计领域。

Abstract: Designing shape memory alloys (SMAs) that meet performance targets while
remaining affordable and sustainable is a complex challenge. In this work, we
focus on optimizing SMA compositions to achieve a desired martensitic start
temperature (Ms) while minimizing cost. To do this, we use machine learning
models as surrogate predictors and apply numerical optimization methods to
search for suitable alloy combinations. We trained two types of machine
learning models, a tree-based ensemble and a neural network, using a dataset of
experimentally characterized alloys and physics-informed features. The
tree-based model was used with a derivative-free optimizer (COBYLA), while the
neural network, which provides gradient information, was paired with a
gradient-based optimizer (TRUST-CONSTR). Our results show that while both
models predict Ms with similar accuracy, the optimizer paired with the neural
network finds better solutions more consistently. COBYLA often converged to
suboptimal results, especially when the starting guess was far from the target.
The TRUST-CONSTR method showed more stable behavior and was better at reaching
alloy compositions that met both objectives. This study demonstrates a
practical approach to exploring new SMA compositions by combining
physics-informed data, machine learning models, and optimization algorithms.
Although the scale of our dataset is smaller than simulation-based efforts, the
use of experimental data improves the reliability of the predictions. The
approach can be extended to other materials where design trade-offs must be
made with limited data.

</details>


### [62] [ERIS: An Energy-Guided Feature Disentanglement Framework for Out-of-Distribution Time Series Classification](https://arxiv.org/abs/2508.14134)
*Xin Wu,Fei Teng,Ji Zhang,Xingwang Li,Yuxuan Liang*

Main category: cs.LG

TL;DR: ERIS框架通过能量引导校准、权重正交性和对抗训练，实现有指导的特征解耦，在时间序列分类的OOD数据上平均提升4.04%准确率


<details>
  <summary>Details</summary>
Motivation: 当前时间序列分类方法在分布外数据上表现不佳，因为模型将领域特定特征和标签相关特征纠缠在一起，形成伪相关，而现有特征解耦方法缺乏语义指导

Method: 提出ERIS端到端框架，包含三个核心机制：能量引导校准提供语义指导、权重级正交性策略强制结构独立性、辅助对抗训练增强鲁棒性

Result: 在四个基准测试上，ERIS比最先进基线平均提升4.04%的准确率

Conclusion: ERIS通过语义指导的特征解耦方法有效解决了时间序列分类中的分布外泛化问题，证明了语义指导对特征解耦的重要性

Abstract: An ideal time series classification (TSC) should be able to capture invariant
representations, but achieving reliable performance on out-of-distribution
(OOD) data remains a core obstacle. This obstacle arises from the way models
inherently entangle domain-specific and label-relevant features, resulting in
spurious correlations. While feature disentanglement aims to solve this,
current methods are largely unguided, lacking the semantic direction required
to isolate truly universal features. To address this, we propose an end-to-end
Energy-Regularized Information for Shift-Robustness (\textbf{ERIS}) framework
to enable guided and reliable feature disentanglement. The core idea is that
effective disentanglement requires not only mathematical constraints but also
semantic guidance to anchor the separation process. ERIS incorporates three key
mechanisms to achieve this goal. Specifically, we first introduce an
energy-guided calibration mechanism, which provides crucial semantic guidance
for the separation, enabling the model to self-calibrate. Additionally, a
weight-level orthogonality strategy enforces structural independence between
domain-specific and label-relevant features, thereby mitigating their
interference. Moreover, an auxiliary adversarial training mechanism enhances
robustness by injecting structured perturbations. Experiments demonstrate that
ERIS improves upon state-of-the-art baselines by an average of 4.04% accuracy
across four benchmarks.

</details>


### [63] [Towards Agent-based Test Support Systems: An Unsupervised Environment Design Approach](https://arxiv.org/abs/2508.14135)
*Collins O. Ogbodo,Timothy J. Rogers,Mattia Dal Borgo,David J. Wagg*

Main category: cs.LG

TL;DR: 提出基于代理的决策支持框架，用于动态模态测试环境中的自适应传感器布局，通过强化学习优化传感器位置


<details>
  <summary>Details</summary>
Motivation: 传统模态测试设计方法静态且刚性，无法适应测试参数变化，影响测试精度和适应性

Method: 使用欠定部分可观测马尔可夫决策过程建模问题，通过双课程学习策略训练通用强化学习代理

Result: 在钢悬臂结构案例研究中验证了方法在优化频率段传感器位置的有效性

Conclusion: 该方法具有鲁棒性和实际应用价值，能显著提升模态测试的准确性和适应性

Abstract: Modal testing plays a critical role in structural analysis by providing
essential insights into dynamic behaviour across a wide range of engineering
industries. In practice, designing an effective modal test campaign involves
complex experimental planning, comprising a series of interdependent decisions
that significantly influence the final test outcome. Traditional approaches to
test design are typically static-focusing only on global tests without
accounting for evolving test campaign parameters or the impact of such changes
on previously established decisions, such as sensor configurations, which have
been found to significantly influence test outcomes. These rigid methodologies
often compromise test accuracy and adaptability. To address these limitations,
this study introduces an agent-based decision support framework for adaptive
sensor placement across dynamically changing modal test environments. The
framework formulates the problem using an underspecified partially observable
Markov decision process, enabling the training of a generalist reinforcement
learning agent through a dual-curriculum learning strategy. A detailed case
study on a steel cantilever structure demonstrates the efficacy of the proposed
method in optimising sensor locations across frequency segments, validating its
robustness and real-world applicability in experimental settings.

</details>


### [64] [STAS: Spatio-Temporal Adaptive Computation Time for Spiking Transformers](https://arxiv.org/abs/2508.14138)
*Donghwa Kang,Doohyun Kim,Sang-Ki Ko,Jinkyu Lee,Brent ByungHoon Kang,Hyeongboo Baek*

Main category: cs.LG

TL;DR: STAS框架通过协同设计静态架构和动态计算策略，解决了脉冲神经网络在视觉Transformer中应用自适应计算时间的两大核心问题，实现了空间和时间维度的token剪枝，显著降低能耗同时提升准确率。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络(SNNs)虽然比人工神经网络(ANNs)更节能，但由于多时间步操作特性导致高延迟和计算开销。现有的动态计算方法针对空间、时间或架构特定冗余进行优化，但缺乏统一框架。自适应计算时间(ACT)原则为统一方法提供了基础，但在SNN-based视觉Transformer中的应用受到时间相似性前提违反和静态架构不适配两大问题的阻碍。

Method: 提出STAS框架，包含两个核心模块：1) 集成脉冲补丁分割(I-SPS)模块：建立时间稳定性，创建统一输入表示，解决时间不相似性的架构问题；2) 自适应脉冲自注意力(A-SSA)模块：在空间和时间两个维度上进行token剪枝。

Result: 在CIFAR-10、CIFAR-100和ImageNet数据集上验证，STAS分别减少能耗45.9%、43.8%和30.1%，同时相比SOTA模型提升了准确率。

Conclusion: STAS通过协同设计架构和计算策略，成功解决了SNN-based视觉Transformer中应用ACT的挑战，实现了显著的能耗降低和性能提升，为脉冲神经网络的高效部署提供了有效解决方案。

Abstract: Spiking neural networks (SNNs) offer energy efficiency over artificial neural
networks (ANNs) but suffer from high latency and computational overhead due to
their multi-timestep operational nature. While various dynamic computation
methods have been developed to mitigate this by targeting spatial, temporal, or
architecture-specific redundancies, they remain fragmented. While the
principles of adaptive computation time (ACT) offer a robust foundation for a
unified approach, its application to SNN-based vision Transformers (ViTs) is
hindered by two core issues: the violation of its temporal similarity
prerequisite and a static architecture fundamentally unsuited for its
principles. To address these challenges, we propose STAS (Spatio-Temporal
Adaptive computation time for Spiking transformers), a framework that
co-designs the static architecture and dynamic computation policy. STAS
introduces an integrated spike patch splitting (I-SPS) module to establish
temporal stability by creating a unified input representation, thereby solving
the architectural problem of temporal dissimilarity. This stability, in turn,
allows our adaptive spiking self-attention (A-SSA) module to perform
two-dimensional token pruning across both spatial and temporal axes.
Implemented on spiking Transformer architectures and validated on CIFAR-10,
CIFAR-100, and ImageNet, STAS reduces energy consumption by up to 45.9%, 43.8%,
and 30.1%, respectively, while simultaneously improving accuracy over SOTA
models.

</details>


### [65] [Topological Data Analysis for Unsupervised Anomaly Detection and Customer Segmentation on Banking Data](https://arxiv.org/abs/2508.14136)
*Leonardo Aldo Alejandro Barberi,Linda Maria De Cave*

Main category: cs.LG

TL;DR: 本文介绍拓扑数据分析(TDA)在银行数据异常检测和客户细分中的应用，使用Mapper算法和持久同调技术从拓扑角度发现数据模式


<details>
  <summary>Details</summary>
Motivation: 将抽象的拓扑数学理论与实际工业应用相结合，为银行业提供无监督的异常检测和客户细分解决方案

Method: 采用Mapper算法和持久同调技术，开发无监督程序来分析客户银行数据的拓扑信息

Result: 框架能够发现客户数据中有意义的模式，产生可操作的业务洞察

Conclusion: 拓扑数据分析为银行业提供了结合数学理论和实际应用价值的有效工具

Abstract: This paper introduces advanced techniques of Topological Data Analysis (TDA)
for unsupervised anomaly detection and customer segmentation in banking data.
Using the Mapper algorithm and persistent homology, we develop unsupervised
procedures that uncover meaningful patterns in customers' banking data by
exploiting topological information. The framework we present in this paper
yields actionable insights that combine the abstract mathematical subject of
topology with real-life use cases that are useful in industry.

</details>


### [66] [Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs](https://arxiv.org/abs/2508.14140)
*Orestis Konstantaropoulos,Stelios Manolis Smirnakis,Maria Papadopouli*

Main category: cs.LG

TL;DR: G2GNet是一种受生物神经网络启发的稀疏模块化架构，通过模仿小鼠视觉皮层功能连接模式，在保持高精度的同时实现高达75%的稀疏度，性能优于全连接模型。


<details>
  <summary>Details</summary>
Motivation: 受生物神经网络模块化、分层和稀疏连接结构的启发，探索如何将功能连接模式应用于人工神经网络设计，以实现计算效率、功能专业化和鲁棒性的平衡。

Method: 引入G2GNet架构，在前馈层间施加稀疏模块化连接；采用动态稀疏训练机制在训练过程中修剪和再生连接；提出基于激活相关性的类赫布重连规则。

Result: 在Fashion-MNIST、CIFAR-10和CIFAR-100等标准视觉基准测试中，G2GNet以显著更少的参数实现了比全连接模型更高的准确率（提升达4.3%），同时达到75%的稀疏度。

Conclusion: 这是首个将生物观察到的功能连接模式作为结构偏置融入ANN设计的架构，证明了生物启发方法在提高神经网络效率和性能方面的有效性。

Abstract: The structure of biological neural circuits-modular, hierarchical, and
sparsely interconnected-reflects an efficient trade-off between wiring cost,
functional specialization, and robustness. These principles offer valuable
insights for artificial neural network (ANN) design, especially as networks
grow in depth and scale. Sparsity, in particular, has been widely explored for
reducing memory and computation, improving speed, and enhancing generalization.
Motivated by systems neuroscience findings, we explore how patterns of
functional connectivity in the mouse visual cortex-specifically,
ensemble-to-ensemble communication, can inform ANN design. We introduce G2GNet,
a novel architecture that imposes sparse, modular connectivity across
feedforward layers. Despite having significantly fewer parameters than fully
connected models, G2GNet achieves superior accuracy on standard vision
benchmarks. To our knowledge, this is the first architecture to incorporate
biologically observed functional connectivity patterns as a structural bias in
ANN design. We complement this static bias with a dynamic sparse training (DST)
mechanism that prunes and regrows edges during training. We also propose a
Hebbian-inspired rewiring rule based on activation correlations, drawing on
principles of biological plasticity. G2GNet achieves up to 75% sparsity while
improving accuracy by up to 4.3% on benchmarks, including Fashion-MNIST,
CIFAR-10, and CIFAR-100, outperforming dense baselines with far fewer
computations.

</details>


### [67] [Learning to Learn the Macroscopic Fundamental Diagram using Physics-Informed and meta Machine Learning techniques](https://arxiv.org/abs/2508.14137)
*Amalie Roark,Serio Agriesti,Francisco Camara Pereira,Guido Cantelmo*

Main category: cs.LG

TL;DR: 提出基于元学习的宏观基本图估计框架，解决交通网络中检测器数据不足的问题，通过多城市数据训练模型，在数据稀缺的城市中显著提升流量预测精度。


<details>
  <summary>Details</summary>
Motivation: 宏观基本图(MFD)是描述聚合交通动态的重要工具，但传统估计方法需要大量检测器数据，而实际应用中往往数据稀缺。需要开发能够适应不同城市、不同检测器配置的通用估计方法。

Method: 采用元学习框架，结合多任务物理信息神经网络(MT-PINN)，利用多个城市的交通数据训练模型，使其能够快速适应新城市的MFD估计任务，即使检测器数量有限。

Result: 在流量预测方面平均MSE改进约17500-36000（取决于检测器子集），相比传统迁移学习方法表现更优，能够成功泛化到不同拓扑结构的城市环境。

Conclusion: 元学习框架有效解决了MFD估计中的数据稀缺问题，证明了在检测器数量有限的情况下，元学习相比传统方法具有更好的泛化能力和性能表现。

Abstract: The Macroscopic Fundamental Diagram is a popular tool used to describe
traffic dynamics in an aggregated way, with applications ranging from traffic
control to incident analysis. However, estimating the MFD for a given network
requires large numbers of loop detectors, which is not always available in
practice. This article proposes a framework harnessing meta-learning, a
subcategory of machine learning that trains models to understand and adapt to
new tasks on their own, to alleviate the data scarcity challenge. The developed
model is trained and tested by leveraging data from multiple cities and
exploiting it to model the MFD of other cities with different shares of
detectors and topological structures. The proposed meta-learning framework is
applied to an ad-hoc Multi-Task Physics-Informed Neural Network, specifically
designed to estimate the MFD. Results show an average MSE improvement in flow
prediction ranging between ~ 17500 and 36000 (depending on the subset of loop
detectors tested). The meta-learning framework thus successfully generalizes
across diverse urban settings and improves performance on cities with limited
data, demonstrating the potential of using meta-learning when a limited number
of detectors is available. Finally, the proposed framework is validated against
traditional transfer learning approaches and tested with FitFun, a
non-parametric model from the literature, to prove its transferability.

</details>


### [68] [Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2508.14285)
*Liyi Zhang,Jake Snell,Thomas L. Griffiths*

Main category: cs.LG

TL;DR: 提出了ABMLL方法，一种计算高效的贝叶斯元学习框架，用于提升LoRA微调后大语言模型的泛化能力和不确定性量化


<details>
  <summary>Details</summary>
Motivation: 现有提升LLM泛化能力的方法（如上下文提示优化和元学习）存在内存和计算成本高的问题，需要长上下文提示或二阶梯度更新

Method: 基于摊销贝叶斯元学习框架，重新定义LoRA中的任务特定参数和全局参数，使用新超参数平衡重构精度和参数保真度

Result: 在Unified-QA和CrossFit数据集上测试，ABMLL在准确率和预期校准误差方面均优于现有方法，可扩展到Llama3-8B等大模型

Conclusion: ABMLL提供了一种计算高效的解决方案，既能提升LLM的泛化性能，又能改善不确定性量化，适用于大规模语言模型

Abstract: Fine-tuning large language models (LLMs) with low-rank adaptaion (LoRA) is a
cost-effective way to incorporate information from a specific dataset. However,
it is often unclear how well the fine-tuned LLM will generalize, i.e., how well
it will perform on unseen datasets. Methods have been proposed to improve
generalization by optimizing with in-context prompts, or by using meta-learning
to fine-tune LLMs. However, these methods are expensive in memory and
computation, requiring either long-context prompts or saving copies of
parameters and using second-order gradient updates. To address these
challenges, we propose Amortized Bayesian Meta-Learning for LoRA (ABMLL). This
method builds on amortized Bayesian meta-learning for smaller models, adapting
this approach to LLMs while maintaining its computational efficiency. We
reframe task-specific and global parameters in the context of LoRA and use a
set of new hyperparameters to balance reconstruction accuracy and the fidelity
of task-specific parameters to the global ones. ABMLL provides effective
generalization and scales to large models such as Llama3-8B. Furthermore, as a
result of using a Bayesian framework, ABMLL provides improved uncertainty
quantification. We test ABMLL on Unified-QA and CrossFit datasets and find that
it outperforms existing methods on these benchmarks in terms of both accuracy
and expected calibration error.

</details>


### [69] [GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation](https://arxiv.org/abs/2508.14302)
*Amirmohsen Sattarifard,Sepehr Lavasani,Ehsan Imani,Kunlin Zhang,Hanlin Xu,Fengyu Sun,Negar Hassanpour,Chao Gao*

Main category: cs.LG

TL;DR: GLASS是一种无需训练的动态剪枝方法，通过结合局部激活和全局重要性统计，在边缘设备上高效部署大语言模型，显著优于现有零剪枝方法。


<details>
  <summary>Details</summary>
Motivation: 在边缘硬件上部署大语言模型需要动态剪枝来减少计算量，但现有静态方法或基于预测器的方法存在单一稀疏模式或额外运行时开销的问题，零剪枝方法在短提示/长生成场景中效果不佳。

Method: 提出A/I-GLASS方法：基于激活和影响的全局-局部神经重要性聚合，使用排名聚合技术结合提示局部统计和模型固有全局统计来动态选择前馈网络单元。

Result: 在多个LLM和基准测试中，GLASS显著优于先前的无训练方法，特别是在具有挑战性的长文本生成场景中，且不依赖辅助预测器或增加推理开销。

Conclusion: GLASS提供了一种有效的训练免费动态剪枝解决方案，能够在边缘设备上高效部署大语言模型，同时保持生成质量。

Abstract: Deploying Large Language Models (LLMs) on edge hardware demands aggressive,
prompt-aware dynamic pruning to reduce computation without degrading quality.
Static or predictor-based schemes either lock in a single sparsity pattern or
incur extra runtime overhead, and recent zero-shot methods that rely on
statistics from a single prompt fail on short prompt and/or long generation
scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local
neural importance Aggregation for feed-forward network SparSification, two
training-free methods that dynamically select FFN units using a
rank-aggregation of prompt local and model-intrinsic global neuron statistics.
Empirical results across multiple LLMs and benchmarks demonstrate that GLASS
significantly outperforms prior training-free methods, particularly in
challenging long-form generation scenarios, without relying on auxiliary
predictors or adding any inference overhead.

</details>


### [70] [Learning Time-Varying Convexifications of Multiple Fairness Measures](https://arxiv.org/abs/2508.14311)
*Quan Zhou,Jakub Marecek,Robert Shorten*

Main category: cs.LG

TL;DR: 论文提出了一种在有限图结构反馈下学习时变多重公平性度量凸化的方法，用于处理多个群体和个体公平性概念的权重学习问题。


<details>
  <summary>Details</summary>
Motivation: 随着对多重公平性度量需求的增加，需要同时考虑多个群体和个体公平性概念。这些公平性正则化器的相对权重是未知的、可能随时间变化，并且需要实时学习。

Method: 采用有限图结构反馈的方式来学习时变的多重公平性度量的凸化表示，处理权重参数的动态调整问题。

Result: 该方法能够有效处理多重公平性度量的权重学习问题，适应时变的环境需求。

Conclusion: 提出的学习框架为解决多重公平性度量的动态权重调整问题提供了有效解决方案，特别适用于需要实时调整公平性权重的应用场景。

Abstract: There is an increasing appreciation that one may need to consider multiple
measures of fairness, e.g., considering multiple group and individual fairness
notions. The relative weights of the fairness regularisers are a priori
unknown, may be time varying, and need to be learned on the fly. We consider
the learning of time-varying convexifications of multiple fairness measures
with limited graph-structured feedback.

</details>


### [71] [Beyond Turing: Memory-Amortized Inference as a Foundation for Cognitive Computation](https://arxiv.org/abs/2508.14143)
*Xin Li*

Main category: cs.LG

TL;DR: 本文提出了记忆摊销推理（MAI）框架，将认知建模为对记忆循环的推理而非梯度下降重计算，通过结构重用实现熵最小化和上下文感知推理，为通用智能提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 智能本质上是非遍历的，不是从零开始的均匀采样或优化，而是基于先前推理轨迹的结构化重用。当前AI面临计算瓶颈，需要更高效的推理方法。

Method: 引入记忆摊销推理（MAI）框架，将认知建模为对记忆循环的推理，使用delta同调理论分析，将每个皮层柱建模为对循环一致记忆状态的局部推理算子。

Result: 建立了MAI与强化学习的时间反转对偶性：RL从奖励向前传播价值，MAI从记忆向后重建潜在原因。这为能量高效推理提供了路径。

Conclusion: MAI提供了一个基于结构、重用和记忆的统一生物学基础智能理论，为人工通用智能（AGI）的实现提供了深刻启示。

Abstract: Intelligence is fundamentally non-ergodic: it emerges not from uniform
sampling or optimization from scratch, but from the structured reuse of prior
inference trajectories. We introduce Memory-Amortized Inference (MAI) as a
formal framework in which cognition is modeled as inference over latent cycles
in memory, rather than recomputation through gradient descent. MAI systems
encode inductive biases via structural reuse, minimizing entropy and enabling
context-aware, structure-preserving inference. This approach reframes cognitive
systems not as ergodic samplers, but as navigators over constrained latent
manifolds, guided by persistent topological memory. Through the lens of
delta-homology, we show that MAI provides a principled foundation for
Mountcastle's Universal Cortical Algorithm, modeling each cortical column as a
local inference operator over cycle-consistent memory states. Furthermore, we
establish a time-reversal duality between MAI and reinforcement learning:
whereas RL propagates value forward from reward, MAI reconstructs latent causes
backward from memory. This inversion paves a path toward energy-efficient
inference and addresses the computational bottlenecks facing modern AI. MAI
thus offers a unified, biologically grounded theory of intelligence based on
structure, reuse, and memory. We also briefly discuss the profound implications
of MAI for achieving artificial general intelligence (AGI).

</details>


### [72] [Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS](https://arxiv.org/abs/2508.14313)
*Can Jin,Yang Zhou,Qixin Zhang,Hongwu Peng,Di Zhang,Marco Pavone,Ligong Han,Zhang-Wei Hong,Tong Che,Dimitris N. Metaxas*

Main category: cs.LG

TL;DR: AIRL-S统一了强化学习和搜索方法，通过对抗逆强化学习直接从正确推理轨迹学习动态过程奖励模型，无需人工标注，在多个推理任务中性能提升9%，达到GPT-4o水平


<details>
  <summary>Details</summary>
Motivation: 现有测试时缩放方法存在两大问题：强化学习方法不稳定且样本效率低；搜索方法需要昂贵的人工标注且容易受分布偏移影响。需要一种统一的解决方案

Method: 结合对抗逆强化学习(AIRL)和组相对策略优化(GRPO)，直接从正确推理轨迹学习密集动态过程奖励模型，该模型同时作为RL的评论家和搜索的启发式函数

Result: 在8个基准测试（数学、科学推理、代码生成）中平均性能提升9%，达到GPT-4o水平，在多种搜索算法中均优于使用标注数据训练的基线PRM

Conclusion: RL训练的奖励函数本身就是搜索的最佳过程奖励模型，为LLM复杂推理任务提供了鲁棒且成本效益高的解决方案

Abstract: Test-time scaling (TTS) for large language models (LLMs) has thus far fallen
into two largely separate paradigms: (1) reinforcement learning (RL) methods
that optimize sparse outcome-based rewards, yet suffer from instability and low
sample efficiency; and (2) search-based techniques guided by independently
trained, static process reward models (PRMs), which require expensive human- or
LLM-generated labels and often degrade under distribution shifts. In this
paper, we introduce AIRL-S, the first natural unification of RL-based and
search-based TTS. Central to AIRL-S is the insight that the reward function
learned during RL training inherently represents the ideal PRM for guiding
downstream search. Specifically, we leverage adversarial inverse reinforcement
learning (AIRL) combined with group relative policy optimization (GRPO) to
learn a dense, dynamic PRM directly from correct reasoning traces, entirely
eliminating the need for labeled intermediate process data. At inference, the
resulting PRM simultaneously serves as the critic for RL rollouts and as a
heuristic to effectively guide search procedures, facilitating robust reasoning
chain extension, mitigating reward hacking, and enhancing cross-task
generalization. Experimental results across eight benchmarks, including
mathematics, scientific reasoning, and code generation, demonstrate that our
unified approach improves performance by 9 % on average over the base model,
matching GPT-4o. Furthermore, when integrated into multiple search algorithms,
our PRM consistently outperforms all baseline PRMs trained with labeled data.
These results underscore that, indeed, your reward function for RL is your best
PRM for search, providing a robust and cost-effective solution to complex
reasoning tasks in LLMs.

</details>


### [73] [Noise Robust One-Class Intrusion Detection on Dynamic Graphs](https://arxiv.org/abs/2508.14192)
*Aleksei Liuliakov,Alexander Schulz,Luca Hermes,Barbara Hammer*

Main category: cs.LG

TL;DR: 提出概率化TGN-SVDD模型，通过高斯分布预测增强网络入侵检测在噪声数据下的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 网络入侵检测领域面临污染和噪声数据输入的鲁棒性挑战，需要提升在噪声环境下的检测准确性

Method: 开发概率化TGN-SVDD模型，为每个网络事件预测高斯分布参数，自然处理噪声对抗样本

Result: 在添加合成噪声的CIC-IDS2017数据集上实验显示，相比基线TGN-SVDD模型检测性能显著提升，特别是在高噪声水平下

Conclusion: 概率化方法能有效增强网络入侵检测模型对噪声数据的鲁棒性，为实际应用中的噪声环境提供了更好的解决方案

Abstract: In the domain of network intrusion detection, robustness against contaminated
and noisy data inputs remains a critical challenge. This study introduces a
probabilistic version of the Temporal Graph Network Support Vector Data
Description (TGN-SVDD) model, designed to enhance detection accuracy in the
presence of input noise. By predicting parameters of a Gaussian distribution
for each network event, our model is able to naturally address noisy
adversarials and improve robustness compared to a baseline model. Our
experiments on a modified CIC-IDS2017 data set with synthetic noise demonstrate
significant improvements in detection performance compared to the baseline
TGN-SVDD model, especially as noise levels increase.

</details>


### [74] [A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques for Autonomous Cyber Operations](https://arxiv.org/abs/2508.14340)
*Konur Tholl,Mariam El Mezouar,Ranwa Al Mallah*

Main category: cs.LG

TL;DR: 本研究在自主网络操作(ACO)中引入教师引导技术，显著提升了强化学习代理的训练效率和早期性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有的ACO应用需要智能体从零开始学习，导致收敛速度慢且早期性能差。虽然教师引导技术在其他领域已显示出潜力，但尚未应用于ACO领域。

Method: 在模拟CybORG环境中实现了四种不同的教师引导技术，并进行了比较评估。

Result: 教师集成能够显著提高训练效率，包括早期策略性能和收敛速度方面都有明显改善。

Conclusion: 教师引导技术对自主网络安全具有潜在的重要价值，能够有效解决ACO中学习效率低下的问题。

Abstract: Autonomous Cyber Operations (ACO) rely on Reinforcement Learning (RL) to
train agents to make effective decisions in the cybersecurity domain. However,
existing ACO applications require agents to learn from scratch, leading to slow
convergence and poor early-stage performance. While teacher-guided techniques
have demonstrated promise in other domains, they have not yet been applied to
ACO. In this study, we implement four distinct teacher-guided techniques in the
simulated CybORG environment and conduct a comparative evaluation. Our results
demonstrate that teacher integration can significantly improve training
efficiency in terms of early policy performance and convergence speed,
highlighting its potential benefits for autonomous cybersecurity.

</details>


### [75] [Reliability comparison of vessel trajectory prediction models via Probability of Detection](https://arxiv.org/abs/2508.14198)
*Zahra Rastin,Kathrin Donandt,Dirk Söffker*

Main category: cs.LG

TL;DR: 该研究评估了不同深度学习方法的船舶轨迹预测性能，重点关注不同交通复杂度下的模型表现和可靠性评估，通过检测概率分析量化模型可靠性


<details>
  <summary>Details</summary>
Motivation: 现有的船舶轨迹预测模型忽视了特定交通场景的复杂度，缺乏可靠性评估，需要更全面的评估方法来提升内河航行的安全性

Method: 使用检测概率分析来量化模型在不同交通场景下的可靠性，将所有模型在按预测期间交通情况分类的测试样本上进行评估，获得每个类别的性能指标和可靠性估计

Result: 综合评估结果提供了对不同预测方法优缺点及其可靠性的深入理解，包括可以保证安全预测的预测时间范围

Conclusion: 这些发现可以为开发更可靠的船舶轨迹预测方法提供信息，从而提高未来内河航行的安全性和效率

Abstract: This contribution addresses vessel trajectory prediction (VTP), focusing on
the evaluation of different deep learning-based approaches. The objective is to
assess model performance in diverse traffic complexities and compare the
reliability of the approaches. While previous VTP models overlook the specific
traffic situation complexity and lack reliability assessments, this research
uses a probability of detection analysis to quantify model reliability in
varying traffic scenarios, thus going beyond common error distribution
analyses. All models are evaluated on test samples categorized according to
their traffic situation during the prediction horizon, with performance metrics
and reliability estimates obtained for each category. The results of this
comprehensive evaluation provide a deeper understanding of the strengths and
weaknesses of the different prediction approaches, along with their reliability
in terms of the prediction horizon lengths for which safe forecasts can be
guaranteed. These findings can inform the development of more reliable vessel
trajectory prediction approaches, enhancing safety and efficiency in future
inland waterways navigation.

</details>


### [76] [Generative AI Against Poaching: Latent Composite Flow Matching for Wildlife Conservation](https://arxiv.org/abs/2508.14342)
*Lingkai Kong,Haichuan Wang,Charles A. Emogor,Vincent Börsch-Supan,Lily Xu,Milind Tambe*

Main category: cs.LG

TL;DR: 本文提出了一种基于流匹配的生成模型来预测偷猎行为，通过整合占用检测模型和复合流初始化方法，解决了偷猎数据不完整和稀缺的问题，在乌干达国家公园数据集上取得了更好的预测精度。


<details>
  <summary>Details</summary>
Motivation: 偷猎对野生动物和生物多样性构成重大威胁。现有基于线性模型或决策树的偷猎预测方法缺乏表达能力，无法捕捉复杂的非线性时空模式。生成模型特别是流匹配提供了更灵活的替代方案，但面临偷猎事件检测不完整和数据有限两个主要障碍。

Method: 1. 将流匹配与基于占用的检测模型集成，在潜在空间中训练流以推断底层占用状态，解决检测不完整问题；2. 采用从线性模型预测初始化的复合流而非标准扩散模型中的随机噪声，注入先验知识并提高泛化能力，缓解数据稀缺问题。

Result: 在乌干达两个国家公园的数据集上进行评估，结果显示该方法在预测准确性方面取得了持续提升。

Conclusion: 该方法通过整合检测模型和复合流初始化，有效解决了偷猎预测中的检测不完整和数据稀缺问题，为保护干预措施提供了更准确的预测工具。

Abstract: Poaching poses significant threats to wildlife and biodiversity. A valuable
step in reducing poaching is to forecast poacher behavior, which can inform
patrol planning and other conservation interventions. Existing poaching
prediction methods based on linear models or decision trees lack the
expressivity to capture complex, nonlinear spatiotemporal patterns. Recent
advances in generative modeling, particularly flow matching, offer a more
flexible alternative. However, training such models on real-world poaching data
faces two central obstacles: imperfect detection of poaching events and limited
data. To address imperfect detection, we integrate flow matching with an
occupancy-based detection model and train the flow in latent space to infer the
underlying occupancy state. To mitigate data scarcity, we adopt a composite
flow initialized from a linear-model prediction rather than random noise which
is the standard in diffusion models, injecting prior knowledge and improving
generalization. Evaluations on datasets from two national parks in Uganda show
consistent gains in predictive accuracy.

</details>


### [77] [Graph Concept Bottleneck Models](https://arxiv.org/abs/2508.14255)
*Haotian Xu,Tsui-Wei Weng,Lam M. Nguyen,Tengfei Ma*

Main category: cs.LG

TL;DR: GraphCBMs通过构建潜在概念图来增强概念瓶颈模型，考虑概念间的相关性，提升分类性能和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有概念瓶颈模型假设概念条件独立，忽略了概念间的隐藏关系，限制了模型性能

Method: 构建潜在概念图来建模概念间关系，与概念瓶颈模型结合，保留可解释性的同时增强性能

Result: 在真实图像分类任务中表现优异，提供更好的概念结构信息，支持更有效的干预，在不同训练和架构设置下保持稳健

Conclusion: GraphCBMs通过显式建模概念间关系，显著提升了概念瓶颈模型的性能和实用性

Abstract: Concept Bottleneck Models (CBMs) provide explicit interpretations for deep
neural networks through concepts and allow intervention with concepts to adjust
final predictions. Existing CBMs assume concepts are conditionally independent
given labels and isolated from each other, ignoring the hidden relationships
among concepts. However, the set of concepts in CBMs often has an intrinsic
structure where concepts are generally correlated: changing one concept will
inherently impact its related concepts. To mitigate this limitation, we propose
GraphCBMs: a new variant of CBM that facilitates concept relationships by
constructing latent concept graphs, which can be combined with CBMs to enhance
model performance while retaining their interpretability. Our experiment
results on real-world image classification tasks demonstrate Graph CBMs offer
the following benefits: (1) superior in image classification tasks while
providing more concept structure information for interpretability; (2) able to
utilize latent concept graphs for more effective interventions; and (3) robust
in performance across different training and architecture settings.

</details>


### [78] [Organ-Agents: Virtual Human Physiology Simulator via LLMs](https://arxiv.org/abs/2508.14357)
*Rihao Chang,He Jiao,Weizhi Nie,Honglin Guo,Keliang Xie,Zhenhua Wu,Lina Zhao,Yunpeng Bai,Yongtao Ma,Lanjun Wang,Yuting Su,Xi Gao,Weijie Wang,Nicu Sebe,Bruno Lepri,Bingwei Sun*

Main category: cs.LG

TL;DR: Organ-Agents是一个基于大语言模型的多代理框架，通过模拟人体9个生理系统的125个变量来创建数字孪生，在脓毒症患者数据上表现出高精度仿真能力，并获得临床医生认可。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型的新进展来模拟复杂生理系统，为重症监护提供可信、可解释的数字孪生，用于精准诊断、治疗模拟和假设测试。

Method: 采用多代理框架，每个代理模拟特定生理系统（如心血管、肾脏、免疫系统）。通过监督微调系统特定的时间序列数据，然后使用动态参考选择和错误校正进行强化引导协调。

Result: 在4,509名保留患者上实现高仿真精度（各系统MSE<0.16），外部验证显示在分布偏移下性能稳定。15名重症医生确认了真实性和生理合理性（平均评分3.9和3.7）。合成数据训练的预警分类器AUROC下降<0.04。

Conclusion: Organ-Agents是一个可信、可解释且可推广的数字孪生平台，能够准确重现关键多系统事件，支持替代治疗策略的反事实模拟，在重症监护领域具有重要应用价值。

Abstract: Recent advances in large language models (LLMs) have enabled new
possibilities in simulating complex physiological systems. We introduce
Organ-Agents, a multi-agent framework that simulates human physiology via
LLM-driven agents. Each Simulator models a specific system (e.g.,
cardiovascular, renal, immune). Training consists of supervised fine-tuning on
system-specific time-series data, followed by reinforcement-guided coordination
using dynamic reference selection and error correction. We curated data from
7,134 sepsis patients and 7,895 controls, generating high-resolution
trajectories across 9 systems and 125 variables. Organ-Agents achieved high
simulation accuracy on 4,509 held-out patients, with per-system MSEs <0.16 and
robustness across SOFA-based severity strata. External validation on 22,689 ICU
patients from two hospitals showed moderate degradation under distribution
shifts with stable simulation. Organ-Agents faithfully reproduces critical
multi-system events (e.g., hypotension, hyperlactatemia, hypoxemia) with
coherent timing and phase progression. Evaluation by 15 critical care
physicians confirmed realism and physiological plausibility (mean Likert
ratings 3.9 and 3.7). Organ-Agents also enables counterfactual simulations
under alternative sepsis treatment strategies, generating trajectories and
APACHE II scores aligned with matched real-world patients. In downstream early
warning tasks, classifiers trained on synthetic data showed minimal AUROC drops
(<0.04), indicating preserved decision-relevant patterns. These results
position Organ-Agents as a credible, interpretable, and generalizable digital
twin for precision diagnosis, treatment simulation, and hypothesis testing in
critical care.

</details>


### [79] [Online Incident Response Planning under Model Misspecification through Bayesian Learning and Belief Quantization](https://arxiv.org/abs/2508.14385)
*Kim Hammar,Tao Li*

Main category: cs.LG

TL;DR: MOBAL是一种在线贝叶斯学习方法，用于网络攻击事件响应规划，能够在模型不准确的情况下通过迭代学习优化响应决策。


<details>
  <summary>Details</summary>
Motivation: 现有的事件响应决策支持框架需要详细的系统模型，但在实际网络攻击中信息往往不完整或不准确，限制了这些框架的实用性。

Method: 提出MOBAL方法：通过贝叶斯学习迭代修正模型猜想，将推测模型量化为有限马尔可夫模型，并利用动态规划进行高效的在线响应规划。

Result: 理论证明贝叶斯学习相对于信息反馈是渐近一致的，建立了模型错误指定和量化误差的界限。在CAGE-2基准测试中，MOBAL在适应性和对模型错误指定的鲁棒性方面优于现有技术。

Conclusion: MOBAL方法能够有效解决网络事件响应中的模型不准确问题，通过在线学习和模型量化实现了快速且鲁棒的决策支持。

Abstract: Effective responses to cyberattacks require fast decisions, even when
information about the attack is incomplete or inaccurate. However, most
decision-support frameworks for incident response rely on a detailed system
model that describes the incident, which restricts their practical utility. In
this paper, we address this limitation and present an online method for
incident response planning under model misspecification, which we call MOBAL:
Misspecified Online Bayesian Learning. MOBAL iteratively refines a conjecture
about the model through Bayesian learning as new information becomes available,
which facilitates model adaptation as the incident unfolds. To determine
effective responses online, we quantize the conjectured model into a finite
Markov model, which enables efficient response planning through dynamic
programming. We prove that Bayesian learning is asymptotically consistent with
respect to the information feedback. Additionally, we establish bounds on
misspecification and quantization errors. Experiments on the CAGE-2 benchmark
show that MOBAL outperforms the state of the art in terms of adaptability and
robustness to model misspecification.

</details>


### [80] [Exact Shapley Attributions in Quadratic-time for FANOVA Gaussian Processes](https://arxiv.org/abs/2508.14499)
*Majid Mohammadi,Krikamol Muandet,Ilaria Tiddi,Annette Ten Teije,Siu Lun Chau*

Main category: cs.LG

TL;DR: 提出了针对FANOVA高斯过程的精确Shapley值计算方法，将计算复杂度从指数级降低到二次方，支持局部和全局解释，并能捕捉不确定性。


<details>
  <summary>Details</summary>
Motivation: Shapley值是特征重要性归因的重要方法，但计算复杂度随特征数量指数增长，特别是在概率模型（如高斯过程）中需要额外计算高阶矩，限制了实际应用。

Method: 利用FANOVA GP的闭式Möbius表示，引入递归算法（受牛顿恒等式启发），为局部解释定义随机合作博弈，为全局解释引入基于方差的值函数，实现二次时间复杂度的精确计算。

Result: 实现了在二次时间内计算精确的随机Shapley值，既能捕捉期望贡献又能处理不确定性，为结构化概率模型提供可扩展、公理合理且不确定性感知的解释。

Conclusion: 该方法显著提升了可解释AI的实用性，通过更高效、精确且考虑不确定性的特征归因，增强了概率模型预测的可解释性。

Abstract: Shapley values are widely recognized as a principled method for attributing
importance to input features in machine learning. However, the exact
computation of Shapley values scales exponentially with the number of features,
severely limiting the practical application of this powerful approach. The
challenge is further compounded when the predictive model is probabilistic - as
in Gaussian processes (GPs) - where the outputs are random variables rather
than point estimates, necessitating additional computational effort in modeling
higher-order moments. In this work, we demonstrate that for an important class
of GPs known as FANOVA GP, which explicitly models all main effects and
interactions, *exact* Shapley attributions for both local and global
explanations can be computed in *quadratic time*. For local, instance-wise
explanations, we define a stochastic cooperative game over function components
and compute the exact stochastic Shapley value in quadratic time only,
capturing both the expected contribution and uncertainty. For global
explanations, we introduce a deterministic, variance-based value function and
compute exact Shapley values that quantify each feature's contribution to the
model's overall sensitivity. Our methods leverage a closed-form (stochastic)
M\"{o}bius representation of the FANOVA decomposition and introduce recursive
algorithms, inspired by Newton's identities, to efficiently compute the mean
and variance of Shapley values. Our work enhances the utility of explainable
AI, as demonstrated by empirical studies, by providing more scalable,
axiomatically sound, and uncertainty-aware explanations for predictions
generated by structured probabilistic models.

</details>


### [81] [Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks](https://arxiv.org/abs/2508.14536)
*Saman Yazdannik,Morteza Tayefi,Shamim Sanisales*

Main category: cs.LG

TL;DR: 提出基于切比雪夫多项式的Ch-DQN架构，在CartPole-v1任务中比标准DQN性能提升39%，但多项式阶数选择很关键


<details>
  <summary>Details</summary>
Motivation: 标准多层感知机在近似复杂价值函数时效率不足，需要更有效的特征表示方法来提升DQN性能

Method: 将切比雪夫多项式基集成到DQN框架中，利用其强大的函数逼近特性构建Ch-DQN模型

Result: 在参数数量相当的情况下，Ch-DQN（N=4）比标准DQN性能提升约39%，但高阶多项式（N=8）反而有害

Conclusion: 正交多项式基在深度强化学习中具有潜力，但需要谨慎选择模型复杂度，存在性能与复杂度的权衡

Abstract: The performance of Deep Q-Networks (DQN) is critically dependent on the
ability of its underlying neural network to accurately approximate the
action-value function. Standard function approximators, such as multi-layer
perceptrons, may struggle to efficiently represent the complex value landscapes
inherent in many reinforcement learning problems. This paper introduces a novel
architecture, the Chebyshev-DQN (Ch-DQN), which integrates a Chebyshev
polynomial basis into the DQN framework to create a more effective feature
representation. By leveraging the powerful function approximation properties of
Chebyshev polynomials, we hypothesize that the Ch-DQN can learn more
efficiently and achieve higher performance. We evaluate our proposed model on
the CartPole-v1 benchmark and compare it against a standard DQN with a
comparable number of parameters. Our results demonstrate that the Ch-DQN with a
moderate polynomial degree (N=4) achieves significantly better asymptotic
performance, outperforming the baseline by approximately 39\%. However, we also
find that the choice of polynomial degree is a critical hyperparameter, as a
high degree (N=8) can be detrimental to learning. This work validates the
potential of using orthogonal polynomial bases in deep reinforcement learning
while also highlighting the trade-offs involved in model complexity.

</details>


### [82] [FedRAIN-Lite: Federated Reinforcement Algorithms for Improving Idealised Numerical Weather and Climate Models](https://arxiv.org/abs/2508.14315)
*Pritthijit Nath,Sebastian Schemm,Henry Moss,Peter Haynes,Emily Shuckburgh,Mark Webb*

Main category: cs.LG

TL;DR: FedRAIN-Lite是一个联邦强化学习框架，通过将智能体分配到纬度带来模拟GCM的空间分解，实现局部参数学习和全局聚合。DDPG算法在简化气候模型中表现最佳，收敛更快且误差更低。


<details>
  <summary>Details</summary>
Motivation: 传统气候模型的子网格参数化是静态且离线调优的，无法适应不断变化的气候状态，需要开发能够在线学习和自适应调整的参数化方法。

Method: 使用联邦强化学习框架，将智能体分配到不同纬度带进行局部参数学习，并定期进行全局聚合。在简化的能量平衡气候模型（ebm-v1到ebm-v3）上测试了三种RL算法。

Result: DDPG算法在ebm-v2和ebm-v3设置中 consistently 优于静态和单智能体基线，在热带和中纬度地区具有更快的收敛速度和更低的区域加权RMSE。

Conclusion: DDPG的跨超参数迁移能力和低计算成本使其非常适合地理自适应参数学习，为高复杂度GCM提供了可扩展的路径，并为物理对齐的在线学习气候模型提供了原型。

Abstract: Sub-grid parameterisations in climate models are traditionally static and
tuned offline, limiting adaptability to evolving states. This work introduces
FedRAIN-Lite, a federated reinforcement learning (FedRL) framework that mirrors
the spatial decomposition used in general circulation models (GCMs) by
assigning agents to latitude bands, enabling local parameter learning with
periodic global aggregation. Using a hierarchy of simplified energy-balance
climate models, from a single-agent baseline (ebm-v1) to multi-agent ensemble
(ebm-v2) and GCM-like (ebm-v3) setups, we benchmark three RL algorithms under
different FedRL configurations. Results show that Deep Deterministic Policy
Gradient (DDPG) consistently outperforms both static and single-agent
baselines, with faster convergence and lower area-weighted RMSE in tropical and
mid-latitude zones across both ebm-v2 and ebm-v3 setups. DDPG's ability to
transfer across hyperparameters and low computational cost make it well-suited
for geographically adaptive parameter learning. This capability offers a
scalable pathway towards high-complexity GCMs and provides a prototype for
physically aligned, online-learning climate models that can evolve with a
changing climate. Code accessible at
https://github.com/p3jitnath/climate-rl-fedrl.

</details>


### [83] [Adaptively Robust LLM Inference Optimization under Prediction Uncertainty](https://arxiv.org/abs/2508.14544)
*Zixi Chen,Yinyu Ye,Zijie Zhou*

Main category: cs.LG

TL;DR: 该论文研究LLM推理调度优化问题，提出基于机器学习预测输出长度的算法来最小化总延迟。设计了保守算法A_max和自适应算法A_min，后者通过动态调整输出长度估计实现对数级竞争比。


<details>
  <summary>Details</summary>
Motivation: LLM推理是在线多任务服务过程，能耗高且输出长度未知，这给调度效率带来挑战。需要解决输出长度不确定性来优化调度并降低功耗。

Method: 提出两种算法：A_max基于预测输出长度的上界进行保守调度；A_min自适应算法以预测下界为初始估计，在推理过程中动态调整。利用机器学习进行输出长度区间预测。

Result: A_min算法在数值模拟中表现接近后见之明调度器，证明其高效性和鲁棒性。该算法仅依赖预测区间的下界，这在实践中更具优势。

Conclusion: A_min算法通过动态调整输出长度估计，有效解决了LLM推理调度中的不确定性挑战，实现了接近最优的性能，且对预测误差具有鲁棒性。

Abstract: We study the problem of optimizing Large Language Model (LLM) inference
scheduling to minimize total latency. LLM inference is an online and multi-task
service process and also heavily energy consuming by which a pre-trained LLM
processes input requests and generates output tokens sequentially. Therefore,
it is vital to improve its scheduling efficiency and reduce the power
consumption while a great amount of prompt requests are arriving. A key
challenge in LLM inference scheduling is that while the prompt length is known
upon arrival, the output length, which critically impacts memory usage and
processing time, is unknown. To address this uncertainty, we propose algorithms
that leverage machine learning to predict output lengths, assuming the
prediction provides an interval classification (min-max range) for each
request.
  We first design a conservative algorithm, $\mathcal{A}_{\max}$, which
schedules requests based on the upper bound of predicted output lengths to
prevent memory overflow. However, this approach is overly conservative: as
prediction accuracy decreases, performance degrades significantly due to
potential overestimation. To overcome this limitation, we propose
$\mathcal{A}_{\min}$, an adaptive algorithm that initially treats the predicted
lower bound as the output length and dynamically refines this estimate during
inferencing. We prove that $\mathcal{A}_{\min}$ achieves a log-scale
competitive ratio. Through numerical simulations, we demonstrate that
$\mathcal{A}_{\min}$ often performs nearly as well as the hindsight scheduler,
highlighting both its efficiency and robustness in practical scenarios.
Moreover, $\mathcal{A}_{\min}$ relies solely on the lower bound of the
prediction interval--an advantageous design choice since upper bounds on output
length are typically more challenging to predict accurately.

</details>


### [84] [Multi-view Graph Condensation via Tensor Decomposition](https://arxiv.org/abs/2508.14330)
*Nícolas Roque dos Santos,Dawon Ahn,Diego Minatel,Alneu de Andrade Lopes,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 该论文提出了GCTD方法，通过张量分解实现多视图图压缩，有效减少大规模图的计算需求，同时保持GNN性能，在多个数据集上取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在大规模图上的训练存在巨大计算挑战，现有图压缩方法依赖计算密集的双层优化且缺乏可解释性，而张量分解技术虽然透明高效但尚未应用于图压缩领域。

Method: 提出基于张量分解的多视图图压缩方法GCTD，利用张量分解技术学习原始图的线性或多线性函数，生成紧凑的合成图，同时保持原始节点与合成节点之间的映射关系。

Result: 在六个真实数据集上的实验表明，GCTD能有效减小图规模，在三个数据集上准确率提升达4.0%，在大图上与现有方法相比具有竞争力。

Conclusion: 张量分解技术能够成功应用于图压缩任务，GCTD方法在保持性能的同时提供了更好的可解释性和计算效率，为大规模图学习提供了新的解决方案。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable results in various
real-world applications, including drug discovery, object detection, social
media analysis, recommender systems, and text classification. In contrast to
their vast potential, training them on large-scale graphs presents significant
computational challenges due to the resources required for their storage and
processing. Graph Condensation has emerged as a promising solution to reduce
these demands by learning a synthetic compact graph that preserves the
essential information of the original one while maintaining the GNN's
predictive performance. Despite their efficacy, current graph condensation
approaches frequently rely on a computationally intensive bi-level
optimization. Moreover, they fail to maintain a mapping between synthetic and
original nodes, limiting the interpretability of the model's decisions. In this
sense, a wide range of decomposition techniques have been applied to learn
linear or multi-linear functions from graph data, offering a more transparent
and less resource-intensive alternative. However, their applicability to graph
condensation remains unexplored. This paper addresses this gap and proposes a
novel method called Multi-view Graph Condensation via Tensor Decomposition
(GCTD) to investigate to what extent such techniques can synthesize an
informative smaller graph and achieve comparable downstream task performance.
Extensive experiments on six real-world datasets demonstrate that GCTD
effectively reduces graph size while preserving GNN performance, achieving up
to a 4.0\ improvement in accuracy on three out of six datasets and competitive
performance on large graphs compared to existing approaches. Our code is
available at https://anonymous.4open.science/r/gctd-345A.

</details>


### [85] [NeRC: Neural Ranging Correction through Differentiable Moving Horizon Location Estimation](https://arxiv.org/abs/2508.14336)
*Xu Weng,K. V. Ling,Haochen Liu,Bingheng Wang,Kun Cao*

Main category: cs.LG

TL;DR: 提出NeRC端到端神经网络框架，使用易于获取的位置真值而非难以标注的测距误差来训练网络，通过可微分移动视界估计和欧几里得距离场成本图实现高精度GNSS定位


<details>
  <summary>Details</summary>
Motivation: 城市环境中移动设备GNSS定位面临信号传播复杂和硬件质量低导致的测距误差问题，传统数据驱动方法需要标注测距误差但标注困难

Method: 端到端神经网络框架，使用位置真值训练，采用可微分移动视界估计处理测量序列并进行梯度反向传播，引入欧几里得距离场成本图减少对标注位置的需求

Result: 在公开基准和自收集数据集上验证了定位精度的显著提升，并在边缘设备上验证了实时性能

Conclusion: NeRC框架通过端到端学习有效解决了GNSS定位中的测距误差问题，无需困难的测距误差标注，实现了高精度和实时性能

Abstract: GNSS localization using everyday mobile devices is challenging in urban
environments, as ranging errors caused by the complex propagation of satellite
signals and low-quality onboard GNSS hardware are blamed for undermining
positioning accuracy. Researchers have pinned their hopes on data-driven
methods to regress such ranging errors from raw measurements. However, the
grueling annotation of ranging errors impedes their pace. This paper presents a
robust end-to-end Neural Ranging Correction (NeRC) framework, where
localization-related metrics serve as the task objective for training the
neural modules. Instead of seeking impractical ranging error labels, we train
the neural network using ground-truth locations that are relatively easy to
obtain. This functionality is supported by differentiable moving horizon
location estimation (MHE) that handles a horizon of measurements for
positioning and backpropagates the gradients for training. Even better, as a
blessing of end-to-end learning, we propose a new training paradigm using
Euclidean Distance Field (EDF) cost maps, which alleviates the demands on
labeled locations. We evaluate the proposed NeRC on public benchmarks and our
collected datasets, demonstrating its distinguished improvement in positioning
accuracy. We also deploy NeRC on the edge to verify its real-time performance
for mobile devices.

</details>


### [86] [ELATE: Evolutionary Language model for Automated Time-series Engineering](https://arxiv.org/abs/2508.14667)
*Andrew Murray,Danial Dervovic,Michael Cashmore*

Main category: cs.LG

TL;DR: ELATE使用语言模型和进化框架自动进行时间序列特征工程，平均提升预测精度8.4%


<details>
  <summary>Details</summary>
Motivation: 传统时间序列特征工程需要手动操作且耗时，现有自动化方法依赖穷举枚举，计算成本高且缺乏领域知识

Method: 结合语言模型和进化框架，使用时序统计量和特征重要性指标来引导和剪枝特征，语言模型提出上下文相关的特征变换

Result: 在多个领域平均提升预测精度8.4%

Conclusion: ELATE有效自动化了时间序列特征工程，显著提升了预测性能

Abstract: Time-series prediction involves forecasting future values using machine
learning models. Feature engineering, whereby existing features are transformed
to make new ones, is critical for enhancing model performance, but is often
manual and time-intensive. Existing automation attempts rely on exhaustive
enumeration, which can be computationally costly and lacks domain-specific
insights. We introduce ELATE (Evolutionary Language model for Automated
Time-series Engineering), which leverages a language model within an
evolutionary framework to automate feature engineering for time-series data.
ELATE employs time-series statistical measures and feature importance metrics
to guide and prune features, while the language model proposes new,
contextually relevant feature transformations. Our experiments demonstrate that
ELATE improves forecasting accuracy by an average of 8.4% across various
domains.

</details>


### [87] [On the Interplay between Graph Structure and Learning Algorithms in Graph Neural Networks](https://arxiv.org/abs/2508.14338)
*Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 该论文研究了图神经网络中学习算法与图结构的相互作用，通过谱图理论分析SGD和岭回归的过度风险，揭示了图结构对学习算法性能的影响，并为GNN算法设计提供实践指导。


<details>
  <summary>Details</summary>
Motivation: 现有理论研究主要关注插值机制下的收敛速度，且仅提供学习动态与图结构之间的粗略联系。本文旨在填补这一空白，研究在泛化机制下学习算法的过度风险与图结构的关系。

Method: 将传统学习理论设置扩展到GNNs背景，通过谱图理论分析SGD和岭回归的过度风险曲线，比较不同图结构（规则图vs幂律图）对算法性能的影响，并扩展到多层线性GNNs分析。

Result: 建立了图结构与学习算法性能之间的耦合关系，揭示了过度风险曲线中的非各向同性效应增加现象，为理解GNNs中的过平滑问题提供了新视角。实证结果与理论预测一致。

Conclusion: 研究展示了图结构、GNNs和学习算法之间的耦合关系，为实践中GNN算法设计和选择提供了重要见解，特别是在处理不同图结构时的性能优化。

Abstract: This paper studies the interplay between learning algorithms and graph
structure for graph neural networks (GNNs). Existing theoretical studies on the
learning dynamics of GNNs primarily focus on the convergence rates of learning
algorithms under the interpolation regime (noise-free) and offer only a crude
connection between these dynamics and the actual graph structure (e.g., maximum
degree). This paper aims to bridge this gap by investigating the excessive risk
(generalization performance) of learning algorithms in GNNs within the
generalization regime (with noise). Specifically, we extend the conventional
settings from the learning theory literature to the context of GNNs and examine
how graph structure influences the performance of learning algorithms such as
stochastic gradient descent (SGD) and Ridge regression. Our study makes several
key contributions toward understanding the interplay between graph structure
and learning in GNNs. First, we derive the excess risk profiles of SGD and
Ridge regression in GNNs and connect these profiles to the graph structure
through spectral graph theory. With this established framework, we further
explore how different graph structures (regular vs. power-law) impact the
performance of these algorithms through comparative analysis. Additionally, we
extend our analysis to multi-layer linear GNNs, revealing an increasing
non-isotropic effect on the excess risk profile, thereby offering new insights
into the over-smoothing issue in GNNs from the perspective of learning
algorithms. Our empirical results align with our theoretical predictions,
\emph{collectively showcasing a coupling relation among graph structure, GNNs
and learning algorithms, and providing insights on GNN algorithm design and
selection in practice.}

</details>


### [88] [AFABench: A Generic Framework for Benchmarking Active Feature Acquisition](https://arxiv.org/abs/2508.14734)
*Valter Schütz,Han Wu,Reza Rezvan,Linus Aronsson,Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: 提出了AFABench，首个用于主动特征获取（AFA）的标准化基准框架，包含多样化数据集和评估方法，解决了该领域缺乏系统评估标准的问题。


<details>
  <summary>Details</summary>
Motivation: 由于实际应用中获取所有特征成本高昂或不切实际，AFA方法需要动态选择信息量最大的特征子集。但现有方法缺乏公平系统的评估基准，阻碍了研究进展。

Method: 开发了包含合成和真实数据集的基准框架AFABench，支持多种获取策略（静态、贪婪、强化学习等），采用模块化设计便于新方法集成，并创建了专门测试前瞻能力的合成数据集AFAContext。

Result: 评估了各类代表性算法，揭示了不同AFA策略之间的关键权衡，为贪婪选择方法的局限性提供了实证证据，并提供了对未来研究有价值的见解。

Conclusion: AFABench填补了AFA领域基准测试的空白，为方法比较和评估提供了标准化平台，有助于推动该领域的系统化研究和发展。

Abstract: In many real-world scenarios, acquiring all features of a data instance can
be expensive or impractical due to monetary cost, latency, or privacy concerns.
Active Feature Acquisition (AFA) addresses this challenge by dynamically
selecting a subset of informative features for each data instance, trading
predictive performance against acquisition cost. While numerous methods have
been proposed for AFA, ranging from greedy information-theoretic strategies to
non-myopic reinforcement learning approaches, fair and systematic evaluation of
these methods has been hindered by the lack of standardized benchmarks. In this
paper, we introduce AFABench, the first benchmark framework for AFA. Our
benchmark includes a diverse set of synthetic and real-world datasets, supports
a wide range of acquisition policies, and provides a modular design that
enables easy integration of new methods and tasks. We implement and evaluate
representative algorithms from all major categories, including static, greedy,
and reinforcement learning-based approaches. To test the lookahead capabilities
of AFA policies, we introduce a novel synthetic dataset, AFAContext, designed
to expose the limitations of greedy selection. Our results highlight key
trade-offs between different AFA strategies and provide actionable insights for
future research. The benchmark code is available at:
https://github.com/Linusaronsson/AFA-Benchmark.

</details>


### [89] [Cross-Modality Controlled Molecule Generation with Diffusion Language Model](https://arxiv.org/abs/2508.14748)
*Yunzhe Zhang,Yifei Wang,Khanh Vinh Nguyen,Pengyu Hong*

Main category: cs.LG

TL;DR: CMCM-DLM是一个基于扩散语言模型的跨模态分子生成方法，能够在预训练模型基础上通过添加可训练模块来支持多模态约束，无需重新训练整个模型。


<details>
  <summary>Details</summary>
Motivation: 现有的SMILES扩散模型通常只支持单模态约束，需要为每个新约束重新训练模型。而实际应用中需要处理多模态约束，且约束可能随时间变化，因此需要一种能够灵活扩展预训练模型的方法。

Method: 在预训练扩散模型基础上添加两个可训练模块：结构控制模块(SCM)和性质控制模块(PCM)。生成过程分为两个阶段：第一阶段使用SCM在早期扩散步骤中注入结构约束，第二阶段使用PCM在后期推理中引导分子性质优化。

Result: 在多个数据集上的实验结果表明，该方法具有高效性和适应性，能够有效支持跨模态分子生成。

Conclusion: CMCM-DLM在药物发现应用的分子生成方面取得了显著进展，提供了一种灵活扩展预训练模型支持多模态约束的有效方法。

Abstract: Current SMILES-based diffusion models for molecule generation typically
support only unimodal constraint. They inject conditioning signals at the start
of the training process and require retraining a new model from scratch
whenever the constraint changes. However, real-world applications often involve
multiple constraints across different modalities, and additional constraints
may emerge over the course of a study. This raises a challenge: how to extend a
pre-trained diffusion model not only to support cross-modality constraints but
also to incorporate new ones without retraining. To tackle this problem, we
propose the Cross-Modality Controlled Molecule Generation with Diffusion
Language Model (CMCM-DLM), demonstrated by two distinct cross modalities:
molecular structure and chemical properties. Our approach builds upon a
pre-trained diffusion model, incorporating two trainable modules, the Structure
Control Module (SCM) and the Property Control Module (PCM), and operates in two
distinct phases during the generation process. In Phase I, we employs the SCM
to inject structural constraints during the early diffusion steps, effectively
anchoring the molecular backbone. Phase II builds on this by further
introducing PCM to guide the later stages of inference to refine the generated
molecules, ensuring their chemical properties match the specified targets.
Experimental results on multiple datasets demonstrate the efficiency and
adaptability of our approach, highlighting CMCM-DLM's significant advancement
in molecular generation for drug discovery applications.

</details>


### [90] [A Non-Asymptotic Convergent Analysis for Scored-Based Graph Generative Model via a System of Stochastic Differential Equations](https://arxiv.org/abs/2508.14351)
*Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 本文首次对基于分数的图生成模型（SGGMs）进行了非渐近收敛性分析，揭示了图结构拓扑特性等独特因素对收敛边界的影响，并提供了超参数选择和归一化技术的理论指导。


<details>
  <summary>Details</summary>
Motivation: 基于分数的图生成模型在药物发现和蛋白质合成等关键应用中表现出色，但其理论行为特别是收敛性方面研究不足。与单SDE控制的传统SGMs不同，SGGMs涉及耦合的SDE系统，现有收敛分析不适用。

Method: 对SGGMs进行非渐近收敛分析，重点研究三种图生成范式的收敛边界：(1)固定图结构的特征生成；(2)固定节点特征的图结构生成；(3)图结构和节点特征的联合生成。使用合成图模型进行实证验证。

Result: 分析揭示了SGGMs特有的影响因素（如图结构拓扑特性），提供了超参数选择（采样步数和扩散长度）的理论见解，并证明归一化等技术可改善收敛性。实证结果与理论预测一致。

Conclusion: 该工作深化了对SGGMs的理论理解，证明了其在关键领域的适用性，并为设计有效模型提供了实用指导，填补了图生成模型收敛性理论分析的空白。

Abstract: Score-based graph generative models (SGGMs) have proven effective in critical
applications such as drug discovery and protein synthesis. However, their
theoretical behavior, particularly regarding convergence, remains
underexplored. Unlike common score-based generative models (SGMs), which are
governed by a single stochastic differential equation (SDE), SGGMs involve a
system of coupled SDEs. In SGGMs, the graph structure and node features are
governed by separate but interdependent SDEs. This distinction makes existing
convergence analyses from SGMs inapplicable for SGGMs. In this work, we present
the first non-asymptotic convergence analysis for SGGMs, focusing on the
convergence bound (the risk of generative error) across three key graph
generation paradigms: (1) feature generation with a fixed graph structure, (2)
graph structure generation with fixed node features, and (3) joint generation
of both graph structure and node features. Our analysis reveals several unique
factors specific to SGGMs (e.g., the topological properties of the graph
structure) which affect the convergence bound. Additionally, we offer
theoretical insights into the selection of hyperparameters (e.g., sampling
steps and diffusion length) and advocate for techniques like normalization to
improve convergence. To validate our theoretical findings, we conduct a
controlled empirical study using synthetic graph models, and the results align
with our theoretical predictions. This work deepens the theoretical
understanding of SGGMs, demonstrates their applicability in critical domains,
and provides practical guidance for designing effective models.

</details>


### [91] [PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning](https://arxiv.org/abs/2508.14765)
*Ruheng Wang,Hang Zhang,Trieu Nguyen,Shasha Feng,Hao-Wei Pang,Xiang Yu,Li Xiao,Peter Zhiping Zhang*

Main category: cs.LG

TL;DR: PepThink-R1是一个结合大语言模型、思维链微调和强化学习的肽设计框架，通过单体级修改推理来生成具有优化药理特性的环肽，在性能和可解释性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前治疗性肽设计面临序列空间巨大、实验数据有限以及生成模型可解释性差等挑战，需要开发能够进行可解释设计选择并优化多种药理特性的新方法。

Method: 整合大语言模型与思维链监督微调和强化学习，在序列生成过程中显式推理单体级修改，通过定制奖励函数平衡化学有效性和性质改进，自主探索多样化序列变体。

Result: PepThink-R1生成的环肽在亲脂性、稳定性和暴露度方面显著提升，在优化成功率和可解释性方面均优于通用大语言模型（如GPT-5）和领域特定基线模型。

Conclusion: 这是首个结合显式推理与强化学习驱动性质控制的大语言模型肽设计框架，为治疗发现提供了可靠且透明的肽优化方法。

Abstract: Designing therapeutic peptides with tailored properties is hindered by the
vastness of sequence space, limited experimental data, and poor
interpretability of current generative models. To address these challenges, we
introduce PepThink-R1, a generative framework that integrates large language
models (LLMs) with chain-of-thought (CoT) supervised fine-tuning and
reinforcement learning (RL). Unlike prior approaches, PepThink-R1 explicitly
reasons about monomer-level modifications during sequence generation, enabling
interpretable design choices while optimizing for multiple pharmacological
properties. Guided by a tailored reward function balancing chemical validity
and property improvements, the model autonomously explores diverse sequence
variants. We demonstrate that PepThink-R1 generates cyclic peptides with
significantly enhanced lipophilicity, stability, and exposure, outperforming
existing general LLMs (e.g., GPT-5) and domain-specific baseline in both
optimization success and interpretability. To our knowledge, this is the first
LLM-based peptide design framework that combines explicit reasoning with
RL-driven property control, marking a step toward reliable and transparent
peptide optimization for therapeutic discovery.

</details>


### [92] [SBGD: Improving Graph Diffusion Generative Model via Stochastic Block Diffusion](https://arxiv.org/abs/2508.14352)
*Junwei Su,Shan Wu*

Main category: cs.LG

TL;DR: SBGD模型通过将图表示细化到块图空间，解决了图扩散生成模型在可扩展性和尺寸泛化方面的挑战，显著降低内存需求并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 图扩散生成模型(GDGMs)面临内存需求高、难以扩展到大型图，以及尺寸泛化能力差的问题，限制了其在实际大规模图数据中的应用。

Method: 提出随机块图扩散(SBGD)模型，将图表示细化到块图空间，融入基于真实图模式的结构先验，降低内存复杂度并改善尺寸泛化。

Result: SBGD实现了显著的内存改进(最高6倍)，同时保持或超越最先进方法的图生成性能，并能更好地泛化到未见过的图尺寸。

Conclusion: SBGD不仅是可扩展且有效的GDGM，还展示了生成建模中的模块化原则，为通过分解复杂任务来探索生成模型提供了新途径。

Abstract: Graph diffusion generative models (GDGMs) have emerged as powerful tools for
generating high-quality graphs. However, their broader adoption faces
challenges in \emph{scalability and size generalization}. GDGMs struggle to
scale to large graphs due to their high memory requirements, as they typically
operate in the full graph space, requiring the entire graph to be stored in
memory during training and inference. This constraint limits their feasibility
for large-scale real-world graphs. GDGMs also exhibit poor size generalization,
with limited ability to generate graphs of sizes different from those in the
training data, restricting their adaptability across diverse applications. To
address these challenges, we propose the stochastic block graph diffusion
(SBGD) model, which refines graph representations into a block graph space.
This space incorporates structural priors based on real-world graph patterns,
significantly reducing memory complexity and enabling scalability to large
graphs. The block representation also improves size generalization by capturing
fundamental graph structures. Empirical results show that SBGD achieves
significant memory improvements (up to 6$\times$) while maintaining comparable
or even superior graph generation performance relative to state-of-the-art
methods. Furthermore, experiments demonstrate that SBGD better generalizes to
unseen graph sizes. The significance of SBGD extends beyond being a scalable
and effective GDGM; it also exemplifies the principle of modularization in
generative modeling, offering a new avenue for exploring generative models by
decomposing complex tasks into more manageable components.

</details>


### [93] [Graph Structure Learning with Temporal Graph Information Bottleneck for Inductive Representation Learning](https://arxiv.org/abs/2508.14859)
*Jiafeng Xiong,Rizos Sakellariou*

Main category: cs.LG

TL;DR: GTGIB框架结合图结构学习和时序图信息瓶颈，通过两步结构增强器优化节点邻域，并在时序图中应用信息瓶颈原则来正则化边和特征，在四个真实数据集上实现了优异的链接预测性能。


<details>
  <summary>Details</summary>
Motivation: 时序图学习中存在两个主要挑战：有效表示未见节点以及减轻噪声或冗余图信息。需要一种能够同时处理这两个问题的框架来提升动态网络中的表示学习效果。

Method: 提出GTGIB框架，整合图结构学习(GSL)和时序图信息瓶颈(TGIB)。设计了两步GSL结构增强器来丰富和优化节点邻域，并通过变分近似推导出可处理的TGIB目标函数来正则化边和特征。

Result: 在四个真实数据集上的链接预测实验中，GTGIB在归纳设置下优于所有现有方法，在转导设置下也实现了显著且一致的改进。

Conclusion: GTGIB通过结合图结构学习和信息瓶颈原则，有效解决了时序图学习中的节点表示和噪声过滤问题，为动态网络分析提供了强大的框架。

Abstract: Temporal graph learning is crucial for dynamic networks where nodes and edges
evolve over time and new nodes continuously join the system. Inductive
representation learning in such settings faces two major challenges:
effectively representing unseen nodes and mitigating noisy or redundant graph
information. We propose GTGIB, a versatile framework that integrates Graph
Structure Learning (GSL) with Temporal Graph Information Bottleneck (TGIB). We
design a novel two-step GSL-based structural enhancer to enrich and optimize
node neighborhoods and demonstrate its effectiveness and efficiency through
theoretical proofs and experiments. The TGIB refines the optimized graph by
extending the information bottleneck principle to temporal graphs, regularizing
both edges and features based on our derived tractable TGIB objective function
via variational approximation, enabling stable and efficient optimization.
GTGIB-based models are evaluated to predict links on four real-world datasets;
they outperform existing methods in all datasets under the inductive setting,
with significant and consistent improvement in the transductive setting.

</details>


### [94] [Disentanglement in T-space for Faster and Distributed Training of Diffusion Models with Fewer Latent-states](https://arxiv.org/abs/2508.14413)
*Samarth Gupta,Raghudeep Gadde,Rui Chen,Aleix M. Martinez*

Main category: cs.LG

TL;DR: 该论文挑战了扩散模型需要大量潜在状态/时间步的传统假设，证明通过精心选择噪声调度，只需少量甚至单个潜在状态就能达到与上千步模型相当的性能，并提出解耦模型实现4-6倍的加速收敛。


<details>
  <summary>Details</summary>
Motivation: 挑战扩散模型需要大量时间步的固有假设，探索在少量潜在状态下实现高质量生成的可能性，以大幅提升训练效率。

Method: 1. 精心选择噪声调度方案 2. 在少量潜在状态（T~32）下训练扩散模型 3. 进一步推进到单个潜在状态的完全解耦 4. 组合多个独立训练的单潜在状态模型

Result: 1. 32步模型性能与1000步模型相当 2. 单潜在状态模型能生成高质量样本 3. 解耦模型在多个指标上实现4-6倍的加速收敛 4. 在两个不同数据集上验证了有效性

Conclusion: 扩散模型并不需要大量时间步，通过合理的噪声调度和模型解耦，可以大幅减少计算成本同时保持生成质量，为高效扩散模型设计提供了新思路。

Abstract: We challenge a fundamental assumption of diffusion models, namely, that a
large number of latent-states or time-steps is required for training so that
the reverse generative process is close to a Gaussian. We first show that with
careful selection of a noise schedule, diffusion models trained over a small
number of latent states (i.e. $T \sim 32$) match the performance of models
trained over a much large number of latent states ($T \sim 1,000$). Second, we
push this limit (on the minimum number of latent states required) to a single
latent-state, which we refer to as complete disentanglement in T-space. We show
that high quality samples can be easily generated by the disentangled model
obtained by combining several independently trained single latent-state models.
We provide extensive experiments to show that the proposed disentangled model
provides 4-6$\times$ faster convergence measured across a variety of metrics on
two different datasets.

</details>


### [95] [Personalized Counterfactual Framework: Generating Potential Outcomes from Wearable Data](https://arxiv.org/abs/2508.14432)
*Ajan Subramanian,Amir M. Rahmani*

Main category: cs.LG

TL;DR: 这篇论文提出了一种从可穿戴传感器数据学习个人化反事实模型的框架，用于探索生活方式改变的假想场景和生理变化。


<details>
  <summary>Details</summary>
Motivation: 可穿戴传感器产生的复杂纵向数据流导致个性化健康监测面临挑战，需要方法来从中提取可执行的洞察。

Method: 通过多模态相似性分析扩充个人数据集，使用时序PC算法发现预测关系，然后用梯度提升机器训练模型来量化个人特异效应。

Result: 评估显示了合理的预测准确性（心率MAE 4.71 bpm）和高反事实可信度（中位数0.9643），并显示了不同个体对生活方式改变的显著差异性。

Conclusion: 该框枵为探索个人化健康动态和生成关于个体对生活方式改变响应的假设提供了工具。

Abstract: Wearable sensor data offer opportunities for personalized health monitoring,
yet deriving actionable insights from their complex, longitudinal data streams
is challenging. This paper introduces a framework to learn personalized
counterfactual models from multivariate wearable data. This enables exploring
what-if scenarios to understand potential individual-specific outcomes of
lifestyle choices. Our approach first augments individual datasets with data
from similar patients via multi-modal similarity analysis. We then use a
temporal PC (Peter-Clark) algorithm adaptation to discover predictive
relationships, modeling how variables at time t-1 influence physiological
changes at time t. Gradient Boosting Machines are trained on these discovered
relationships to quantify individual-specific effects. These models drive a
counterfactual engine projecting physiological trajectories under hypothetical
interventions (e.g., activity or sleep changes). We evaluate the framework via
one-step-ahead predictive validation and by assessing the plausibility and
impact of interventions. Evaluation showed reasonable predictive accuracy
(e.g., mean heart rate MAE 4.71 bpm) and high counterfactual plausibility
(median 0.9643). Crucially, these interventions highlighted significant
inter-individual variability in response to hypothetical lifestyle changes,
showing the framework's potential for personalized insights. This work provides
a tool to explore personalized health dynamics and generate hypotheses on
individual responses to lifestyle changes.

</details>


### [96] [DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization](https://arxiv.org/abs/2508.14460)
*Shuaijie She,Yu Bao,Yu Lu,Lu Xu,Tao Li,Wenhao Zhu,Shujian Huang,Shanbo Cheng,Lu Lu,Yuxuan Wang*

Main category: cs.LG

TL;DR: DuPO是一个基于对偶学习的偏好优化框架，通过广义对偶性生成无标注反馈，解决了传统方法对标注数据和可验证任务的依赖，在翻译、数学推理等任务上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR方法对昂贵标注数据和可验证任务的依赖，以及传统对偶学习仅限于严格对偶任务对（如翻译和回译）的限制，寻求一种更通用、可扩展的无标注优化范式。

Method: 将原始任务输入分解为已知和未知组件，构建对偶任务来使用原始输出和已知信息重建未知部分（如通过数学解反推隐藏变量），重建质量作为自监督奖励来优化原始任务，利用LLM单模型实例化两个任务的能力。

Result: 在756个翻译方向上平均提升2.13 COMET分数，在三个数学推理挑战基准上平均提升6.4个百分点，作为推理时重排序器性能提升9.3个百分点（以计算换精度）。

Conclusion: DuPO是一个可扩展、通用且无需标注的LLM优化范式，在各种任务上展现出显著性能提升，为大规模语言模型优化提供了新思路。

Abstract: We present DuPO, a dual learning-based preference optimization framework that
generates annotation-free feedback via a generalized duality. DuPO addresses
two key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s
reliance on costly labels and applicability restricted to verifiable tasks, and
traditional dual learning's restriction to strictly dual task pairs (e.g.,
translation and back-translation). Specifically, DuPO decomposes a primal
task's input into known and unknown components, then constructs its dual task
to reconstruct the unknown part using the primal output and known information
(e.g., reversing math solutions to recover hidden variables), broadening
applicability to non-invertible tasks. The quality of this reconstruction
serves as a self-supervised reward to optimize the primal task, synergizing
with LLMs' ability to instantiate both tasks via a single model. Empirically,
DuPO achieves substantial gains across diverse tasks: it enhances the average
translation quality by 2.13 COMET over 756 directions, boosts the mathematical
reasoning accuracy by an average of 6.4 points on three challenge benchmarks,
and enhances performance by 9.3 points as an inference-time reranker (trading
computation for accuracy). These results position DuPO as a scalable, general,
and annotation-free paradigm for LLM optimization.

</details>


### [97] [Fast Symbolic Regression Benchmarking](https://arxiv.org/abs/2508.14481)
*Viktor Martinek*

Main category: cs.LG

TL;DR: 本文提出了一种改进的符号回归基准测试方法，通过引入可接受表达式列表和早期终止机制，显著提高了基准测试的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归基准测试存在两个主要问题：过度强调恢复特定表达式形式，以及依赖计算机代数系统评估成功。此外，现有基准在发现表达式后仍继续搜索，浪费计算资源。

Method: 引入精心策划的可接受表达式列表和回调机制实现早期终止。基于Yoshitomo等人提出的SRSD基准问题，对SymbolicRegression.jl和TiSR两个SR包进行基准测试。

Result: 新方法将SymbolicRegression.jl的重新发现率从26.7%提升到44.7%，计算开销减少41.2%。TiSR的重新发现率为69.4%，节省63%的时间。

Conclusion: 提出的改进基准测试方法不仅提高了符号回归算法的重新发现率，还显著减少了计算成本，为符号回归算法的评估提供了更有效的框架。

Abstract: Symbolic regression (SR) uncovers mathematical models from data. Several
benchmarks have been proposed to compare the performance of SR algorithms.
However, existing ground-truth rediscovery benchmarks overemphasize the
recovery of "the one" expression form or rely solely on computer algebra
systems (such as SymPy) to assess success. Furthermore, existing benchmarks
continue the expression search even after its discovery. We improve upon these
issues by introducing curated lists of acceptable expressions, and a callback
mechanism for early termination. As a starting point, we use the symbolic
regression for scientific discovery (SRSD) benchmark problems proposed by
Yoshitomo et al., and benchmark the two SR packages SymbolicRegression.jl and
TiSR. The new benchmarking method increases the rediscovery rate of
SymbolicRegression.jl from 26.7%, as reported by Yoshitomo et at., to 44.7%.
Performing the benchmark takes 41.2% less computational expense. TiSR's
rediscovery rate is 69.4%, while performing the benchmark saves 63% time.

</details>


### [98] [On the notion of missingness for path attribution explainability methods in medical settings: Guiding the selection of medically meaningful baselines](https://arxiv.org/abs/2508.14482)
*Alexander Geiger,Lars Wagner,Daniel Rueckert,Dirk Wilhelm,Alissa Jell*

Main category: cs.LG

TL;DR: 该论文提出使用变分自编码器生成反事实基线来改进医学深度学习模型的可解释性，相比传统基线方法能产生更忠实和医学相关的归因结果。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学领域的可解释性面临挑战，传统基线选择（如全零输入）在医学上下文中往往语义无意义，因为缺失信息本身可能具有临床意义。

Method: 提出反事实引导的方法，使用变分自编码器生成临床正常但与输入接近的反事实基线，这种方法与生成模型无关，可适用于任何合适的反事实方法。

Result: 在三个不同的医学数据集上评估表明，反事实基线相比标准基线选择能产生更忠实和医学相关的归因结果。

Conclusion: 反事实基线为医学深度学习模型提供了一种更准确的特征缺失表示方法，提高了模型解释的临床相关性和可信度。

Abstract: The explainability of deep learning models remains a significant challenge,
particularly in the medical domain where interpretable outputs are critical for
clinical trust and transparency. Path attribution methods such as Integrated
Gradients rely on a baseline input representing the absence of relevant
features ("missingness"). Commonly used baselines, such as all-zero inputs, are
often semantically meaningless, especially in medical contexts where
missingness can itself be informative. While alternative baseline choices have
been explored, existing methods lack a principled approach to dynamically
select baselines tailored to each input. In this work, we examine the notion of
missingness in the medical setting, analyze its implications for baseline
selection, and introduce a counterfactual-guided approach to address the
limitations of conventional baselines. We argue that a clinically normal but
input-close counterfactual represents a more accurate representation of a
meaningful absence of features in medical data. To implement this, we use a
Variational Autoencoder to generate counterfactual baselines, though our
concept is generative-model-agnostic and can be applied with any suitable
counterfactual method. We evaluate the approach on three distinct medical data
sets and empirically demonstrate that counterfactual baselines yield more
faithful and medically relevant attributions compared to standard baseline
choices.

</details>


### [99] [Semantic Energy: Detecting LLM Hallucination Beyond Entropy](https://arxiv.org/abs/2508.14496)
*Huan Ma,Jiadong Pan,Jing Liu,Yan Chen,Joey Tianyi Zhou,Guangyu Wang,Qinghua Hu,Hua Wu,Changqing Zhang,Haifeng Wang*

Main category: cs.LG

TL;DR: 提出Semantic Energy框架，通过直接使用倒数第二层logits结合玻尔兹曼能量分布，改进LLM幻觉检测和不确定性估计


<details>
  <summary>Details</summary>
Motivation: 现有语义熵方法依赖softmax后概率，无法捕捉模型内在不确定性，在某些场景下效果不佳

Method: 在倒数第二层logits上操作，结合语义聚类和玻尔兹曼能量分布来估计不确定性

Result: 在多个基准测试中显著提升幻觉检测和不确定性估计性能

Conclusion: Semantic Energy为下游应用提供更可靠的幻觉检测信号，优于传统语义熵方法

Abstract: Large Language Models (LLMs) are being increasingly deployed in real-world
applications, but they remain susceptible to hallucinations, which produce
fluent yet incorrect responses and lead to erroneous decision-making.
Uncertainty estimation is a feasible approach to detect such hallucinations.
For example, semantic entropy estimates uncertainty by considering the semantic
diversity across multiple sampled responses, thus identifying hallucinations.
However, semantic entropy relies on post-softmax probabilities and fails to
capture the model's inherent uncertainty, causing it to be ineffective in
certain scenarios. To address this issue, we introduce Semantic Energy, a novel
uncertainty estimation framework that leverages the inherent confidence of LLMs
by operating directly on logits of penultimate layer. By combining semantic
clustering with a Boltzmann-inspired energy distribution, our method better
captures uncertainty in cases where semantic entropy fails. Experiments across
multiple benchmarks show that Semantic Energy significantly improves
hallucination detection and uncertainty estimation, offering more reliable
signals for downstream applications such as hallucination detection.

</details>


### [100] [Artificial Intelligence-Based Multiscale Temporal Modeling for Anomaly Detection in Cloud Services](https://arxiv.org/abs/2508.14503)
*Lian Lian,Yilin Li,Song Han,Renzi Meng,Sibo Wang,Ming Wang*

Main category: cs.LG

TL;DR: 提出基于Transformer架构的多尺度特征感知异常检测方法，通过改进的时序建模和多尺度特征融合，在云服务监控数据上实现优越的异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决云服务环境中时序建模和尺度感知特征表示的局限性，传统方法难以有效捕捉长程依赖和多尺度异常模式。

Method: 使用改进的Transformer模块进行时序建模，引入多尺度特征构建路径和注意力加权融合模块，动态调整不同尺度特征的贡献度。

Result: 在精确率、召回率、AUC和F1-score等关键指标上优于主流基线模型，在各种扰动条件下保持强稳定性和检测性能。

Conclusion: 该方法在复杂云环境中展现出优异的异常检测能力，为云服务监控提供了有效的解决方案。

Abstract: This study proposes an anomaly detection method based on the Transformer
architecture with integrated multiscale feature perception, aiming to address
the limitations of temporal modeling and scale-aware feature representation in
cloud service environments. The method first employs an improved Transformer
module to perform temporal modeling on high-dimensional monitoring data, using
a self-attention mechanism to capture long-range dependencies and contextual
semantics. Then, a multiscale feature construction path is introduced to
extract temporal features at different granularities through downsampling and
parallel encoding. An attention-weighted fusion module is designed to
dynamically adjust the contribution of each scale to the final decision,
enhancing the model's robustness in anomaly pattern modeling. In the input
modeling stage, standardized multidimensional time series are constructed,
covering core signals such as CPU utilization, memory usage, and task
scheduling states, while positional encoding is used to strengthen the model's
temporal awareness. A systematic experimental setup is designed to evaluate
performance, including comparative experiments and hyperparameter sensitivity
analysis, focusing on the impact of optimizers, learning rates, anomaly ratios,
and noise levels. Experimental results show that the proposed method
outperforms mainstream baseline models in key metrics, including precision,
recall, AUC, and F1-score, and maintains strong stability and detection
performance under various perturbation conditions, demonstrating its superior
capability in complex cloud environments.

</details>


### [101] [Great GATsBi: Hybrid, Multimodal, Trajectory Forecasting for Bicycles using Anticipation Mechanism](https://arxiv.org/abs/2508.14523)
*Kevin Riehl,Shaimaa K. El-Baklish,Anastasios Kouvelas,Michail A. Makridis*

Main category: cs.LG

TL;DR: 提出了Great GATsBi框架，这是一个基于领域知识的混合多模态自行车轨迹预测模型，结合物理建模和社会建模来处理自行车运动的双重特性。


<details>
  <summary>Details</summary>
Motivation: 虽然大多数交通事故死亡涉及自行车，但现有研究主要关注行人和机动车辆，自行车轨迹预测研究不足，对道路安全至关重要。

Method: 使用图注意力网络建模社会交互，结合物理基础建模（受机动车辆启发）和社会基础建模（受行人运动启发），包含衰减历史轨迹和预期未来轨迹数据。

Result: 提出的物理模型（短期预测表现好）和社会模型（长期预测表现好）的集成方法超越了最先进性能，并通过大规模控制骑行实验验证了框架性能。

Conclusion: 该混合框架成功处理了自行车运动的双重特性，在轨迹预测和社会交互建模方面表现出色，为自行车安全应用提供了有效解决方案。

Abstract: Accurate prediction of road user movement is increasingly required by many
applications ranging from advanced driver assistance systems to autonomous
driving, and especially crucial for road safety. Even though most traffic
accident fatalities account to bicycles, they have received little attention,
as previous work focused mainly on pedestrians and motorized vehicles. In this
work, we present the Great GATsBi, a domain-knowledge-based, hybrid, multimodal
trajectory prediction framework for bicycles. The model incorporates both
physics-based modeling (inspired by motorized vehicles) and social-based
modeling (inspired by pedestrian movements) to explicitly account for the dual
nature of bicycle movement. The social interactions are modeled with a graph
attention network, and include decayed historical, but also anticipated, future
trajectory data of a bicycles neighborhood, following recent insights from
psychological and social studies. The results indicate that the proposed
ensemble of physics models -- performing well in the short-term predictions --
and social models -- performing well in the long-term predictions -- exceeds
state-of-the-art performance. We also conducted a controlled mass-cycling
experiment to demonstrate the framework's performance when forecasting bicycle
trajectories and modeling social interactions with road users.

</details>


### [102] [FedEve: On Bridging the Client Drift and Period Drift for Cross-device Federated Learning](https://arxiv.org/abs/2508.14539)
*Tao Shen,Zexi Li,Didi Zhu,Ziyu Zhao,Chao Wu,Fei Wu*

Main category: cs.LG

TL;DR: 本文研究了联邦学习中由于部分客户端参与导致的period drift问题，发现其与client drift相互作用会严重影响性能，提出了FedEve方法来补偿这两种漂移效应。


<details>
  <summary>Details</summary>
Motivation: 跨设备联邦学习中，部分客户端参与会导致每个通信轮次参与客户端的数据分布与全体客户端分布存在偏差（period drift），这种漂移比client drift更具危害性，但尚未得到充分研究。

Method: 提出了predict-observe框架和FedEve方法，通过让period drift和client drift相互补偿来减轻整体影响，理论证明可以降低模型更新的方差。

Result: 理论分析表明方法能减少模型更新方差，大量实验证明在非独立同分布数据的跨设备设置中优于其他方法。

Conclusion: period drift是跨设备联邦学习中的重要问题，与client drift相互作用会加剧性能下降，提出的FedEve方法能有效补偿这两种漂移效应，提升模型性能。

Abstract: Federated learning (FL) is a machine learning paradigm that allows multiple
clients to collaboratively train a shared model without exposing their private
data. Data heterogeneity is a fundamental challenge in FL, which can result in
poor convergence and performance degradation. Client drift has been recognized
as one of the factors contributing to this issue resulting from the multiple
local updates in FedAvg. However, in cross-device FL, a different form of drift
arises due to the partial client participation, but it has not been studied
well. This drift, we referred as period drift, occurs as participating clients
at each communication round may exhibit distinct data distribution that
deviates from that of all clients. It could be more harmful than client drift
since the optimization objective shifts with every round.
  In this paper, we investigate the interaction between period drift and client
drift, finding that period drift can have a particularly detrimental effect on
cross-device FL as the degree of data heterogeneity increases. To tackle these
issues, we propose a predict-observe framework and present an instantiated
method, FedEve, where these two types of drift can compensate each other to
mitigate their overall impact. We provide theoretical evidence that our
approach can reduce the variance of model updates. Extensive experiments
demonstrate that our method outperforms alternatives on non-iid data in
cross-device settings.

</details>


### [103] [Cooperative SGD with Dynamic Mixing Matrices](https://arxiv.org/abs/2508.14565)
*Soumya Sarkar,Shweta Jain*

Main category: cs.LG

TL;DR: 本文提出了一个统一框架，用于分析具有动态拓扑和客户端选择的分布式SGD算法，相比现有工作提供了改进或匹配的理论收敛保证。


<details>
  <summary>Details</summary>
Motivation: 传统分布式SGD算法假设固定拓扑和均匀节点贡献，但实验表明这些假设是次优的。动态拓扑和非均匀聚合策略可以显著提升模型性能。

Method: 开发了一个统一的理论框架，涵盖多种基于局部更新SGD的分布式算法，支持动态拓扑变化和客户端选择机制。

Result: 该框架为动态拓扑下的分布式SGD算法提供了改进或至少匹配现有工作的理论收敛保证。

Conclusion: 动态拓扑和非均匀聚合策略在分布式SGD训练中具有重要价值，提出的统一框架为这类算法提供了坚实的理论基础。

Abstract: One of the most common methods to train machine learning algorithms today is
the stochastic gradient descent (SGD). In a distributed setting, SGD-based
algorithms have been shown to converge theoretically under specific
circumstances. A substantial number of works in the distributed SGD setting
assume a fixed topology for the edge devices. These papers also assume that the
contribution of nodes to the global model is uniform. However, experiments have
shown that such assumptions are suboptimal and a non uniform aggregation
strategy coupled with a dynamically shifting topology and client selection can
significantly improve the performance of such models. This paper details a
unified framework that covers several Local-Update SGD-based distributed
algorithms with dynamic topologies and provides improved or matching
theoretical guarantees on convergence compared to existing work.

</details>


### [104] [A Comprehensive Evaluation of the Sensitivity of Density-Ratio Estimation Based Fairness Measurement in Regression](https://arxiv.org/abs/2508.14576)
*Abdalwahab Almajed,Maryam Tabar,Peyman Najafirad*

Main category: cs.LG

TL;DR: 本研究探讨了回归问题中基于密度比估计的公平性测量方法对底层估计算法选择的敏感性，发现不同核心算法会显著影响公平性测量结果甚至产生不一致的结论。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用逻辑回归分类器进行密度比估计来测量回归模型的公平性，但未研究不同密度比估计算法对公平性测量结果的影响，存在研究空白。

Method: 开发了多种基于不同密度比估计核心的公平性测量方法，并系统比较这些核心算法对公平性测量结果的影响。

Result: 实验结果表明，密度比估计核心的选择会显著影响公平性测量结果，甚至导致对不同算法相对公平性的评估产生不一致的结论。

Conclusion: 基于密度比估计的回归公平性测量方法存在重大问题，需要进一步研究以提高其可靠性。

Abstract: The prevalence of algorithmic bias in Machine Learning (ML)-driven approaches
has inspired growing research on measuring and mitigating bias in the ML
domain. Accordingly, prior research studied how to measure fairness in
regression which is a complex problem. In particular, recent research proposed
to formulate it as a density-ratio estimation problem and relied on a Logistic
Regression-driven probabilistic classifier-based approach to solve it. However,
there are several other methods to estimate a density ratio, and to the best of
our knowledge, prior work did not study the sensitivity of such fairness
measurement methods to the choice of underlying density ratio estimation
algorithm. To fill this gap, this paper develops a set of fairness measurement
methods with various density-ratio estimation cores and thoroughly investigates
how different cores would affect the achieved level of fairness. Our
experimental results show that the choice of density-ratio estimation core
could significantly affect the outcome of fairness measurement method, and
even, generate inconsistent results with respect to the relative fairness of
various algorithms. These observations suggest major issues with density-ratio
estimation based fairness measurement in regression and a need for further
research to enhance their reliability.

</details>


### [105] [DualNILM: Energy Injection Identification Enabled Disaggregation with Deep Multi-Task Learning](https://arxiv.org/abs/2508.14600)
*Xudong Wang,Guoming Tang,Junyu Xue,Srinivasan Keshav,Tongxin Li,Chris Ding*

Main category: cs.LG

TL;DR: DualNILM是一个基于Transformer的多任务学习框架，用于解决智能家居中分布式能源接入导致的非侵入式负载监测性能下降问题，能够同时识别电器状态和注入能量。


<details>
  <summary>Details</summary>
Motivation: 随着太阳能板、电池存储等分布式能源在智能家居中的广泛应用，传统的非侵入式负载监测方法仅依赖电表数据，无法有效处理分布式能源注入能量对电器功率特征的干扰，导致监测性能显著下降。

Method: 提出DualNILM框架，采用深度多任务学习方法，结合序列到点和序列到序列策略，基于Transformer架构捕捉聚合功率消耗模式中的多尺度时间依赖关系，实现电器状态识别和能量注入识别的双重任务。

Result: 在包含电器级能耗和能量注入的自收集和合成开源NILM数据集上进行验证，实验结果表明DualNILM在双重任务上保持优异性能，显著优于传统方法。

Conclusion: DualNILM有效解决了分布式能源接入对NILM系统的挑战，为智能家居和建筑应用中的精细能耗监测提供了更可靠的解决方案。

Abstract: Non-Intrusive Load Monitoring (NILM) offers a cost-effective method to obtain
fine-grained appliance-level energy consumption in smart homes and building
applications. However, the increasing adoption of behind-the-meter energy
sources, such as solar panels and battery storage, poses new challenges for
conventional NILM methods that rely solely on at-the-meter data. The injected
energy from the behind-the-meter sources can obscure the power signatures of
individual appliances, leading to a significant decline in NILM performance. To
address this challenge, we present DualNILM, a deep multi-task learning
framework designed for the dual tasks of appliance state recognition and
injected energy identification in NILM. By integrating sequence-to-point and
sequence-to-sequence strategies within a Transformer-based architecture,
DualNILM can effectively capture multi-scale temporal dependencies in the
aggregate power consumption patterns, allowing for accurate appliance state
recognition and energy injection identification. We conduct validation of
DualNILM using both self-collected and synthesized open NILM datasets that
include both appliance-level energy consumption and energy injection. Extensive
experimental results demonstrate that DualNILM maintains an excellent
performance for the dual tasks in NILM, much outperforming conventional
methods.

</details>


### [106] [Measuring IIA Violations in Similarity Choices with Bayesian Models](https://arxiv.org/abs/2508.14615)
*Hugo Sales Corrêa,Suryanarayana Sankagiri,Daniel Ratton Figueiredo,Matthias Grossglauser*

Main category: cs.LG

TL;DR: 该论文提出了两种统计方法来检验相似性选择中的IIA假设，发现了显著的IIA违反现象，表明需要新的考虑情境效应的相似性选择模型。


<details>
  <summary>Details</summary>
Motivation: 相似性选择数据在信息检索和嵌入学习等场景中很常见，但经典的基于度量的模型假设无关选项独立性(IIA)。虽然IIA违反在其他离散选择场景中已被发现，但在相似性选择领域研究较少，因为目标依赖的特性使IIA检验变得复杂。

Method: 提出了两种统计检验方法：经典的拟合优度检验和基于后验预测检查(PPC)的贝叶斯方法。收集了两个数据集：一个设计用于引发IIA违反，另一个来自相同项目宇宙的随机选择集。

Result: 在两个数据集上都检测到了显著的IIA违反，且违反程度相当。PPC检验显示人群是同质的，表明IIA违反是由选择集内的情境效应（具体是选项间的交互作用）驱动的。

Conclusion: 研究结果强调了需要开发新的相似性选择模型来考虑情境效应，因为经典的IIA假设在相似性选择场景中不成立。

Abstract: Similarity choice data occur when humans make choices among alternatives
based on their similarity to a target, e.g., in the context of information
retrieval and in embedding learning settings. Classical metric-based models of
similarity choice assume independence of irrelevant alternatives (IIA), a
property that allows for a simpler formulation. While IIA violations have been
detected in many discrete choice settings, the similarity choice setting has
received scant attention. This is because the target-dependent nature of the
choice complicates IIA testing. We propose two statistical methods to test for
IIA: a classical goodness-of-fit test and a Bayesian counterpart based on the
framework of Posterior Predictive Checks (PPC). This Bayesian approach, our
main technical contribution, quantifies the degree of IIA violation beyond its
mere significance. We curate two datasets: one with choice sets designed to
elicit IIA violations, and another with randomly generated choice sets from the
same item universe. Our tests confirmed significant IIA violations on both
datasets, and notably, we find a comparable degree of violation between them.
Further, we devise a new PPC test for population homogeneity. Results show that
the population is indeed homogenous, suggesting that the IIA violations are
driven by context effects -- specifically, interactions within the choice sets.
These results highlight the need for new similarity choice models that account
for such context effects.

</details>


### [107] [A Fuzzy-Enhanced Explainable AI Framework for Flight Continuous Descent Operations Classification](https://arxiv.org/abs/2508.14618)
*Amin Noroozi,Sandaruwan K. Sethunge,Elham Norouzi,Phat T. Phan,Kavinda U. Waduge,Md. Arafatur Rahman*

Main category: cs.LG

TL;DR: 本研究提出了一个模糊增强可解释AI框架(FEXAI)，结合模糊逻辑、机器学习和SHAP分析，用于分析连续下降操作(CDO)性能影响因素，准确率超过90%并提供可解释规则。


<details>
  <summary>Details</summary>
Motivation: 连续下降操作(CDO)具有减少燃油消耗、排放和噪音等优势，但现有研究缺乏系统性分析其性能影响因素的方法，且相关优化方法缺乏航空领域所需的透明度和可解释性。

Method: 收集1,094个航班的ADS-B数据（29个特征），应用机器学习模型和SHAP分析分类CDO遵守水平并排序特征重要性，使用最重要的三个特征构建模糊规则分类器。

Result: 所有模型分类准确率超过90%，识别出平均下降速率、下降段数量和平均航向变化是最重要的CDO性能预测因子，FEXAI提供了可读性强的操作规则。

Conclusion: FEXAI方法为操作决策支持提供了新途径，可集成到航空工具中实现实时建议，在不同操作条件下保持CDO遵守率。

Abstract: Continuous Descent Operations (CDO) involve smooth, idle-thrust descents that
avoid level-offs, reducing fuel burn, emissions, and noise while improving
efficiency and passenger comfort. Despite its operational and environmental
benefits, limited research has systematically examined the factors influencing
CDO performance. Moreover, many existing methods in related areas, such as
trajectory optimization, lack the transparency required in aviation, where
explainability is critical for safety and stakeholder trust. This study
addresses these gaps by proposing a Fuzzy-Enhanced Explainable AI (FEXAI)
framework that integrates fuzzy logic with machine learning and SHapley
Additive exPlanations (SHAP) analysis. For this purpose, a comprehensive
dataset of 29 features, including 11 operational and 18 weather-related
features, was collected from 1,094 flights using Automatic Dependent
Surveillance-Broadcast (ADS-B) data. Machine learning models and SHAP were then
applied to classify flights' CDO adherence levels and rank features by
importance. The three most influential features, as identified by SHAP scores,
were then used to construct a fuzzy rule-based classifier, enabling the
extraction of interpretable fuzzy rules. All models achieved classification
accuracies above 90%, with FEXAI providing meaningful, human-readable rules for
operational users. Results indicated that the average descent rate within the
arrival route, the number of descent segments, and the average change in
directional heading during descent were the strongest predictors of CDO
performance. The FEXAI method proposed in this study presents a novel pathway
for operational decision support and could be integrated into aviation tools to
enable real-time advisories that maintain CDO adherence under varying
operational conditions.

</details>


### [108] [Clinical semantics for lung cancer prediction](https://arxiv.org/abs/2508.14627)
*Luis H. John,Jan A. Kors,Jenna M. Reps,Peter R. Rijnbeek,Egill A. Fridgeirsson*

Main category: cs.LG

TL;DR: 该研究通过将SNOMED医学术语层次结构映射到双曲空间生成Poincaré嵌入，整合临床语义信息来改进肺癌发病预测，在深度学习模型中实现了适度的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有临床预测模型通常忽略临床概念间的语义关系，本研究旨在通过整合领域特定的语义信息来改进肺癌发病预测。

Method: 从SNOMED分类法构建临床知识图谱，使用黎曼随机梯度下降生成Poincaré嵌入，并将这些嵌入整合到ResNet和Transformer深度学习架构中。

Result: 使用预训练Poincaré嵌入相比使用随机初始化欧几里得嵌入的基线模型，在判别性能上获得了适度且一致的改进。ResNet模型（特别是使用10维Poincaré嵌入的）显示出更好的校准性能。

Conclusion: 将临床知识图谱嵌入双曲空间并整合到深度学习模型中，可以通过保留用于预测的临床术语的层次结构来改善肺癌发病预测，这展示了将数据驱动特征提取与既定临床知识相结合的可行方法。

Abstract: Background: Existing clinical prediction models often represent patient data
using features that ignore the semantic relationships between clinical
concepts. This study integrates domain-specific semantic information by mapping
the SNOMED medical term hierarchy into a low-dimensional hyperbolic space using
Poincar\'e embeddings, with the aim of improving lung cancer onset prediction.
  Methods: Using a retrospective cohort from the Optum EHR dataset, we derived
a clinical knowledge graph from the SNOMED taxonomy and generated Poincar\'e
embeddings via Riemannian stochastic gradient descent. These embeddings were
then incorporated into two deep learning architectures, a ResNet and a
Transformer model. Models were evaluated for discrimination (area under the
receiver operating characteristic curve) and calibration (average absolute
difference between observed and predicted probabilities) performance.
  Results: Incorporating pre-trained Poincar\'e embeddings resulted in modest
and consistent improvements in discrimination performance compared to baseline
models using randomly initialized Euclidean embeddings. ResNet models,
particularly those using a 10-dimensional Poincar\'e embedding, showed enhanced
calibration, whereas Transformer models maintained stable calibration across
configurations.
  Discussion: Embedding clinical knowledge graphs into hyperbolic space and
integrating these representations into deep learning models can improve lung
cancer onset prediction by preserving the hierarchical structure of clinical
terminologies used for prediction. This approach demonstrates a feasible method
for combining data-driven feature extraction with established clinical
knowledge.

</details>


### [109] [Understanding Data Influence with Differential Approximation](https://arxiv.org/abs/2508.14648)
*Haoru Tan,Sitong Wu,Xiuzhe Wu,Wang Wang,Bo Zhao,Zeke Xie,Gui-Song Xia,Xiaojuan Qi*

Main category: cs.LG

TL;DR: Diff-In是一种新的样本影响力近似方法，通过累积连续学习步骤中的影响力差异来估计样本影响力，无需模型凸性假设，计算效率高且可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有数据分析工具在准确性方面存在不足，许多工具假设神经网络损失函数是凸的，这限制了方法的有效实施。需要一种更准确且不依赖凸性假设的影响力估计方法。

Method: 提出Diff-In方法，将样本影响力定义为在连续训练迭代中变化的累积和。使用二阶近似来高精度近似这些差异项，通过计算Hessian矩阵和梯度的乘积来实现高效计算，该乘积可以通过一阶梯度的有限差分来有效近似。

Result: 理论分析和大量实验证明，Diff-In相比现有影响力估计器具有显著更低的近似误差。在三个数据中心任务（数据清洗、数据删除和核心集选择）的多个基准数据集上表现出优越性能，特别是在大规模视觉语言预训练的数据剪枝实验中，能够扩展到数百万数据点并优于强基线方法。

Conclusion: Diff-In提供了一种准确、高效且可扩展的样本影响力估计方法，克服了现有方法对模型凸性的依赖，在多种数据中心应用中表现出色，具有重要的实际应用价值。

Abstract: Data plays a pivotal role in the groundbreaking advancements in artificial
intelligence. The quantitative analysis of data significantly contributes to
model training, enhancing both the efficiency and quality of data utilization.
However, existing data analysis tools often lag in accuracy. For instance, many
of these tools even assume that the loss function of neural networks is convex.
These limitations make it challenging to implement current methods effectively.
In this paper, we introduce a new formulation to approximate a sample's
influence by accumulating the differences in influence between consecutive
learning steps, which we term Diff-In. Specifically, we formulate the
sample-wise influence as the cumulative sum of its changes/differences across
successive training iterations. By employing second-order approximations, we
approximate these difference terms with high accuracy while eliminating the
need for model convexity required by existing methods. Despite being a
second-order method, Diff-In maintains computational complexity comparable to
that of first-order methods and remains scalable. This efficiency is achieved
by computing the product of the Hessian and gradient, which can be efficiently
approximated using finite differences of first-order gradients. We assess the
approximation accuracy of Diff-In both theoretically and empirically. Our
theoretical analysis demonstrates that Diff-In achieves significantly lower
approximation error compared to existing influence estimators. Extensive
experiments further confirm its superior performance across multiple benchmark
datasets in three data-centric tasks: data cleaning, data deletion, and coreset
selection. Notably, our experiments on data pruning for large-scale
vision-language pre-training show that Diff-In can scale to millions of data
points and outperforms strong baselines.

</details>


### [110] [Improving Fairness in Graph Neural Networks via Counterfactual Debiasing](https://arxiv.org/abs/2508.14683)
*Zengyi Wo,Chang Liu,Yumeng Wang,Minglai Shao,Wenjun Wang*

Main category: cs.LG

TL;DR: Fair-ICD：一种基于反事实数据增强的图神经网络公平性提升方法，通过生成多样化的邻居环境来学习无偏节点表示，同时保持预测准确性


<details>
  <summary>Details</summary>
Motivation: 现有GNN公平性方法通过过滤敏感信息（如边丢弃或特征掩码）可能无意中消除非敏感特征，导致预测准确性和公平性之间的平衡受损

Method: 使用反事实数据增强创建多样化邻居环境，在消息传递前生成增强图，然后使用对抗判别器来减少传统GNN分类器的预测偏差

Result: 在标准数据集上使用三种GNN骨干网络的实验表明，Fair-ICD显著提高了公平性指标，同时保持了较高的预测性能

Conclusion: Fair-ICD方法在适度条件下确保了GNN的公平性，有效解决了公平性和准确性之间的权衡问题

Abstract: Graph Neural Networks (GNNs) have been successful in modeling
graph-structured data. However, similar to other machine learning models, GNNs
can exhibit bias in predictions based on attributes like race and gender.
Moreover, bias in GNNs can be exacerbated by the graph structure and
message-passing mechanisms. Recent cutting-edge methods propose mitigating bias
by filtering out sensitive information from input or representations, like edge
dropping or feature masking. Yet, we argue that such strategies may
unintentionally eliminate non-sensitive features, leading to a compromised
balance between predictive accuracy and fairness. To tackle this challenge, we
present a novel approach utilizing counterfactual data augmentation for bias
mitigation. This method involves creating diverse neighborhoods using
counterfactuals before message passing, facilitating unbiased node
representations learning from the augmented graph. Subsequently, an adversarial
discriminator is employed to diminish bias in predictions by conventional GNN
classifiers. Our proposed technique, Fair-ICD, ensures the fairness of GNNs
under moderate conditions. Experiments on standard datasets using three GNN
backbones demonstrate that Fair-ICD notably enhances fairness metrics while
preserving high predictive performance.

</details>


### [111] [Addressing Graph Anomaly Detection via Causal Edge Separation and Spectrum](https://arxiv.org/abs/2508.14684)
*Zengyi Wo,Wenjun Wang,Minglai Shao,Chang Liu,Yumeng Wang,Yueheng Sun*

Main category: cs.LG

TL;DR: 提出基于因果边分离的谱神经网络CES2-GAD，用于解决异配图异常检测问题，通过分离同配和异配边并使用混合谱滤波器捕获信号


<details>
  <summary>Details</summary>
Motivation: 现实世界中异常实体往往通过添加合法连接来隐藏异常链接，形成异配结构，现有GNN方法难以处理这种异配性问题，特别是在谱域的研究有限

Method: 1. 使用因果干预将原始图分离为同配边和异配边；2. 使用多种混合谱滤波器从分割图中捕获信号；3. 将多信号表示拼接后输入分类器进行异常预测

Result: 在真实数据集上的大量实验证明了所提出方法的有效性

Conclusion: CES2-GAD方法能够有效处理异配图中的异常检测问题，通过谱分析和因果边分离技术解决了现有方法的局限性

Abstract: In the real world, anomalous entities often add more legitimate connections
while hiding direct links with other anomalous entities, leading to
heterophilic structures in anomalous networks that most GNN-based techniques
fail to address. Several works have been proposed to tackle this issue in the
spatial domain. However, these methods overlook the complex relationships
between node structure encoding, node features, and their contextual
environment and rely on principled guidance, research on solving spectral
domain heterophilic problems remains limited. This study analyzes the spectral
distribution of nodes with different heterophilic degrees and discovers that
the heterophily of anomalous nodes causes the spectral energy to shift from low
to high frequencies. To address the above challenges, we propose a spectral
neural network CES2-GAD based on causal edge separation for anomaly detection
on heterophilic graphs. Firstly, CES2-GAD will separate the original graph into
homophilic and heterophilic edges using causal interventions. Subsequently,
various hybrid-spectrum filters are used to capture signals from the segmented
graphs. Finally, representations from multiple signals are concatenated and
input into a classifier to predict anomalies. Extensive experiments with
real-world datasets have proven the effectiveness of the method we proposed.

</details>


### [112] [CaTE Data Curation for Trustworthy AI](https://arxiv.org/abs/2508.14741)
*Mary Versa Clemens-Sewall,Christopher Cervantes,Emma Rafkin,J. Neil Otte,Tom Magelinski,Libby Lewis,Michelle Liu,Dana Udwin,Monique Kirkman-Bey*

Main category: cs.LG

TL;DR: 该报告提供了在AI系统数据精炼阶段提升可信过程的实践指南，包括定义、步骤序列和工具实现


<details>
  <summary>Details</summary>
Motivation: 为AI系统开发团队提供在数据精炼阶段促进可信过程的实用指导

Method: 定义数据精炼和可信过程概念，描述一系列核心步骤和并行路径，包括优缺点、前提条件和工具实现

Result: 综合了来自学术文献的数据精炼工具和方法，形成了一套多样但协调的实践方法集

Conclusion: 该报告为读者提供了提升AI可信过程的完整工具套件和实践指南

Abstract: This report provides practical guidance to teams designing or developing
AI-enabled systems for how to promote trustworthiness during the data curation
phase of development. In this report, the authors first define data, the data
curation phase, and trustworthiness. We then describe a series of steps that
the development team, especially data scientists, can take to build a
trustworthy AI-enabled system. We enumerate the sequence of core steps and
trace parallel paths where alternatives exist. The descriptions of these steps
include strengths, weaknesses, preconditions, outcomes, and relevant
open-source software tool implementations. In total, this report is a synthesis
of data curation tools and approaches from relevant academic literature, and
our goal is to equip readers with a diverse yet coherent set of practices for
improving AI trustworthiness.

</details>


### [113] [MissionHD: Data-Driven Refinement of Reasoning Graph Structure through Hyperdimensional Causal Path Encoding and Decoding](https://arxiv.org/abs/2508.14746)
*Sanggeon Yun,Raheeb Hassan,Ryozo Masukawa,Mohsen Imani*

Main category: cs.LG

TL;DR: 提出了D-GSR方法，通过下游任务数据直接优化LLM生成的推理图结构，使用超维度计算框架MissionHD进行图结构精炼，在视频异常检测任务上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有图结构精炼方法不适用于LLM生成的新型、无数据集的推理图，这些图与下游视觉任务（如视频异常检测）存在对齐问题。

Method: 提出数据驱动的图结构精炼范式D-GSR，使用超维度计算框架MissionHD，通过高效的编码-解码过程，在下游任务信号的指导下精炼图结构。

Result: 在具有挑战性的视频异常检测和视频异常识别基准测试中，使用精炼后的图结构获得了显著的性能提升。

Conclusion: 该方法作为有效的预处理步骤，能够显著改善LLM生成的推理图与下游视觉任务的对齐问题。

Abstract: Reasoning graphs from Large Language Models (LLMs) are often misaligned with
downstream visual tasks such as video anomaly detection (VAD). Existing Graph
Structure Refinement (GSR) methods are ill-suited for these novel, dataset-less
graphs. We introduce Data-driven GSR (D-GSR), a new paradigm that directly
optimizes graph structure using downstream task data, and propose MissionHD, a
hyperdimensional computing (HDC) framework to operationalize it. MissionHD uses
an efficient encode-decode process to refine the graph, guided by the
downstream task signal. Experiments on challenging VAD and VAR benchmarks show
significant performance improvements when using our refined graphs, validating
our approach as an effective pre-processing step.

</details>


### [114] [HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents](https://arxiv.org/abs/2508.14751)
*Thomas Carta,Clément Romac,Loris Gaven,Pierre-Yves Oudeyer,Olivier Sigaud,Sylvain Lamprier*

Main category: cs.LG

TL;DR: HERAKLES是一个分层自目标AI代理框架，通过将已掌握的目标编译到低级策略中，动态扩展高级策略可用的子目标空间，利用LLM作为高级控制器进行目标分解和泛化。


<details>
  <summary>Details</summary>
Motivation: 解决开放环境中AI代理需要高效学习日益复杂、抽象和异构目标的问题，现有方法使用专家定义的子目标空间和预训练低级策略，不适合目标空间自然多样化的开放场景。

Method: 采用两层分层架构：低级策略由小型快速神经网络执行已编译的技能，高级策略使用LLM进行目标分解和泛化，动态扩展子目标空间。

Result: 在Crafter环境中验证，该框架能有效随目标复杂度扩展，通过技能编译提高样本效率，使代理能够随时间稳健适应新挑战。

Conclusion: HERAKLES框架通过动态技能编译和LLM高级控制，为开放环境中的分层自目标学习提供了有效的解决方案，实现了目标复杂度的可扩展性和适应性。

Abstract: Open-ended AI agents need to be able to learn efficiently goals of increasing
complexity, abstraction and heterogeneity over their lifetime. Beyond sampling
efficiently their own goals, autotelic agents specifically need to be able to
keep the growing complexity of goals under control, limiting the associated
growth in sample and computational complexity. To adress this challenge, recent
approaches have leveraged hierarchical reinforcement learning (HRL) and
language, capitalizing on its compositional and combinatorial generalization
capabilities to acquire temporally extended reusable behaviours. Existing
approaches use expert defined spaces of subgoals over which they instantiate a
hierarchy, and often assume pre-trained associated low-level policies. Such
designs are inadequate in open-ended scenarios, where goal spaces naturally
diversify across a broad spectrum of difficulties. We introduce HERAKLES, a
framework that enables a two-level hierarchical autotelic agent to continuously
compile mastered goals into the low-level policy, executed by a small, fast
neural network, dynamically expanding the set of subgoals available to the
high-level policy. We train a Large Language Model (LLM) to serve as the
high-level controller, exploiting its strengths in goal decomposition and
generalization to operate effectively over this evolving subgoal space. We
evaluate HERAKLES in the open-ended Crafter environment and show that it scales
effectively with goal complexity, improves sample efficiency through skill
compilation, and enables the agent to adapt robustly to novel challenges over
time.

</details>


### [115] [Federated Distillation on Edge Devices: Efficient Client-Side Filtering for Non-IID Data](https://arxiv.org/abs/2508.14769)
*Ahmed Mujtaba,Gleb Radchenko,Radu Prodan,Marc Masana*

Main category: cs.LG

TL;DR: 提出EdgeFD方法，通过KMeans密度比估计器简化客户端计算，无需服务器端过滤，在异构数据场景下实现高效联邦蒸馏


<details>
  <summary>Details</summary>
Motivation: 现有联邦蒸馏方法需要复杂的知识共享策略，客户端需通过计算密集的统计密度比估计器识别分布内代理数据，服务器端过滤引入延迟

Method: EdgeFD使用基于KMeans的高效密度比估计器，在客户端有效过滤分布内和分布外代理数据，无需预训练教师模型和服务器端过滤

Result: 在强非IID、弱非IID和IID数据分布场景下，EdgeFD优于最先进方法，准确率接近IID场景，计算开销显著降低

Conclusion: EdgeFD提高了联邦蒸馏的可扩展性和实际应用性，特别适合资源受限的边缘设备部署

Abstract: Federated distillation has emerged as a promising collaborative machine
learning approach, offering enhanced privacy protection and reduced
communication compared to traditional federated learning by exchanging model
outputs (soft logits) rather than full model parameters. However, existing
methods employ complex selective knowledge-sharing strategies that require
clients to identify in-distribution proxy data through computationally
expensive statistical density ratio estimators. Additionally, server-side
filtering of ambiguous knowledge introduces latency to the process. To address
these challenges, we propose a robust, resource-efficient EdgeFD method that
reduces the complexity of the client-side density ratio estimation and removes
the need for server-side filtering. EdgeFD introduces an efficient KMeans-based
density ratio estimator for effectively filtering both in-distribution and
out-of-distribution proxy data on clients, significantly improving the quality
of knowledge sharing. We evaluate EdgeFD across diverse practical scenarios,
including strong non-IID, weak non-IID, and IID data distributions on clients,
without requiring a pre-trained teacher model on the server for knowledge
distillation. Experimental results demonstrate that EdgeFD outperforms
state-of-the-art methods, consistently achieving accuracy levels close to IID
scenarios even under heterogeneous and challenging conditions. The
significantly reduced computational overhead of the KMeans-based estimator is
suitable for deployment on resource-constrained edge devices, thereby enhancing
the scalability and real-world applicability of federated distillation. The
code is available online for reproducibility.

</details>


### [116] [Synthetic Adaptive Guided Embeddings (SAGE): A Novel Knowledge Distillation Method](https://arxiv.org/abs/2508.14783)
*Suleyman Olcay Polat,Poli A. Nemkova,Mark V. Albert*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的适应性模型萌荐框架，通过动态数据增帽和矢量化萌荐来提高学生模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统模型萌荐方法存在计算开销大和沿用性有限的问题，需要一种更高效的知识转移方法来支持资源受限环境中的模型部署。

Method: 使用UMAP降维和最近邻采样识别学生模型失败高的区域，生成针对性的合成示例；引入轻量级教师-学生接口，跳过教师输入层直接在矢量表示上进行萌荐。

Result: 在标准NLP测试集上，66M参数的学生模型持续超越基线方法，在QNLI上达到91.2%，在SST-2上达到92.3%，同时训练周期更少。

Conclusion: 识别失败的数据增帽和矢量化萌荐方法为高效的模型压缩提供了有前景的解决方案。

Abstract: Model distillation enables the transfer of knowledge from large-scale models
to compact student models, facilitating deployment in resource-constrained
environments. However, conventional distillation approaches often suffer from
computational overhead and limited generalization. We propose a novel adaptive
distillation framework that dynamically augments training data in regions of
high student model loss. Using UMAP-based dimensionality reduction and nearest
neighbor sampling, our method identifies underperforming regions in the
embedding space and generates targeted synthetic examples to guide student
learning. To further improve efficiency, we introduce a lightweight
teacher-student interface that bypasses the teacher's input layer, enabling
direct distillation on vectorized representations. Experiments across standard
NLP benchmarks demonstrate that our 66M-parameter student model consistently
matches or surpasses established baselines, achieving 91.2% on QNLI and 92.3%
on SST-2, while training with fewer epochs. These results highlight the promise
of loss-aware data augmentation and vectorized distillation for efficient and
effective model compression.

</details>


### [117] [A Guide for Manual Annotation of Scientific Imagery: How to Prepare for Large Projects](https://arxiv.org/abs/2508.14801)
*Azim Ahmadzadeh,Rohan Adhyapak,Armin Iraji,Kartik Chaurasiya,V Aparna,Petrus C. Martens*

Main category: cs.LG

TL;DR: 这篇论文提供了一个域无关的图像注释项目准备指南，重点关注科学图像注释项目的管理挑战和最佳实践。


<details>
  <summary>Details</summary>
Motivation: 虽然手动注释图像数据需求很高，但管理复杂而成本高昂的注释项目却缺乏充分讨论。领导这些项目需要应对多样化且相互关联的挑战，而这些挑战往往超出了特定领域专家的知识范围。

Method: 基于作者管理大型手动注释项目的丰富经验，论文提供了一个系统性的准备指南，涵盖成功指标、注释对象、项目目标、数据可用性和关键团队角色等核心概念。同时讨论了各种人为偏见并推荐了提高注释质量和效率的工具技术。

Result: 论文提供了一个完整的注释项目管理框架，包括从数据收集到资源分配、从偏见减少到注释员培训的各个方面。通过提供实用的建议和最佳实践，为管理复杂注释项目提供了具体的指导。

Conclusion: 论文的目标是鼓励更多的研究和框架开发，以建立一个全面的知识库，从而降低各个领域手动注释项目的成本。这个域无关的指南有助于提高注释项目的成功率和效果。

Abstract: Despite the high demand for manually annotated image data, managing complex
and costly annotation projects remains under-discussed. This is partly due to
the fact that leading such projects requires dealing with a set of diverse and
interconnected challenges which often fall outside the expertise of specific
domain experts, leaving practical guidelines scarce. These challenges range
widely from data collection to resource allocation and recruitment, from
mitigation of biases to effective training of the annotators. This paper
provides a domain-agnostic preparation guide for annotation projects, with a
focus on scientific imagery. Drawing from the authors' extensive experience in
managing a large manual annotation project, it addresses fundamental concepts
including success measures, annotation subjects, project goals, data
availability, and essential team roles. Additionally, it discusses various
human biases and recommends tools and technologies to improve annotation
quality and efficiency. The goal is to encourage further research and
frameworks for creating a comprehensive knowledge base to reduce the costs of
manual annotation projects across various fields.

</details>


### [118] [Source-Guided Flow Matching](https://arxiv.org/abs/2508.14807)
*Zifan Wang,Alice Harting,Matthieu Barreau,Michael M. Zavlanos,Karl H. Johansson*

Main category: cs.LG

TL;DR: 提出了Source-Guided Flow Matching (SGFM)框架，通过直接修改源分布而非概率流向量场来实现生成模型引导，保持预训练向量场不变，将引导问题转化为源分布采样问题。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型引导方法通过修改概率流向量场来添加引导场，这种方法可能破坏预训练模型的性能。作者希望找到一种更灵活且能保持预训练向量场完整性的引导方法。

Method: SGFM框架直接修改源分布，保持预训练向量场不变。将引导问题转化为从源分布采样的明确定义问题，允许用户根据具体问题灵活选择采样方法。

Result: 理论证明SGFM能准确恢复目标分布，提供了使用近似采样器和近似向量场时的Wasserstein误差界限。在合成2D基准、图像数据集和物理信息生成任务上的实验证明了框架的有效性和灵活性。

Conclusion: SGFM提供了一种灵活且理论保证的生成模型引导方法，与最优流匹配模型良好集成，保持了向量场生成的直接传输映射特性。

Abstract: Guidance of generative models is typically achieved by modifying the
probability flow vector field through the addition of a guidance field. In this
paper, we instead propose the Source-Guided Flow Matching (SGFM) framework,
which modifies the source distribution directly while keeping the pre-trained
vector field intact. This reduces the guidance problem to a well-defined
problem of sampling from the source distribution. We theoretically show that
SGFM recovers the desired target distribution exactly. Furthermore, we provide
bounds on the Wasserstein error for the generated distribution when using an
approximate sampler of the source distribution and an approximate vector field.
The key benefit of our approach is that it allows the user to flexibly choose
the sampling method depending on their specific problem. To illustrate this, we
systematically compare different sampling methods and discuss conditions for
asymptotically exact guidance. Moreover, our framework integrates well with
optimal flow matching models since the straight transport map generated by the
vector field is preserved. Experimental results on synthetic 2D benchmarks,
image datasets, and physics-informed generative tasks demonstrate the
effectiveness and flexibility of the proposed framework.

</details>


### [119] [Enhancing Contrastive Link Prediction With Edge Balancing Augmentation](https://arxiv.org/abs/2508.14808)
*Chen-Hao Chang,Hui-Ju Hung,Chia-Hsun Lu,Chih-Ya Shen*

Main category: cs.LG

TL;DR: 本文提出了CoEBA方法，通过边平衡增强和新的对比损失函数来改进图链接预测性能，在8个基准数据集上显著优于现有最先进模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比学习的链接预测方法存在两个主要缺陷：缺乏理论分析支持，以及对节点度考虑不足。

Method: 提出边平衡增强(EBA)方法调整节点度作为图增强，并设计新的对比损失函数，形成CoEBA框架。

Result: 在8个基准数据集上的实验结果表明，CoEBA显著优于其他最先进的链接预测模型。

Conclusion: 该研究为对比学习在图链接预测中的应用提供了首个正式理论分析，并通过EBA增强和新的对比损失有效提升了模型性能。

Abstract: Link prediction is one of the most fundamental tasks in graph mining, which
motivates the recent studies of leveraging contrastive learning to enhance the
performance. However, we observe two major weaknesses of these studies: i) the
lack of theoretical analysis for contrastive learning on link prediction, and
ii) inadequate consideration of node degrees in contrastive learning. To
address the above weaknesses, we provide the first formal theoretical analysis
for contrastive learning on link prediction, where our analysis results can
generalize to the autoencoder-based link prediction models with contrastive
learning. Motivated by our analysis results, we propose a new graph
augmentation approach, Edge Balancing Augmentation (EBA), which adjusts the
node degrees in the graph as the augmentation. We then propose a new approach,
named Contrastive Link Prediction with Edge Balancing Augmentation (CoEBA),
that integrates the proposed EBA and the proposed new contrastive losses to
improve the model performance. We conduct experiments on 8 benchmark datasets.
The results demonstrate that our proposed CoEBA significantly outperforms the
other state-of-the-art link prediction models.

</details>


### [120] [Successive Halving with Learning Curve Prediction via Latent Kronecker Gaussian Processes](https://arxiv.org/abs/2508.14818)
*Jihao Andreas Lin,Nicolas Mayoraz,Steffen Rendle,Dima Kuzmin,Emil Praun,Berivan Isik*

Main category: cs.LG

TL;DR: 通过学习曲线预测指导成功一半算法，尝试免让慢热起的超参被过早剪除，但需要完整的学习曲线数据进行训练


<details>
  <summary>Details</summary>
Motivation: 成功一半算法依赖中间性能评估做资源分配决策，容易过早剪除初期表现慢但最终性能优秀的超参

Method: 使用括尾科克高斯过程进行学习曲线预测，用预测结果指导成功一半算法的资源分配

Result: 预测方法能达到竞争性能，但与在标准方法上投入更多资源相比并非帕紴最优，因为需要完整的学习曲线进行训练

Conclusion: 学习曲线预测指导成功一半算法具有潜力，但需要利用现有的学习曲线数据来减轻对完整运行曲线的依赖

Abstract: Successive Halving is a popular algorithm for hyperparameter optimization
which allocates exponentially more resources to promising candidates. However,
the algorithm typically relies on intermediate performance values to make
resource allocation decisions, which can cause it to prematurely prune slow
starters that would eventually become the best candidate. We investigate
whether guiding Successive Halving with learning curve predictions based on
Latent Kronecker Gaussian Processes can overcome this limitation. In a
large-scale empirical study involving different neural network architectures
and a click prediction dataset, we compare this predictive approach to the
standard approach based on current performance values. Our experiments show
that, although the predictive approach achieves competitive performance, it is
not Pareto optimal compared to investing more resources into the standard
approach, because it requires fully observed learning curves as training data.
However, this downside could be mitigated by leveraging existing learning curve
data.

</details>


### [121] [On Defining Neural Averaging](https://arxiv.org/abs/2508.14832)
*Su Hyeong Lee,Richard Ngo*

Main category: cs.LG

TL;DR: 本文提出了Amortized Model Ensembling (AME)框架，用于在无训练数据的情况下通过模型权重平均来合成单一神经网络，该方法将模型差异视为伪梯度来指导权重更新，在分布外场景中表现优于传统模型集成方法。


<details>
  <summary>Details</summary>
Motivation: 研究如何在无训练数据访问的情况下，仅使用预训练模型的最终权重来合成单一神经网络，探索神经网络平均的正式定义和方法。

Method: 提出Amortized Model Ensembling (AME)框架，将模型差异作为伪梯度进行元优化，实现数据无关的模型权重聚合，能够恢复模型汤方法并支持更灵活的自适应集成策略。

Result: 实验表明AME产生的平均神经网络解决方案在性能上优于单个专家模型和模型汤基线，特别是在分布外设置中表现突出。

Conclusion: AME提供了一个原则性和可推广的无数据模型权重聚合概念，为神经网络平均提供了理论基础和实践方法。

Abstract: What does it even mean to average neural networks? We investigate the problem
of synthesizing a single neural network from a collection of pretrained models,
each trained on disjoint data shards, using only their final weights and no
access to training data. In forming a definition of neural averaging, we take
insight from model soup, which appears to aggregate multiple models into a
singular model while enhancing generalization performance. In this work, we
reinterpret model souping as a special case of a broader framework: Amortized
Model Ensembling (AME) for neural averaging, a data-free meta-optimization
approach that treats model differences as pseudogradients to guide neural
weight updates. We show that this perspective not only recovers model soup but
enables more expressive and adaptive ensembling strategies. Empirically, AME
produces averaged neural solutions that outperform both individual experts and
model soup baselines, especially in out-of-distribution settings. Our results
suggest a principled and generalizable notion of data-free model weight
aggregation and defines, in one sense, how to perform neural averaging.

</details>


### [122] [Multimodal Quantum Vision Transformer for Enzyme Commission Classification from Biochemical Representations](https://arxiv.org/abs/2508.14844)
*Murat Isik,Mandeep Kaur Saggi,Humaira Gowher,Sabre Kais*

Main category: cs.LG

TL;DR: 提出了一种新颖的多模态量子机器学习框架，通过整合蛋白质序列嵌入、量子电子描述符、分子图结构和2D分子图像表示四种生化模态，显著提升了酶功能分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 准确预测酶功能仍然是计算生物学的主要挑战之一，特别是对于结构注释有限或序列同源性较低的酶。现有方法主要依赖单一模态信息，难以全面捕捉酶功能的立体电子相互作用机制。

Method: 采用量子视觉变换器（QVT）作为主干网络，配备模态特定编码器和统一的交叉注意力融合模块。整合了四种互补生化模态：蛋白质序列嵌入、量子电子描述符、分子图结构和2D分子图像表示，以捕捉酶功能背后的关键立体电子相互作用。

Result: 实验结果表明，多模态QVT模型达到了85.1%的top-1准确率，显著优于仅使用序列的基线方法，并比其他QML模型取得了更好的性能结果。

Conclusion: 该多模态量子机器学习框架通过整合多种生化信息源，有效提升了酶功能预测的准确性，为理解酶功能的立体电子机制提供了新的计算工具。

Abstract: Accurately predicting enzyme functionality remains one of the major
challenges in computational biology, particularly for enzymes with limited
structural annotations or sequence homology. We present a novel multimodal
Quantum Machine Learning (QML) framework that enhances Enzyme Commission (EC)
classification by integrating four complementary biochemical modalities:
protein sequence embeddings, quantum-derived electronic descriptors, molecular
graph structures, and 2D molecular image representations. Quantum Vision
Transformer (QVT) backbone equipped with modality-specific encoders and a
unified cross-attention fusion module. By integrating graph features and
spatial patterns, our method captures key stereoelectronic interactions behind
enzyme function. Experimental results demonstrate that our multimodal QVT model
achieves a top-1 accuracy of 85.1%, outperforming sequence-only baselines by a
substantial margin and achieving better performance results compared to other
QML models.

</details>


### [123] [Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent](https://arxiv.org/abs/2508.14853)
*Sajib Biswas,Mao Nishino,Samuel Jacob Chacko,Xiuwen Liu*

Main category: cs.LG

TL;DR: 提出一种基于指数梯度下降和Bregman投影的本征优化方法，直接优化对抗后缀令牌的松弛one-hot编码，有效破解多种大语言模型。


<details>
  <summary>Details</summary>
Motivation: 尽管RLHF等对齐技术在典型提示上取得成功，但LLMs仍然容易受到通过精心设计的对抗触发器实现的越狱攻击。现有方法要么依赖离散令牌空间的低效搜索，要么直接优化连续嵌入，但后者无法应用于专有模型且投影回离散令牌会降低攻击效果。

Method: 使用指数梯度下降结合Bregman投影，直接优化对抗后缀令牌的松弛one-hot编码，确保每个令牌的优化one-hot编码始终保持在概率单纯形内。

Result: 在5个开源LLM和4个对抗行为数据集上的评估显示，该方法相比三种最先进基线方法获得更高的成功率和更快的收敛速度。还能生成跨多个提示有效的通用对抗后缀，并展示优化后缀到不同LLMs的可迁移性。

Conclusion: 提出的本征优化方法为LLM越狱攻击提供了高效有效的解决方案，在攻击成功率和收敛速度方面均优于现有方法，同时支持通用对抗后缀生成和跨模型迁移。

Abstract: As large language models (LLMs) are increasingly deployed in critical
applications, ensuring their robustness and safety alignment remains a major
challenge. Despite the overall success of alignment techniques such as
reinforcement learning from human feedback (RLHF) on typical prompts, LLMs
remain vulnerable to jailbreak attacks enabled by crafted adversarial triggers
appended to user prompts. Most existing jailbreak methods either rely on
inefficient searches over discrete token spaces or direct optimization of
continuous embeddings. While continuous embeddings can be given directly to
selected open-source models as input, doing so is not feasible for proprietary
models. On the other hand, projecting these embeddings back into valid discrete
tokens introduces additional complexity and often reduces attack effectiveness.
We propose an intrinsic optimization method which directly optimizes relaxed
one-hot encodings of the adversarial suffix tokens using exponentiated gradient
descent coupled with Bregman projection, ensuring that the optimized one-hot
encoding of each token always remains within the probability simplex. We
provide theoretical proof of convergence for our proposed method and implement
an efficient algorithm that effectively jailbreaks several widely used LLMs.
Our method achieves higher success rates and faster convergence compared to
three state-of-the-art baselines, evaluated on five open-source LLMs and four
adversarial behavior datasets curated for evaluating jailbreak methods. In
addition to individual prompt attacks, we also generate universal adversarial
suffixes effective across multiple prompts and demonstrate transferability of
optimized suffixes to different LLMs.

</details>


### [124] [Squeezed Diffusion Models](https://arxiv.org/abs/2508.14871)
*Jyotirmai Singh,Samar Khanna,James Burgess*

Main category: cs.LG

TL;DR: 该论文提出了挤压扩散模型(SDM)，通过数据感知的各向异性噪声缩放来改进扩散模型性能，在多个数据集上实现了FID指标15%的提升。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型使用各向同性高斯噪声，忽略了数据的结构信息。受量子压缩态根据海森堡不确定性原理重新分配不确定性的启发，作者希望通过对主成分方向进行各向异性噪声缩放来更好地学习重要数据特征。

Method: 提出了两种配置：(i)海森堡扩散模型-在主轴上缩放噪声并在正交方向进行逆缩放补偿；(ii)标准SDM变体-仅缩放主轴。在CIFAR-10/100和CelebA-64数据集上进行实验。

Result: 反直觉地发现，适度的反压缩（即在主轴上增加方差）能持续改善FID达15%，并将精确率-召回率边界推向更高召回率。

Conclusion: 简单的数据感知噪声整形可以在不改变架构的情况下带来稳健的生成性能提升，证明了数据相关噪声缩放的有效性。

Abstract: Diffusion models typically inject isotropic Gaussian noise, disregarding
structure in the data. Motivated by the way quantum squeezed states
redistribute uncertainty according to the Heisenberg uncertainty principle, we
introduce Squeezed Diffusion Models (SDM), which scale noise anisotropically
along the principal component of the training distribution. As squeezing
enhances the signal-to-noise ratio in physics, we hypothesize that scaling
noise in a data-dependent manner can better assist diffusion models in learning
important data features. We study two configurations: (i) a Heisenberg
diffusion model that compensates the scaling on the principal axis with inverse
scaling on orthogonal directions and (ii) a standard SDM variant that scales
only the principal axis. Counterintuitively, on CIFAR-10/100 and CelebA-64,
mild antisqueezing - i.e. increasing variance on the principal axis -
consistently improves FID by up to 15% and shifts the precision-recall frontier
toward higher recall. Our results demonstrate that simple, data-aware noise
shaping can deliver robust generative gains without architectural changes.

</details>


### [125] [Compute-Optimal Scaling for Value-Based Deep RL](https://arxiv.org/abs/2508.14881)
*Preston Fu,Oleh Rybkin,Zhiyuan Zhou,Michal Nauman,Pieter Abbeel,Sergey Levine,Aviral Kumar*

Main category: cs.LG

TL;DR: 这篇论文研究了深度强化学习中的计算最优化扩缩问题，发现大批处理大尺度模型时更有效，并提出了TD过拟合现象和相应的计算资源分配指南


<details>
  <summary>Details</summary>
Motivation: 随着模型越来越大、训练成本越来越高，需要在固定计算预算下完成最大化性能的计算最优化扩缩，而强化学习在这方面的研究较语言模型少

Method: 研究在线值基深度强化学习中的计算分配问题，分析模型容量和更新比率(UTD)两个主要轴尾的相互作用

Result: 发现了TD-过拟合现象：小模型中增加批处大小会快速容影响Q函数准确性，但大模型中没有这种效应，从而能够有效利用大批处进行扩缩

Conclusion: 提供了选择批处大小和UTD以优化计算使用的指南，为深度强化学习的计算最优化扩缩提供了基础研究

Abstract: As models grow larger and training them becomes expensive, it becomes
increasingly important to scale training recipes not just to larger models and
more data, but to do so in a compute-optimal manner that extracts maximal
performance per unit of compute. While such scaling has been well studied for
language modeling, reinforcement learning (RL) has received less attention in
this regard. In this paper, we investigate compute scaling for online,
value-based deep RL. These methods present two primary axes for compute
allocation: model capacity and the update-to-data (UTD) ratio. Given a fixed
compute budget, we ask: how should resources be partitioned across these axes
to maximize sample efficiency? Our analysis reveals a nuanced interplay between
model size, batch size, and UTD. In particular, we identify a phenomenon we
call TD-overfitting: increasing the batch quickly harms Q-function accuracy for
small models, but this effect is absent in large models, enabling effective use
of large batch size at scale. We provide a mental model for understanding this
phenomenon and build guidelines for choosing batch size and UTD to optimize
compute usage. Our findings provide a grounded starting point for
compute-optimal scaling in deep RL, mirroring studies in supervised learning
but adapted to TD learning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [126] [Large Language Models are Highly Aligned with Human Ratings of Emotional Stimuli](https://arxiv.org/abs/2508.14214)
*Mattson Ogg,Chace Ashcraft,Ritwik Bose,Raphael Norman-Tenazas,Michael Wolmetz*

Main category: cs.AI

TL;DR: GPT-4o在情感评价任务中与人类表现高度相似，特别是在快乐评分方面，但在唤醒度评分上存在差异。LLM在五分类情感框架中比二维情感维度模型表现更好，但评分比人类更加同质化。


<details>
  <summary>Details</summary>
Motivation: 了解大型语言模型如何评估情感刺激对于其在日常生活中的应用至关重要，需要研究LLM与人类在情感评价方面的对齐程度。

Method: 使用人类先前评定的情感词汇和图像数据集，让多个流行LLM进行相同的评分任务，比较LLM与人类评分的一致性。

Result: GPT-4o在大多数评分维度上与人类参与者高度相似（r ≥ 0.9），快乐评分对齐度最高，唤醒度评分对齐度较低。LLM在五分类情感框架中表现更好，但评分比人类更加同质化。

Conclusion: LLM在情感刺激解释方面与人类存在相似性和差异，这些发现有助于理解生物智能与人工智能在关键行为领域的异同，为LLM在情感相关角色中的应用提供参考。

Abstract: Emotions exert an immense influence over human behavior and cognition in both
commonplace and high-stress tasks. Discussions of whether or how to integrate
large language models (LLMs) into everyday life (e.g., acting as proxies for,
or interacting with, human agents), should be informed by an understanding of
how these tools evaluate emotionally loaded stimuli or situations. A model's
alignment with human behavior in these cases can inform the effectiveness of
LLMs for certain roles or interactions. To help build this understanding, we
elicited ratings from multiple popular LLMs for datasets of words and images
that were previously rated for their emotional content by humans. We found that
when performing the same rating tasks, GPT-4o responded very similarly to human
participants across modalities, stimuli and most rating scales (r = 0.9 or
higher in many cases). However, arousal ratings were less well aligned between
human and LLM raters, while happiness ratings were most highly aligned. Overall
LLMs aligned better within a five-category (happiness, anger, sadness, fear,
disgust) emotion framework than within a two-dimensional (arousal and valence)
organization. Finally, LLM ratings were substantially more homogenous than
human ratings. Together these results begin to describe how LLM agents
interpret emotional stimuli and highlight similarities and differences among
biological and artificial intelligence in key behavioral domains.

</details>


### [127] [Explaining Hitori Puzzles: Neurosymbolic Proof Staging for Sequential Decisions](https://arxiv.org/abs/2508.14294)
*Maria Leonor Pacheco,Fabio Somenzi,Dananjay Srinivas,Ashutosh Trivedi*

Main category: cs.AI

TL;DR: 提出一种结合决策程序和大型语言模型的神经符号方法，用于解释复杂决策序列，并以Hitori数独游戏为例验证其有效性


<details>
  <summary>Details</summary>
Motivation: 需要为复杂决策序列提供可解释性，结合符号推理和神经网络的各自优势，Hitori游戏同时包含适合符号解释的局部约束和适合视觉解释的连通性约束

Method: 开发神经符号方法，结合SAT求解器和LLM，SAT求解器处理局部约束的解析证明，LLM处理连通性约束的视觉解释

Result: 实现了辅助人类解决Hitori谜题的工具，实验证据表明该方法有效

Conclusion: 神经符号方法能够有效结合符号推理和神经网络的优势，为复杂决策提供全面的解释，Hitori游戏是验证这种灵活组合的理想测试平台

Abstract: We propose a neurosymbolic approach to the explanation of complex sequences
of decisions that combines the strengths of decision procedures and Large
Language Models (LLMs). We demonstrate this approach by producing explanations
for the solutions of Hitori puzzles. The rules of Hitori include local
constraints that are effectively explained by short resolution proofs. However,
they also include a connectivity constraint that is more suitable for visual
explanations. Hence, Hitori provides an excellent testing ground for a flexible
combination of SAT solvers and LLMs. We have implemented a tool that assists
humans in solving Hitori puzzles, and we present experimental evidence of its
effectiveness.

</details>


### [128] [Automated Optimization Modeling through Expert-Guided Large Language Model Reasoning](https://arxiv.org/abs/2508.14410)
*Beinuo Yang,Qishen Zhou,Junyi Li,Xingchen Su,Simon Hu*

Main category: cs.AI

TL;DR: 本文提出了ORThought框架，通过思维链推理利用专家级优化建模原则来自动化优化建模过程，解决了现有LLM方法在基准标注错误率高、评估范围窄和计算效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 优化建模过程耗时且容易出错，严重依赖领域专家。虽然大语言模型在解决这些问题方面显示出潜力，但现有方法存在基准标注错误率高（达42%）、评估范围仅限于最优值、以及计算效率低等关键局限性。

Method: 首先通过系统错误校正和更全面的标注增强现有数据集；引入LogiOR物流领域新基准；提出ORThought框架，利用思维链推理应用专家级优化建模原则来自动化优化建模过程。

Result: ORThought在广泛实证评估中优于现有方法（包括多智能体框架），在复杂优化问题上表现出特别显著的优势。

Conclusion: 提供了方法的系统分析，识别了关键成功因素和失败模式，为基于LLM的优化建模未来研究提供了宝贵见解。

Abstract: Optimization Modeling (OM) is essential for solving complex decision-making
problems. However, the process remains time-consuming and error-prone, heavily
relying on domain experts. While Large Language Models (LLMs) show promise in
addressing these challenges through their natural language understanding and
reasoning capabilities, current approaches face three critical limitations:
high benchmark labeling error rates reaching up to 42\%, narrow evaluation
scope that only considers optimal values, and computational inefficiency due to
heavy reliance on multi-agent systems or model fine-tuning. In this work, we
first enhance existing datasets through systematic error correction and more
comprehensive annotation. Additionally, we introduce LogiOR, a new optimization
modeling benchmark from the logistics domain, containing more complex problems
with standardized annotations. Furthermore, we present ORThought, a novel
framework that leverages expert-level optimization modeling principles through
chain-of-thought reasoning to automate the OM process. Through extensive
empirical evaluation, we demonstrate that ORThought outperforms existing
approaches, including multi-agent frameworks, with particularly significant
advantages on complex optimization problems. Finally, we provide a systematic
analysis of our method, identifying critical success factors and failure modes,
providing valuable insights for future research on LLM-based optimization
modeling.

</details>


### [129] [The Agent Behavior: Model, Governance and Challenges in the AI Digital Age](https://arxiv.org/abs/2508.14415)
*Qiang Zhang,Pei Yan,Yijia Xu,Chuanpo Fu,Yong Fang,Yang Liu*

Main category: cs.AI

TL;DR: 本文提出了网络行为生命周期模型和A4A范式，通过6阶段分析和5维度差异来区分人类与AI代理行为，解决信任、责任等挑战。


<details>
  <summary>Details</summary>
Motivation: AI代理在网络环境中越来越像人类行为，导致信任、责任、伦理和安全等方面的监管困难，需要系统性的区分框架。

Method: 提出网络行为生命周期模型（6阶段分析）和人类-代理行为差异模型（5维度：决策机制、执行效率、意图-行为一致性、行为惯性、非理性模式），并通过红蓝队实战验证。

Result: 模型有效区分了人类与代理行为差异，在渗透测试和防御等实际场景中得到验证。

Conclusion: 为安全可信的人机协作提供了理论基础和技术路线图，未来研究方向包括动态认知治理架构、行为差异量化和元治理协议栈。

Abstract: Advancements in AI have led to agents in networked environments increasingly
mirroring human behavior, thereby blurring the boundary between artificial and
human actors in specific contexts. This shift brings about significant
challenges in trust, responsibility, ethics, security and etc. The difficulty
in supervising of agent behaviors may lead to issues such as data contamination
and unclear accountability. To address these challenges, this paper proposes
the "Network Behavior Lifecycle" model, which divides network behavior into 6
stages and systematically analyzes the behavioral differences between humans
and agents at each stage. Based on these insights, the paper further introduces
the "Agent for Agent (A4A)" paradigm and the "Human-Agent Behavioral Disparity
(HABD)" model, which examine the fundamental distinctions between human and
agent behaviors across 5 dimensions: decision mechanism, execution efficiency,
intention-behavior consistency, behavioral inertia, and irrational patterns.
The effectiveness of the model is verified through real-world cases such as red
team penetration and blue team defense. Finally, the paper discusses future
research directions in dynamic cognitive governance architecture, behavioral
disparity quantification, and meta-governance protocol stacks, aiming to
provide a theoretical foundation and technical roadmap for secure and
trustworthy human-agent collaboration.

</details>


### [130] [Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs](https://arxiv.org/abs/2508.14564)
*Luca Annese,Sabrina Patania,Silvia Serino,Tom Foulsham,Silvia Rossi,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.AI

TL;DR: 该研究探讨了使用规划器生成的结构化示例是否能提升LLM智能体的视角采择能力，发现虽然能略微减少澄清请求，但结构化示例本身不足以实现稳健的视角采择


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在需要主动感知、协作推理和视角采择的任务中仍面临挑战，研究旨在通过结构化示例改进LLM智能体的表现

Method: 提出结构化解决方案处理流程，生成三种类型的示例（G型、E型、L型），并将其转换为"思考-行动"示例，在ReAct框架中测试LLM智能体

Result: L型示例略微减少了澄清请求和总体行动步骤，但未能带来一致性的改进。智能体在基本注意力过滤任务中成功，但在需要心理化遮挡空间或权衡认知行动成本的场景中表现不佳

Conclusion: 仅靠结构化示例不足以实现稳健的视角采择，需要明确的信念追踪、成本建模和更丰富的环境来实现LLM智能体的社会性协作

Abstract: Recent advances in large language models (LLMs) and reasoning frameworks have
opened new possibilities for improving the perspective -taking capabilities of
autonomous agents. However, tasks that involve active perception, collaborative
reasoning, and perspective taking (understanding what another agent can see or
knows) pose persistent challenges for current LLM-based systems. This study
investigates the potential of structured examples derived from transformed
solution graphs generated by the Fast Downward planner to improve the
performance of LLM-based agents within a ReAct framework. We propose a
structured solution-processing pipeline that generates three distinct
categories of examples: optimal goal paths (G-type), informative node paths
(E-type), and step-by-step optimal decision sequences contrasting alternative
actions (L-type). These solutions are further converted into ``thought-action''
examples by prompting an LLM to explicitly articulate the reasoning behind each
decision. While L-type examples slightly reduce clarification requests and
overall action steps, they do not yield consistent improvements. Agents are
successful in tasks requiring basic attentional filtering but struggle in
scenarios that required mentalising about occluded spaces or weighing the costs
of epistemic actions. These findings suggest that structured examples alone are
insufficient for robust perspective-taking, underscoring the need for explicit
belief tracking, cost modelling, and richer environments to enable socially
grounded collaboration in LLM-based agents.

</details>


### [131] [LeanGeo: Formalizing Competitional Geometry problems in Lean](https://arxiv.org/abs/2508.14644)
*Chendong Song,Zihan Wang,Frederick Pu,Haiming Wang,Xiaohan Lin,Junqi Liu,Jia Li,Zhengying Liu*

Main category: cs.AI

TL;DR: LeanGeo是一个基于Lean 4定理证明器的统一几何形式化系统，包含高级几何定理库和LeanGeo-Bench基准测试，用于解决竞赛级几何问题并验证AI推理能力


<details>
  <summary>Details</summary>
Motivation: 现有几何求解系统难以统一表达问题且难以与其他数学领域集成，几何证明依赖直观图表导致验证困难

Method: 在Lean 4定理证明器中构建统一的形式化系统，包含综合的几何定理库，并与Mathlib集成，创建包含IMO等高级问题的形式化基准测试

Result: 开发了LeanGeo系统和LeanGeo-Bench基准，评估了最先进大语言模型在该基准上的表现，展示了自动几何推理的当前能力和局限

Conclusion: LeanGeo为几何问题提供了统一的严格验证框架，揭示了自动几何推理需要进一步发展的需求，相关资源已开源

Abstract: Geometry problems are a crucial testbed for AI reasoning capabilities. Most
existing geometry solving systems cannot express problems within a unified
framework, thus are difficult to integrate with other mathematical fields.
Besides, since most geometric proofs rely on intuitive diagrams, verifying
geometry problems is particularly challenging. To address these gaps, we
introduce LeanGeo, a unified formal system for formalizing and solving
competition-level geometry problems within the Lean 4 theorem prover. LeanGeo
features a comprehensive library of high-level geometric theorems with Lean's
foundational logic, enabling rigorous proof verification and seamless
integration with Mathlib. We also present LeanGeo-Bench, a formal geometry
benchmark in LeanGeo, comprising problems from the International Mathematical
Olympiad (IMO) and other advanced sources. Our evaluation demonstrates the
capabilities and limitations of state-of-the-art Large Language Models on this
benchmark, highlighting the need for further advancements in automated
geometric reasoning. We open source the theorem library and the benchmark of
LeanGeo at https://github.com/project-numina/LeanGeo/tree/master.

</details>


### [132] [Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent Framework with LLM and Knowledge Graph Integration](https://arxiv.org/abs/2508.14654)
*Peilin Ji,Xiao Xue,Simeng Wang,Wenhao Yan*

Main category: cs.AI

TL;DR: 提出了H-J分层多智能体框架，通过知识引导提示、熵约束生成和反馈驱动优化，解决极端降雨下城市应急调度中的多目标权衡、动态环境适应和LLM策略稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 极端降雨事件频发导致城市内涝，造成交通拥堵和服务中断，现有方法在感知、全局优化和多智能体协调方面存在不足，无法有效处理竞争目标间的权衡、动态环境变化和LLM策略的不稳定性。

Method: H-J分层多智能体框架，整合知识引导提示、熵约束生成和反馈驱动优化，建立从多源感知到策略执行和持续优化的闭环管道。

Result: 在真实城市拓扑和降雨数据上的实验表明，H-J在交通流畅度、任务成功率和系统鲁棒性方面优于基于规则和强化学习的基线方法。

Conclusion: 基于LLM的不确定性感知和知识约束方法有望增强城市防洪响应的韧性。

Abstract: In recent years, the increasing frequency of extreme urban rainfall events
has posed significant challenges to emergency scheduling systems. Urban
flooding often leads to severe traffic congestion and service disruptions,
threatening public safety and mobility. However, effective decision making
remains hindered by three key challenges: (1) managing trade-offs among
competing goals (e.g., traffic flow, task completion, and risk mitigation)
requires dynamic, context-aware strategies; (2) rapidly evolving environmental
conditions render static rules inadequate; and (3) LLM-generated strategies
frequently suffer from semantic instability and execution inconsistency.
Existing methods fail to align perception, global optimization, and multi-agent
coordination within a unified framework. To tackle these challenges, we
introduce H-J, a hierarchical multi-agent framework that integrates
knowledge-guided prompting, entropy-constrained generation, and feedback-driven
optimization. The framework establishes a closed-loop pipeline spanning from
multi-source perception to strategic execution and continuous refinement. We
evaluate H-J on real-world urban topology and rainfall data under three
representative conditions: extreme rainfall, intermittent bursts, and daily
light rain. Experiments show that H-J outperforms rule-based and
reinforcement-learning baselines in traffic smoothness, task success rate, and
system robustness. These findings highlight the promise of uncertainty-aware,
knowledge-constrained LLM-based approaches for enhancing resilience in urban
flood response.

</details>


### [133] [MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers](https://arxiv.org/abs/2508.14704)
*Ziyang Luo,Zhiqi Shen,Wenzhuo Yang,Zirui Zhao,Prathyusha Jwalapuram,Amrita Saha,Doyen Sahoo,Silvio Savarese,Caiming Xiong,Junnan Li*

Main category: cs.AI

TL;DR: MCP-Universe是首个专门为评估LLM与真实MCP服务器交互的综合性基准测试，涵盖6个核心领域11个服务器，发现即使是SOTA模型在真实任务中仍存在显著性能限制。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试过于简单，无法捕捉真实应用挑战，如长时程推理和大规模陌生工具空间，需要更全面的评估标准。

Method: 构建包含6个领域11个MCP服务器的基准测试，采用执行式评估器（格式评估器、静态评估器和动态评估器）进行严格评估。

Result: 顶级模型如GPT-5(43.72%)、Grok-4(33.33%)和Claude-4.0-Sonnet(29.44%)表现显著受限，面临长上下文和未知工具挑战。

Conclusion: MCP-Universe填补了关键空白，提供了可扩展的开源评估框架，促进MCP生态系统的创新发展。

Abstract: The Model Context Protocol has emerged as a transformative standard for
connecting large language models to external data sources and tools, rapidly
gaining adoption across major AI providers and development platforms. However,
existing benchmarks are overly simplistic and fail to capture real application
challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To
address this critical gap, we introduce MCP-Universe, the first comprehensive
benchmark specifically designed to evaluate LLMs in realistic and hard tasks
through interaction with real-world MCP servers. Our benchmark encompasses 6
core domains spanning 11 different MCP servers: Location Navigation, Repository
Management, Financial Analysis, 3D Design, Browser Automation, and Web
Searching. To ensure rigorous evaluation, we implement execution-based
evaluators, including format evaluators for agent format compliance, static
evaluators for time-invariant content matching, and dynamic evaluators that
automatically retrieve real-time ground truth for temporally sensitive tasks.
Through extensive evaluation of leading LLMs, we find that even SOTA models
such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit
significant performance limitations. In addition, our benchmark poses a
significant long-context challenge for LLM agents, as the number of input
tokens increases rapidly with the number of interaction steps. Moreover, it
introduces an unknown-tools challenge, as LLM agents often lack familiarity
with the precise usage of the MCP servers. Notably, enterprise-level agents
like Cursor cannot achieve better performance than standard ReAct frameworks.
Beyond evaluation, we open-source our extensible evaluation framework with UI
support, enabling researchers and practitioners to seamlessly integrate new
agents and MCP servers while fostering innovation in the rapidly evolving MCP
ecosystem.

</details>


### [134] [Data-Driven Probabilistic Evaluation of Logic Properties with PAC-Confidence on Mealy Machines](https://arxiv.org/abs/2508.14710)
*Swantje Plambeck,Ali Salamati,Eyke Huellermeier,Goerschwin Fey*

Main category: cs.AI

TL;DR: 通过活跃学习和PAC学习框架，从CPS系统收集数据来预测有限时间界限内的安全概率，并提供统计信心保障


<details>
  <summary>Details</summary>
Motivation: CPS系统需要强大的模型来进行验证、诊断和调试，但手动提取模型困难，需要数据驱动方法来解决这些任务

Method: 采用活跃学习范式，基于Mealy机离散抽象，通过PAC学习框架来预测系统在有限时间界限内的安全概率，并提供统计信心保障

Result: 在自动车道保持系统的案例研究中验证了方法的有效性

Conclusion: 该方法为CPS系统提供了一种可靠的数据驱动方式来评估安全概率，并在离散逻辑和概率达到分析之间建立了联系

Abstract: Cyber-Physical Systems (CPS) are complex systems that require powerful models
for tasks like verification, diagnosis, or debugging. Often, suitable models
are not available and manual extraction is difficult. Data-driven approaches
then provide a solution to, e.g., diagnosis tasks and verification problems
based on data collected from the system. In this paper, we consider CPS with a
discrete abstraction in the form of a Mealy machine. We propose a data-driven
approach to determine the safety probability of the system on a finite horizon
of n time steps. The approach is based on the Probably Approximately Correct
(PAC) learning paradigm. Thus, we elaborate a connection between discrete logic
and probabilistic reachability analysis of systems, especially providing an
additional confidence on the determined probability. The learning process
follows an active learning paradigm, where new learning data is sampled in a
guided way after an initial learning set is collected. We validate the approach
with a case study on an automated lane-keeping system.

</details>


### [135] [Privileged Self-Access Matters for Introspection in AI](https://arxiv.org/abs/2508.14802)
*Siyuan Song,Harvey Lederman,Jennifer Hu,Kyle Mahowald*

Main category: cs.AI

TL;DR: 本文探讨AI模型是否能够内省，提出了比现有"轻量级"定义更"厚重"的内省定义，并通过LLM温度参数推理实验证明模型可能表现出轻量级内省但缺乏真正的内省能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型的发展，判断其是否具备内省能力成为一个重要的实践问题，但目前缺乏统一的内省定义标准。

Method: 提出新的内省定义：任何能通过比第三方可用方法更可靠且计算成本相当或更低的过程获取内部状态信息的过程。通过LLM推理其内部温度参数的实验进行验证。

Result: 实验显示LLM可能表现出轻量级内省的表象，但按照作者提出的更严格定义，它们未能实现有意义的内省。

Conclusion: 需要更严格的内省定义来准确评估AI模型的内省能力，当前LLM虽然能表现出某些内省特征，但缺乏真正的内省机制。

Abstract: Whether AI models can introspect is an increasingly important practical
question. But there is no consensus on how introspection is to be defined.
Beginning from a recently proposed ''lightweight'' definition, we argue instead
for a thicker one. According to our proposal, introspection in AI is any
process which yields information about internal states through a process more
reliable than one with equal or lower computational cost available to a third
party. Using experiments where LLMs reason about their internal temperature
parameters, we show they can appear to have lightweight introspection while
failing to meaningfully introspect per our proposed definition.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [136] [Accelerating K-Core Computation in Temporal Graphs](https://arxiv.org/abs/2508.14147)
*Zhuo Ma,Dong Wen,Hanchen Wang,Wentao Li,Wenjie Zhang,Xuemin Lin*

Main category: cs.DB

TL;DR: 基于核时间概念的新算法，优化了时序图中枚所有时序k核的效率和可扩展性


<details>
  <summary>Details</summary>
Motivation: 现有时序k核枚举算法效率低下，可扩展性差，需要更高效的解决方案

Method: 提出了一种基于核时间概念的新算法，证明算法运行时间仅为结果集大小的线性函数

Result: 计算核时间的成本较低，整体运行时间与结果集大小密切相关，达到了该场景下的最优性

Conclusion: 新算法通过核时间概念显著提高了时序k核枚举的效率和可扩展性，为大规模时序图分析提供了有效解决方案

Abstract: We address the problem of enumerating all temporal k-cores given a query time
range and a temporal graph, which suffers from poor efficiency and scalability
in the state-of-the-art solution. Motivated by an existing concept called core
times, we propose a novel algorithm to compute all temporal $k$-cores based on
core times and prove that the algorithmic running time is bounded by the size
of all resulting temporal k-cores, which is optimal in this scenario.
Meanwhile, we show that the cost of computing core times is much lower, which
demonstrates the close relevance between our overall running time and the
result size.

</details>


### [137] [Efficient Size Constraint Community Search over Heterogeneous Information Networks](https://arxiv.org/abs/2508.14356)
*Xinjian Zhang,Lu Chen,Chengfei Liu,Rui Zhou,Bo Ning*

Main category: cs.DB

TL;DR: 本文针对异构信息网络(HIN)中带规模约束的社区搜索问题，提出了(k,P)-truss模型来衡量社区内聚性，并开发了高效的精确算法和启发式算法来解决这个NP难问题。


<details>
  <summary>Details</summary>
Motivation: 现有HIN社区搜索工作大多忽视了实际应用中常见的规模约束问题，需要开发能够识别包含查询节点的指定规模最内聚社区的解决方案。

Method: 提出改进的(k,P)-truss模型衡量社区内聚性；开发基于分支定界(B&B)的框架；设计边界剪枝、分支策略、全排序和候选集缩减等优化技术；提出启发式算法获取高质量初始解；开发两种分别基于边和节点枚举的精确算法。

Result: 在真实数据集上的大量实验证明了所提方法的有效性和高效性，能够成功识别指定规模的最内聚社区。

Conclusion: 本文首次将规模约束社区搜索问题引入HIN领域，提出的(k,P)-truss模型和高效算法框架为解决这一NP难问题提供了有效方案，实验验证了方法的优越性能。

Abstract: The goal of community search in heterogeneous information networks (HINs) is
to identify a set of closely related target nodes that includes a query target
node. In practice, a size constraint is often imposed due to limited resources,
which has been overlooked by most existing HIN community search works. In this
paper, we introduce the size-bounded community search problem to HIN data.
Specifically, we propose a refined (k, P)-truss model to measure community
cohesiveness, aiming to identify the most cohesive community of size s that
contains the query node. We prove that this problem is NP-hard. To solve this
problem, we develop a novel B\&B framework that efficiently generates target
node sets of size s. We then tailor novel bounding, branching, total ordering,
and candidate reduction optimisations, which enable the framework to
efficiently lead to an optimum result. We also design a heuristic algorithm
leveraging structural properties of HINs to efficiently obtain a high-quality
initial solution, which serves as a global lower bound to further enhance the
above optimisations. Building upon these, we propose two exact algorithms that
enumerate combinations of edges and nodes, respectively. Extensive experiments
on real-world datasets demonstrate the effectiveness and efficiency of the
proposed methods.

</details>


### [138] [A DBMS-independent approach for capturing provenance polynomials through query rewriting](https://arxiv.org/abs/2508.14608)
*Paulo Pintor,Rogério Costa,José Moreira*

Main category: cs.DB

TL;DR: 提出了一种基于查询重写的SQL查询溯源多项式标注方法，支持SPJUA操作和嵌套查询，实现了完整的基于半环理论的溯源多项式实现，并在性能和可扩展性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前溯源多项式实现存在局限性，无法很好地处理聚合和嵌套查询，且与特定数据库管理系统紧密耦合，影响了互操作性和广泛应用。

Method: 采用查询重写方法为SQL查询添加溯源多项式标注，通过递归传播溯源注释，支持SPJUA操作和嵌套查询，实现了基于半环理论的完整实现。

Result: 实验评估表明，该方法实现了文献中提出的理论形式化，在性能和可扩展性方面优于现有方法，表现出更好的性能表现。

Conclusion: 该方法提供了一个全面的溯源多项式实现方案，具有DBMS无关性，支持复杂的查询操作，在性能和实用性方面都有显著提升。

Abstract: In today's data-driven ecosystems, ensuring data integrity, traceability and
accountability is important. Provenance polynomials constitute a powerful
formalism for tracing the origin and the derivations made to produce database
query results. Despite their theoretical expressiveness, current
implementations have limitations in handling aggregations and nested queries,
and some of them and tightly coupled to a single Database Management System
(DBMS), hindering interoperability and broader applicability.
  This paper presents a query rewriting-based approach for annotating
Structured Query Language (SQL) queries with provenance polynomials. The
proposed methods are DBMS-independent and support
Select-Projection-Join-Union-Aggregation (SPJUA) operations and nested queries,
through recursive propagation of provenance annotations. This constitutes the
first full implementation of semiring-based theory for provenance polynomials
extended with semimodule structures. It also presents an experimental
evaluation to assess the validity of the proposed methods and compare the
performance against state-of-the-art systems using benchmark data and queries.
The results indicate that our solution delivers a comprehensive implementation
of the theoretical formalisms proposed in the literature, and demonstrates
improved performance and scalability, outperforming existing methods.

</details>
