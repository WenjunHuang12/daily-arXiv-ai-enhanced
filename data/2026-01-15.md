<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 13]
- [cs.IT](#cs.IT) [Total: 29]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [On the Fair Allocation to Asymmetric Agents with Binary XOS Valuations](https://arxiv.org/abs/2601.09299)
*Ziheng Chen,Bo Li,Zihan Luo,Jialin Zhang*

Main category: cs.GT

TL;DR: 本文研究了带有二进制边际值的XOS估值下的不公平分配问题，将APS近似比提升至1/2（匹配上界），并将WMMS的1/n近似比从加性估值扩展到XOS估值。


<details>
  <summary>Details</summary>
Motivation: 在不可分割物品分配中，XOS估值比加性估值更一般化。先前研究在二进制边际值下获得了0.1222-APS近似比，但存在0.5的上界。本文旨在缩小这一差距，同时在不对称设置下改进近似比，并将WMMS的结果扩展到更一般的估值类。

Method: 针对不对称设置下的二进制XOS估值，设计了多项式时间算法来寻找1/2-APS分配。对于WMMS，将Farhadi等人的1/n近似结果从加性估值扩展到XOS估值，并证明即使在二进制边际值下也无法改进这一近似比。

Result: 1. 在不对称设置下，将二进制XOS估值的APS近似比从1/6提升至1/2，匹配已知上界；2. 提供了多项式时间算法；3. 将WMMS的1/n近似比从加性估值扩展到XOS估值，并证明该比率为紧界。

Conclusion: 本文在XOS估值下的公平分配问题上取得了重要进展：对于APS，达到了最优近似比1/2；对于WMMS，证明了1/n近似比在XOS估值下也是紧的，这与二进制加性估值下存在精确WMMS分配形成鲜明对比。

Abstract: We study the problem of allocating $m$ indivisible goods among $n$ agents, where each agent's valuation is fractionally subadditive (XOS). With respect to AnyPrice Share (APS) fairness, Kulkarni et al. (2024) showed that, when agents have binary marginal values, a $0.1222$-APS allocation can be found in polynomial time, and there exists an instance where no allocation is better than $0.5$-approximate APS. Very recently, Feige and Grinberg (2025) extended the problem to the asymmetric case, where agents may have different entitlements, and improved the approximation ratio to $1/6$ for general XOS valuations. In this work, we focus on the asymmetric setting with binary XOS valuations, and further improve the approximation ratio to $1/2$, which matches the known upper bound. We also present a polynomial-time algorithm to compute such an allocation. Beyond APS fairness, we also study the weighted maximin share (WMMS) fairness. Farhadi et al. (2019) showed that, a $1/n$-WMMS allocation always exists for agents with general additive valuations, and that this approximation ratio is tight. We extend this result to general XOS valuations, where a $1/n$-WMMS allocation still exists, and this approximation ratio cannot be improved even when marginal values are binary. This shows a sharp contrast to binary additive valuations, where an exact WMMS allocation exists and can be found in polynomial time.

</details>


### [2] [Measuring the benefits of lying in MARA under egalitarian social welfare](https://arxiv.org/abs/2601.09354)
*Jonathan Carrero,Ismael Rodriguez,Fernando Rubio*

Main category: cs.GT

TL;DR: 使用遗传算法分析在平等主义社会福利分配中，代理人撒谎对其收益的影响


<details>
  <summary>Details</summary>
Motivation: 在平等主义社会福利分配中，代理人为了获得更有价值的资源，有动机谎报自己的真实偏好。本文旨在分析这种撒谎行为在不同情境下的实际收益。

Method: 使用遗传算法来评估在不同情况下撒谎的收益，通过算法模拟代理人撒谎行为对资源分配结果的影响。

Result: 通过遗传算法的实验分析，量化了代理人在不同情境下通过撒谎可能获得的收益，揭示了撒谎策略的有效性。

Conclusion: 在平等主义社会福利分配机制中，代理人确实可以通过策略性谎报偏好获得显著收益，遗传算法为分析这种策略行为提供了有效的工具。

Abstract: When some resources are to be distributed among a set of agents following egalitarian social welfare, the goal is to maximize the utility of the agent whose utility turns out to be minimal. In this context, agents can have an incentive to lie about their actual preferences, so that more valuable resources are assigned to them. In this paper we analyze this situation, and we present a practical study where genetic algorithms are used to assess the benefits of lying under different situations.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [3] [Navigating Ideation Space: Decomposed Conceptual Representations for Positioning Scientific Ideas](https://arxiv.org/abs/2601.08901)
*Yuexi Shen,Minqian Liu,Dawei Zhou,Lifu Huang*

Main category: cs.IR

TL;DR: 提出Ideation Space框架，将科学知识分解为问题、方法、发现三个维度，实现细粒度文献检索和概念新颖性评估


<details>
  <summary>Details</summary>
Motivation: 科学发现是累积过程，需要将新思想置于现有知识体系中。当前挑战：如何从快速增长文献中识别概念相关的前期工作，并评估新思想与现有研究的区别。现有嵌入方法将不同概念方面混为单一表示，无法支持细粒度检索；LLM评估器存在奉承偏见，无法提供区分性新颖性评估

Method: 引入Ideation Space结构化表示，将科学知识分解为研究问题、方法论、核心发现三个维度，通过对比学习训练。基于此提出分层子空间检索框架进行高效目标文献检索，以及分解新颖性评估算法识别思想的哪些方面是新颖的

Result: 实验显示显著改进：Recall@30达到0.329（比基线提升16.7%），思想转换检索Hit Rate@30达到0.643，新颖性评估与专家判断相关性达到0.37

Conclusion: 该工作为加速和评估科学发现的未来研究提供了有前景的范式

Abstract: Scientific discovery is a cumulative process and requires new ideas to be situated within an ever-expanding landscape of existing knowledge. An emerging and critical challenge is how to identify conceptually relevant prior work from rapidly growing literature, and assess how a new idea differentiates from existing research. Current embedding approaches typically conflate distinct conceptual aspects into single representations and cannot support fine-grained literature retrieval; meanwhile, LLM-based evaluators are subject to sycophancy biases, failing to provide discriminative novelty assessment. To tackle these challenges, we introduce the Ideation Space, a structured representation that decomposes scientific knowledge into three distinct dimensions, i.e., research problem, methodology, and core findings, each learned through contrastive training. This framework enables principled measurement of conceptual distance between ideas, and modeling of ideation transitions that capture the logical connections within a proposed idea. Building upon this representation, we propose a Hierarchical Sub-Space Retrieval framework for efficient, targeted literature retrieval, and a Decomposed Novelty Assessment algorithm that identifies which aspects of an idea are novel. Extensive experiments demonstrate substantial improvements, where our approach achieves Recall@30 of 0.329 (16.7% over baselines), our ideation transition retrieval reaches Hit Rate@30 of 0.643, and novelty assessment attains 0.37 correlation with expert judgments. In summary, our work provides a promising paradigm for future research on accelerating and evaluating scientific discovery.

</details>


### [4] [Fine Grained Evaluation of LLMs-as-Judges](https://arxiv.org/abs/2601.08919)
*Sourav Saha,Mandar Mitra*

Main category: cs.IR

TL;DR: LLMs作为相关性评估者在文档层面表现良好，但在段落级评估中准确性有限，需要人类监督


<details>
  <summary>Details</summary>
Motivation: 扩展现有研究，探索LLMs作为信息检索相关性评估者的能力，不仅评估文档相关性，还能识别文档中的相关段落

Method: 使用INEX创建的维基百科测试集，提示LLMs判断文档相关性并高亮相关段落，与人类评估者的标注进行对比

Result: LLMs在文档层面评估表现良好，但在段落级评估准确性有限，需要人类监督才能达到最佳效果

Conclusion: LLMs作为评估者需要人类监督，特别是在需要精确识别相关段落时，不能完全替代人类判断

Abstract: A good deal of recent research has focused on how Large Language Models
  (LLMs) may be used as `judges' in place of humans to evaluate the quality
  of the output produced by various text / image processing systems. Within
  this broader context, a number of studies have investigated the specific
  question of how effectively LLMs can be used as relevance assessors for
  the standard ad hoc task in Information Retrieval (IR). We extend these
  studies by looking at additional questions. Most importantly, we use a
  Wikipedia based test collection created by the INEX initiative, and
  prompt LLMs to not only judge whether documents are relevant /
  non-relevant, but to highlight relevant passages in documents that it
  regards as useful. The human relevance assessors involved in creating
  this collection were given analogous instructions, i.e., they were asked
  to highlight all passages within a document that respond to the
  information need expressed in a query. This enables us to evaluate the
  quality of LLMs as judges not only at the document level, but to also
  quantify how often these `judges' are right for the right reasons.
  Our findings suggest that LLMs-as-judges work best under human
  supervision.

</details>


### [5] [LLMs Meet Isolation Kernel: Lightweight, Learning-free Binary Embeddings for Fast Retrieval](https://arxiv.org/abs/2601.09159)
*Zhibo Zhang,Yang Xu,Kai Ming Ting,Cam-Tu Nguyen*

Main category: cs.IR

TL;DR: 提出IKE方法，将LLM高维嵌入转换为二进制嵌入，显著降低存储和检索开销，同时保持检索精度


<details>
  <summary>Details</summary>
Motivation: LLM嵌入通常维度很高，导致存储和检索开销大。现有方法如MRL和CSR虽然能缓解问题，但仍存在检索精度下降的问题

Method: 使用Isolation Kernel（IK）将LLM嵌入转换为二进制嵌入。IKE是基于随机分区的集成方法，能够稳健估计LLM嵌入空间的理想核函数

Result: 在多个文本检索数据集上，IKE相比LLM嵌入实现高达16.7倍更快的检索和16倍更低的内存使用，同时保持相当或更好的精度。相比CSR和其他压缩方法，IKE在检索效率和效果之间达到最佳平衡

Conclusion: IKE是一种轻量级、基于二进制编码的方法，能够有效解决LLM嵌入的高维问题，在保持检索精度的同时显著提升效率

Abstract: Large language models (LLMs) have recently enabled remarkable progress in text representation. However, their embeddings are typically high-dimensional, leading to substantial storage and retrieval overhead. Although recent approaches such as Matryoshka Representation Learning (MRL) and Contrastive Sparse Representation (CSR) alleviate these issues to some extent, they still suffer from retrieval accuracy degradation. This paper proposes \emph{Isolation Kernel Embedding} or IKE, a learning-free method that transforms an LLM embedding into a binary embedding using Isolation Kernel (IK). IKE is an ensemble of diverse (random) partitions, enabling robust estimation of ideal kernel in the LLM embedding space, thus reducing retrieval accuracy loss as the ensemble grows. Lightweight and based on binary encoding, it offers low memory footprint and fast bitwise computation, lowering retrieval latency. Experiments on multiple text retrieval datasets demonstrate that IKE offers up to 16.7x faster retrieval and 16x lower memory usage than LLM embeddings, while maintaining comparable or better accuracy. Compared to CSR and other compression methods, IKE consistently achieves the best balance between retrieval efficiency and effectiveness.

</details>


### [6] [Why not Collaborative Filtering in Dual View? Bridging Sparse and Dense Models](https://arxiv.org/abs/2601.09286)
*Hanze Guo,Jianxun Lian,Xiao Zhou*

Main category: cs.IR

TL;DR: 提出SaD框架，结合稀疏和稠密视图解决推荐系统中冷门物品建模的SNR瓶颈问题，通过双向对齐机制提升性能


<details>
  <summary>Details</summary>
Motivation: 当前基于稠密嵌入的协同过滤方法在处理冷门物品时存在信号噪声比(SNR)天花板问题，参数化稠密模型在数据稀疏情况下SNR会下降

Method: 提出SaD统一框架，结合稠密嵌入的语义表达能力和稀疏交互模式的结构可靠性，采用轻量级双向对齐机制：稠密视图通过注入语义相关性丰富稀疏视图，稀疏视图通过显式结构信号正则化稠密模型

Result: 实验表明，在双视图对齐下，即使是简单的矩阵分解式稠密模型也能达到最先进性能；SaD在真实基准测试中始终优于强基线，在BarsMatch排行榜上排名第一

Conclusion: SaD框架展示了从双视角利用协同过滤的持久力量，具有即插即用特性，可无缝应用于广泛的现有推荐模型

Abstract: Collaborative Filtering (CF) remains the cornerstone of modern recommender systems, with dense embedding--based methods dominating current practice. However, these approaches suffer from a critical limitation: our theoretical analysis reveals a fundamental signal-to-noise ratio (SNR) ceiling when modeling unpopular items, where parameter-based dense models experience diminishing SNR under severe data sparsity. To overcome this bottleneck, we propose SaD (Sparse and Dense), a unified framework that integrates the semantic expressiveness of dense embeddings with the structural reliability of sparse interaction patterns. We theoretically show that aligning these dual views yields a strictly superior global SNR. Concretely, SaD introduces a lightweight bidirectional alignment mechanism: the dense view enriches the sparse view by injecting semantic correlations, while the sparse view regularizes the dense model through explicit structural signals. Extensive experiments demonstrate that, under this dual-view alignment, even a simple matrix factorization--style dense model can achieve state-of-the-art performance. Moreover, SaD is plug-and-play and can be seamlessly applied to a wide range of existing recommender models, highlighting the enduring power of collaborative filtering when leveraged from dual perspectives. Further evaluations on real-world benchmarks show that SaD consistently outperforms strong baselines, ranking first on the BarsMatch leaderboard. The code is publicly available at https://github.com/harris26-G/SaD.

</details>


### [7] [On-Device Large Language Models for Sequential Recommendation](https://arxiv.org/abs/2601.09306)
*Xin Xia,Hongzhi Yin,Shane Culpepper*

Main category: cs.IR

TL;DR: OD-LLM：首个专门为顺序推荐任务设计的任务自适应压缩框架，通过低秩结构压缩和标记化归一化技术，在资源受限设备上高效部署LLM，模型尺寸减半时性能无损失。


<details>
  <summary>Details</summary>
Motivation: 设备端推荐在延迟、隐私和网络不稳定场景中至关重要。虽然大语言模型在顺序推荐任务中表现出色，但其巨大的内存占用和计算开销使得在资源受限设备上部署成为高风险挑战。

Method: 提出OD-LLM框架，集成两种互补压缩策略：1）使用奇异值分解的低秩结构压缩算法减少参数冗余；2）新颖的标记化归一化技术以更好地配合低秩分解过程。同时采用渐进对齐算法逐层细化目标模型参数，最小化高压缩比下的性能损失。

Result: 在顺序推荐基准测试中，当部署模型尺寸减半时，OD-LLM相比原始推荐模型在有效性方面无损失，证明了该框架的效能和可扩展性。

Conclusion: OD-LLM为实时设备端解决方案提供了实用替代方案，能够替代昂贵的远程执行大语言模型，在资源受限设备上实现高效准确的顺序推荐。

Abstract: On-device recommendation is critical for a number of real-world applications, especially in scenarios that have agreements on execution latency, user privacy, and robust functionality when internet connectivity is unstable or even impossible. While large language models (LLMs) can now provide exceptional capabilities that model user behavior for sequential recommendation tasks, their substantial memory footprint and computational overhead make the deployment on resource-constrained devices a high risk proposition. In this paper, we propose OD-LLM, the first task-adaptive compression framework explicitly designed to provide efficient and accurate on-device deployment of LLMs for sequential recommendation tasks. OD-LLM uniquely integrates two complementary compression strategies: a low-rank structural compression algorithm which uses Singular Value Decomposition (SVD) to significantly reduce parameter redundancy in the model, and a novel tokenization normalization technique that better complements the low-rank decomposition process being used. Additionally, to minimize any potential performance degradation when using higher compression ratios, a novel progressive alignment algorithm is used to iteratively refine the parameters required layerwise in the target model. Empirical evaluations conducted on sequential recommendation benchmarks show that OD-LLM exhibits no loss in effectiveness when compared to the original recommendation model, when the deployed model size is halved. These promising results demonstrate the efficacy and scalability of OD-LLM, making this novel solution a practical alternative for real-time, on-device solutions wishing to replace expensive, remotely executed LLMs.

</details>


### [8] [LISP -- A Rich Interaction Dataset and Loggable Interactive Search Platform](https://arxiv.org/abs/2601.09366)
*Jana Isabelle Friese,Andreas Konstantin Kruff,Philipp Schaer,Norbert Fuhr,Nicola Ferro*

Main category: cs.IR

TL;DR: 提出了一个可重复使用的数据集和基础设施，用于研究交互式信息检索中的人类搜索行为，包含61名参与者的详细交互日志和用户特征，支持可重复性研究。


<details>
  <summary>Details</summary>
Motivation: 交互式信息检索(IIR)领域需要研究人类搜索行为，但缺乏包含详细交互日志和用户特征的可重复使用数据集，限制了研究个体和情境因素对搜索行为的影响。

Method: 收集了61名参与者（122个会话）的详细交互日志，结合用户特征（感知速度、主题兴趣、搜索专业知识、人口统计信息），提供完整的研究设置文档、基于网页的感知速度测试框架。

Result: 创建了一个包含丰富用户特征和交互数据的可重复使用数据集，通过示例分析展示了数据集的潜力，所有资源作为开放访问发布。

Conclusion: 该工作为IIR社区提供了支持可重复研究和资源共享的基础设施，使研究人员能够研究影响搜索行为的个体和情境因素，并开发或验证考虑这种变异性的用户模拟器。

Abstract: We present a reusable dataset and accompanying infrastructure for studying human search behavior in Interactive Information Retrieval (IIR). The dataset combines detailed interaction logs from 61 participants (122 sessions) with user characteristics, including perceptual speed, topic-specific interest, search expertise, and demographic information. To facilitate reproducibility and reuse, we provide a fully documented study setup, a web-based perceptual speed test, and a framework for conducting similar user studies. Our work allows researchers to investigate individual and contextual factors affecting search behavior, and to develop or validate user simulators that account for such variability. We illustrate the datasets potential through an illustrative analysis and release all resources as open-access, supporting reproducible research and resource sharing in the IIR community.

</details>


### [9] [Dissecting Judicial Reasoning in U.S. Copyright Damage Awards](https://arxiv.org/abs/2601.09459)
*Pei-Chi Lo,Thomas Y. Lu*

Main category: cs.IR

TL;DR: 本文提出了一种基于话语分析和LLM的新方法，用于量化分析版权损害赔偿判决中的司法推理模式，解决了传统法律分析中推理模式不透明的问题。


<details>
  <summary>Details</summary>
Motivation: 版权损害赔偿判决中的司法推理存在核心挑战：尽管联邦法院遵循1976年《版权法》，但不同司法管辖区对法律的解释和因素权重存在广泛差异。这种不一致性给诉讼当事人带来了不可预测性，并掩盖了法律决策的实证基础。

Method: 提出了一种新颖的基于话语分析的LLM方法，整合修辞结构理论（RST）和代理工作流程。采用三阶段流水线：数据集构建、话语分析和代理特征提取，将司法意见解析为层次化话语结构，识别推理组件并提取特征标签及相应的话语子树。

Result: 在分析版权损害赔偿裁决时，话语增强的LLM分析优于传统方法，同时揭示了各巡回法院之间因素权重的未量化变化。

Conclusion: 该研究为计算法律分析提供了方法论进步，并为司法推理提供了实践见解，对寻求预测工具的法律从业者、研究法律原则应用的学者以及面临版权法不一致问题的政策制定者具有重要意义。

Abstract: Judicial reasoning in copyright damage awards poses a core challenge for computational legal analysis. Although federal courts follow the 1976 Copyright Act, their interpretations and factor weightings vary widely across jurisdictions. This inconsistency creates unpredictability for litigants and obscures the empirical basis of legal decisions. This research introduces a novel discourse-based Large Language Model (LLM) methodology that integrates Rhetorical Structure Theory (RST) with an agentic workflow to extract and quantify previously opaque reasoning patterns from judicial opinions. Our framework addresses a major gap in empirical legal scholarship by parsing opinions into hierarchical discourse structures and using a three-stage pipeline, i.e., Dataset Construction, Discourse Analysis, and Agentic Feature Extraction. This pipeline identifies reasoning components and extract feature labels with corresponding discourse subtrees. In analyzing copyright damage rulings, we show that discourse-augmented LLM analysis outperforms traditional methods while uncovering unquantified variations in factor weighting across circuits. These findings offer both methodological advances in computational legal analysis and practical insights into judicial reasoning, with implications for legal practitioners seeking predictive tools, scholars studying legal principle application, and policymakers confronting inconsistencies in copyright law.

</details>


### [10] [Bridging Semantic Understanding and Popularity Bias with LLMs](https://arxiv.org/abs/2601.09478)
*Renqiang Luo,Dong Zhang,Yupeng Gao,Wen Shi,Mingliang Hou,Jiaying Liu,Zhe Wang,Shuo Yu*

Main category: cs.IR

TL;DR: FairLRM是一个基于大语言模型的推荐框架，通过语义理解来消除流行度偏差，将偏差分解为物品侧和用户侧组件，显著提升公平性和推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统大多将流行度偏差的语义理解简化为多样性增强或长尾覆盖问题，忽视了偏差本身的因果起源深层语义层，导致去偏效果和推荐准确性受限。

Method: 提出FairLRM框架，基于大语言模型进行推荐，将流行度偏差分解为物品侧和用户侧组件，使用结构化指令提示增强模型对全局物品分布和个体用户偏好的理解。

Result: 经验评估显示FairLRM显著提升了公平性和推荐准确性，提供了更具语义感知和可信赖的流行度偏差理解方法。

Conclusion: FairLRM通过大语言模型增强对流行度偏差的语义理解，为推荐系统提供了更有效的去偏方法，实现了公平性和准确性的双重提升。

Abstract: Semantic understanding of popularity bias is a crucial yet underexplored challenge in recommender systems, where popular items are often favored at the expense of niche content. Most existing debiasing methods treat the semantic understanding of popularity bias as a matter of diversity enhancement or long-tail coverage, neglecting the deeper semantic layer that embodies the causal origins of the bias itself. Consequently, such shallow interpretations limit both their debiasing effectiveness and recommendation accuracy. In this paper, we propose FairLRM, a novel framework that bridges the gap in the semantic understanding of popularity bias with Recommendation via Large Language Model (RecLLM). FairLRM decomposes popularity bias into item-side and user-side components, using structured instruction-based prompts to enhance the model's comprehension of both global item distributions and individual user preferences. Unlike traditional methods that rely on surface-level features such as "diversity" or "debiasing", FairLRM improves the model's ability to semantically interpret and address the underlying bias. Through empirical evaluation, we show that FairLRM significantly enhances both fairness and recommendation accuracy, providing a more semantically aware and trustworthy approach to enhance the semantic understanding of popularity bias. The implementation is available at https://github.com/LuoRenqiang/FairLRM.

</details>


### [11] [Unifying Search and Recommendation in LLMs via Gradient Multi-Subspace Tuning](https://arxiv.org/abs/2601.09496)
*Jujia Zhao,Zihan Wang,Shuaiqun Pan,Suzan Verberne,Zhaochun Ren*

Main category: cs.IR

TL;DR: GEMS提出了一种参数高效微调框架，通过多子空间分解和零空间投影来解决搜索与推荐统一建模中的梯度冲突和知识偏移问题。


<details>
  <summary>Details</summary>
Motivation: 搜索和推荐在在线平台中具有互补作用，统一建模具有重要价值。现有基于LLM的统一方法采用全参数微调，计算成本高且可扩展性差。参数高效微调虽然更实用，但面临两个关键挑战：跨任务的梯度冲突和微调数据导致的用户意图理解偏移。

Method: GEMS框架包含两个核心组件：1）多子空间分解：将共享和任务特定的优化信号解耦到互补的低秩子空间，减少破坏性梯度干扰；2）零空间投影：将参数更新约束到与通用知识空间正交的子空间，减轻用户意图理解的偏移。

Result: 在基准数据集上的广泛实验表明，GEMS在搜索和推荐任务上始终优于最先进的基线方法，实现了卓越的有效性。

Conclusion: GEMS通过创新的参数高效微调框架，成功解决了搜索与推荐统一建模中的梯度冲突和知识偏移问题，为LLM在S&R统一建模中的实际应用提供了可行的解决方案。

Abstract: Search and recommendation (S&R) are core to online platforms, addressing explicit intent through queries and modeling implicit intent from behaviors, respectively. Their complementary roles motivate a unified modeling paradigm. Early studies to unify S&R adopt shared encoders with task-specific heads, while recent efforts reframe item ranking in both S&R as conditional generation. The latter holds particular promise, enabling end-to-end optimization and leveraging the semantic understanding of LLMs. However, existing methods rely on full fine-tuning, which is computationally expensive and limits scalability. Parameter-efficient fine-tuning (PEFT) offers a more practical alternative but faces two critical challenges in unifying S&R: (1) gradient conflicts across tasks due to divergent optimization objectives, and (2) shifts in user intent understanding caused by overfitting to fine-tuning data, which distort general-domain knowledge and weaken LLM reasoning. To address the above issues, we propose Gradient Multi-Subspace Tuning (GEMS), a novel framework that unifies S&R with LLMs while alleviating gradient conflicts and preserving general-domain knowledge. GEMS introduces (1) \textbf{Multi-Subspace Decomposition}, which disentangles shared and task-specific optimization signals into complementary low-rank subspaces, thereby reducing destructive gradient interference, and (2) \textbf{Null-Space Projection}, which constrains parameter updates to a subspace orthogonal to the general-domain knowledge space, mitigating shifts in user intent understanding. Extensive experiments on benchmark datasets show that GEMS consistently outperforms the state-of-the-art baselines across both search and recommendation tasks, achieving superior effectiveness.

</details>


### [12] [TEMPO: A Realistic Multi-Domain Benchmark for Temporal Reasoning-Intensive Retrieval](https://arxiv.org/abs/2601.09523)
*Abdelrahman Abdallah,Mohammed Ali,Muhammad Abdul-Mageed,Adam Jatowt*

Main category: cs.IR

TL;DR: TEMPO是首个结合时间推理与推理密集型检索的基准测试，包含1,730个复杂查询和3,976个分解步骤，评估显示现有检索系统在时间推理方面面临重大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有时间QA基准主要关注新闻语料中的简单事实查询，而推理密集型检索基准缺乏时间基础。现实世界的信息需求通常需要理解时间演变和跨时期证据合成。

Method: 构建TEMPO基准，包含13个领域的1,730个复杂查询，采用分步检索规划（3,976个分解步骤），为每个步骤映射黄金文档，并引入Temporal Coverage@k和Temporal Precision@k等新型时间指标。

Result: 评估12个检索系统显示显著挑战：最佳模型(DiVeR)仅达到32.0 NDCG@10和71.4% Temporal Coverage@10，表明在检索时间完整证据方面存在困难。

Conclusion: TEMPO为改进检索和RAG系统中的时间推理提供了一个具有挑战性的基准，现有系统在时间推理方面仍有很大提升空间。

Abstract: Existing temporal QA benchmarks focus on simple fact-seeking queries from news corpora, while reasoning-intensive retrieval benchmarks lack temporal grounding. However, real-world information needs often require reasoning about temporal evolution and synthesizing evidence across time periods. We introduce TEMPO, the first benchmark combining temporal reasoning with reasoning-intensive retrieval across 13 domains. TEMPO features: (1) 1,730 complex queries requiring deep temporal reasoning such as tracking changes, identifying trends, or comparing cross-period evidence; (2) step-wise retrieval planning with 3,976 decomposed steps and gold documents mapped to each step for multi-hop evaluation; and (3) novel temporal metrics including Temporal Coverage@k and Temporal Precision@k measuring whether results span required time periods. Evaluation of 12 retrieval systems reveals substantial challenges: the best model (DiVeR) achieves only 32.0 NDCG@10 and 71.4\% Temporal Coverage@10, demonstrating difficulty in retrieving temporally complete evidence. We believe TEMPO provides a challenging benchmark for improving temporal reasoning in retrieval and RAG systems. Our code and data are available at https://github.com/tempo-bench/Tempo. See also our official website: https://tempo-bench.github.io/.

</details>


### [13] [SpatCode: Rotary-based Unified Encoding Framework for Efficient Spatiotemporal Vector Retrieval](https://arxiv.org/abs/2601.09530)
*Bingde Hu,Enhao Pan,Wanjing Zhou,Yang Gao,Zunlei Feng,Hao Zhong*

Main category: cs.IR

TL;DR: 提出统一的时空向量检索框架，通过旋转编码、增量更新和加权检索算法，实现高效、灵活的时空信息检索。


<details>
  <summary>Details</summary>
Motivation: 现有时空检索方法通常是传统向量搜索系统的扩展，依赖外部过滤器或专用索引来纳入时空约束，导致效率低下、架构复杂，且处理异构模态的灵活性有限。

Method: 1) 基于旋转的统一编码方法，将时间和位置嵌入旋转位置向量；2) 循环增量更新机制，支持高效滑动窗口更新；3) 加权兴趣检索算法，自适应平衡模态权重。

Result: 在多个真实数据集上的实验表明，该框架在检索准确性和效率方面显著优于现有基线方法，并在动态数据演化下保持鲁棒性。

Conclusion: 提出的方法为智能系统中可扩展的时空信息检索提供了有效且实用的解决方案。

Abstract: Spatiotemporal vector retrieval has emerged as a critical paradigm in modern information retrieval, enabling efficient access to massive, heterogeneous data that evolve over both time and space. However, existing spatiotemporal retrieval methods are often extensions of conventional vector search systems that rely on external filters or specialized indices to incorporate temporal and spatial constraints, leading to inefficiency, architectural complexity, and limited flexibility in handling heterogeneous modalities. To overcome these challenges, we present a unified spatiotemporal vector retrieval framework that integrates temporal, spatial, and semantic cues within a coherent similarity space while maintaining scalability and adaptability to continuous data streams. Specifically, we propose (1) a Rotary-based Unified Encoding Method that embeds time and location into rotational position vectors for consistent spatiotemporal representation; (2) a Circular Incremental Update Mechanism that supports efficient sliding-window updates without global re-encoding or index reconstruction; and (3) a Weighted Interest-based Retrieval Algorithm that adaptively balances modality weights for context-aware and personalized retrieval. Extensive experiments across multiple real-world datasets demonstrate that our framework substantially outperforms state-of-the-art baselines in both retrieval accuracy and efficiency, while maintaining robustness under dynamic data evolution. These results highlight the effectiveness and practicality of the proposed approach for scalable spatiotemporal information retrieval in intelligent systems.

</details>


### [14] [Examining DOM Coordinate Effectiveness For Page Segmentation](https://arxiv.org/abs/2601.09543)
*Jason Carpenter,Faaiq Bilal,Eman Ramadan,Zhi-Li Zhang*

Main category: cs.IR

TL;DR: 该研究发现网页分割中DOM坐标优于视觉坐标，简单向量比复杂向量表现更好，通过优化匹配可提升分割准确率至74%


<details>
  <summary>Details</summary>
Motivation: 网页数据规模庞大且非结构化，现有基于DOM的方法使用视觉位置或树结构坐标构建聚类向量，但这些向量的构建和组件价值缺乏深入分析，需要理解不同坐标对网页分割的影响

Method: 提出并详细检验DOM坐标，比较视觉坐标与DOM坐标的性能，分析简单向量与复杂向量的效果，研究向量、聚类算法和页面的匹配机制

Result: 视觉坐标比DOM坐标平均低20-30%性能；简单向量（单坐标）在68.2%的页面中表现优于复杂向量；通过优化匹配可实现74%的整体分割准确率，比简单应用向量提升20%

Conclusion: 挑战了当前分割向量构建的正统观念，表明没有通用向量，DOM坐标优于视觉坐标，简单向量常优于复杂向量，优化匹配机制对网页分割至关重要

Abstract: Web pages form a cornerstone of available data for daily human consumption and with the rise of LLM-based search and learning systems a treasure trove of valuable data. The scale of this data and its unstructured format still continue to grow requiring ever more robust automated extraction and retrieval mechanisms. Existing work, leveraging the web pages Document Object Model (DOM), often derives clustering vectors from coordinates informed by the DOM such as visual placement or tree structure. The construction and component value of these vectors often go unexamined. Our work proposes and examines DOM coordinates in a detail to understand their impact on web page segmentation. Our work finds that there is no one-size-fits-all vector, and that visual coordinates under-perform compared to DOM coordinates by about 20-30% on average. This challenges the necessity of including visual coordinates in clustering vectors. Further, our work finds that simple vectors, comprised of single coordinates, fare better than complex vectors constituting 68.2% of the top performing vectors of the pages examined. Finally, we find that if a vector, clustering algorithm, and page are properly matched, one can achieve overall high segmentation accuracy at 74%. This constitutes a 20% improvement over a naive application of vectors. Conclusively, our results challenge the current orthodoxy for segmentation vector creation, opens up the possibility to optimize page segmentation via clustering on DOM coordinates, and highlights the importance of finding mechanisms to match the best approach for web page segmentation.

</details>


### [15] [MM-BRIGHT: A Multi-Task Multimodal Benchmark for Reasoning-Intensive Retrieval](https://arxiv.org/abs/2601.09562)
*Abdelrahman Abdallah,Mohamed Darwish Mounis,Mahmoud Abdalla,Mahmoud SalahEldin Kasem,Mostafa Farouk Senussi,Mohamed Mahmoud,Mohammed Ali,Adam Jatowt,Hyun-Soo Kang*

Main category: cs.IR

TL;DR: MM-BRIGHT是首个针对推理密集型检索的多模态基准测试，包含2,803个真实世界查询，涵盖29个技术领域，评估模型在文本到文本、多模态到文本、多模态到图像和多模态到多模态四种任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有检索基准主要针对文本查询，而现实世界查询常包含图表、截图等多模态元素，需要深度推理才能找到相关文档。当前缺乏评估多模态推理检索能力的基准。

Method: 构建包含2,803个真实世界查询的数据集，涵盖29个技术领域，设计四种复杂度递增的任务：文本到文本、多模态到文本、多模态到图像、多模态到多模态检索。

Result: 最先进模型在所有任务上都表现不佳：BM25在纯文本检索上仅获得8.5 nDCG@10，最佳多模态模型Nomic-Vision在多模态到文本检索上仅27.6 nDCG@10，甚至不如最佳纯文本模型DiVeR（32.2）。

Conclusion: MM-BRIGHT揭示了当前检索模型在多模态推理方面的显著不足，为下一代更好整合视觉推理的检索模型提供了测试平台。

Abstract: Existing retrieval benchmarks primarily consist of text-based queries where keyword or semantic matching is usually sufficient. Many real-world queries contain multimodal elements, particularly, images such as diagrams, charts, and screenshots that require intensive reasoning to identify relevant documents. To address this gap, we introduce MM-BRIGHT, the first multimodal benchmark for reasoning-intensive retrieval. Our dataset consists of 2,803 real-world queries spanning 29 diverse technical domains, with four tasks of increasing complexity: text-to-text, multimodal-to-text, multimodal-to-image, and multimodal-to-multimodal retrieval. Extensive evaluation reveals that state-of-the-art models struggle across all tasks: BM25 achieves only 8.5 nDCG@10 on text-only retrieval, while the best multimodal model Nomic-Vision reaches just 27.6 nDCG@10 on multimodal-to-text retrieval actually underperforming the best text-only model (DiVeR: 32.2). These results highlight substantial headroom and position MM-BRIGHT as a testbed for next-generation retrieval models that better integrate visual reasoning. Our code and data are available at https://github.com/mm-bright/MM-BRIGHT. See also our official website: https://mm-bright.github.io/.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [16] [Two-dimensional Entanglement-assisted Quantum Quasi-cyclic Low-density Parity-check Codes](https://arxiv.org/abs/2601.08927)
*Pavan Kumar,Shayan Srinivasa Garani*

Main category: cs.IT

TL;DR: 该论文研究了二维经典准循环LDPC码中2g-环的存在条件，构建了多种girth大于4或6的码族，并基于这些经典码构造了具有良好纠错能力的二维EA量子LDPC码。


<details>
  <summary>Details</summary>
Motivation: 研究二维经典QC-LDPC码中环的存在条件，旨在构建具有较大围长(girth)的码，以减少Tanner图中的短环，从而改善LDPC码的性能。进一步将经典码构造方法扩展到量子领域，构建具有良好纠错能力的EA量子LDPC码。

Method: 1. 推导二维经典QC-LDPC码Tanner图中2g-环存在的一般条件
2. 通过堆叠p×p×p张量(p为奇素数)构造girth大于4的二维经典QC-LDPC码族
3. 对于复合数p，提出两种通过类似张量堆叠得到的二维经典LDPC码族，分别实现girth大于4和大于6
4. 基于构造的经典码，推导两种二维EA量子LDPC码族：一种来自一对二进制经典码，另一种来自单个无4环的经典码

Result: 1. 建立了二维经典QC-LDPC码中2g-环存在的理论条件
2. 成功构造了多个具有不同围长特性的码族：奇素数p时girth>4；复合数p时分别实现girth>4和girth>6
3. 所有提出的经典码都具有至少p×p的擦除纠错能力
4. 构造的EA量子LDPC码继承了经典码的纠错能力，且第一种仅需1个ebit，第二种基于无4环的经典码

Conclusion: 论文系统研究了二维经典QC-LDPC码的环结构，建立了理论条件并构造了多种高性能码族。进一步将经典构造方法扩展到量子领域，成功构建了具有良好纠错能力的EA量子LDPC码，为量子纠错码设计提供了新思路。

Abstract: For any positive integer $g \ge 2$, we derive general conditions for the existence of a $2g$-cycle in the Tanner graph of two-dimensional ($2$-D) classical quasi-cyclic (QC) low-density parity-check (LDPC) codes. Based on these conditions, we construct a family of $2$-D classical QC-LDPC codes with girth greater than $4$ by stacking $p \times p \times p$ tensors, where $p$ is an odd prime. Furthermore, for composite values of $p$, we propose two additional families of $2$-D classical LDPC codes obtained via similar tensor stacking. In this case, one family achieves girth greater than $4$, while the other attains girth greater than $6$. All the proposed $2$-D classical QC-LDPC codes exhibit an erasure correction capability of at least $p \times p$. Based on the constructed classical $2$-D QC-LDPC codes, we derive two families of $2$-D entanglement-assisted (EA) quantum low-density parity-check (QLDPC) codes. The first family of $2$-D EA-QLDPC codes is obtained from a pair of binary $2$-D classical LDPC codes and is designed such that the unassisted part of the Tanner graph of the resulting EA-QLDPC code is free of cycles of length four, while requiring only a single ebit to be shared across the quantum transceiver. The second family is constructed from a single $2$-D classical LDPC code whose Tanner graph is free from $4$-cycles. Moreover, the constructed EA-QLDPC codes inherit an erasure correction capability of $p \times p$, as the underlying classical codes possess the same erasure correction property.

</details>


### [17] [A Local Characterization of $f$-Divergences Yielding PSD Mutual-Information Matrices](https://arxiv.org/abs/2601.08929)
*Zachary Roberston*

Main category: cs.IT

TL;DR: 研究f-互信息矩阵在弱依赖条件下的正半定性，给出了局部特征：当且仅当f在1处解析且泰勒系数非负时，矩阵正半定。


<details>
  <summary>Details</summary>
Motivation: 研究变量索引的f-互信息矩阵何时是正半定的，这不同于分布间散度的度量性质，关注的是变量索引矩阵的正定性。

Method: 结合副本嵌入将单项式项转化为Gram矩阵，通过副本强制约简为正定点积核，应用Schoenberg-Berg-Christensen-Ressel分类理论。

Result: 给出了局部特征：存在δ>0使得在弱依赖条件下，M^{(f)}正半定当且仅当f在1处解析且泰勒系数a_m≥0。负系数会给出反例，非解析凸散度被排除。

Conclusion: f-互信息矩阵的正半定性要求f在1处解析且泰勒系数非负，这与散度本身的度量性质不同，为变量索引矩阵的正定性提供了理论特征。

Abstract: We study when the variable-indexed matrix of pairwise \(f\)-mutual informations \(M^{(f)}_{ij}=I_f(X_i;X_j)\) is positive semidefinite (PSD). Let \(f:(0,\infty)\to\mathbb{R}\) be convex with \(f(1)=0\), finite in a neighborhood of \(1\), and with \(f(0)<\infty\) so that diagonal terms are finite. We give a sharp \emph{local} characterization around independence: there exists \(δ=δ(f)>0\) such that for every \(n\) and every finite-alphabet family \((X_1,\ldots,X_n)\) whose pairwise joint-to-product ratios lie in \((1-δ,1+δ)\), the matrix \(M^{(f)}\) is PSD if and only if \(f\) is analytic at \(1\) with a convergent expansion \(f(t)=\sum_{m=2}^{\infty} a_m (t-1)^m\) and \(a_m\ge 0\) on a neighborhood of \(1\). Consequently, any negative Taylor coefficient yields an explicit finite-alphabet counterexample under arbitrarily weak dependence, and non-analytic convex divergences (e.g.\ total variation) are excluded. This PSD requirement is distinct from Hilbertian/metric properties of divergences between distributions (e.g.\ \(\sqrt{\mathrm{JS}}\)): we study PSD of the \emph{variable-indexed} mutual-information matrix. The proof combines a replica embedding that turns monomial terms into Gram matrices with a replica-forcing reduction to positive-definite dot-product kernels, enabling an application of the Schoenberg--Berg--Christensen--Ressel classification.

</details>


### [18] [On the Information Leakage Envelope of the Gaussian Mechanism](https://arxiv.org/abs/2601.08986)
*Sara Saeidian*

Main category: cs.IT

TL;DR: 本文研究了高斯机制的点态最大泄漏包络，推导了高斯秘密下的闭式解，并扩展到一般无界秘密，特别是强对数凹先验。


<details>
  <summary>Details</summary>
Motivation: 研究高斯机制的信息泄漏边界，特别是在任意后处理下以高概率成立的最小泄漏界，这对于隐私保护分析很重要。

Method: 推导高斯机制在高斯秘密下的确定性PML包络闭式解，然后扩展到一般无界秘密，利用Brascamp-Lieb不等式证明强对数凹先验满足条件。

Result: 得到了高斯秘密下PML包络的闭式表达式，并证明强对数凹先验的PML包络与高斯情况一致。

Conclusion: 高斯机制的PML包络在高斯秘密下有闭式解，且该结果可扩展到强对数凹先验，为隐私泄漏分析提供了理论工具。

Abstract: We study the pointwise maximal leakage (PML) envelope of the Gaussian mechanism, which characterizes the smallest information leakage bound that holds with high probability under arbitrary post-processing. For the Gaussian mechanism with a Gaussian secret, we derive a closed-form expression for the deterministic PML envelope for sufficiently small failure probabilities. We then extend this result to general unbounded secrets by identifying a sufficient condition under which the envelope coincides with the Gaussian case. In particular, we show that strongly log-concave priors satisfy this condition via an application of the Brascamp-Lieb inequality.

</details>


### [19] [An Information-Theoretic Perspective on LLM Tokenizers](https://arxiv.org/abs/2601.09039)
*Mete Erdogan,Abhiram Gorle,Shubham Chandak,Mert Pilanci,Tsachy Weissman*

Main category: cs.IT

TL;DR: 论文研究LLM分词器作为结构化压缩器的作用，分析分词训练规模如何影响压缩效率和诱导的统计结构，提出压缩感知的分词器设计方法。


<details>
  <summary>Details</summary>
Motivation: LLM分词器作为结构化压缩器，通过将文本映射到离散标记序列，决定了标记数量（计算和上下文使用）以及下游模型看到的统计结构。尽管分词器在LLM流程中起核心作用，但分词、压缩效率和诱导结构之间的关系尚未被充分理解。

Method: 1) 将预训练的GPT系列分词器作为黑盒压缩器在不同领域进行基准测试；2) 研究不同配置（词汇量、训练规模、领域）下的学习分词器；3) 将分词作为通用压缩的变换，引入压缩感知的BPE变体；4) 采用信道视角，引入容量利用指标来分析分词器行为。

Result: 实证表明分词器训练规模重新分配熵：随着训练数据增加，标记流在总体上变得更加多样化（更高的一元熵），但在上下文中明显更可预测（更低的更高阶条件熵）。这表明分词吸收了大量的短程规律性，但这些增益在训练-测试领域不匹配时会退化。

Conclusion: 研究结果揭示了压缩、诱导结构和领域转移鲁棒性之间的各种权衡，并激励了原则性的、压缩感知的分词器设计。

Abstract: Large language model (LLM) tokenizers act as structured compressors: by mapping text to discrete token sequences, they determine token count (and thus compute and context usage) and the statistical structure seen by downstream models. Despite their central role in LLM pipelines, the link between tokenization, compression efficiency and induced structure is not well understood. We empirically demonstrate that tokenizer training scale redistributes entropy: as training data grows, the token stream becomes more diverse in aggregate (higher unigram entropy) yet markedly more predictable in-context (lower higher-order conditional entropies), indicating that tokenization absorbs substantial short-range regularity although these gains degrade under train-test domain mismatch. To ground these observations, we first benchmark i) pretrained GPT-family tokenizers as black-box compressors across various domains, and ii) learned tokenizers across configurations spanning vocabulary size, training scale, and domain. Next, we study tokenization as a transform for universal compression and introduce a compression-aware BPE variant. Finally, we adopt a channel lens and introduce capacity-utilization metrics to analyze tokenizer behaviour and outline implications for downstream modeling. Put together, our results expose various trade-offs between compression, induced structure, and robustness under domain shift, and motivate principled, compression-aware tokenizer design.

</details>


### [20] [Hybrid Mono- and Bi-static OFDM-ISAC via BS-UE Cooperation: Closed-Form CRLB and Coverage Analysis](https://arxiv.org/abs/2601.09057)
*Xiaoli Xu,Yong Zeng*

Main category: cs.IT

TL;DR: 提出一种混合单/双基地感知框架，利用ISAC系统中基站和用户设备协作，无需额外频谱或小区间协调，显著提升感知性能。


<details>
  <summary>Details</summary>
Motivation: 现有单基地和双基地感知模式各有局限，需要开发更高效的感知框架来提升ISAC系统的感知性能，同时避免额外资源开销。

Method: 基于3GPP支持的感知模式，构建混合单/双基地感知框架；推导目标定位和速度估计的CRLB闭式解；使用有效参数估计算法和加权MSE融合方法进行验证。

Result: 当BS-目标-UE形成接近直角三角形的有利几何时，混合感知相比纯单/双基地感知有显著性能增益；感知覆盖随BS-UE距离先改善后下降；感知精度随UE密度增加而提升。

Conclusion: 提出的混合感知框架在ISAC系统中具有显著性能优势，为网络部署和UE选择提供了理论指导，可有效提升感知覆盖和精度。

Abstract: This paper proposes a hybrid mono- and bi-static sensing framework, by leveraging the base station (BS) and user equipment (UE) cooperation in integrated sensing and communication (ISAC) systems. This scheme is built on 3GPP-supported sensing modes, and it does not incur any extra spectrum cost or inter-cell coordination. To reveal the fundamental performance limit of the proposed hybrid sensing mode, we derive closed-form Cramér-Rao lower bound (CRLB) for sensing target localization and velocity estimation, as functions of target and UE positions. The results reveal that significant performance gains can be achieved over the purely mono- or bi-static sensing, especially when the BS-target-UE form a favorable geometry, which is close to a right triangle. The analytical results are validated by simulations using effective parameter estimation algorithm and weighted mean square error (MSE) fusion method. Based on the derived sensing bound, we further analyze the sensing coverage by varying the UE positions, which shows that sensing coverage first improves then degrades as the BS-UE separation increases. Furthermore, the sensing accuracy for a potential target with best UE selection is derived as a function of the UE density in the network.

</details>


### [21] [Overcoming the Shadow: Bending Airy Beams for Radiative Near-Field Multi-User Access in Half-Space Blockage Scenarios](https://arxiv.org/abs/2601.09098)
*Yifeng Qin,Jing Chen,Zhi Hao Jiang,Zhi Ning Chen,Yongming Huang*

Main category: cs.IT

TL;DR: 提出利用艾里光束的自弯曲特性来缓解下一代无线通信中大规模天线阵列面临的半空间遮挡问题，无需额外硬件即可恢复阴影区域连接


<details>
  <summary>Details</summary>
Motivation: 下一代大规模天线阵列通信进入辐射近场区域，但高频近场链路在室内环境中极易受到墙壁、角落等半空间障碍物的遮挡，导致传统近场聚焦波束在阴影区域出现严重能量截断和有效秩崩溃，而可重构智能表面等硬件解决方案不实用

Method: 基于格林函数建立辐射近场多用户信道模型，分析刀边障碍物后的奇异值崩溃；设计艾里模拟波束成形方案，优化弯曲轨迹以恢复有效信道秩；提出艾里零陷导向方法，将振荡零陷与明亮区域用户对齐以抑制干扰

Result: 仿真显示，提出的边缘"骑行"艾里策略相比传统近场聚焦，在阴影链路上实现超过20dB的信噪比改善，恢复全秩连接，基本消除几何阴影中的中断，在典型室内ELAA配置下多用户频谱效率提高约35%

Conclusion: 艾里光束的自弯曲特性能够在不依赖可重构智能表面的情况下，为半空间遮挡场景提供稳健的辐射近场多用户接入，有效解决刀边阴影问题

Abstract: The move to next-generation wireless communications with extremely large-scale antenna arrays (ELAAs) brings the communications into the radiative near-field (RNF) region, where distance-aware focusing is feasible. However, high-frequency RNF links are highly vulnerable to blockage in indoor environments dominated by half-space obstacles (walls, corners) that create knife-edge shadows. Conventional near-field focused beams offer high gain in line-of-sight (LoS) scenarios but suffer from severe energy truncation and effective-rank collapse in shadowed regions, making hardware remedies such as reconfigurable intelligent surfaces (RIS) impractical. We propose a beamforming strategy that exploits the auto-bending property of Airy beams to mitigate half-space blockage without additional hardware. The Airy beam is designed to ``ride'' the diffraction edge, accelerating its main lobe into the shadow to restore connectivity. Our contributions are threefold: (i) a Green's function-based RNF multi-user channel model that analytically reveals singular-value collapse behind knife-edge obstacles; (ii) an Airy analog beamforming scheme that optimizes the bending trajectory to recover the effective channel rank; and (iii) an Airy null-steering method that aligns oscillatory nulls with bright-region users to suppress interference in mixed shadow/bright scenarios. Simulations show that the proposed edge-riding Airy strategy achieves an SNR improvement of over 20 dB and restores full-rank connectivity in shadowed links compared to conventional RNF focusing, virtually eliminating outage in geometric shadows and increasing multi-user spectral efficiency by approximately 35\% under typical indoor ELAA configurations. These results demonstrate robust RNF multi-user access in half-space blockage scenarios without relying on RIS.

</details>


### [22] [The .serva Standard: One Primitive for All AI Cost Reduced, Barriers Removed](https://arxiv.org/abs/2601.09124)
*Rachel St. Clair,John Austin Cook,Peter Sutor,Victor Cavero,Garrett Mindt*

Main category: cs.IT

TL;DR: ServaStack提出了一种革命性的AI基础设施解决方案，通过.serva格式和Chimera引擎实现数据格式与计算引擎的统一，大幅提升能效和存储效率。


<details>
  <summary>Details</summary>
Motivation: 当前AI基础设施面临两大危机：计算负载（训练和推理的能源与资本成本不可持续）和数据混乱（80%项目精力消耗在数据准备上）。现有方法将这两个问题分开处理，导致生态系统复杂性增加。

Method: 提出ServaStack：包含通用数据格式(.serva)和通用AI计算引擎(Chimera)。.serva格式基于激光全息原理实现无损压缩，Chimera引擎将计算操作转换为表示空间，直接在.serva文件上计算而无需解压缩。

Result: 内部基准测试显示：能效提升30-374倍（减少96-99%），无损存储压缩4-34倍，计算负载减少68倍且无精度损失。在十亿次每日迭代的超大规模下，每PB每训练周期可节省485万美元。

Conclusion: ServaStack通过统一数据格式和计算引擎，将AI开发瓶颈从基础设施转移到想象力，实现任何数据流向任何模型在任何硬件上的新范式。

Abstract: Artificial Intelligence (AI) infrastructure faces two compounding crises. Compute payload - the unsustainable energy and capital costs of training and inference - threatens to outpace grid capacity and concentrate capability among a handful of organizations. Data chaos - the 80% of project effort consumed by preparation, conversion, and preprocessing - strangles development velocity and locks datasets to single model architectures. Current approaches treat these as separate problems, managing each with incremental optimization while increasing ecosystem complexity. This paper presents ServaStack: a universal data format (.serva) paired with a universal AI compute engine (Chimera). The .serva format achieves lossless compression by encoding information using laser holography principles, while Chimera converts compute operations into a representational space where computation occurs directly on .serva files without decompression. The result is automatic data preprocessing. The Chimera engine enables any existing model to operate on .serva data without retraining, preserving infrastructure investments while revamping efficiency. Internal benchmarks demonstrate 30-374x energy efficiency improvements (96-99% reduction), 4x-34x lossless storage compression, and 68x compute payload reduction without accuracy loss when compared to RNN, CNN, and MLP models on FashionMNIST and MNIST datasets. At hyperscale with one billion daily iterations, these gains translate to $4.85M savings per petabyte per training cycle. When any data flows to any model on any hardware, the AI development paradigm shifts. The bottleneck moves from infrastructure to imagination.

</details>


### [23] [Movable Antenna Assisted Dual-Polarized Multi-Cell Cooperative AirComp: An Alternating Optimization Approach](https://arxiv.org/abs/2601.09137)
*Mingyu Hu,Nan Liu,Wei Kang*

Main category: cs.IT

TL;DR: 提出基于双极化可移动天线(D-PMA)的多小区协作空中计算框架，通过联合优化组合矩阵、极化向量、天线位置和用户发射系数来最小化均方误差，显著提升分布式优化性能。


<details>
  <summary>Details</summary>
Motivation: 空中计算(AirComp)是分布式优化的关键技术，但传统固定单极化基站阵列无法充分利用空间自由度且存在极化失配问题，限制了系统性能。

Method: 提出多小区协作空中计算框架，采用双极化可移动天线(D-PMA)，通过交替优化算法：组合矩阵和发射系数采用闭式更新，极化向量使用SCA和SDR方法优化，天线位置采用梯度法更新，并开发基于统计信道的天线位置优化方案。

Result: 数值结果表明，所提可移动双极化方案在瞬时信道和统计信道下均优于可移动单极化和固定天线基准方案。

Conclusion: 双极化可移动天线架构能有效提升空中计算性能，通过联合优化天线参数可显著降低均方误差，为分布式优化系统提供更优的通信解决方案。

Abstract: Over-the-air computation (AirComp) is a key enabler for distributed optimization, since it leverages analog waveform superposition to perform aggregation and thereby mitigates the communication bottleneck caused by iterative information exchange. However, AirComp is sensitive to wireless environment and conventional systems with fixed single-polarized base-station arrays cannot fully exploit spatial degrees of freedom while also suffering from polarization mismatch. To overcome these limitations, this paper proposes a multi-cell cooperative air-computation framework assisted by dual-polarized movable antennas (D-PMA), and formulates a mean squared error (MSE) minimization problem by jointly optimizing the combining matrix, polarization vectors, antenna positions, and user transmit coefficients. The resulting problem is highly nonconvex, so an alternating algorithm is developed in which closed-form updates are obtained for the combining matrix and transmit coefficients. Then a method based on successive convex approximation (SCA) and semidefinite relaxation (SDR) is proposed to refine polarization vectors, and the antenna positions are updated using a gradient-based method. In addition, we develop a statistical-channel-based scheme for optimizing the antenna locations, and we further present the corresponding algorithm to efficiently obtain the solution. Numerical results show that the proposed movable dual-polarized scheme consistently outperforms movable single-polarized and fixed-antenna baselines under both instantaneous and statistical channels.

</details>


### [24] [Reducing The Sub-packetization Level of Optimal-Access Cooperative MSR Codes](https://arxiv.org/abs/2601.09188)
*Yaqian Zhang,Jingke Xu*

Main category: cs.IT

TL;DR: 提出了一种降低最优访问合作MSR码子分组化的新方法，针对两个擦除情况，将子分组化从r^(C(n,2))降低到r^(C(n,2)-⌊n/r⌋(C(r,2)-1))


<details>
  <summary>Details</summary>
Motivation: 现有合作MSR码的子分组化较大，导致高复杂度和大量磁盘I/O操作。需要设计具有最优访问特性的合作MSR码，同时降低子分组化以提高实际性能。

Method: 首先设计两个关键的MDS阵列码用于修复特定两种擦除模式，然后以这两个码为构建块，通过多次堆叠构建出具有最优访问特性的合作MSR码。

Result: 获得了子分组化ℓ=r^(C(n,2)-⌊n/r⌋(C(r,2)-1))的最优访问合作MSR码，相比现有技术(ℓ=r^(C(n,2)))减少了1/r^(⌊n/r⌋(C(r,2)-1))倍。

Conclusion: 成功降低了最优访问合作MSR码的子分组化，提高了实际应用的可行性，为存储系统提供了更高效的纠删码方案。

Abstract: Cooperative MSR codes are a kind of storage codes which enable optimal-bandwidth repair of any $h\geq2$ node erasures in a cooperative way, while retaining the minimum storage as an $[n,k]$ MDS code. Each code coordinate (node) is assumed to store an array of $\ell$ symbols, where $\ell$ is termed as sub-packetization. Large sub-packetization tends to induce high complexity, large input/output in practice. To address the disk IO capability, a cooperative MSR code is said to have optimal-access property, if during node repair, the amount of data accessed at each helper node meets a theoretical lower bound.
  In this paper, we focus on reducing the sub-packetization of optimal-access cooperative MSR codes with two erasures. At first, we design two crucial MDS array codes for repairing a specific repair pattern of two erasures with optimal access. Then, using the two codes as building blocks and by stacking up of the two codes for several times, we obtain an optimal-access cooperative MSR code with two erasures. The derived code has sub-packetization $\ell=r^{\binom{n}{2}-\lfloor\frac{n}{r}\rfloor(\binom{r}{2}-1)}$ where $r=n-k$, and it reduces $\ell$ by a fraction of $1/r^{\lfloor\frac{n}{r}\rfloor(\binom{r}{2}-1)}$ compared with the state of the art ($\ell=r^{\binom{n}{2}}$).

</details>


### [25] [Second-Order Asymptotics of Two-Sample Tests](https://arxiv.org/abs/2601.09196)
*K V Harsha,Jithin Ravi,Tobias Koch*

Main category: cs.IT

TL;DR: 提出了一种基于任意散度的两样本检验推广方法，称为散度检验，证明了其具有最优的一阶指数衰减率，且当使用不变散度时能达到与Gutman检验相同的二阶渐近性能。


<details>
  <summary>Details</summary>
Motivation: Gutman检验使用Jensen-Shannon散度进行两样本检验，但能否推广到任意散度？这些推广后的检验性能如何？能否达到最优的误差指数衰减率？

Method: 提出散度检验框架，用任意散度替代JS散度，比较两个样本经验分布的散度值与阈值。分析其类型II错误概率的指数衰减特性，研究一阶和二阶渐近性能。

Result: 1) 任意散度的散度检验都能达到最优一阶指数衰减率；2) 使用不变散度的散度检验能达到与Gutman检验相同的二阶渐近性能；3) 证明了Gutman检验是两样本检验问题的GLRT；4) 建立了两样本检验与鲁棒拟合优度检验的联系。

Conclusion: 散度检验为两样本检验提供了灵活的框架，在保持最优一阶性能的同时，通过选择合适的散度可以控制高阶渐近特性，为实际应用提供了理论指导。

Abstract: In two-sampling testing, one observes two independent sequences of independent and identically distributed random variables distributed according to the distributions $P_1$ and $P_2$ and wishes to decide whether $P_1=P_2$ (null hypothesis) or $P_1\neq P_2$ (alternative hypothesis). The Gutman test for this problem compares the empirical distributions of the observed sequences and decides on the null hypothesis if the Jensen-Shannon (JS) divergence between these empirical distributions is below a given threshold. This paper proposes a generalization of the Gutman test, termed \emph{divergence test}, which replaces the JS divergence by an arbitrary divergence. For this test, the exponential decay of the type-II error probability for a fixed type-I error probability is studied. First, it is shown that the divergence test achieves the optimal first-order exponent, irrespective of the choice of divergence. Second, it is demonstrated that the divergence test with an invariant divergence achieves the same second-order asymptotics as the Gutman test. In addition, it is shown that the Gutman test is the GLRT for the two-sample testing problem, and a connection between two-sample testing and robust goodness-of-fit testing is established.

</details>


### [26] [On Polar Coding with Feedback](https://arxiv.org/abs/2601.09222)
*Ling Liu,Qi Cao,Liping Li,Baoming Bai*

Main category: cs.IT

TL;DR: 研究反馈辅助下极化码的性能，反馈虽不提升信道容量但能显著改善有限长度性能，通过启用"精灵辅助"解码和更灵活的构造阈值


<details>
  <summary>Details</summary>
Motivation: 尽管已知反馈不会改善无记忆信道的容量，但反馈可能改善极化码在有限长度下的性能，因为反馈能实现精灵辅助解码并为极化码构造提供更灵活的阈值

Method: 提出反馈辅助的极化码构造方法，启用精灵辅助连续消除(SC)解码，并准确表征精灵辅助SC解码下错误事件的分布

Result: 反馈能显著提升极化码的有限长度性能，提出的分布表征方法也能用于预测接近容量时标准SC解码的性能

Conclusion: 反馈虽不提升信道容量，但通过启用精灵辅助解码和更灵活的构造阈值，能显著改善极化码的有限长度性能，提出的错误分布表征方法具有实用价值

Abstract: In this work, we investigate the performance of polar codes with the assistance of feedback in communication systems. Although it is well known that feedback does not improve the capacity of memoryless channels, we show that the finite length performance of polar codes can be significantly improved as feedback enables genie-aided decoding and allows more flexible thresholds for the polar coding construction. To analyze the performance under the new construction, we then propose an accurate characterization of the distribution of the error event under the genie-aided successive cancellation (SC) decoding. This characterization can be also used to predict the performance of the standard SC decoding of polar codes with rates close to capacity.

</details>


### [27] [A Theoretical Framework for Rate-Distortion Limits in Learned Image Compression](https://arxiv.org/abs/2601.09254)
*Changshuo Wang,Zijian Liang,Kai Niu,Ping Zhang*

Main category: cs.IT

TL;DR: 提出一个理论框架分析学习图像压缩的率失真极限，将性能损失分解为方差估计、量化策略和上下文建模三个关键组件，为设计更高效的学习压缩系统提供新见解。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的神经编解码器取得了显著的经验结果，但它们与信息论极限的距离仍不清楚。本文旨在填补这一空白，提供一个系统性的理论框架来分析学习图像压缩的率失真极限。

Method: 1) 推导高斯假设下最优潜在方差作为二阶矩，提供基于超先验估计的原则性替代方案；2) 量化均匀量化与从反向注水定理推导的高斯测试信道之间的差距；3) 将框架扩展到上下文建模，证明准确的均值预测能显著降低熵。通过联合仿真和端到端训练，推导理论率失真极限的紧密且可操作的近似。

Result: 提出了一个结构可解释的框架，与真实压缩模块对齐，支持细粒度分析。该方法提供了对理论率失真极限的紧密且可操作的近似，为设计更高效的学习压缩系统提供了新见解。

Conclusion: 该理论框架为分析学习图像压缩的率失真极限提供了系统方法，将性能损失分解为三个关键组件，为理解和改进神经编解码器的性能提供了理论基础，有助于设计更接近信息论极限的学习压缩系统。

Abstract: We present a novel systematic theoretical framework to analyze the rate-distortion (R-D) limits of learned image compression. While recent neural codecs have achieved remarkable empirical results, their distance from the information-theoretic limit remains unclear. Our work addresses this gap by decomposing the R-D performance loss into three key components: variance estimation, quantization strategy, and context modeling. First, we derive the optimal latent variance as the second moment under a Gaussian assumption, providing a principled alternative to hyperprior-based estimation. Second, we quantify the gap between uniform quantization and the Gaussian test channel derived from the reverse water-filling theorem. Third, we extend our framework to include context modeling, and demonstrate that accurate mean prediction yields substantial entropy reduction. Unlike prior R-D estimators, our method provides a structurally interpretable perspective that aligns with real compression modules and enables fine-grained analysis. Through joint simulation and end-to-end training, we derive a tight and actionable approximation of the theoretical R-D limits, offering new insights into the design of more efficient learned compression systems.

</details>


### [28] [Regenerating codes with minimal disk I/O cost achieving optimal tradeoff between storage and repair bandwidth](https://arxiv.org/abs/2601.09300)
*Minhan Gao,Kenneth Shum*

Main category: cs.IT

TL;DR: 提出一种基于gammoids理论的编码方案，可在单节点故障时实现存储与修复带宽之间的最优权衡，并支持无限次节点修复。


<details>
  <summary>Details</summary>
Motivation: 分布式存储系统中需要平衡多个性能指标：修复带宽（网络资源）和磁盘I/O成本（辅助节点读取的数据量）。最优I/O成本要求修复时读取的数据包数量与发送的数据包数量相等，这种无编码修复模式计算开销最小。现有方案难以同时实现这些目标。

Method: 基于gammoids理论（一类特殊的图拟阵）设计编码方案。该方案在单节点故障且所有存活节点参与修复时，能够实现存储与修复带宽之间的所有最优权衡点。

Result: 证明了该方案可以在固定大小的有限域上实现存储与修复带宽之间的所有最优权衡点，并且能够容忍无限次的节点修复迭代。

Conclusion: 基于gammoids理论的编码方案能够同时实现无编码修复（最小化I/O成本和计算开销）和存储-修复带宽的最优权衡，为分布式存储系统提供了高效的修复机制。

Abstract: There are multiple performance metrics in the design of coding schemes for distributed storage systems. The first metric is called repair bandwidth, which measures the network resources required during the repair process. Another critical metric for repair efficiency is disk I/O cost, defined as the amount of data packets accessed at helper nodes to repair the failed node. In an encoding scheme with optimal I/O cost, the number of packets sent to the newcomer is exactly the same as the number of packets read from memory. This mode of repair is referred to as uncoded repair, as no coding operations are performed at the helper node. In addition to minimizing disk I/O cost, an uncoded repair mechanism has the advantage of incurring minimal computational overhead at the helper node. In this paper, we demonstrate that for single node failures, if all surviving nodes participate in the repair of the failed node, we can achieve all points on the fundamental tradeoff curve between storage and repair bandwidth. The design of the proposed encoding scheme is based on the theory of gammoids, a specialized class of graph-based matroids. We prove that this scheme can tolerate an unlimited number of node repair iterations over a field of fixed size.

</details>


### [29] [An Information Theoretic Proof of the Radon-Nikodym Theorem](https://arxiv.org/abs/2601.09308)
*Peter Harremoës*

Main category: cs.IT

TL;DR: 该论文讨论了Radon-Nikodym定理在信息论中的重要性，指出虽然该定理在测度论教材中常见，但在概率论和信息论教材中常被省略，因为证明被认为过于困难。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是强调Radon-Nikodym定理在信息论基础概念（如香农熵、f-散度等）定义中的核心作用，并指出当前教学中的一个问题：许多概率论和信息论教材省略了这个重要定理的证明，因为被认为过于困难。

Method: 论文采用理论分析的方法，通过讨论Radon-Nikodym定理在信息论中的具体应用，以及对比不同学科教材对该定理的处理方式，来阐明其重要性。

Result: 论文明确了Radon-Nikodym定理在信息论基础中的关键地位，揭示了当前教学中存在的一个空白：概率论和信息论教材往往省略这个重要定理的证明，可能导致学生对相关概念的理解不够深入。

Conclusion: Radon-Nikodym定理是信息论基础概念定义的核心，虽然其证明在测度论教材中常见，但在概率论和信息论教学中常被省略，这可能会影响学生对信息论基础的理解深度。

Abstract: The Radon-Nikodym theorem plays a significant role in the definition of Shannon entropy, f-divergences, and other basic quantities in information theory. The existence of Radon Nikodym derivates appear in many text books in measure theory but in text books on probability or information theory it is often omitted because the proof is often considered to be too difficult.

</details>


### [30] [Contraction of Rényi Divergences for Discrete Channels: Properties and Applications](https://arxiv.org/abs/2601.09328)
*Adrien Vandenbroucque,Amedeo Roberto Esposito,Michael Gastpar*

Main category: cs.IT

TL;DR: 本文研究了Rényi散度的强数据处理常数性质，与φ-散度进行对比，发现当α>1时收缩性质显著不同，并将∞-Rényi散度与ε-局部差分隐私关联，最后应用于马尔可夫链收敛速度分析。


<details>
  <summary>Details</summary>
Motivation: 研究Rényi散度的强数据处理常数性质，探索其与已深入研究的φ-散度之间的异同，特别是不同α阶数下的收缩特性差异。

Method: 通过理论分析Rényi散度的强数据处理常数，与φ-散度进行系统对比，特别关注不同α阶数下的收缩性质变化，并建立∞-Rényi散度与ε-局部差分隐私的关联。

Result: 发现当α>1时，Rényi散度的收缩性质与φ-散度存在显著差异；揭示了∞-Rényi散度的特定收缩特性及其与ε-局部差分隐私的关系；将结果应用于马尔可夫链收敛速度分析。

Conclusion: Rényi散度的收缩性质为分析马尔可夫链收敛速度提供了新视角，补充了文献中常见的L^α-范数收缩分析方法，特别是在α>1时展现出与φ-散度不同的特性。

Abstract: This work explores properties of Strong Data-Processing constants for Rényi Divergences. Parallels are made with the well-studied $\varphi$-Divergences, and it is shown that the order $α$ of Rényi Divergences dictates whether certain properties of the contraction of $\varphi$-Divergences are mirrored or not. In particular, we demonstrate that when $α>1$, the contraction properties can deviate quite strikingly from those of $\varphi$-Divergences. We also uncover specific characteristics of contraction for the $\infty$-Rényi Divergence and relate it to $\varepsilon$-Local Differential Privacy. The results are then applied to bound the speed of convergence of Markov chains, where we argue that the contraction of Rényi Divergences offers a new perspective on the contraction of $L^α$-norms commonly studied in the literature.

</details>


### [31] [Generalized Schalkwijk-Kailath Coding for Autoregressive Gaussian Channels](https://arxiv.org/abs/2601.09329)
*Jun Su,Guangyue Han,Shlomo Shamai*

Main category: cs.IT

TL;DR: 提出了一种用于AR(p)高斯信道的高斯随机编码方案SK(2)，推广了经典的Schalkwijk-Kailath编码方案，并给出了可达率的闭式表达式，证明SK方案并非普遍最优。


<details>
  <summary>Details</summary>
Motivation: 经典Schalkwijk-Kailath(SK)编码方案在特定高斯信道中表现优异，但需要验证其在更一般的AR(p)高斯信道中是否仍然保持最优性，解决Butman提出的猜想。

Method: 提出SK(2)高斯随机编码方案，作为经典SK方案的推广，适用于AR(p)高斯信道，通过构造性编码方法获得可达率的闭式表征。

Result: 获得了SK(2)编码方案的可达率闭式表达式，证明经典SK方案在AR(p)高斯信道中并非普遍最优，从而推翻了Butman的猜想。

Conclusion: SK(2)编码方案成功推广了经典SK方案，证明了SK方案在更一般的信道模型中并非最优，解决了长期存在的理论问题。

Abstract: We propose a Gaussian random coding scheme for AR($p$) Gaussian channels that generalizes the celebrated Schalkwijk-Kailath (SK) coding scheme. This constructive coding scheme, termed the SK(2) coding scheme, yields a closed-form characterization for the corresponding achievable rate. Among many others, this result shows that the celebrated SK coding scheme is not universally optimal, and therefore, disprove the conjecture proposed by Butman in \cite{butman1976linear}.

</details>


### [32] [A Constructive Method to Minimize the Index of Coincidence under Marginal Constraints](https://arxiv.org/abs/2601.09347)
*Pierre Jean-Claude Robert Bertrand*

Main category: cs.IT

TL;DR: 论文提出了一种最小化联合分布重合指数的构造性方法，解决了边际约束下该优化问题的一般情况。


<details>
  <summary>Details</summary>
Motivation: 重合指数最小化在信息论中有多个应用，但现有闭式解仅适用于强可行性条件，而该条件在实践中很少满足。

Method: 首先证明强可行性条件的适用性随维度增长而消失，然后分析最优耦合的结构特征（单调阶梯状零值模式），基于此提出显式迭代构造算法。

Result: 证明了迭代构造在有限步内收敛到最小化解，为重合指数最小化问题提供了完整的构造性解决方案。

Conclusion: 论文解决了边际约束下重合指数最小化的一般情况，提供了实用的构造性算法，填补了现有理论仅适用于理想条件的空白。

Abstract: We consider the problem of minimizing the index of coincidence of a joint distribution under fixed marginal constraints. This objective is motivated by several applications in information theory, where the index of coincidence naturally arises. A closed-form solution is known when the marginals satisfy a strong feasibility condition, but this condition is rarely met in practice. We first show that the measure of the set of marginals for which condition applies vanishes as the dimension grows. We then characterize the structure of the optimal coupling in the general case, proving that it exhibits a monotone staircase of zero entries. Based on this structure, we propose an explicit iterative construction and prove that it converges in finitely many steps to a minimizer. Main result of the paper is a complete constructive solution of index-of-coincidence minimization.

</details>


### [33] [Asymptotic Rate Bounds and Constructions for the Inclusive Variant of Disjunct Matrices](https://arxiv.org/abs/2601.09362)
*Yuto Mizunuma,Yuichiro Fujiwara*

Main category: cs.IT

TL;DR: 本文首次建立了包含性分离矩阵的渐近正速率下界，匹配了已知上界（相差对数因子），为抑制剂复杂模型下的鲁棒可扩展群组测试提供了理论保证。


<details>
  <summary>Details</summary>
Motivation: 包含性分离矩阵在抑制剂复杂模型下的群组测试中具有重要应用价值，但其渐近行为一直未被充分研究，特别是能否实现渐近正速率这一可扩展设计的关键问题尚未解决。

Method: 采用概率方法证明渐近下界，并基于此提出了简单高效的随机构造；进一步通过去随机化技术获得了确定性多项式时间构造。

Result: 首次建立了包含性分离矩阵的非平凡渐近下界，该下界与已知最强上界仅相差对数因子；同时提供了随机和确定性两种构造方法。

Conclusion: 该研究阐明了在一般抑制剂复杂模型下鲁棒可扩展群组测试的渐近潜力，为实际应用提供了理论基础和构造方法。

Abstract: Disjunct matrices, also known as cover-free families and superimposed codes, are combinatorial arrays widely used in group testing. Among their variants, those that satisfy an additional combinatorial property called inclusiveness form a special class suitable for computationally efficient and highly error-tolerant group testing under the general inhibitor complex model, a broad framework that subsumes practical settings such as DNA screening. Despite this relevance, the asymptotic behavior of the inclusive variant of disjunct matrices has remained largely unexplored. In particular, it was not previously known whether this variant can achieve an asymptotically positive rate, a requirement for scalable group testing designs. In this work, we establish the first nontrivial asymptotic lower bound on the maximum achievable rate of the inclusive variant, which matches the strongest known upper bound up to a logarithmic factor. Our proof is based on the probabilistic method and yields a simple and efficient randomized construction. Furthermore, we derandomize this construction to obtain a deterministic polynomial-time construction. These results clarify the asymptotic potential of robust and scalable group testing under the general inhibitor complex model.

</details>


### [34] [On Decoding First- and Second-Order BiD Codes](https://arxiv.org/abs/2601.09390)
*Devansh Jain,Lakshmi Prasad Natarajan*

Main category: cs.IT

TL;DR: 本文提出了一阶BiD码的快速最大似然和max-log-MAP解码器，并针对二阶BiD码设计了性能接近ML解码器的置信传播解码器


<details>
  <summary>Details</summary>
Motivation: BiD码是一种新的代数码族，在比特MAP解码下能达到擦除信道容量，且具有比Reed-Muller码更大的最小距离。然而，需要开发高效的解码算法来充分发挥其性能优势。

Method: 1. 为一阶BiD码设计快速最大似然(ML)和max-log-MAP解码器
2. 识别二阶BiD码的最小权重奇偶校验
3. 确定二阶BiD码的"投影"特性（类似于RM码文献中的概念）
4. 基于这些结果设计置信传播解码器

Result: 1. 成功设计了一阶BiD码的高效解码算法
2. 对于块长度为81和243的二阶BiD码，置信传播解码器的性能在最大似然解码器的1 dB范围内

Conclusion: 本文为BiD码开发了有效的解码算法，特别是为二阶BiD码设计的置信传播解码器在性能上接近最优的最大似然解码器，为BiD码的实际应用奠定了基础。

Abstract: BiD codes, which are a new family of algebraic codes of length $3^m$, achieve the erasure channel capacity under bit-MAP decoding and offer asymptotically larger minimum distance than Reed-Muller (RM) codes. In this paper we propose fast maximum-likelihood (ML) and max-log-MAP decoders for first-order BiD codes. For second-order codes, we identify their minimum-weight parity checks and ascertain a code property known as 'projection' in the RM coding literature. We use these results to design a belief propagation decoder that performs within 1 dB of ML decoder for block lengths 81 and 243.

</details>


### [35] [A Generalized Leakage Interpretation of Alpha-Mutual Information](https://arxiv.org/abs/2601.09406)
*Akira Kamatsuka,Takahiro Yoshida*

Main category: cs.IT

TL;DR: 该论文提出了α-互信息在广义g-泄露框架下的统一解释，将α参数解释为对手的风险厌恶度量


<details>
  <summary>Details</summary>
Motivation: 现有信息流量化框架中，α-互信息缺乏统一的解释。论文旨在在广义决策问题的对抗性框架下，为α-互信息提供新的解释，特别是将α参数与对手的风险态度联系起来。

Method: 采用基于对抗性广义决策问题的扩展框架，利用Kolmogorov-Nagumo均值和q-对数来表征对抗性收益，在该框架下重新解释α-互信息。

Result: 成功建立了α-互信息与广义g-泄露的统一联系，证明了α参数可以解释为对手风险厌恶程度的度量，为信息流量化提供了新的理论视角。

Conclusion: 该研究为α-互信息提供了在对抗性决策框架下的统一解释，将信息论概念与风险分析联系起来，为信息安全领域的量化分析提供了新的理论基础。

Abstract: This paper presents a unified interpretation of $α$-mutual information ($α$-MI) in terms of generalized $g$-leakage. Specifically, we present a novel interpretation of $α$-MI within an extended framework for quantitative information flow based on adversarial generalized decision problems. This framework employs the Kolmogorov-Nagumo mean and the $q$-logarithm to characterize adversarial gain. Furthermore, we demonstrate that, within this framework, the parameter $α$ can be interpreted as a measure of the adversary's risk aversion.

</details>


### [36] [Dobrushin Coefficients of Private Mechanisms Beyond Local Differential Privacy](https://arxiv.org/abs/2601.09498)
*Leonhard Grosse,Sara Saeidian,Tobias J. Oechtering,Mikael Skoglund*

Main category: cs.IT

TL;DR: 研究具有有界逐点最大泄漏(PML)的离散马尔可夫核的Dobrushin系数，该定义在c→0时恢复局部差分隐私(LDP)，推导收缩界并提供实现机制，扩展到f-散度


<details>
  <summary>Details</summary>
Motivation: 研究具有有界逐点最大泄漏(PML)的离散马尔可夫核的收缩性质，该框架扩展了局部差分隐私(LDP)的概念，允许分析更广泛的隐私机制

Method: 使用Dobrushin系数分析离散马尔可夫核的收缩性质，推导基于PML保证的收缩界，构造实现这些界的机制，并通过Binette不等式扩展到f-散度

Result: 得到了PML机制的可实现收缩界，构造了达到这些界的机制，对LDP机制得到更紧的界，并将结果扩展到任意离散核和f-散度

Conclusion: 该工作建立了PML框架下离散马尔可夫核收缩性的系统分析，为隐私机制设计提供了理论保证，扩展了LDP分析的范围

Abstract: We investigate Dobrushin coefficients of discrete Markov kernels that have bounded pointwise maximal leakage (PML) with respect to all distributions with a minimum probability mass bounded away from zero by a constant $c>0$. This definition recovers local differential privacy (LDP) for $c\to 0$. We derive achievable bounds on contraction in terms of a kernels PML guarantees, and provide mechanism constructions that achieve the presented bounds. Further, we extend the results to general $f$-divergences by an application of Binette's inequality. Our analysis yields tighter bounds for mechanisms satisfying LDP and extends beyond the LDP regime to any discrete kernel.

</details>


### [37] [Error Exponents for Randomised List Decoding](https://arxiv.org/abs/2601.09519)
*Henrique K. Miyamoto,Sheng Yang*

Main category: cs.IT

TL;DR: 研究随机列表解码的随机编码错误指数，分析固定列表大小和指数增长列表大小两种机制下的性能


<details>
  <summary>Details</summary>
Motivation: 探索随机列表解码在错误指数方面的理论性能，特别是当解码器根据码字解码度量概率随机选择L个消息时的性能界限

Method: 分析两种机制：固定列表大小下推导集合紧致的随机编码错误指数；列表大小随块长度指数增长时提供非平凡下界

Result: 对于固定列表大小，推导出集合紧致的随机编码错误指数，但匹配度量下不改善普通解码的错误指数；对于指数增长列表大小，在高码率下获得紧致的非平凡下界

Conclusion: 随机列表解码在固定列表大小下对匹配度量无性能提升，但在列表大小指数增长时在高码率下能获得紧致的错误指数下界

Abstract: This paper studies random-coding error exponents of randomised list decoding, in which the decoder randomly selects $L$ messages with probabilities proportional to the decoding metric of the codewords. The exponents (or bounds) are given for mismatched, and then particularised to matched and universal decoding metrics. Two regimes are studied: for fixed list size, we derive an ensemble-tight random-coding error exponent, and show that, for the matched metric, it does not improve the error exponent of ordinary decoding. For list sizes growing exponentially with the block-length, we provide a non-trivial lower bound to the error exponent that is tight at high rates under the matched metric.

</details>


### [38] [A Finite-Sample Strong Converse for Binary Hypothesis Testing via (Reverse) Rényi Divergence](https://arxiv.org/abs/2601.09550)
*Roberto Bruno,Adrien Vandenbroucque,Amedeo Roberto Esposito*

Main category: cs.IT

TL;DR: 该论文研究了有限样本下非对称错误约束的二元假设检验问题，利用"反向"Rényi散度推导了新型非渐近界，建立了强逆定理，并展示了在Type I错误指数衰减时Type II错误的精确行为。


<details>
  <summary>Details</summary>
Motivation: 现有有限样本假设检验结果存在局限性，特别是在非对称错误约束下缺乏精确的收敛行为分析。作者旨在改进现有有限样本界，建立更严格的逆定理，并精确刻画Type I和Type II错误之间的权衡关系。

Method: 采用"反向"Rényi散度作为分析工具，推导非渐近的Type II错误概率界限。通过数学分析建立强逆定理，并研究当Type I错误以指数速率c衰减时Type II错误的精确收敛行为。

Result: 1) 提出了基于反向Rényi散度的新型非渐近界；2) 建立了强逆定理；3) 证明了当c > D(P₁∥P₀)时Type II错误指数收敛到1，当c < D(P₁∥P₀)时指数收敛到0；4) 数值实验显示所提逆界严格优于现有有限样本结果。

Conclusion: 该工作为有限样本假设检验提供了更精确的理论框架，反向Rényi散度是分析非对称错误约束的有效工具，所得结果改进了现有文献中的有限样本界，并揭示了错误概率之间的精确权衡关系。

Abstract: This work investigates binary hypothesis testing between $H_0\sim P_0$ and $H_1\sim P_1$ in the finite-sample regime under asymmetric error constraints. By employing the ``reverse" Rényi divergence, we derive novel non-asymptotic bounds on the Type II error probability which naturally establish a strong converse result. Furthermore, when the Type I error is constrained to decay exponentially with a rate $c$, we show that the Type II error converges to 1 exponentially fast if $c$ exceeds the Kullback-Leibler divergence $D(P_1\|P_0)$, and vanishes exponentially fast if $c$ is smaller. Finally, we present numerical examples demonstrating that the proposed converse bounds strictly improve upon existing finite-sample results in the literature.

</details>


### [39] [On Linear Estimators for some Stable Vectors](https://arxiv.org/abs/2601.09554)
*Rayan Chouity,Charbel Hannoun,Jihad Fahs,Ibrahim Abou-Faycal*

Main category: cs.IT

TL;DR: 论文研究了联合稳定随机变量的估计问题，证明了在两种依赖模型下条件均值估计器都是线性的，并找到了最优线性估计器。


<details>
  <summary>Details</summary>
Motivation: 研究联合稳定随机变量的估计问题，特别是在非高斯稳定分布下的线性估计性质，扩展高斯分布中的经典结果。

Method: 考虑两种依赖模型：1) 两个独立稳定变量的线性变换；2) 次高斯对称α稳定向量。分析条件均值估计器的性质，并寻找最优线性估计器。

Result: 在这两种模型下，条件均值估计器都是线性的。对于次高斯对称α稳定向量，条件均值估计器与最优线性估计器相同，这推广了高斯分布中条件均值是最优线性最小均方误差估计器的经典结果。

Conclusion: 论文证明了在联合稳定随机变量的两种依赖模型中，线性估计具有良好性质，特别是次高斯对称α稳定向量的结果推广了高斯分布中的经典结论。

Abstract: We consider the estimation problem for jointly stable random variables. Under two specific dependency models: a linear transformation of two independent stable variables and a sub-Gaussian symmetric $α$-stable (S$α$S) vector, we show that the conditional mean estimator is linear in both cases. Moreover, we find dispersion optimal linear estimators. Interestingly, for the sub-Gaussian (S$α$S) vector, both estimators are identical generalizing the well-known Gaussian result of the conditional mean being the best linear minimum-mean square estimator.

</details>


### [40] [The Spectral Representations Of The Simple Hypothesis Testing Problem](https://arxiv.org/abs/2601.09564)
*Barış Nakiboğlu*

Main category: cs.IT

TL;DR: 论文研究了假设检验问题中II类错误概率（体积）关于I类错误概率（体积）的凸共轭（Legendre变换），推导了原始熵谱的表达式，并利用该对偶表征得到了乘积测度情况下的最新界。


<details>
  <summary>Details</summary>
Motivation: 研究假设检验中I类错误和II类错误之间的基本关系，通过凸共轭变换建立对偶表征，为分析假设检验性能提供新的理论工具。

Method: 利用似然比分位数的性质，推导随机检测器情况下II类错误体积的凸共轭（原始熵谱），通过标准谱恒等式将其表示为似然比互补分布函数的积分。对于乘积测度情况，结合Berry-Esseen定理和高斯Mills比性质进行分析。

Result: 得到了II类错误体积凸共轭的解析表达式（原始熵谱），建立了对偶表征，并推导出乘积测度情况下具有最新性能的误差界，包括有倾斜和无倾斜两种情况。

Conclusion: 通过凸共轭变换成功建立了假设检验中I类和II类错误之间的对偶关系，为分析假设检验性能提供了新的理论框架，并在乘积测度情况下得到了最优的误差界。

Abstract: The convex conjugate (i.e., the Legendre transform) of Type II error probability (volume) as a function of Type I error probability (volume) is determined for the hypothesis testing problem with randomized detectors. The derivation relies on properties of likelihood ratio quantiles and is general enough to extend to the case of $σ$-finite measures in all non-trivial cases. The convex conjugate of the Type II error volume, called the primitive entropy spectrum, is expressed as an integral of the complementary distribution function of the likelihood ratio using a standard spectral identity. The resulting dual characterization of the Type II error volume leads to state of the art bounds for the case of product measures via Berry--Esseen theorem through a brief analysis relying on properties of the Gaussian Mills ratio, both with and without tilting.

</details>


### [41] [On the Error Probability of RPA Decoding of Reed-Muller Codes over BMS Channels](https://arxiv.org/abs/2601.09581)
*Dorsa Fathollahi,V. Arvind Rameshwar,V. Lalitha*

Main category: cs.IT

TL;DR: 本文分析了RPA解码器在一般BMS信道下对RM码的性能，将先前仅针对BSC信道的结果推广到所有BMS信道，证明了在码率较低时能达到渐近消失的错误概率。


<details>
  <summary>Details</summary>
Motivation: 先前的研究仅证明了RPA解码器在二进制对称信道（BSC）下对低码率RM码能达到渐近消失的错误概率。本文旨在将这一结果推广到更一般的二进制无记忆对称（BMS）信道，消除对信道的限制性假设。

Method: 通过建立RPA投影操作与极化码"信道组合"阶段的等价关系，避免了额外假设。利用这种等价性，对一阶RM码（RPA解码器的"基础情况"）在最大似然解码下的错误概率应用通用的联合界，该界适用于所有BMS信道。

Result: 证明了对于所有BMS信道，当RM码的阶数大致按log log n增长时（其中n为码长），RPA解码器在n趋于无穷大时能达到渐近消失的错误概率。

Conclusion: 本文成功将RPA解码器的性能分析从BSC信道推广到所有BMS信道，证明了在低码率条件下，该解码器对RM码具有普适的渐近性能保证。

Abstract: We analyze the performance of the Recursive Projection-Aggregation (RPA) decoder of Ye and Abbe (2020), for Reed-Muller (RM) codes, over general binary memoryless symmetric (BMS) channels. Our work is a significant generalization of a recent result of Rameshwar and Lalitha (2025) that showed that the RPA decoder provably achieves vanishing error probabilities for "low-rate" RM codes, over the binary symmetric channel (BSC). While a straightforward generalization of the proof strategy in that paper will require additional, restrictive assumptions on the BMS channel, our technique, which employs an equivalence between the RPA projection operation and a part of the "channel combining" phase in polar codes, requires no such assumptions. Interestingly, such an equivalence allows for the use of a generic union bound on the error probability of the first-order RM code (the "base case" of the RPA decoder), under maximum-likelihood decoding, which holds for any BMS channel. We then exploit these observations in the proof strategy outlined in the work of Rameshwar and Lalitha (2025), and argue that, much like in the case of the BSC, one can obtain vanishing error probabilities, in the large $n$ limit (where $n$ is the blocklength), for RM orders that scale roughly as $\log \log n$, for all BMS channels.

</details>


### [42] [Secret sharing with additive access structures from correlated random variables](https://arxiv.org/abs/2601.09640)
*David Miller,Rémi A. Chou*

Main category: cs.IT

TL;DR: 将基于相关随机性和公共通信的秘密共享模型推广到支持动态访问结构序列（称为加法访问结构），证明存在策略在每个时间步达到与固定访问结构模型相同的秘密速率，并在阈值访问结构时达到容量


<details>
  <summary>Details</summary>
Motivation: 传统秘密共享模型针对固定访问结构设计，但实际应用中访问结构可能随时间动态变化。需要将现有模型推广到支持动态访问结构序列，其中访问结构可以单调增长，且分发者仅在变化发生时才知道这些变化

Method: 提出加法访问结构模型，允许访问结构随时间单调增长（参与者子集可被添加）。基于相关随机性和公共通信的框架，证明存在秘密共享策略能够处理这种动态变化

Result: 证明存在策略在每个时间步达到与固定访问结构模型相同的秘密速率；当访问结构为阈值访问结构时，存在容量达到策略

Conclusion: 成功将秘密共享模型推广到动态访问结构场景，保持了与固定结构相同的性能，特别在阈值访问结构下能够达到容量极限

Abstract: We generalize secret-sharing models that rely on correlated randomness and public communication, originally designed for a fixed access structure, to support a sequence of dynamic access structures, which we term an Additive Access Structure. Specifically, the access structure is allowed to monotonically grow by having any subset of participants added to it at a given time step, and the dealer only learns of these changes to the access structure on the time step that they occur. For this model, we prove the existence of a secret sharing strategy that achieves the same secret rate at each time step as the best known strategy for the fixed access structure version of this model. We also prove that there exists a strategy that is capacity-achieving at any time step where the access structure is a threshold access structure.

</details>


### [43] [Counting and Entropy Bounds for Structure-Avoiding Spatially-Coupled LDPC Constructions](https://arxiv.org/abs/2601.09674)
*Lei Huang*

Main category: cs.IT

TL;DR: 本文量化了QC-SC-LDPC码设计空间的大小和结构，推导了满足结构避免约束的划分矩阵数量的显式下界，并提供了MT算法输出解多样性的可计算下界。


<details>
  <summary>Details</summary>
Motivation: 设计具有低错误平层的大耦合存储QC-SC-LDPC码需要消除由边扩展和提升引起的特定有害子结构（如短环）。需要量化可行设计空间的大小和结构。

Method: 基于Clique Lovász Local Lemma (CLLL)设计原则和Moser-Tardos (MT)构造方法，使用定量CLLL推导满足结构避免约束的划分矩阵数量的显式下界，通过Rényi熵界分析MT分布，提供MT算法输出解多样性的可计算下界。

Result: 获得了划分矩阵数量的下界、行/列置换下非等价解数量的界限，以及MT算法输出不同解数量的可计算下界。针对消除4环的特殊情况，得到了系统参数函数的闭式界。

Conclusion: 该工作为QC-SC-LDPC码设计提供了量化框架，为随机化构造提供了具体的多样性保证，并为内存/提升参数的选择和剩余搜索空间的估计提供了原则性方法。

Abstract: Designing large coupling memory quasi-cyclic spatially-coupled LDPC (QC-SC-LDPC) codes with low error floors requires eliminating specific harmful substructures (e.g., short cycles) induced by edge spreading and lifting. Building on our work~\cite{r15} that introduced a Clique Lovász Local Lemma (CLLL)-based design principle and a Moser--Tardos (MT)-type constructive approach, this work quantifies the size and structure of the feasible design space. Using the quantitative CLLL, we derive explicit lower bounds on the number of partition matrices satisfying a given family of structure-avoidance constraints, and further obtain bounds on the number of non-equivalent solutions under row/column permutations. Moreover, via Rényi-entropy bounds for the MT distribution, we provide a computable lower bound on the number of distinct solutions that the MT algorithm can output, giving a concrete diversity guarantee for randomized constructions. Specializations for eliminating 4-cycle candidates yield closed-form bounds as functions of system parameters, offering a principled way to size memory/lifting and to estimate the remaining search space.

</details>


### [44] [Progress on the Courtade-Kumar Conjecture: Optimal High-Noise Entropy Bounds and Generalized Coordinate-wise Mutual Information](https://arxiv.org/abs/2601.09679)
*Adel Javanmard,David P. Woodruff*

Main category: cs.IT

TL;DR: 本文解决了Courtade-Kumar猜想相关的两个重要问题：1) 证明了任意布尔函数（无论偏置）的输出与各噪声输入坐标的互信息之和的上界为1-H(α)；2) 在高噪声区域改进了渐近熵展开的误差界至O(λ²)，扩展了猜想成立的噪声参数范围。


<details>
  <summary>Details</summary>
Motivation: Courtade-Kumar猜想认为独裁函数在布尔超立方体上最大化函数输出与其噪声输入之间的互信息。该猜想在信息论和布尔函数分析中具有重要意义，但先前结果仅限于平衡布尔函数，且在高噪声区域的误差界不够精确。

Method: 1) 通过数学证明解决了Courtade和Kumar提出的开放问题，将互信息和的上界结果推广到任意偏置的布尔函数；2) 在高噪声区域采用精化分析，推导出渐近熵展开的最优误差界O(λ²)，其中λ=(1-2α)²，改进了先前已知的界。

Result: 1) 证明了对于任意布尔函数，输出与各噪声输入坐标的互信息之和≤1-H(α)，其中α是二进制对称信道的噪声参数；2) 获得了渐近熵展开的O(λ²)最优误差界，这导致了对高信息量函数的尖锐线性傅里叶集中界，并显著扩展了猜想成立的噪声参数λ的范围。

Conclusion: 本文在Courtade-Kumar猜想的研究中取得了两个重要进展：将互信息和的上界推广到所有布尔函数，并在高噪声区域获得了最优误差界，为最终解决该猜想提供了重要工具，扩展了布尔函数分析与信息论交叉领域的研究边界。

Abstract: The Courtade-Kumar conjecture posits that dictatorship functions maximize the mutual information between the function's output and a noisy version of its input over the Boolean hypercube. We present two significant advancements related to this conjecture. First, we resolve an open question posed by Courtade and Kumar, proving that for any Boolean function (regardless of bias), the sum of mutual information between the function's output and the individual noisy input coordinates is bounded by $1-H(α)$, where $α$ is the noise parameter of the Binary Symmetric Channel. This generalizes their previous result which was restricted to balanced Boolean functions. Second, we advance the study of the main conjecture in the high noise regime. We establish an optimal error bound of $O(λ^2)$ for the asymptotic entropy expansion, where $λ= (1-2α)^2$, improving upon the previous best-known bounds. This refined analysis leads to a sharp, linear Fourier concentration bound for highly informative functions and significantly extends the range of the noise parameter $λ$ for which the conjecture is proven to hold.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [45] [An Almost-Optimal Upper Bound on the Push Number of the Torus Puzzle](https://arxiv.org/abs/2601.08989)
*Matteo Caporrella,Stefano Leucci*

Main category: cs.DS

TL;DR: 本文研究了Torus Puzzle的推数(push number)问题，提出了一种在更严格模型下使用O(mn·log max{m,n})单位旋转求解的算法，将推数上界从O(mn·max{m,n})改进到O(mn·log max{m,n})。


<details>
  <summary>Details</summary>
Motivation: Torus Puzzle的推数(push number)问题目前理解不足，已知上界为O(mn·max{m,n})，而下界为Ω(mn)，存在Θ(max{m,n})的差距。本文旨在缩小这一差距。

Method: 提出了一种在比原始谜题更严格模型下的算法，该算法使用O(mn·log max{m,n})单位旋转来求解Torus Puzzle。这种更严格的模型意味着算法结果可以直接应用于原始谜题的推数上界。

Result: 将推数的上界从O(mn·max{m,n})改进到O(mn·log max{m,n})，将已知上下界之间的差距从Θ(max{m,n})缩小到Θ(log max{m,n})。

Conclusion: 通过设计更高效的算法，显著改进了Torus Puzzle推数的上界，缩小了与下界之间的理论差距，为这一组合优化问题提供了更好的理论理解。

Abstract: We study the Torus Puzzle, a solitaire game in which the elements of an input $m \times n$ matrix need to be rearranged into a target configuration via a sequence of unit rotations (i.e., circular shifts) of rows and/or columns. Amano et al.\ proposed a more permissive variant of the above puzzle, where each row and column rotation can shift the involved elements by any amount of positions. The number of rotations needed to solve the puzzle in the original and in the permissive variants of the puzzle are respectively known as the \emph{push number} and the \emph{drag number}, where the latter is always smaller than or equal to the former and admits an existential lower bound of $Ω(mn)$. While this lower bound is matched by an $O(mn)$ upper bound, the push number is not so well understood. Indeed, to the best of our knowledge, only an $O(mn \cdot \max\{ m, n \})$ upper bound is currently known. In this paper, we provide an algorithm that solves the Torus Puzzle using $O(mn \cdot \log \max \{m, n\})$ unit rotations in a model that is more restricted than that of the original puzzle. This implies a corresponding upper bound on the push number and reduces the gap between the known upper and lower bounds from $Θ(\max\{m,n\})$ to $Θ(\log \max\{m, n\})$.

</details>


### [46] [A Grouped Sorting Queue Supporting Dynamic Updates for Timer Management in High-Speed Network Interface Cards](https://arxiv.org/abs/2601.09081)
*Zekun Wang,Binghao Yue,Weitao Pan,Jianyi Shi,Yue Hao*

Main category: cs.DS

TL;DR: 本文提出支持更新和分组排序操作的硬件优先级队列，用于网络接口卡(NIC)的定时器管理，相比现有设计显著提升了性能和资源效率。


<details>
  <summary>Details</summary>
Motivation: 随着网络功能硬件卸载，网络接口卡需要处理大量有状态、高精度、高吞吐量的任务，定时器成为关键组件。现有定时器管理方案存在软件负载重、精度低、缺乏硬件更新支持和溢出问题。

Method: 提出两种新颖的优先级队列操作：更新操作和分组排序。更新操作通过基本操作的组合和传播来修改队列中元素的优先级；分组排序机制通过建立组边界优先级来改变排序过程和元素插入位置，确保溢出后的正确计时行为。采用一维脉动阵列和移位寄存器的混合架构实现。

Result: 4K深度、16位定时器队列在28nm工艺下达到500MHz以上（175Mpps，12ns精度），在FPGA上达到300MHz以上（116Mpps）。相比现有设计，LUT和FF使用量分别减少31%和25%。

Conclusion: 提出的硬件优先级队列设计有效解决了现有定时器管理方案的问题，为网络接口卡提供了高性能、高精度、低资源消耗的定时器管理解决方案。

Abstract: With the hardware offloading of network functions, network interface cards (NICs) undertake massive stateful, high-precision, and high-throughput tasks, where timers serve as a critical enabling component. However, existing timer management schemes suffer from heavy software load, low precision, lack of hardware update support, and overflow. This paper proposes two novel operations for priority queues--update and group sorting--to enable hardware timer management. To the best of our knowledge, this work presents the first hardware priority queue to support an update operation through the composition and propagation of basic operations to modify the priorities of elements within the queue. The group sorting mechanism ensures correct timing behavior post-overflow by establishing a group boundary priority to alter the sorting process and element insertion positions. Implemented with a hybrid architecture of a one-dimension (1D) systolic array and shift registers, our design is validated through packet-level simulations for flow table timeout management. Results demonstrate that a 4K-depth, 16-bit timer queue achieves over 500 MHz (175 Mpps, 12 ns precision) in a 28nm process and over 300 MHz (116 Mpps) on an FPGA. Critically, it reduces LUTs and FFs usage by 31% and 25%, respectively, compared to existing designs.

</details>


### [47] [Dynamic Hierarchical $j$-Tree Decomposition and Its Applications](https://arxiv.org/abs/2601.09139)
*Gramoz Goranci,Monika Henzinger,Peter Kiss,Ali Momeni,Gernot Zöcklein*

Main category: cs.DS

TL;DR: 提出动态层次分解框架，在边更新的有容量无向图上实现多对数近似比和亚线性更新时间的割优化问题算法


<details>
  <summary>Details</summary>
Motivation: 针对动态图中割优化问题（如最小割、稀疏割等）需要高效动态算法，现有方法难以同时实现多对数近似比和亚线性更新时间的挑战

Method: 开发动态层次分解框架，维护Madry的层次j-树分解变体，核心是能处理顶点分裂的动态割稀疏化算法，基于Abraham等人的森林打包方法并加入新结构洞察

Result: 实现多对数近似比和O(n^ε)摊销更新时间，为稀疏割、多路割、多割等问题首次提供亚线性时间内多对数近似的完全动态算法

Conclusion: 提出的动态层次分解框架为割优化问题提供了新的近似与时间权衡，动态割稀疏化构造具有独立研究价值

Abstract: We develop a new algorithmic framework for designing approximation algorithms for cut-based optimization problems on capacitated undirected graphs that undergo edge insertions and deletions. Specifically, our framework dynamically maintains a variant of the hierarchical $j$-tree decomposition of [Madry FOCS'10], achieving a poly-logarithmic approximation factor to the graph's cut structure and supporting edge updates in $O(n^ε)$ amortized update time, for any arbitrarily small constant $ε\in (0,1)$.
  Consequently, we obtain new trade-offs between approximation and update/query time for fundamental cut-based optimization problems in the fully dynamic setting, including all-pairs minimum cuts, sparsest cut, multi-way cut, and multi-cut. For the last three problems, these trade-offs give the first fully-dynamic algorithms achieving poly-logarithmic approximation in sub-linear time per operation.
  The main technical ingredient behind our dynamic hierarchy is a dynamic cut-sparsifier algorithm that can handle vertex splits with low recourse. This is achieved by white-boxing the dynamic cut sparsifier construction of [Abraham et al. FOCS'16], based on forest packing, together with new structural insights about the maintenance of these forests under vertex splits. Given the versatility of cut sparsification in both the static and dynamic graph algorithms literature, we believe this construction may be of independent interest.

</details>


### [48] [Computational Complexity of Swish](https://arxiv.org/abs/2601.09289)
*Takashi Horiyama,Takehiro Ito,Jun Kawahara,Shin-ichi Minato,Akira Suzuki,Ryuhei Uehara,Yutaro Yamaguchi*

Main category: cs.DS

TL;DR: Swish卡牌游戏在两符号每卡情况下是NP完全的，解决了之前开放的问题，完成了该游戏计算复杂度的完整分类


<details>
  <summary>Details</summary>
Motivation: 解决Swish卡牌游戏在两符号每卡情况下的计算复杂度问题，这是原始游戏的情况，之前的研究只解决了一符号和三符号的情况

Method: 证明当允许单次翻转或180度旋转时是NP难的，扩展到允许所有三种变换的原始设置；当不允许变换时提供多项式时间算法

Result: Swish在两符号每卡情况下是NP完全的，结合已知结果完成了对Swish计算复杂度的完整分类

Conclusion: 完成了Swish游戏计算复杂度的完整分类：一符号每卡是多项式时间可解，两符号每卡是NP完全，三符号或更多每卡也是NP完全

Abstract: Swish is a card game in which players are given cards having symbols (hoops and balls), and find a valid superposition of cards, called a "swish." Dailly, Lafourcade, and Marcadet (FUN 2024) studied a generalized version of Swish and showed that the problem is solvable in polynomial time with one symbol per card, while it is NP-complete with three or more symbols per card. In this paper, we resolve the previously open case of two symbols per card, which corresponds to the original game. We show that Swish is NP-complete for this case. Specifically, we prove the NP-hardness when the allowed transformations of cards are restricted to a single (horizontal or vertical) flip or 180-degree rotation, and extend the results to the original setting allowing all three transformations. In contrast, when neither transformation is allowed, we present a polynomial-time algorithm. Combining known and our results, we establish a complete characterization of the computational complexity of Swish with respect to both the number of symbols per card and the allowed transformations.

</details>


### [49] [Engineering Compressed Matrix Multiplication with the Fast Walsh-Hadamard Transform](https://arxiv.org/abs/2601.09477)
*Joel Andersson,Matti Karppa*

Main category: cs.DS

TL;DR: 实现了Pagh的压缩矩阵乘法算法，通过FFT或FWHT构建矩阵草图来估计矩阵乘积，在乘积矩阵稀疏或少数元素占主导时性能优于DGEMM，FWHT版本比FFT快4倍，比MKL DGEMM快40倍。


<details>
  <summary>Details</summary>
Motivation: 传统矩阵乘法（如DGEMM）在处理稀疏或少数大元素占主导的乘积矩阵时效率不高，需要更高效的算法来利用这种结构特性。

Method: 实现Pagh的压缩矩阵乘法算法，使用随机草图技术估计矩阵乘积。采用FFT进行快速多项式乘法，并创新性地用FWHT替代FFT，保持相同正确性和方差保证。开发了精心优化的多线程CPU实现。

Result: FWHT版本比FFT版本快4倍；在乘积矩阵稀疏或少数大元素占主导时，比Intel MKL的DGEMM快40倍，且估计误差概率低。提供了开源实现和NumPy兼容的Python绑定。

Conclusion: 压缩矩阵乘法算法在实际应用中可行，FWHT替代FFT能显著提升性能。该算法在特定矩阵结构下优于传统方法，为稀疏或非均匀矩阵乘法提供了高效解决方案。

Abstract: We present an implementation of Pagh's compressed matrix multiplication algorithm, a randomized algorithm that constructs sketches of matrices to compute an unbiased estimate of their product. By leveraging fast polynomial multiplication via the FFT, the algorithm achieves high performance when the product matrix is sparse or contains only a small number of entries with magnitudes significantly larger than the rest. We show empirically that the algorithm is practical and can outperform state-of-the-art DGEMM implementations when the product matrix has few nonzero entries or is otherwise dominated by a small subset of elements with large magnitude. As a minor theoretical contribution, we replace the FFT with the Fast Walsh-Hadamard Transform (FWHT) in sketched multiplication, preserving all correctness and variance guarantees of the original algorithm.
  Experiments with our carefully engineered multithreaded CPU implementation for dense double-precision matrices on 64-core CPU nodes across a range of synthetic benchmarks, exhibiting variable sparsity patterns, show that the FWHT variant is up to 4 times faster than the FFT-based version. Under favorable sparsity and magnitude patterns in the product matrix, our FWHT-based implementation achieves a speedup of up to 40 over DGEMM from Intel MKL, with low probability of error in the estimates. Our implementation is released as free software and comes with NumPy-compatible Python bindings.

</details>


### [50] [How many users have been here for a long time? Efficient solutions for counting long aggregated visits](https://arxiv.org/abs/2601.09489)
*Peyman Afshani,Rezaul Chowdhury,Inge Li Gørtz,Mayank Goswami,Francesco Silvestri,Mariafiore Tognon*

Main category: cs.DS

TL;DR: 本文研究Counting Long Aggregated Visits问题：给定n个用户和m个区域，每个用户在区域中花费时间，查询包含r个区域，任务是统计在这些查询区域中总时间至少为k的不同用户数量。提出了多种精确和近似数据结构，以及下界分析。


<details>
  <summary>Details</summary>
Motivation: 该问题源于大规模移动数据分析中的查询需求，需要高效统计在多个区域中累计停留时间达到阈值的用户数量，这在位置数据分析、用户行为研究等应用中具有重要意义。

Method: 1. 提出精确数据结构，展示空间-时间权衡；2. 基于采样和草图技术的近似解决方案；3. 在几何设置中（区域为ℝ^d中的点，查询为超矩形），开发改进性能的精确数据结构。

Result: 提出了多种支持Counting Long Aggregated Visits查询的数据结构，包括精确和近似方法，并在几何设置中获得了更好的性能。同时提供了条件性和无条件性下界分析。

Conclusion: 本文系统研究了Counting Long Aggregated Visits问题，提供了从精确到近似、从一般到几何特化的多层次解决方案，为大规模移动数据分析中的复杂查询提供了理论基础和实用工具。

Abstract: This paper addresses the Counting Long Aggregated Visits problem, which is defined as follows. We are given $n$ users and $m$ regions, where each user spends some time visiting some regions. For a parameter $k$ and a query consisting of a subset of $r$ regions, the task is to count the number of distinct users whose aggregate time spent visiting the query regions is at least $k$. This problem is motivated by queries arising in the analysis of large-scale mobility datasets. We present several exact and approximate data structures for supporting counting long aggregated visits, as well as conditional and unconditional lower bounds. First, we describe an exact data structure that exhibits a space-time tradeoff, as well as efficient approximate solutions based on sampling and sketching techniques. We then study the problem in geometric settings where regions are points in $\mathbb{R}^d$ and queries are hyperrectangles, and derive exact data structures that achieve improved performance in these structured spaces.

</details>


### [51] [Permutation Matching Under Parikh Budgets: Linear-Time Detection, Packing, and Disjoint Selection](https://arxiv.org/abs/2601.09577)
*MD Nazmul Alam Shanto,Md. Tanzeem Rahat,Md. Manzurul Hasan*

Main category: cs.DS

TL;DR: 该论文提出了一个统一的滑动窗口框架，用于解决排列（乱序/阿贝尔）模式匹配问题，包括检测、优化和打包变体，实现了线性时间复杂度的算法。


<details>
  <summary>Details</summary>
Motivation: 虽然传统的排列模式匹配存在检测问题已有线性时间滑动窗口解决方案，但许多实际应用需要超越单纯检测的优化和打包变体，需要更通用的框架来处理这些扩展问题。

Method: 提出了基于维护模式P与当前文本窗口之间Parikh向量差异的统一滑动窗口框架，并在此基础上开发了最大可行子串问题（MFSP）的解决方案，以及非重叠匹配选择问题的贪心算法。

Result: 实现了O(n + σ)时间复杂度的排列匹配算法，其中σ = |Σ|；解决了MFSP问题，同样在O(n + σ)时间内完成；证明了贪心最早完成策略可以获得最大基数的不相交匹配集合。

Conclusion: 该研究提供了简洁、可证明正确的算法，具有紧密的界限，并将基于频率的字符串匹配与打包式优化原语连接起来，为排列模式匹配的扩展问题提供了统一的解决方案框架。

Abstract: We study permutation (jumbled/Abelian) pattern matching over a general alphabet $Σ$. Given a pattern P of length m and a text T of length n, the classical task is to decide whether T contains a length-m substring whose Parikh vector equals that of P . While this existence problem admits a linear-time sliding-window solution, many practical applications require optimization and packing variants beyond mere detection. We present a unified sliding-window framework based on maintaining the Parikh-vector difference between P and the current window of T , enabling permutation matching in O(n + σ) time and O(σ) space, where σ = |Σ|. Building on this foundation, we introduce a combinatorial-optimization variant that we call Maximum Feasible Substring under Pattern Supply (MFSP): find the longest substring S of T whose symbol counts are component-wise bounded by those of P . We show that MFSP can also be solved in O(n + σ) time via a two-pointer feasibility maintenance algorithm, providing an exact packing interpretation of P as a resource budget. Finally, we address non-overlapping occurrence selection by modeling each permutation match as an equal-length interval and proving that a greedy earliest-finishing strategy yields a maximum-cardinality set of disjoint matches, computable in linear time once all matches are enumerated. Our results provide concise, provably correct algorithms with tight bounds, and connect frequency-based string matching to packing-style optimization primitives.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [52] [Honesty-Aware Multi-Agent Framework for High-Fidelity Synthetic Data Generation in Digital Psychiatric Intake Doctor-Patient Interactions](https://arxiv.org/abs/2601.09216)
*Xinyuan Zhang,Zijian Wang,Chang Dao,Juexiao Zhou*

Main category: cs.DB

TL;DR: 提出多智能体合成框架，模拟患者欺骗行为生成高质量、可公开的合成精神病学就诊记录，用于研究欺骗感知的精神病学评估和训练自适应对话系统。


<details>
  <summary>Details</summary>
Motivation: 精神病学就诊和评估面临数据稀缺和不可靠自我报告（如隐瞒或夸大）的基本挑战，需要能够模拟患者欺骗行为的高质量合成数据。

Method: 基于DAIC-WOZ访谈构建丰富患者档案，模拟四角色工作流：患者（在主题相关诚实状态下完成自评量表和半结构化访谈）、评估者（基于人口统计学和主诉选择工具）、评估员（基于评分员管理量表进行访谈、跟踪怀疑并完成评分）、诊断师（整合所有证据形成诊断总结）。

Result: 通过四种互补评估验证框架：诊断一致性和严重程度分级、思维链消融、临床真实性和欺骗建模的人类评估、基于LLM的比较评估，生成涵盖多种障碍和严重程度的语料库。

Conclusion: 该框架能够生成高保真合成精神病学就诊记录，支持欺骗感知精神病学评估的受控研究，以及自适应对话系统的训练和评估。

Abstract: Data scarcity and unreliable self-reporting -- such as concealment or exaggeration -- pose fundamental challenges to psychiatric intake and assessment. We propose a multi-agent synthesis framework that explicitly models patient deception to generate high-fidelity, publicly releasable synthetic psychiatric intake records. Starting from DAIC-WOZ interviews, we construct enriched patient profiles and simulate a four-role workflow: a \emph{Patient} completes self-rated scales and participates in a semi-structured interview under a topic-dependent honesty state; an \emph{Assessor} selects instruments based on demographics and chief complaints; an \emph{Evaluator} conducts the interview grounded in rater-administered scales, tracks suspicion, and completes ratings; and a \emph{Diagnostician} integrates all evidence into a diagnostic summary. Each case links the patient profile, self-rated and rater-administered responses, interview transcript, diagnostic summary, and honesty state. We validate the framework through four complementary evaluations: diagnostic consistency and severity grading, chain-of-thought ablations, human evaluation of clinical realism and dishonesty modeling, and LLM-based comparative evaluation. The resulting corpus spans multiple disorders and severity levels, enabling controlled study of dishonesty-aware psychiatric assessment and the training and evaluation of adaptive dialogue agents.

</details>


### [53] [TiInsight: A SQL-based Automated Exploratory Data Analysis System through Large Language Models](https://arxiv.org/abs/2601.09404)
*Jun-Peng Zhu,Boyan Niu,Peng Cai,Zheming Ni,Kai Xu,Jiajun Huang,Shengbo Ma,Bing Wang,Xuan Zhou,Guanglei Bao,Donghui Zhang,Liu Tang,Qi Liu*

Main category: cs.DB

TL;DR: TiInsight是一个基于SQL的自动化跨领域探索性数据分析系统，利用大语言模型实现从自然语言查询到SQL生成和数据可视化的完整流程。


<details>
  <summary>Details</summary>
Motivation: 现有的SQL探索性数据分析方法缺乏跨领域分析能力，且对大语言模型潜力的探索不足。需要开发一个能够自动化处理跨领域数据探索的系统。

Method: TiInsight提供用户友好的GUI界面，采用分层数据上下文生成、问题澄清与分解、文本到SQL转换（TiSQL）以及数据可视化（TiChart）的跨领域探索分析流程。

Result: 已在PingCAP生产环境中实现并部署TiInsight系统，使用代表性数据集展示了其功能，提供了演示视频。

Conclusion: TiInsight成功构建了一个基于SQL的自动化跨领域探索性数据分析系统，通过大语言模型实现了从自然语言到数据洞察的完整转换流程。

Abstract: The SQL-based exploratory data analysis has garnered significant attention within the data analysis community. The emergence of large language models (LLMs) has facilitated the paradigm shift from manual to automated data exploration. However, existing methods generally lack the ability for cross-domain analysis, and the exploration of LLMs capabilities remains insufficient. This paper presents TiInsight, an SQL-based automated cross-domain exploratory data analysis system. First, TiInsight offers a user-friendly GUI enabling users to explore data using natural language queries. Second, TiInsight offers a robust cross-domain exploratory data analysis pipeline: hierarchical data context (i.e., HDC) generation, question clarification and decomposition, text-to-SQL (i.e., TiSQL), and data visualization (i.e., TiChart). Third, we have implemented and deployed TiInsight in the production environment of PingCAP and demonstrated its capabilities using representative datasets. The demo video is available at https://youtu.be/JzYFyYd-emI.

</details>
