<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 8]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.IT](#cs.IT) [Total: 9]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [A Hybrid Multimodal Deep Learning Framework for Intelligent Fashion Recommendation](https://arxiv.org/abs/2511.07573)
*Kamand Kalashi,Babak Teimourpour*

Main category: cs.IR

TL;DR: 提出了一种混合多模态深度学习框架，用于时尚推荐，同时解决服装搭配兼容性预测和互补物品检索两个关键任务。


<details>
  <summary>Details</summary>
Motivation: 在线时尚平台的快速发展对能够理解视觉和文本线索的智能推荐系统提出了日益增长的需求。

Method: 利用CLIP架构的视觉和文本编码器获取时尚物品的联合潜在表示，然后集成到统一特征向量中，通过transformer编码器处理。引入'outfit token'建模物品间的整体关系，使用'target item token'表示所需物品描述进行检索。

Result: 在Polyvore数据集上，兼容性预测的AUC达到0.95；在Fill-in-the-Blank指标下，互补物品检索准确率达到69.24%。

Conclusion: 该方法在两个任务上都表现出强大性能，突显了多模态学习在时尚推荐中的有效性。

Abstract: The rapid expansion of online fashion platforms has created an increasing demand for intelligent recommender systems capable of understanding both visual and textual cues. This paper proposes a hybrid multimodal deep learning framework for fashion recommendation that jointly addresses two key tasks: outfit compatibility prediction and complementary item retrieval. The model leverages the visual and textual encoders of the CLIP architecture to obtain joint latent representations of fashion items, which are then integrated into a unified feature vector and processed by a transformer encoder. For compatibility prediction, an "outfit token" is introduced to model the holistic relationships among items, achieving an AUC of 0.95 on the Polyvore dataset. For complementary item retrieval, a "target item token" representing the desired item description is used to retrieve compatible items, reaching an accuracy of 69.24% under the Fill-in-the-Blank (FITB) metric. The proposed approach demonstrates strong performance across both tasks, highlighting the effectiveness of multimodal learning for fashion recommendation.

</details>


### [2] [TurkEmbed4Retrieval: Turkish Embedding Model for Retrieval Task](https://arxiv.org/abs/2511.07595)
*Özay Ezerceli,Gizem Gümüşçekiçci,Tuğba Erkoç,Berke Özenç*

Main category: cs.IR

TL;DR: 开发了TurkEmbed4Retrieval模型，通过微调MS MARCO TR数据集，在土耳其语检索任务上实现了SOTA性能，比Turkish colBERT提升了19.26%。


<details>
  <summary>Details</summary>
Motivation: 为土耳其语信息检索任务开发专门的检索模型，提升现有模型的性能表现。

Method: 在TurkEmbed基础模型上，使用MS MARCO TR数据集进行微调，采用Matryoshka表示学习和定制的多负例排序损失等先进训练技术。

Result: 在Scifact TR数据集的关键检索指标上，比Turkish colBERT提升了19.26%，为土耳其语信息检索建立了新的基准。

Conclusion: TurkEmbed4Retrieval模型在土耳其语检索任务上表现出色，显著超越了现有最佳模型，为相关领域提供了新的技术标杆。

Abstract: In this work, we introduce TurkEmbed4Retrieval, a retrieval specialized variant of the TurkEmbed model originally designed for Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. By fine-tuning the base model on the MS MARCO TR dataset using advanced training techniques, including Matryoshka representation learning and a tailored multiple negatives ranking loss, we achieve SOTA performance for Turkish retrieval tasks. Extensive experiments demonstrate that our model outperforms Turkish colBERT by 19,26% on key retrieval metrics for the Scifact TR dataset, thereby establishing a new benchmark for Turkish information retrieval.

</details>


### [3] [From IDs to Semantics: A Generative Framework for Cross-Domain Recommendation with Adaptive Semantic Tokenization](https://arxiv.org/abs/2511.08006)
*Peiyu Hu,Wayne Lu,Jia Wang*

Main category: cs.IR

TL;DR: GenCDR是一个生成式跨域推荐框架，通过领域自适应分词、跨域自回归推荐和领域感知前缀树解决传统方法依赖共享ID和LLM方法面临词汇爆炸、领域建模不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统跨域推荐方法依赖共享用户/物品ID，但在实际场景中往往不可用。现有LLM方法面临词汇爆炸问题（物品ID分词困境）和领域特定建模不足的问题。

Method: 1. 领域自适应分词模块：通过通用编码器和领域特定适配器的动态路由生成解耦的语义ID；2. 跨域自回归推荐模块：融合通用和领域特定兴趣建模用户偏好；3. 领域感知前缀树：实现高效准确的生成。

Result: 在多个真实世界数据集上的广泛实验表明，GenCDR显著优于现有最先进的基线方法。

Conclusion: GenCDR通过创新的生成式框架有效解决了跨域推荐中的关键挑战，在推荐准确性和泛化性方面取得了显著提升。

Abstract: Cross-domain recommendation (CDR) is crucial for improving recommendation accuracy and generalization, yet traditional methods are often hindered by the reliance on shared user/item IDs, which are unavailable in most real-world scenarios. Consequently, many efforts have focused on learning disentangled representations through multi-domain joint training to bridge the domain gaps. Recent Large Language Model (LLM)-based approaches show promise, they still face critical challenges, including: (1) the \textbf{item ID tokenization dilemma}, which leads to vocabulary explosion and fails to capture high-order collaborative knowledge; and (2) \textbf{insufficient domain-specific modeling} for the complex evolution of user interests and item semantics. To address these limitations, we propose \textbf{GenCDR}, a novel \textbf{Gen}erative \textbf{C}ross-\textbf{D}omain \textbf{R}ecommendation framework. GenCDR first employs a \textbf{Domain-adaptive Tokenization} module, which generates disentangled semantic IDs for items by dynamically routing between a universal encoder and domain-specific adapters. Symmetrically, a \textbf{Cross-domain Autoregressive Recommendation} module models user preferences by fusing universal and domain-specific interests. Finally, a \textbf{Domain-aware Prefix-tree} enables efficient and accurate generation. Extensive experiments on multiple real-world datasets demonstrate that GenCDR significantly outperforms state-of-the-art baselines. Our code is available in the supplementary materials.

</details>


### [4] [BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives](https://arxiv.org/abs/2511.08029)
*Aarush Sinha,Pavan Kumar S,Roshan Balaji,Nirav Pravinbhai Bhatt*

Main category: cs.IR

TL;DR: 提出BiCA方法，利用PubMed文章中的引用链接生成高质量的困难负样本，用于改进生物医学领域的密集检索模型。


<details>
  <summary>Details</summary>
Motivation: 在生物医学和科学领域，由于源文档和困难负样本难以区分，传统的困难负样本挖掘方法面临挑战。引用文档天然具有上下文相关性但不是重复文档，适合作为困难负样本。

Method: 利用PubMed中20,000篇文章的引用链接进行困难负样本挖掘，对GTE_small和GTE_Base模型进行微调，使用引用信息化的负样本。

Result: 在BEIR数据集上的零样本密集检索任务中，使用nDCG@10指标在领域内和领域外任务上均取得一致改进，在LoTTE的长尾主题上使用Success@5指标优于基线方法。

Conclusion: 利用文档链接结构生成信息丰富的负样本具有巨大潜力，能够以最少的微调实现最先进的性能，展示了高效数据域适应的路径。

Abstract: Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.

</details>


### [5] [DiffuGR: Generative Document Retrieval with Diffusion Language Models](https://arxiv.org/abs/2511.08150)
*Xinpeng Zhao,Yukun Zhao,Zhenyang Li,Mengqi Zhang,Jun Feng,Ran Chen,Ying Zhou,Zhumin Chen,Shuaiqiang Wang,Zhaochun Ren,Dawei Yin,Xin Xin*

Main category: cs.IR

TL;DR: DiffuGR提出了一种基于扩散语言模型的生成式文档检索方法，通过离散扩散过程解决自回归生成方法在DocID生成中的问题，提供灵活的推理速度与精度权衡。


<details>
  <summary>Details</summary>
Motivation: 传统自回归生成检索方法存在两个主要问题：(1) DocID生成与自然语言生成不匹配，早期错误会导致完全错误的检索；(2) 无法动态平衡检索效率与精度的权衡。

Method: 将DocID生成建模为离散扩散过程：训练时通过随机掩码过程破坏DocIDs，学习扩散语言模型在检索感知目标下恢复它们；推理时并行生成DocID tokens并通过可控的去噪步骤进行精炼。

Result: 在基准检索数据集上的实验表明，DiffuGR与强自回归生成检索器具有竞争力，同时通过可变去噪预算提供灵活的速度与精度权衡。

Conclusion: 非自回归扩散模型是生成式文档检索的实用有效替代方案，提供了更好的错误容忍性和运行时控制能力。

Abstract: Generative retrieval (GR) re-frames document retrieval as a sequence-based document identifier (DocID) generation task, memorizing documents with model parameters and enabling end-to-end retrieval without explicit indexing. Existing GR methods are based on auto-regressive generative models, i.e., the token generation is performed from left to right. However, such auto-regressive methods suffer from: (1) mismatch between DocID generation and natural language generation, e.g., an incorrect DocID token generated in early left steps would lead to totally erroneous retrieval; and (2) failure to balance the trade-off between retrieval efficiency and accuracy dynamically, which is crucial for practical applications. To address these limitations, we propose generative document retrieval with diffusion language models, dubbed DiffuGR. It models DocID generation as a discrete diffusion process: during training, DocIDs are corrupted through a stochastic masking process, and a diffusion language model is learned to recover them under a retrieval-aware objective. For inference, DiffuGR attempts to generate DocID tokens in parallel and refines them through a controllable number of denoising steps. In contrast to conventional left-to-right auto-regressive decoding, DiffuGR provides a novel mechanism to first generate more confident DocID tokens and refine the generation through diffusion-based denoising. Moreover, DiffuGR also offers explicit runtime control over the qualitylatency tradeoff. Extensive experiments on benchmark retrieval datasets show that DiffuGR is competitive with strong auto-regressive generative retrievers, while offering flexible speed and accuracy tradeoffs through variable denoising budgets. Overall, our results indicate that non-autoregressive diffusion models are a practical and effective alternative for generative document retrieval.

</details>


### [6] [MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System](https://arxiv.org/abs/2511.08181)
*Seung Hwan Cho,Yujin Yang,Danik Baeck,Minjoo Kim,Young-Min Kim,Heejung Lee,Sangjin Park*

Main category: cs.IR

TL;DR: 提出MARC系统，基于Agentic RAG和图数据库的多模态多任务鸡尾酒推荐系统，解决冷启动问题，通过任务识别路由和反思过程生成高质量答案


<details>
  <summary>Details</summary>
Motivation: 利用模态信息和LLM的推理能力解决推荐系统冷启动问题，结合鸡尾酒领域独特的数据属性和关系特性

Method: 构建图数据库处理鸡尾酒数据，采用Agentic RAG框架，包含任务识别路由和反思过程两个核心流程

Result: 使用200个手工制作的问题评估，通过LLM和人工评估显示图数据库生成的答案质量优于简单向量数据库

Conclusion: MARC系统在冷启动条件下能生成高质量、上下文适当的鸡尾酒推荐答案，图数据库方法优于传统向量数据库

Abstract: Recommender systems (RS) are currently being studied to mitigate limitations during cold-start conditions by leveraging modality information or introducing Agent concepts based on the exceptional reasoning capabilities of Large Language Models (LLMs). Meanwhile, food and beverage recommender systems have traditionally used knowledge graph and ontology concepts due to the domain's unique data attributes and relationship characteristics. On this background, we propose MARC, a multimodal and multi-task cocktail recommender system based on Agentic Retrieval-Augmented Generation (RAG) utilizing graph database under cold-start conditions. The proposed system generates high-quality, contextually appropriate answers through two core processes: a task recognition router and a reflection process. The graph database was constructed by processing cocktail data from Kaggle, and its effectiveness was evaluated using 200 manually crafted questions. The evaluation used both LLM-as-a-judge and human evaluation to demonstrate that answers generated via the graph database outperformed those from a simple vector database in terms of quality. The code is available at https://github.com/diddbwls/cocktail_rec_agentrag

</details>


### [7] [Bid Farewell to Seesaw: Towards Accurate Long-tail Session-based Recommendation via Dual Constraints of Hybrid Intents](https://arxiv.org/abs/2511.08378)
*Xiao Wang,Ke Qin,Dongyang Zhang,Xiurui Xie,Shuang Liang*

Main category: cs.IR

TL;DR: HID框架通过混合意图学习和意图约束损失，解决了会话推荐中长尾物品与推荐准确性的冲突，实现了双赢。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提升长尾物品推荐时会导致准确性下降，形成"跷跷板"效应，主要原因是未能有效识别和约束会话无关的噪声。

Method: 提出HID框架：1) 混合意图学习：使用属性感知谱聚类重构物品到意图的映射，区分目标意图和噪声意图；2) 意图约束损失：通过多样性和准确性两个约束范式来调节物品和会话的表示学习。

Result: 在多个SBR模型和数据集上的实验表明，HID能够同时提升长尾性能和推荐准确性，在长尾推荐系统中达到最先进的性能。

Conclusion: HID框架成功将传统的"跷跷板"冲突转化为"双赢"，通过混合意图和双重约束有效解决了长尾推荐中的核心问题。

Abstract: Session-based recommendation (SBR) aims to predict anonymous users' next interaction based on their interaction sessions. In the practical recommendation scenario, low-exposure items constitute the majority of interactions, creating a long-tail distribution that severely compromises recommendation diversity. Existing approaches attempt to address this issue by promoting tail items but incur accuracy degradation, exhibiting a "see-saw" effect between long-tail and accuracy performance. We attribute such conflict to session-irrelevant noise within the tail items, which existing long-tail approaches fail to identify and constrain effectively. To resolve this fundamental conflict, we propose \textbf{HID} (\textbf{H}ybrid \textbf{I}ntent-based \textbf{D}ual Constraint Framework), a plug-and-play framework that transforms the conventional "see-saw" into "win-win" through introducing the hybrid intent-based dual constraints for both long-tail and accuracy. Two key innovations are incorporated in this framework: (i) \textit{Hybrid Intent Learning}, where we reformulate the intent extraction strategies by employing attribute-aware spectral clustering to reconstruct the item-to-intent mapping. Furthermore, discrimination of session-irrelevant noise is achieved through the assignment of the target and noise intents to each session. (ii) \textit{Intent Constraint Loss}, which incorporates two novel constraint paradigms regarding the \textit{diversity} and \textit{accuracy} to regulate the representation learning process of both items and sessions. These two objectives are unified into a single training loss through rigorous theoretical derivation. Extensive experiments across multiple SBR models and datasets demonstrate that HID can enhance both long-tail performance and recommendation accuracy, establishing new state-of-the-art performance in long-tail recommender systems.

</details>


### [8] [Advancing Scientific Knowledge Retrieval and Reuse with a Novel Digital Library for Machine-Readable Knowledge](https://arxiv.org/abs/2511.08476)
*Hadi Ghaemi,Lauren Snyder,Markus Stocker*

Main category: cs.IR

TL;DR: ORKG reborn是一个新兴的数字图书馆，支持查找、访问和重用机器可读的细粒度科学知识表达，这些表达将科学陈述与其支持的数据和代码证据相关联。


<details>
  <summary>Details</summary>
Motivation: 现有的数字图书馆（如ACM数字图书馆）基于文档中心模型，需要手动或半自动的知识提取、结构化和组织，无法有效支持科学知识的机器化重用。

Method: 提出ORKG reborn系统，发布可重用的科学知识表达（称为"重生"文章），这些表达将科学陈述与支持证据（数据和代码）关联起来，提供机器可读的细粒度知识结构。

Result: 该系统在实际应用中展示了可行性，与现有数字图书馆相比，在信息检索方面具有潜力，能够通过统计方法、软件包、变量或特定约束的数据进行科学知识检索。

Conclusion: 科学知识数据库具有巨大潜力，ORKG reborn为构建这类数据库提供了一条可行途径，能够改变以文档为中心的学术交流模式。

Abstract: Digital libraries for research, such as the ACM Digital Library or Semantic Scholar, do not enable the machine-supported, efficient reuse of scientific knowledge (e.g., in synthesis research). This is because these libraries are based on document-centric models with narrative text knowledge expressions that require manual or semi-automated knowledge extraction, structuring, and organization. We present ORKG reborn, an emerging digital library that supports finding, accessing, and reusing accurate, fine-grained, and reproducible machine-readable expressions of scientific knowledge that relate scientific statements and their supporting evidence in terms of data and code. The rich expressions of scientific knowledge are published as reborn (born-reusable) articles and provide novel possibilities for scientific knowledge retrieval, for instance by statistical methods, software packages, variables, or data matching specific constraints. We describe the proposed system and demonstrate its practical viability and potential for information retrieval in contrast to state-of-the-art digital libraries and document-centric scholarly communication using several published articles in research fields ranging from computer science to soil science. Our work underscores the enormous potential of scientific knowledge databases and a viable approach to their construction.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [9] [Cortex AISQL: A Production SQL Engine for Unstructured Data](https://arxiv.org/abs/2511.07663)
*Paritosh Aggarwal,Bowei Chen,Anupam Datta,Benjamin Han,Boxin Jiang,Nitish Jindal,Zihan Li,Aaron Lin,Pawel Liskowski,Jay Tayade,Dimitris Tsirogiannis,Nathan Wiegand,Weicheng Zhao*

Main category: cs.DB

TL;DR: Snowflake Cortex AISQL 是一个生产级SQL引擎，将语义操作原生集成到SQL中，通过AI感知查询优化、自适应模型级联和语义连接重写三大技术，解决了语义操作在生产规模下的效率挑战。


<details>
  <summary>Details</summary>
Motivation: 传统SQL引擎无法有效优化语义操作，因为语义操作比传统SQL操作更昂贵，具有不同的延迟和吞吐特性，且成本和选择性在查询编译时未知。

Method: 1. AI感知查询优化：将AI推理成本作为首要优化目标；2. 自适应模型级联：通过快速代理模型路由大多数行，不确定情况升级到强大oracle模型；3. 语义连接查询重写：将连接操作的二次时间复杂度降低为线性。

Result: AI感知优化实现2-8倍加速；自适应模型级联实现2-6倍加速并保持90-95%的oracle模型质量；语义连接重写实现15-70倍加速且通常提高预测质量。

Conclusion: AISQL已在Snowflake生产环境中部署，支持客户在分析、搜索和内容理解等多样化工作负载。

Abstract: Snowflake's Cortex AISQL is a production SQL engine that integrates native semantic operations directly into SQL. This integration allows users to write declarative queries that combine relational operations with semantic reasoning, enabling them to query both structured and unstructured data effortlessly. However, making semantic operations efficient at production scale poses fundamental challenges. Semantic operations are more expensive than traditional SQL operations, possess distinct latency and throughput characteristics, and their cost and selectivity are unknown during query compilation. Furthermore, existing query engines are not designed to optimize semantic operations. The AISQL query execution engine addresses these challenges through three novel techniques informed by production deployment data from Snowflake customers. First, AI-aware query optimization treats AI inference cost as a first-class optimization objective, reasoning about large language model (LLM) cost directly during query planning to achieve 2-8$\times$ speedups. Second, adaptive model cascades reduce inference costs by routing most rows through a fast proxy model while escalating uncertain cases to a powerful oracle model, achieving 2-6$\times$ speedups while maintaining 90-95% of oracle model quality. Third, semantic join query rewriting lowers the quadratic time complexity of join operations to linear through reformulation as multi-label classification tasks, achieving 15-70$\times$ speedups with often improved prediction quality. AISQL is deployed in production at Snowflake, where it powers diverse customer workloads across analytics, search, and content understanding.

</details>


### [10] [ACGraph: An Efficient Asynchronous Out-of-Core Graph Processing Framework](https://arxiv.org/abs/2511.07886)
*Dechuang Chen,Sibo Wang,Qintian Guo*

Main category: cs.DB

TL;DR: ACGraph是一个针对SSD环境的异步图处理系统，通过动态块级优先级调度、在线异步工作列表和流水线执行模型，显著提升了外存图处理的性能和I/O效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图数据规模持续增长，超出了单机内存容量，需要外存图处理系统。现有同步执行系统存在I/O效率低下和同步开销大的问题。

Method: 采用动态块级优先级调度器、在线异步工作列表、统一异步I/O与计算的流水线执行模型，以及优化的混合存储格式。

Result: 在BFS、WCC、PPR、PageRank和k-core等算法上，ACGraph在运行时间和I/O效率方面显著优于现有最先进的外存图处理系统。

Conclusion: ACGraph通过异步处理和优化调度机制，有效解决了外存图处理中的I/O瓶颈问题，为大规模图数据处理提供了高效解决方案。

Abstract: Graphs are a ubiquitous data structure in diverse domains such as machine learning, social networks, and data mining. As real-world graphs continue to grow beyond the memory capacity of single machines, out-of-core graph processing systems have emerged as a viable solution. Yet, existing systems that rely on strictly synchronous, iteration-by-iteration execution incur significant overheads. In particular, their scheduling mechanisms lead to I/O inefficiencies, stemming from read and work amplification, and induce costly synchronization stalls hindering sustained disk utilization. To overcome these limitations, we present {\em ACGraph}, a novel asynchronous graph processing system optimized for SSD-based environments with constrained memory resources. ACGraph employs a dynamic, block-centric priority scheduler that adjusts in real time based on workload, along with an online asynchronous worklist that minimizes redundant disk accesses by efficiently reusing active blocks in memory. Moreover, ACGraph unifies asynchronous I/O with computation in a pipelined execution model that maintains sustained I/O activation, and leverages a highly optimized hybrid storage format to expedite access to low-degree vertices. We implement popular graph algorithms, such as Breadth-First Search (BFS), Weakly Connected Components (WCC), personalized PageRank (PPR), PageRank (PR), and $k$-core on ACGraph and demonstrate that ACGraph substantially outperforms state-of-the-art out-of-core graph processing systems in both runtime and I/O efficiency.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [11] [Reliable and Private Utility Signaling for Data Markets](https://arxiv.org/abs/2511.07975)
*Li Peng,Jiayao Zhang,Yihang Wu,Weiran Liu,Jinfei Liu,Zheng Yan,Kui Ren,Lei Zhang,Lin Qu*

Main category: cs.GT

TL;DR: 本文提出了一种基于多方安全计算(MPC)的信号机制，解决了数据市场中隐私与可靠性之间的权衡问题，支持公平的数据估值和知情交易决策。


<details>
  <summary>Details</summary>
Motivation: 数据市场的爆炸性增长凸显了数据共享的重要性，但现有信号机制面临隐私与可靠性的两难困境，阻碍了有效的数据交易决策。

Method: 采用恶意安全的多方安全计算(MPC)来确保信号计算的隐私和鲁棒性，提出基于MPC的哈希验证方案保证输入可靠性，并在多卖家场景中优化了基于MPC的KNN-Shapley方法。

Result: 严格的实验证明了该方法的效率和实用性，能够同时确保隐私和可靠性，防止参与者的次优决策。

Conclusion: 所提出的非TCP信号机制构建能够有效解决数据市场中的隐私-可靠性权衡问题，促进知情的数据交易。

Abstract: The explosive growth of data has highlighted its critical role in driving economic growth through data marketplaces, which enable extensive data sharing and access to high-quality datasets. To support effective trading, signaling mechanisms provide participants with information about data products before transactions, enabling informed decisions and facilitating trading. However, due to the inherent free-duplication nature of data, commonly practiced signaling methods face a dilemma between privacy and reliability, undermining the effectiveness of signals in guiding decision-making.
  To address this, this paper explores the benefits and develops a non-TCP-based construction for a desirable signaling mechanism that simultaneously ensures privacy and reliability. We begin by formally defining the desirable utility signaling mechanism and proving its ability to prevent suboptimal decisions for both participants and facilitate informed data trading. To design a protocol to realize its functionality, we propose leveraging maliciously secure multi-party computation (MPC) to ensure the privacy and robustness of signal computation and introduce an MPC-based hash verification scheme to ensure input reliability. In multi-seller scenarios requiring fair data valuation, we further explore the design and optimization of the MPC-based KNN-Shapley method with improved efficiency. Rigorous experiments demonstrate the efficiency and practicality of our approach.

</details>


### [12] [Centralized Group Equitability and Individual Envy-Freeness in the Allocation of Indivisible Items](https://arxiv.org/abs/2511.07984)
*Ying Wang,Jiaqian Li,Tianze Wei,Hau Chan,Minming Li*

Main category: cs.GT

TL;DR: 研究多组智能体不可分割物品的公平分配问题，考虑智能体间的公平性和集中分配者对组间公平性的关注。提出了CGEQ和CGEQ1等新概念，证明了EF1和CGEQ1分配的存在性并设计了高效算法。


<details>
  <summary>Details</summary>
Motivation: 解决现实场景中集中分配者需要同时确保组间公平和智能体间公平的问题，如学校资源分配到部门、城市住房分配到社区等。

Method: 定义了集中组公平性(CGEQ)和其松弛版本CGEQ1，结合经典的EF和EF1概念，针对不同估值函数类别设计分配算法。

Result: 证明了对于不同类别的估值函数，同时满足EF1和CGEQ1的分配总是存在，并设计了高效计算这些分配的算法。

Conclusion: 提出的框架能有效解决集中分配者视角下的组间公平问题，同时保证智能体间的公平性，为实际应用提供了理论支持。

Abstract: We study the fair allocation of indivisible items for groups of agents from the perspectives of the agents and a centralized allocator. In our setting, the centralized allocator is interested in ensuring the allocation is fair among the groups and between agents. This setting applies to many real-world scenarios, including when a school administrator wants to allocate resources (e.g., office spaces and supplies) to staff members in departments and when a city council allocates limited housing units to various families in need across different communities. To ensure fair allocation between agents, we consider the classical envy-freeness (EF) notion. To ensure fairness among the groups, we define the notion of centralized group equitability (CGEQ) to capture the fairness for the groups from the allocator's perspective. Because an EF or CGEQ allocation does not always exist in general, we consider their corresponding natural relaxations of envy-freeness to one item (EF1) and centralized group equitability up to one item (CGEQ1). For different classes of valuation functions of the agents and the centralized allocator, we show that allocations satisfying both EF1 and CGEQ1 always exist and design efficient algorithms to compute these allocations. We also consider the centralized group maximin share (CGMMS) from the centralized allocator's perspective as a group-level fairness objective with EF1 for agents and present several results.

</details>


### [13] [Nash-equilibrium Seeking Algorithm for Power-Allocation Games on Networks of International Relations](https://arxiv.org/abs/2511.08033)
*Chuanzhe Zhang,Yuke Li,Wenjun Mei*

Main category: cs.GT

TL;DR: 本文扩展了基于符号图的博弈框架，通过修改偏好公理和引入新算法来更好地建模国际安全中的战略互动，并使用1940年历史数据进行验证。


<details>
  <summary>Details</summary>
Motivation: 现有基于符号图的博弈框架过于基础，无法充分捕捉国际关系中复杂的战略情景，需要更细致地建模国家追求生存、保护盟友和攻击对手的行为。

Method: 修改偏好公理以更细致地描述国家行为，并设计新算法证明修正后博弈存在纯策略纳什均衡，使用1940年历史数据验证模型。

Result: 成功扩展了原有框架的现实适用性，能够预测国家生存能力，为网络化安全环境中的战略互动提供更全面的视角。

Conclusion: 通过改进偏好公理和算法设计，增强了基于符号图博弈框架在国际安全分析中的实用性和解释力。

Abstract: In the field of international security, understanding the strategic interactions between countries within a networked context is crucial. Our previous research has introduced a ``games-on-signed graphs'' framework~\cite{LiMorse2022} to analyze these interactions. While the framework is intended to be basic and general, there is much left to be explored, particularly in capturing the complexity of strategic scenarios in international relations. Our paper aims to fill this gap in two key ways. First, we modify the existing preference axioms to allow for a more nuanced understanding of how countries pursue self-survival, defense of allies, and offense toward adversaries. Second, we introduce a novel algorithm that proves the existence of a pure-strategy Nash equilibrium for these revised games. To validate our model, we employ historical data from the year 1940 as the game input and predict countries' survivability. Our contributions thus extend the real-world applicability of the original framework, offering a more comprehensive view of strategic interactions in a networked security environment.

</details>


### [14] [Dividing Indivisible Items for the Benefit of All: It is Hard to Be Fair Without Social Awareness](https://arxiv.org/abs/2511.08160)
*Argyris Deligkas,Eduard Eiben,Tiger-Lily Goldsmith,Dušan Knop,Šimon Schierreich*

Main category: cs.GT

TL;DR: 研究在公平分配不可分割物品时如何同时最大化社会影响，发现问题的复杂度取决于代理人是否具有社会意识。


<details>
  <summary>Details</summary>
Motivation: 传统公平分配模型假设代理人都是自私的，但现实中资源分配会影响整个群体或社会，因此需要同时考虑公平性和社会影响。

Method: 每个代理人有两种加性函数：价值函数和社会影响函数。目标是分配物品以最大化社会影响，同时保持某种公平标准。

Result: 对于无社会意识的代理人，问题通常是NP难的；对于有社会意识的代理人，可以在多项式时间内找到同时满足公平性和最大化社会影响的分配方案。

Conclusion: 社会意识是解决公平分配与社会影响最大化之间权衡的关键因素，但一旦放松社会意识的定义，问题就变得难以处理。

Abstract: In standard fair division models, we assume that all agents are selfish. However, in many scenarios, division of resources has a direct impact on the whole group or even society. Therefore, we study fair allocations of indivisible items that, at the same time, maximize social impact. In this model, each agent is associated with two additive functions that define their value and social impact for each item. The goal is to allocate items so that the social impact is maximized while maintaining some fairness criterion. We reveal that the complexity of the problem heavily depends on whether the agents are socially aware, i.e., they take into consideration the social impact functions. For socially unaware agents, we prove that the problem is NP-hard for a variety of fairness notions, and that it is tractable only for very restricted cases, e.g., if, for every agent, the valuation equals social impact and it is binary. On the other hand, social awareness allows for fair allocations that maximize social impact, and such allocations can be computed in polynomial time. Interestingly, the problem becomes again intractable as soon as the definition of social awareness is relaxed.

</details>


### [15] [Classification in Equilibrium: Structure of Optimal Decision Rules](https://arxiv.org/abs/2511.08347)
*Elizabeth Maggie Penn,John W. Patty*

Main category: cs.GT

TL;DR: 该论文研究了在个体根据分类规则调整行为时的最优分类问题，发现最优规则属于简单可解释的单阈值和双截断规则家族，与传统观点不同，最优分类器可能故意奖励较低似然比的个体或在中间区间集中奖励/惩罚以提高信息质量。


<details>
  <summary>Details</summary>
Motivation: 研究当个体能够通过合规、欺骗或弃权来应对分类规则时，如何设计最优分类规则，考虑个体行为反应对分类效果的影响。

Method: 使用Stackelberg博弈模型，将设计者与人群的互动建模为领导者-跟随者博弈，设计者选择分类规则并预期个体的行为反应。

Result: 在标准单调似然比假设下，最优规则属于单阈值和双截断规则家族，这些规则可能奖励较低似然比的个体或在中间区间集中奖励/惩罚。

Conclusion: 最优分类规则的设计需要充分考虑个体的策略性行为反应，可能与传统直觉相反，通过奖励低似然比个体或中间区间来改善信息质量。

Abstract: This paper characterizes optimal classification when individuals adjust their behavior in response to the classification rule. We model the interaction between a designer and a population as a Stackelberg game: the designer selects a classification rule anticipating how individuals will comply, cheat, or abstain in order to obtain a favorable classification. Under standard monotone likelihood ratio assumptions, optimal rules belong to a small and interpretable family (single-threshold and two-cut rules) that encompass both conventional and counterintuitive designs. Our results depart sharply from prior findings that optimal classifiers reward higher signals: in equilibrium, the designer may deliberately reward those with lower likelihood ratios or concentrate rewards/penalties in a middle band to improve informational quality.

</details>


### [16] [Fair Multi-agent Persuasion with Submodular Constraints](https://arxiv.org/abs/2511.08538)
*Yannan Bai,Kamesh Munagala,Yiheng Shen,Davidson Zhu*

Main category: cs.GT

TL;DR: 本文研究贝叶斯劝说中的选择问题，提出一种信号策略，在接收者有限理性的假设下，实现对数近似的主要化公平分配，显著优于线性近似方法。


<details>
  <summary>Details</summary>
Motivation: 在资源分配中，中介了解代理人的隐藏价值，希望通过设计信息信号和打破平局策略来影响选择结果，使代理人的期望效用（分配乘以价值）达到基于主要化的公平性。

Method: 提出一种信号策略，在任意子模约束下，通过将代理人效用向量表征为不同多面体的基多面体，并使用乘性权重更新方法在多项式时间内产生任意好的加性近似。

Result: 所提出的信号策略实现了对数近似的主要化政策，近似比几乎是最优的，显著优于仅产生线性近似的通用结果。

Conclusion: 该研究为贝叶斯劝说中的公平选择问题提供了有效的解决方案，关键的结构性表征结果可能具有独立的研究价值。

Abstract: We study the problem of selection in the context of Bayesian persuasion. We are given multiple agents with hidden values (or quality scores), to whom resources must be allocated by a welfare-maximizing decision-maker. An intermediary with knowledge of the agents' values seeks to influence the outcome of the selection by designing informative signals and providing tie-breaking policies, so that when the receiver maximizes welfare over the resulting posteriors, the expected utilities of the agents (where utility is defined as allocation times value) achieve certain fairness properties. The fairness measure we will use is majorization, which simultaneously approximately maximizes all symmetric, monotone, concave functions of the utilities. We consider the general setting where the allocation to the agents needs to respect arbitrary submodular constraints, as given by the corresponding polymatroid.
  We present a signaling policy that, under a mild bounded rationality assumption on the receiver, achieves a logarithmically approximate majorized policy in this setting. The approximation ratio is almost best possible, and that significantly outperforms generic results that only yield linear approximations. A key component of our result is a structural characterization showing that the vector of agent utilities for a given signaling policy defines the base polytope of a different polymatroid, a result that may be of independent interest. In addition, we show that an arbitrarily good additive approximation to this vector can be produced in (weakly) polynomial time via the multiplicative weights update method.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [17] [Testing noisy low-degree polynomials for sparsity](https://arxiv.org/abs/2511.07835)
*Yiqiao Bao,Anindya De,Shivam Nadimpalli,Rocco A. Servedio,Nathan White*

Main category: cs.DS

TL;DR: 该论文研究了在噪声评估下测试低阶多项式稀疏性的问题，给出了样本复杂度与维度无关的精确条件，并提供了匹配的常数样本算法。


<details>
  <summary>Details</summary>
Motivation: 扩展Chen等人(2020)从噪声线性函数到一般低阶多项式的稀疏性测试工作，解决在噪声环境下学习稀疏低阶多项式的经典问题。

Method: 定义了可计算函数MSG_X,d(·)，根据稀疏参数s和T的关系，分别设计常数样本算法和证明对数样本下界。技术上将Dinur等人(2007)关于有界函数傅里叶尾的结果推广到有限支撑分布。

Result: 当T ≥ MSG_X,d(s)时，给出独立于维度n的常数样本算法；当T ≤ MSG_X,d(s)-1时，证明即使无噪声也需要Ω(log n)样本。

Conclusion: 精确刻画了低阶多项式稀疏性测试何时具有常数样本复杂度，为噪声环境下学习稀疏多项式提供了理论基础。

Abstract: We consider the problem of testing whether an unknown low-degree polynomial $p$ over $\mathbb{R}^n$ is sparse versus far from sparse, given access to noisy evaluations of the polynomial $p$ at \emph{randomly chosen points}. This is a property-testing analogue of classical problems on learning sparse low-degree polynomials with noise, extending the work of Chen, De, and Servedio (2020) from noisy \emph{linear} functions to general low-degree polynomials.
  Our main result gives a \emph{precise characterization} of when sparsity testing for low-degree polynomials admits constant sample complexity independent of dimension, together with a matching constant-sample algorithm in that regime. For any mean-zero, variance-one finitely supported distribution $\boldsymbol{X}$ over the reals, degree $d$, and any sparsity parameters $s \leq T$, we define a computable function $\mathrm{MSG}_{\boldsymbol{X},d}(\cdot)$, and:
  - For $T \ge \mathrm{MSG}_{\boldsymbol{X},d}(s)$, we give an $O_{s,\boldsymbol{X},d}(1)$-sample algorithm that distinguishes whether a multilinear degree-$d$ polynomial over $\mathbb{R}^n$ is $s$-sparse versus $\varepsilon$-far from $T$-sparse, given examples $(\boldsymbol{x},\, p(\boldsymbol{x}) + \mathrm{noise})_{\boldsymbol{x} \sim \boldsymbol{X}^{\otimes n}}$. Crucially, the sample complexity is \emph{completely independent} of the ambient dimension $n$.
  - For $T \leq \mathrm{MSG}_{\boldsymbol{X},d}(s) - 1$, we show that even without noise, any algorithm given samples $(\boldsymbol{x},p(\boldsymbol{x}))_{\boldsymbol{x} \sim \boldsymbol{X}^{\otimes n}}$ must use $Ω_{\boldsymbol{X},d,s}(\log n)$ examples.
  Our techniques employ a generalization of the results of Dinur et al. (2007) on the Fourier tails of bounded functions over $\{0,1\}^n$ to a broad range of finitely supported distributions, which may be of independent interest.

</details>


### [18] [Model-agnostic super-resolution in high dimensions](https://arxiv.org/abs/2511.07846)
*Xi Chen,Anindya De,Yizhi Huang,Shivam Nadimpalli,Rocco A. Servedio,Tianqi Yang*

Main category: cs.DS

TL;DR: 本文分析了高维超分辨率问题，研究了在d维环面上一般信号的两种重建方式：Wasserstein距离重建需要约exp(d)个傅里叶系数，而"重击者"重建仅需约exp(√d)个系数。


<details>
  <summary>Details</summary>
Motivation: 先前超分辨率研究对信号施加了强建模假设（如空间分离的点源），本文旨在分析更一般的超分辨率问题，考虑d维环面上的完全一般信号，不假设空间分离或有限点源组合。

Method: 研究两种重建概念：1）Wasserstein距离重建，给出截止频率T和噪声幅度κ的上下界匹配结果；2）引入"重击者"重建概念，专注于重建分布的"足够密集"区域。

Result: 对于d维信号，Wasserstein重建需要约exp(d)个傅里叶系数，而"重击者"重建仅需约exp(√d)个系数，两者都给出了匹配的上下界。

Conclusion: 超分辨率问题的复杂度与重建目标密切相关：Wasserstein重建需要指数级更多信息，而"重击者"重建在信息需求上显著更高效，特别是在高维情况下。

Abstract: The problem of \emph{super-resolution}, roughly speaking, is to reconstruct an unknown signal to high accuracy, given (potentially noisy) information about its low-degree Fourier coefficients. Prior results on super-resolution have imposed strong modeling assumptions on the signal, typically requiring that it is a linear combination of spatially separated point sources.
  In this work we analyze a very general version of the super-resolution problem, by considering completely general signals over the $d$-dimensional torus $[0,1)^d$; we do not assume any spatial separation between point sources, or even that the signal is a finite linear combination of point sources. We obtain two sets of results, corresponding to two natural notions of reconstruction.
  - {\bf Reconstruction in Wasserstein distance:} We give essentially matching upper and lower bounds on the cutoff frequency $T$ and the magnitude $κ$ of the noise for which accurate reconstruction in Wasserstein distance is possible. Roughly speaking, our results here show that for $d$-dimensional signals, estimates of $\approx \exp(d)$ many Fourier coefficients are necessary and sufficient for accurate reconstruction under the Wasserstein distance.
  - {\bf "Heavy hitter" reconstruction:} For nonnegative signals (equivalently, probability distributions), we introduce a new notion of "heavy hitter" reconstruction that essentially amounts to achieving high-accuracy reconstruction of all "sufficiently dense" regions of the distribution. Here too we give essentially matching upper and lower bounds on the cutoff frequency $T$ and the magnitude $κ$ of the noise for which accurate reconstruction is possible. Our results show that -- in sharp contrast with Wasserstein reconstruction -- accurate estimates of only $\approx \exp(\sqrt{d})$ many Fourier coefficients are necessary and sufficient for heavy hitter reconstruction.

</details>


### [19] [Deterministic Padded Decompositions and Negative-Weight Shortest Paths](https://arxiv.org/abs/2511.07859)
*Jason Li*

Main category: cs.DS

TL;DR: 提出了第一个在整数加权图上计算负权重单源最短路径的近似线性时间确定性算法


<details>
  <summary>Details</summary>
Motivation: 解决负权重单源最短路径问题的确定性算法在时间复杂度上的改进需求

Method: 使用确定性方法构建有向图上的填充分解作为主要技术

Result: 获得了第一个近似线性时间确定性算法

Conclusion: 该确定性填充分解方法可能具有独立的研究价值

Abstract: We obtain the first near-linear time deterministic algorithm for negative-weight single-source shortest paths on integer-weighted graphs. Our main ingredient is a deterministic construction of a padded decomposition on directed graphs, which may be of independent interest.

</details>


### [20] [Parallel Sampling via Autospeculation](https://arxiv.org/abs/2511.07869)
*Nima Anari,Carlo Baronio,CJ Chen,Alireza Haqi,Frederic Koehler,Anqi Li,Thuy-Duong Vuong*

Main category: cs.DS

TL;DR: 提出并行算法加速两种模型的采样：任意顺序自回归模型和去噪扩散模型，通过并行化将采样时间从O(n)降低到O(n^{1/2})


<details>
  <summary>Details</summary>
Motivation: 标准顺序采样算法需要O(n)时间，希望通过并行化加速采样过程，提高采样效率

Method: 引入推测性拒绝采样技术，利用辅助分布ν近似目标分布μ来加速采样，采用序列级推测而非单步推测

Result: 将期望采样时间从O(n)降低到O(n^{1/2})，改进了任意顺序自回归模型的O(n^{2/3})界限，并为扩散模型在高精度机制下提供了首个并行加速

Conclusion: 提出的推测性拒绝采样技术能有效加速采样过程，特别是在序列级推测的框架下实现了显著的并行加速效果

Abstract: We present parallel algorithms to accelerate sampling via counting in two settings: any-order autoregressive models and denoising diffusion models. An any-order autoregressive model accesses a target distribution $μ$ on $[q]^n$ through an oracle that provides conditional marginals, while a denoising diffusion model accesses a target distribution $μ$ on $\mathbb{R}^n$ through an oracle that provides conditional means under Gaussian noise. Standard sequential sampling algorithms require $\widetilde{O}(n)$ time to produce a sample from $μ$ in either setting. We show that, by issuing oracle calls in parallel, the expected sampling time can be reduced to $\widetilde{O}(n^{1/2})$. This improves the previous $\widetilde{O}(n^{2/3})$ bound for any-order autoregressive models and yields the first parallel speedup for diffusion models in the high-accuracy regime, under the relatively mild assumption that the support of $μ$ is bounded.
  We introduce a novel technique to obtain our results: speculative rejection sampling. This technique leverages an auxiliary ``speculative'' distribution~$ν$ that approximates~$μ$ to accelerate sampling. Our technique is inspired by the well-studied ``speculative decoding'' techniques popular in large language models, but differs in key ways. Firstly, we use ``autospeculation,'' namely we build the speculation $ν$ out of the same oracle that defines~$μ$. In contrast, speculative decoding typically requires a separate, faster, but potentially less accurate ``draft'' model $ν$. Secondly, the key differentiating factor in our technique is that we make and accept speculations at a ``sequence'' level rather than at the level of single (or a few) steps. This last fact is key to unlocking our parallel runtime of $\widetilde{O}(n^{1/2})$.

</details>


### [21] [Forgetting Alternation and Blossoms: A New Framework for Fast Matching Augmentation and Its Applications to Sequential/Distributed/Streaming Computation](https://arxiv.org/abs/2511.08210)
*Taisuke Izumi,Naoki Kitamura,Yutaro Yamaguchi*

Main category: cs.DS

TL;DR: 本文提出了一种新的结构定理来分析一般图中的最短交替路径，避开了复杂的花朵结构。基于此定理开发了比Micali-Vazirani算法更易实现和验证的新算法，并在分布式和半流式设置中提出了新的(1-ε)近似算法。


<details>
  <summary>Details</summary>
Motivation: Micali-Vazirani算法虽然高效，但其正确性证明极其复杂，40多年来一直难以给出完整证明。作者希望开发更易理解和验证的匹配算法。

Method: 提出交替基树(ABT)概念，建立新的结构定理来简化最短交替路径分析，避免处理复杂的花朵结构。基于此开发新算法框架。

Result: 开发了比MV算法更易实现和验证的新算法，并在分布式和半流式设置中提出了高效的(1-ε)近似算法，显著改进了已知的时间上界。

Conclusion: 新框架通过简化交替路径分析，提供了更易理解的匹配算法设计方法，同时为近似算法提供了新的放大技术，具有独立的研究价值。

Abstract: Finding a maximum cardinality matching in a graph is one of the most fundamental problems. An algorithm proposed by Micali and Vazirani (1980) is well-known to solve the problem in $O(m\sqrt{n})$ time, which is still one of the fastest algorithms in general. While the MV algorithm itself is not so complicated and is indeed convincing, its correctness proof is extremely challenging, which can be seen from the history: after the first algorithm paper had appeared in 1980, Vazirani has made several attempts to give a complete proof for more than 40 years. It seems, roughly speaking, caused by the nice but highly complex structure of the shortest alternating paths in general graphs that are deeply intertwined with the so-called (nested) blossoms.
  In this paper, we propose a new structure theorem on the shortest alternating paths in general graphs without taking into the details of blossoms. The high-level idea is to forget the alternation (of matching and non-matching edges) as early as possible. A key ingredient is a notion of alternating base trees (ABTs) introduced by Izumi, Kitamura, and Yamaguchi (2024) to develop a nearly linear-time distributed algorithm. Our structure theorem refines the properties of ABTs exploited in their algorithm, and we also give simpler alternative proofs for them. Based on our structure theorem, we propose a new algorithm, which is slightly slower but more implementable and much easier to confirm its correctness than the MV algorithm.
  As applications of our framework, we also present new $(1 - ε)$-approximation algorithms in the distributed and semi-streaming settings. Both algorithms are deterministic, and substantially improve the best known upper bounds on the running time. The algorithms are built on the top of a novel framework of amplifying approximation factors of given matchings, which is of independent interest.

</details>


### [22] [Fully Dynamic Set Cover: Worst-Case Recourse and Update Time](https://arxiv.org/abs/2511.08485)
*Sayan Bhattacharya,Ruoxu Cen,Debmalya Panigrahi*

Main category: cs.DS

TL;DR: 提出了首个在完全动态集合覆盖问题中同时实现非平凡最坏情况边界的方法，在O(log n)和O(f)两种近似比下都能达到O(log n)的调整次数和f·polylog(n)的更新时间。


<details>
  <summary>Details</summary>
Motivation: 动态集合覆盖问题中，现有方法要么只能保证平摊边界，要么在最坏情况下更新时间优化但调整次数很差(Ω(m))。需要同时优化这两个关键指标。

Method: 开发了完全动态集合覆盖算法，在元素添加或删除时维护近似最优解，同时控制每次更新的调整集合数量和计算时间。

Result: 实现了O(log n)调整次数和f·polylog(n)更新时间的最坏情况保证，支持O(log n)和O(f)两种近似比。

Conclusion: 这是首个在动态集合覆盖中同时实现非平凡最坏情况边界的算法，解决了该领域长期存在的权衡问题。

Abstract: In (fully) dynamic set cover, the goal is to maintain an approximately optimal solution to a dynamically evolving instance of set cover, where in each step either an element is added to or removed from the instance. The two main desiderata of a dynamic set cover algorithm are to minimize at each time-step, the recourse, which is the number of sets removed from or added to the solution, and the update time to compute the updated solution. This problem has been extensively studied over the last decade leading to many results that achieve ever-improving bounds on the recourse and update time, while maintaining a solution whose cost is comparable to that of offline approximation algorithms.
  In this paper, we give the first algorithms to simultaneously achieve non-trivial worst-case bounds for recourse and update time. Specifically, we give fully-dynamic set cover algorithms that simultaneously achieve $O(\log n)$ recourse and $f\cdot \textrm{poly}\log(n)$ update time in the worst-case, for both approximation regimes: $O(\log n)$ and $O(f)$ approximation. (Here, $n, f$ respectively denote the maximum number of elements and maximum frequency of an element across all instances.) Prior to our work, all results for this problem either settled for amortized bounds on recourse and update time, or obtained $f\cdot \textrm{poly}\log(n)$ update time in the worst-case but at the cost of $Ω(m)$ worst-case recourse. (Here, $m$ denotes the number of sets. Note that any algorithm has recourse at most $m$.)

</details>


### [23] [Deterministic Negative-Weight Shortest Paths in Nearly Linear Time via Path Covers](https://arxiv.org/abs/2511.08551)
*Bernhard Haeupler,Yonggang Jiang,Thatchaphol Saranurak*

Main category: cs.DS

TL;DR: 提出了首个确定性近线性时间算法，用于解决有向图中带负权边的单源最短路径问题，时间复杂度为Õ(m)·log(nW)。


<details>
  <summary>Details</summary>
Motivation: 现有的近线性时间算法都依赖于随机化方法，特别是低直径分解，这限制了算法的确定性实现。需要克服这一障碍，为有向图设计确定性算法。

Method: 引入了有向图的新结构原语——路径覆盖(path cover)，该结构在功能上类似于无向图中的邻域覆盖，能够替代随机化方法中的低直径分解。

Result: 算法能够在Õ(m)·log(nW)时间内计算从源点s到所有顶点的距离，或者检测负环，这是首个确定性近线性时间解决方案。

Conclusion: 路径覆盖将成为有向图上设计确定性算法的基本工具，为未来相关研究提供重要基础。

Abstract: We present the first deterministic nearly-linear time algorithm for single-source shortest paths with negative edge weights on directed graphs: given a directed graph $G$ with $n$ vertices, $m$ edges whose weights are integer in $\{-W,\dots,W\}$, our algorithm either computes all distances from a source $s$ or reports a negative cycle in time $\tilde{O}(m)\cdot \log(nW)$ time.
  All known near-linear time algorithms for this problem have been inherently randomized, as they crucially rely on low-diameter decompositions.
  To overcome this barrier, we introduce a new structural primitive for directed graphs called the path cover. This plays a role analogous to neighborhood covers in undirected graphs, which have long been central to derandomizing algorithms that use low-diameter decomposition in the undirected setting. We believe that path covers will serve as a fundamental tool for the design of future deterministic algorithms on directed graphs.

</details>


### [24] [Universal Connection Schedules for Reconfigurable Networking](https://arxiv.org/abs/2511.08556)
*Shaleen Baral,Robert Kleinberg,Sylvan Martin,Henry Rogers,Tegan Wilson,Ruogu Zhang*

Main category: cs.DS

TL;DR: 本文提出了首个用于无感知路由的通用调度方案，能够在单一连接调度中同时实现多个帕累托最优权衡点，解决了现有方法对不同延迟请求类型效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有帕累托最优设计针对特定跳数h进行微调，但在实际数据中心工作负载中，存在混合的低延迟和高延迟请求，使用单一调度的设计会导致效率低下。

Method: 基于循环置换的连接调度，开发了新颖的傅里叶分析方法来分析随机路由，首先使用均匀随机连接调度，然后设计更精细的随机调度，最后通过去随机化Lovett-Meka差异最小化算法实现确定性构造。

Result: 提出的通用调度方案对所有可能的跳数h都接近最优，第一个随机构造在吞吐量上有乘法误差，延迟最优到log N因子；第二个构造在吞吐量上有加法误差，延迟最优到常数因子。

Conclusion: 成功构建了首个无感知路由的通用调度方案，能够在单一调度中灵活适应不同延迟需求，为可重构网络提供了更高效的通信解决方案。

Abstract: Reconfigurable networks are a novel communication paradigm in which the pattern of connectivity between hosts varies rapidly over time. Prior theoretical work explored the inherent tradeoffs between throughput (or, hop-count) and latency, and showed the existence of infinitely many Pareto-optimal designs as the network size tends to infinity. Existing Pareto-optimal designs use a connection schedule which is fine-tuned to the desired hop-count $h$, permitting lower latency as $h$ increases. However, in reality datacenter workloads contain a mix of low-latency and high-latency requests. Using a connection schedule fine-tuned for one request type leads to inefficiencies when serving other types.
  A more flexible and efficient alternative is a {\em universal schedule}, a single connection schedule capable of attaining many Pareto-optimal tradeoff points simultaneously, merely by varying the choice of routing paths. In this work we present the first universal schedules for oblivious routing. Our constructions yield universal schedules which are near-optimal for all possible hop-counts $h$. The key technical idea is to specialize to a type of connection schedule based on cyclic permutations and to develop a novel Fourier-analytic method for analyzing randomized routing on these connection schedules. We first show that a uniformly random connection schedule suffices with multiplicative error in throughput, and latency optimal up to a $\log N$ factor. We then show that a more carefully designed random connection schedule suffices with additive error in throughput, but improved latency optimal up to only constant factors. Finally, we show that our first randomized construction can be made deterministic using a derandomized version of the Lovett-Meka discrepancy minimization algorithm to obtain the same result.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [25] [Group Probability Decoding of Turbo Product Codes over Higher-Order Fields](https://arxiv.org/abs/2511.07731)
*Lukas Rapp,Muriel Médard,Ken R. Duffy*

Main category: cs.IT

TL;DR: 提出基于群概率的Turbo乘积码解码方法，相比传统比特概率解码，能更好地保留比特相关性，在内外相关性场景下分别获得0.3dB和0.7dB的信噪比增益。


<details>
  <summary>Details</summary>
Motivation: 传统Turbo乘积码解码在组件解码器间传递对数似然比时会丢失比特相关性信息，这种相关性可能来自外部干扰或内部解码过程。

Method: 使用群概率解码替代比特概率解码，重新研究非二进制Turbo乘积码，结合符号级ORBGRAND和软输出GRAND来部分保留比特相关性。

Result: 基于群概率的Turbo乘积码解码在内生相关性场景下获得0.3dB信噪比增益，在外生相关性场景下获得0.7dB信噪比增益。

Conclusion: 群概率解码能有效保留比特相关性，显著提升Turbo乘积码的解码性能，特别是在存在相关性的通信场景中。

Abstract: Binary turbo product codes (TPCs) are powerful error-correcting codes constructed from short component codes. Traditionally, turbo product decoding passes log likelihood ratios (LLRs) between the component decoders, inherently losing information when bit correlation exists. Such correlation can arise exogenously from sources like intersymbol interference and endogenously during component code decoding. To preserve these correlations and improve performance, we propose turbo product decoding based on group probabilities. We theoretically predict mutual information and signal-to-noise ratio (SNR) gains of group over bit-probability decoding. To translate these theoretical insights to practice, we revisit non-binary TPCs that naturally support group-probability decoding. We show that any component list decoder that takes group probabilities as input and outputs block-wise soft-output can partially preserve bit correlation, which we demonstrate with symbol-level ORBGRAND combined with soft-output GRAND (SOGRAND). Our results demonstrate that group-probability-based turbo product decoding achieves SNR gains of up to 0.3 dB for endogenous correlation and 0.7 dB for exogenous correlation, compared to bit-probability decoding.

</details>


### [26] [Digital Twin Empowered In-Vehicular Channel Modeling and Wireless Planning in the Terahertz Band](https://arxiv.org/abs/2511.07789)
*Mingjie Zhu,Yejian Lyu,Chong Han*

Main category: cs.IT

TL;DR: 本文研究了300GHz频段的车内无线信道，通过测量和数字孪生技术建立了混合射线追踪-统计信道模型，并用于无线规划分析。


<details>
  <summary>Details</summary>
Motivation: 车联网技术是智能交通系统的关键使能技术，而太赫兹频段提供了丰富的频谱资源来支持超高速、低延迟的车联网通信。

Method: 使用矢量网络分析仪进行信道测量，构建车辆的数字孪生模型，集成到开源射线追踪模拟器Sionna中，建立混合射线追踪-统计信道模型。

Result: 数字孪生赋能的仿真结果与测量数据高度一致，验证了可行性。基于验证模型进行了SINR分析、覆盖概率评估和最优发射器布局。

Conclusion: 这些发现为未来太赫兹车内通信系统的设计和部署提供了有价值的见解。

Abstract: Vehicle-to-everything (V2X) technology has emerged as a key enabler of intelligent transportation systems, while the Terahertz (THz) band offers abundant spectrum resources to support ultra-high-speed and low-latency V2X communications. This paper investigates the in-vehicle wireless channel in the 300~GHz band. First, channel measurement based on vector-network-analyzer (VNA) is conducted under typical V2X scenarios, including with/without human, and window-on/off cases. Then, a digital twin (DT) of the vehicle is constructed from high-resolution point cloud data and a measurement-based material property database. The DT is integrated into an open-source ray-tracing (RT) simulator, Sionna, to model multipath propagation. The DT-empowered simulation results are analyzed and validated with the measurement data, showing strong agreement and validating the feasibility. Finally, a hybrid ray-tracing-statistic channel model is established, combining the RT results and measurement data. Leveraging the validated model, further wireless planning is carried out, including signal-to-interference-plus-noise ratio (SINR) analysis, coverage probability evaluation, and optimal transmitter (Tx) placement. These findings provide valuable insights for the design and deployment of future THz in-vehicle communication systems.

</details>


### [27] [Variable-Length Joint Source-Channel Coding for Semantic Communication](https://arxiv.org/abs/2511.07826)
*Yujie Zhou,Rulong Wang,Yong Xiao,Yingyu Li,Guangming Shi*

Main category: cs.IT

TL;DR: 提出了一种端到端编码框架，解决数字语义通信中联合源信道编码方案与离散变长码字系统不兼容的问题，实现了物理比特级速率控制。


<details>
  <summary>Details</summary>
Motivation: 现有联合源信道编码方案产生连续编码表示，与采用离散变长码字的数字系统不兼容，导致无法实现物理比特级速率控制，影响语义传输效率。

Method: 基于信息瓶颈理论扩展，提出端到端编码框架，通过结构化解码处理码长和内容，构建可端到端训练的编码器，支持直接将数据源压缩到有限码本，使用策略梯度优化不可微操作。

Result: 实验结果表明，E2EC在低比特率下实现高质量推理，性能优于与数字语义通信系统兼容的代表性基线方法。

Conclusion: 所提出的端到端编码框架有效解决了数字语义通信中的兼容性问题，实现了高效的语义传输。

Abstract: This paper investigates a key challenge faced by joint source-channel coding (JSCC) in digital semantic communication (SemCom): the incompatibility between existing JSCC schemes that yield continuous encoded representations and digital systems that employ discrete variable-length codewords. It further results in feasibility issues in achieving physical bit-level rate control via such JSCC approaches for efficient semantic transmission. In this paper, we propose a novel end-to-end coding (E2EC) framework to tackle it. The semantic coding problem is formed by extending the information bottleneck (IB) theory over noisy channels, which is a tradeoff between bit-level communication rate and semantic distortion. With a structural decomposition of encoding to handle code length and content respectively, we can construct an end-to-end trainable encoder that supports the direct compression of a data source into a finite codebook. To optimize our E2EC across non-differentiable operations, e.g., sampling, we use the powerful policy gradient to support gradient-based updates. Experimental results illustrate that E2EC achieves high inference quality with low bit rates, outperforming representative baselines compatible with digital SemCom systems.

</details>


### [28] [Symbol Detection in Multi-channel Multi-tag Ambient Backscatter Communication Under IQ Imbalance](https://arxiv.org/abs/2511.08182)
*Yuxin Li,Guangyue Lu,Yinghui Ye,Liqin Shi,Daniel Benevides da Costa*

Main category: cs.IT

TL;DR: 本文研究了多通道多标签环境反向散射通信中由IQ不平衡引起的图像信道串扰问题，提出了一种包含IQ不平衡参数的符号检测模型，推导了误码率的闭式表达式和最小化误码率的近最优检测阈值，并提出了一种无需先验信息的阈值估计方法。


<details>
  <summary>Details</summary>
Motivation: 在多通道多标签环境反向散射通信中，IQ不平衡导致的图像信道串扰是符号检测面临的主要挑战之一，现有方法在此情况下会导致误码率性能显著下降。

Method: 提出了包含IQ不平衡参数的符号检测模型，推导了误码率闭式表达式和近最优检测阈值，并设计了基于接收样本的阈值估计方法来消除对先验信息的依赖。

Result: 数值结果表明，在IQ不平衡情况下，直接使用现有方法会导致误码率性能显著下降，但通过本文提出的检测阈值可以有效缓解这种性能下降。

Conclusion: 本文提出的符号检测模型和阈值估计方法能够有效解决多通道多标签环境反向散射通信中IQ不平衡导致的图像信道串扰问题，显著提升系统性能。

Abstract: Ambient backscatter communication (AmBC) offers low-cost and low-power connectivity for Internet of Things (IoT), where a backscatter tag (BT) modulates incident signals transmitted by an ambient radio frequency (RF) source and reflects them to its associated AmBC receiver. In multi-channel multi-tag AmBC, one of major challenges from the aspect of symbol detection is the image channel crosstalk, which is induced by the inevitable in-phase/quadrature (IQ) imbalance. To address this issue, in this paper, we study symbol detection in multi-channel multi-tag AmBC under IQ imbalance. Considering the differential encoding scheme at the BTs, we propose a novel symbol detection model that incorporates IQ imbalance parameters, the presence or absence of both the incident signal and the backscattered signal of the image channel. On this basis, considering an energy difference detector at the AmBC receiver, we derive the closed-form expressions for the bit error rate (BER) as well as the near-optimal detection threshold to minimize BER. However, calculating the near-optimal detection threshold requires prior information, such as the IQ imbalance parameters, the presence probability of the incident signal of the image channel and the backscattered signal of the image channel, the signal power of the ambient RF source, and the noise power, which are typically unknown to the AmBC receiver in practice. To eliminate the need for the prior information, we propose a threshold estimation method using the received samples. Numerical results indicate that under IQ imbalance, directly using the existing method leads to a significant degradation in BER performance. However, this degradation can be effectively mitigated by our derived detection threshold.

</details>


### [29] [Dynamic Downlink-Uplink Spectrum Sharing between Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2511.08188)
*Sourav Mukherjee,Bho Matthiesen,Armin Dekorsy,Petar Popovski*

Main category: cs.IT

TL;DR: 提出动态重分配FDD频段的方法来管理密集卫星部署中的干扰，通过联合优化频段分配、用户调度和功率分配，在密集部署中实现高达94%的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 6G网络集成低地球轨道卫星以实现全球连接，但密集巨型星座在共享频段上会产生严重干扰，传统FDD系统的固定频段分配缺乏灵活性。

Method: 制定联合优化问题，结合动态频段分配、用户调度和双向功率分配，使用等效变换、交替优化和工业级混合整数求解器解决非凸混合整数问题。

Result: 数值结果表明，动态FDD频段分配方法相比传统FDD显著提升系统性能，在密集部署中吞吐量提升高达94%。

Conclusion: 动态FDD频段分配是管理密集卫星部署干扰的有效方法，能大幅提升系统吞吐量性能。

Abstract: 6G networks are expected to integrate low Earth orbit satellites to ensure global connectivity by extending coverage to underserved and remote regions. However, the deployment of dense mega-constellations introduces severe interference among satellites operating over shared frequency bands. This is, in part, due to the limited flexibility of conventional frequency division duplex (FDD) systems, where fixed bands for downlink (DL) and uplink (UL) transmissions are employed. In this work, we propose dynamic re-assignment of FDD bands for improved interference management in dense deployments and evaluate the performance gain of this approach. To this end, we formulate a joint optimization problem that incorporates dynamic band assignment, user scheduling, and power allocation in both directions. This non-convex mixed integer problem is solved using a combination of equivalence transforms, alternating optimization, and state-of-the-art industrial-grade mixed integer solvers. Numerical results demonstrate that the proposed approach of dynamic FDD band assignment significantly enhances system performance over conventional FDD, achieving up to 94\% improvement in throughput in dense deployments.

</details>


### [30] [Analysis of SINR Coverage in LEO Satellite Networks through Spatial Network Calculus](https://arxiv.org/abs/2511.08255)
*Yuting Tang,Yufan He,Yi Zhong,Xijun Wang,Tony Q. S. Quek,Howard H. Yang*

Main category: cs.IT

TL;DR: 提出了基于空间网络演算的新分析框架，用于评估低地球轨道卫星网络性能。该框架将卫星位置建模为球面上的强球调节点过程，推导了Nakagami-m和瑞利衰落下的条件覆盖概率下界，并通过Starlink星座验证了模型有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LEO卫星网络性能分析缺乏合适的理论模型，需要开发能够反映卫星间安全距离约束的分析框架。

Method: 将卫星空间位置建模为强球调节点过程，体现卫星间的局部排斥特性；推导Nakagami-m和瑞利衰落下的条件覆盖概率下界；通过与Starlink星座对比验证模型。

Result: 分析结果提供了实际覆盖概率的紧下界，与经验模拟几乎完美匹配（仅1dB偏移），证明该框架是LEO卫星网络的合适理论模型。

Conclusion: 所提出的空间网络演算框架为LEO卫星网络性能评估提供了有效的理论工具，具有低计算复杂度，能够准确预测网络覆盖性能。

Abstract: We introduce a new analytical framework, developed based on the spatial network calculus, for performance assessment of Low Earth Orbit (LEO) satellite networks. Specifically, we model the satellites' spatial positions as a strong ball-regulated point process on the sphere. Under this model, proximal points in space exhibit a locally repulsive property, reflecting the fact that intersatellite links are protected by a safety distance and would not be arbitrarily close. Subsequently, we derive analytical lower bounds on the conditional coverage probabilities under Nakagami-$m$ and Rayleigh fading, respectively. These expressions have a low computational complexity, enabling efficient numerical evaluations. We validate the effectiveness of our theoretical model by contrasting the coverage probability obtained from our analysis with that estimated from a Starlink constellation. The results show that our analysis provides a tight lower bound on the actual value and, surprisingly, matches the empirical simulations almost perfectly with a 1 dB shift. This demonstrates our framework as an appropriate theoretical model for LEO satellite networks.

</details>


### [31] [Robust Dynamic Coded Distributed Storage with Partially Storage Constrained Servers](https://arxiv.org/abs/2511.08278)
*Chen Zhao,Haobo Jia,Zhuqing Jia*

Main category: cs.IT

TL;DR: 本文完全刻画了具有部分存储约束服务器的鲁棒动态编码分布式存储系统的基本极限，包括最小可用服务器数量和读写操作的最小通信成本。


<details>
  <summary>Details</summary>
Motivation: 解决在部分服务器存储受限的情况下，如何实现鲁棒（抵抗服务器丢失）且高效（通信成本低）的读写操作问题。

Method: 基于Jia等人的已有研究，开发了新的逆论据和编码设计方法。

Result: 完全表征了具有部分存储约束服务器的RDCDS系统的基本极限。

Conclusion: 该工作为具有部分存储约束的鲁棒动态编码分布式存储系统提供了完整的理论框架和性能极限分析。

Abstract: We consider the problem of Robust Dynamic Coded Distributed Storage (RDCDS) with partially storage constrained servers where the goal is to enable robust (resilient to server dropouts) and efficient (as measured by the communication costs) read and update operations, subject to the constraint that the storage at $S$ out of $N$ servers is limited by $1/K_c$ the size of the message. Building upon previously established converse arguments and achievability schemes by Jia et al., in this work we develop a set of new converse arguments and coding designs that enable us to completely characterize the fundamental limits of RDCDS with partially storage constrained servers, i.e., the minimum number of available servers for feasible update operation and the minimum communication costs for read and update operations across various server dropout scenarios.

</details>


### [32] [Cartesian square-free codes](https://arxiv.org/abs/2511.08304)
*Cícero Carvalho,Hiram H. López,Rodrigo San-José*

Main category: cs.IT

TL;DR: 本文引入了笛卡尔无平方码，并利用交换代数工具（足迹界）给出了这类码的部分广义汉明权重的显式公式，并将结果推广到射影空间上的求值码。


<details>
  <summary>Details</summary>
Motivation: 广义汉明权重（GHWs）扩展了线性码的最小距离概念，从一维子空间的最小支撑基数扩展到r维子空间的最小支撑基数。研究这类参数有助于更深入地理解线性码的结构特性。

Method: 引入笛卡尔无平方码（由在笛卡尔集上求值无平方单项式生成的线性码），使用交换代数工具特别是足迹界方法，推导广义汉明权重的显式表达式。

Result: 获得了笛卡尔无平方码部分广义汉明权重的显式公式，并将这些结果成功推广到射影空间上的求值码。

Conclusion: 通过交换代数方法，成功建立了笛卡尔无平方码的广义汉明权重理论框架，为研究更广泛类型的求值码提供了有效工具。

Abstract: The generalized Hamming weights (GHWs) of a linear code C extend the concept of minimum distance, which is the minimum cardinality of the support of all one-dimensional subspaces of C, to the minimum cardinality of the support of all r-dimensional subspaces of the code. In this work, we introduce Cartesian square-free codes, which are linear codes generated by evaluating square-free monomials over a Cartesian set. We use commutative algebraic tools, specifically the footprint bound, to provide explicit formulas for some of the GHWs of this family of codes, and we show how we can translate these results to evaluation codes over the projective space.

</details>


### [33] [A General Ziv-Zakai Bound for DoA Estimation in MIMO Radar Systems](https://arxiv.org/abs/2511.08326)
*Mohammadreza Bakhshizadeh Mohajer,Daniela Tuninetti,Luca Barletta*

Main category: cs.IT

TL;DR: 本文推导了共址MIMO雷达系统中DoA估计的Ziv-Zakai下界，提供了适用于多目标场景的闭式表达式，考虑了输入协方差矩阵、目标RCS统计和多快照效应，揭示了MSE与发射天线数、目标数、SNR和发射协方差矩阵的关系。


<details>
  <summary>Details</summary>
Motivation: 传统结果主要针对单输入多输出系统和高斯输入信号，需要开发适用于MIMO雷达系统且考虑实际系统参数（如输入协方差矩阵、多目标场景）的更准确性能边界。

Method: 推导了Ziv-Zakai下界（ZZB），考虑了通用输入协方差矩阵、目标雷达散射截面统计特性和多快照效应，获得了紧凑的闭式表达式。

Result: 数值仿真验证了ZZB在先验主导区域的紧致性，增加发射天线数可压缩过渡到CRB的阈值SNR，目标数变化会改变边界在不同SNR区间的行为。ZZB在低SNR区域比CRB更紧致。

Conclusion: ZZB为MIMO雷达DoA估计提供了比CRB更准确、更紧致的性能边界，特别是在低SNR区域，对系统设计具有重要指导意义。

Abstract: This paper derives a Ziv-Zakai Bound (ZZB) on the Mean Squared Error (MSE) for Direction-of-Arrival (DoA) estimation in co-located Multiple-Input Multiple-Output (MIMO) radar systems and provides closed-form expressions that hold for multi-target scenarios. Unlike classical results that address single-input multiple-output systems with complex Gaussian input signals, the developed ZZB in this paper explicitly accounts for a general input covariance matrix, target radar cross-section statistics and multiple snapshot effects, and admits a compact expression that reveals the dependence of the MSE on the number of transmit antennas, number of targets, Signal-to-Noise Ratio (SNR) and the transmit covariance matrix. Numerical simulations validate the tightness of the ZZB in the a priori dominated region and show how the increase of the number of transmit antennas compresses the threshold SNR for the transition to the Cramer-Rao bound (CRB) while the variation of the number of targets shifts the bound's behavior across SNR regimes. The analytical results and numerical simulations demonstrate that the ZZB is tighter than the CRB, particularly in the low SNR regime.

</details>
