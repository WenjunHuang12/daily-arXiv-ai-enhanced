{"id": "2510.16388", "categories": ["cs.DB", "H.2; H.4.0"], "pdf": "https://arxiv.org/pdf/2510.16388", "abs": "https://arxiv.org/abs/2510.16388", "authors": ["Doriana Armenise", "Ginevra Battello", "Andrea Brunello", "Lorenza Driul", "Angelo Montanari", "Elisa Rizzante", "Nicola Saccomanno", "Andrea Salvador", "Serena Xodo", "Silvia Zermano"], "title": "Unified Peripartum Database with Natural-Language-to-SQL Capabilities at Udine University Hospital: Design and Prototype", "comment": null, "summary": "The fragmentation of obstetric information across electronic health record\nmodules, device repositories, and laboratory systems, as it is common in\nhospitals, hinders both intrapartum care and reproducible research. In this\nwork, we present a practical blueprint for transforming heterogeneous\nperipartum records into computable, queryable assets by designing and\nprototyping a unified peripartum relational database with\nnatural-language-to-SQL (NL2SQL) capabilities at the Obstetrics Clinic of Udine\nUniversity Hospital. Requirements were co-defined with clinicians and\nformalized as an Entity-Relationship diagram, from which the logical schema and\nSQL implementation of the database were then derived. The latter integrates\nheterogeneous sources to connect maternal anamnestic and longitudinal history,\ncurrent-pregnancy findings, intrapartum course, and delivery and neonatal\noutcomes. The NL2SQL layer enables clinicians to pose natural-language queries\nto the system, lowering barriers to audit and exploratory analysis."}
{"id": "2510.16470", "categories": ["cs.DB", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16470", "abs": "https://arxiv.org/abs/2510.16470", "authors": ["Elham Khabiri", "Jeffrey O. Kephart", "Fenno F. Heath III", "Srideepika Jayaraman", "Fateh A. Tipu", "Yingjie Li", "Dhruv Shah", "Achille Fokoue", "Anu Bhamidipaty"], "title": "Declarative Techniques for NL Queries over Heterogeneous Data", "comment": null, "summary": "In many industrial settings, users wish to ask questions in natural language,\nthe answers to which require assembling information from diverse structured\ndata sources. With the advent of Large Language Models (LLMs), applications can\nnow translate natural language questions into a set of API calls or database\ncalls, execute them, and combine the results into an appropriate natural\nlanguage response. However, these applications remain impractical in realistic\nindustrial settings because they do not cope with the data source heterogeneity\nthat typifies such environments. In this work, we simulate the heterogeneity of\nreal industry settings by introducing two extensions of the popular Spider\nbenchmark dataset that require a combination of database and API calls. Then,\nwe introduce a declarative approach to handling such data heterogeneity and\ndemonstrate that it copes with data source heterogeneity significantly better\nthan state-of-the-art LLM-based agentic or imperative code generation systems.\nOur augmented benchmarks are available to the research community."}
{"id": "2510.17089", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.17089", "abs": "https://arxiv.org/abs/2510.17089", "authors": ["Christian Imenkamp", "Andrea Maldonado", "Hendrik Reiter", "Martin Werner", "Wilhelm Hasselbring", "Agnes Koschmider", "Andrea Burattin"], "title": "AVOCADO: The Streaming Process Mining Challenge", "comment": "12 pages, 4 figures", "summary": "Streaming process mining deals with the real-time analysis of streaming data.\nEvent streams require algorithms capable of processing data incrementally. To\nsystematically address the complexities of this domain, we propose AVOCADO, a\nstandardized challenge framework that provides clear structural divisions:\nseparating the concept and instantiation layers of challenges in streaming\nprocess mining for algorithm evaluation. The AVOCADO evaluates algorithms on\nstreaming-specific metrics like accuracy, Mean Absolute Error (MAE), Root Mean\nSquare Error (RMSE), Processing Latency, and robustness. This initiative seeks\nto foster innovation and community-driven discussions to advance the field of\nstreaming process mining. We present this framework as a foundation and invite\nthe community to contribute to its evolution by suggesting new challenges, such\nas integrating metrics for system throughput and memory consumption, and\nexpanding the scope to address real-world stream complexities like out-of-order\nevent arrival."}
{"id": "2510.17301", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17301", "abs": "https://arxiv.org/abs/2510.17301", "authors": ["Panos Kalnis. Shuo Shang", "Christian S. Jensen"], "title": "Comprehending Spatio-temporal Data via Cinematic Storytelling using Large Language Models", "comment": "5 pages", "summary": "Spatio-temporal data captures complex dynamics across both space and time,\nyet traditional visualizations are complex, require domain expertise and often\nfail to resonate with broader audiences. Here, we propose MapMuse, a\nstorytelling-based framework for interpreting spatio-temporal datasets,\ntransforming them into compelling, narrative-driven experiences. We utilize\nlarge language models and employ retrieval augmented generation (RAG) and\nagent-based techniques to generate comprehensive stories. Drawing on principles\ncommon in cinematic storytelling, we emphasize clarity, emotional connection,\nand audience-centric design. As a case study, we analyze a dataset of taxi\ntrajectories. Two perspectives are presented: a captivating story based on a\nheat map that visualizes millions of taxi trip endpoints to uncover urban\nmobility patterns; and a detailed narrative following a single long taxi\njourney, enriched with city landmarks and temporal shifts. By portraying\nlocations as characters and movement as plot, we argue that data storytelling\ndrives insight, engagement, and action from spatio-temporal information. The\ncase study illustrates how MapMuse can bridge the gap between data complexity\nand human understanding. The aim of this short paper is to provide a glimpse to\nthe potential of the cinematic storytelling technique as an effective\ncommunication tool for spatio-temporal data, as well as to describe open\nproblems and opportunities for future research."}
{"id": "2510.17234", "categories": ["cs.MM", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17234", "abs": "https://arxiv.org/abs/2510.17234", "authors": ["Yuyang Hong", "Qi Yang", "Tao Zhang", "Zili Wang", "Zhaojin Fu", "Kun Ding", "Bin Fan", "Shiming Xiang"], "title": "Taming Modality Entanglement in Continual Audio-Visual Segmentation", "comment": null, "summary": "Recently, significant progress has been made in multi-modal continual\nlearning, aiming to learn new tasks sequentially in multi-modal settings while\npreserving performance on previously learned ones. However, existing methods\nmainly focus on coarse-grained tasks, with limitations in addressing modality\nentanglement in fine-grained continual learning settings. To bridge this gap,\nwe introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to\ncontinuously segment new classes guided by audio. Through comprehensive\nanalysis, two critical challenges are identified: 1) multi-modal semantic\ndrift, where a sounding objects is labeled as background in sequential tasks;\n2) co-occurrence confusion, where frequent co-occurring classes tend to be\nconfused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework\nis designed to address these challenges. Specifically, for multi-modal semantic\ndrift, a Multi-modal Sample Selection (MSS) strategy is proposed to select\nsamples with high modal consistency for rehearsal. Meanwhile, for co-occurence\nconfusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed,\nallowing for the increase of rehearsal sample frequency of those confusable\nclasses during training process. Moreover, we construct three audio-visual\nincremental scenarios to verify effectiveness of our method. Comprehensive\nexperiments demonstrate that our method significantly outperforms single-modal\ncontinual learning methods."}
{"id": "2510.16385", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.16385", "abs": "https://arxiv.org/abs/2510.16385", "authors": ["Naoyuki Kamiyama"], "title": "The Strongly Stable Roommates Problem and Linear Programming", "comment": null, "summary": "The stable roommates problem is a non-bipartite version of the stable\nmatching problem in a bipartite graph. In this paper, we consider the stable\nroommates problem with ties. In particular, we focus on strong stability, which\nis one of the main stability concepts in the stable roommates problem with\nties. We propose a new polynomial-time algorithm for the problem of checking\nthe existence of a strongly stable matching in the stable roommates problem\nwith ties. More concretely, we extend the linear programming approach of\nAbeledo and Blum to the stable roommates problem with strict preferences to our\nproblem."}
{"id": "2510.16055", "categories": ["cs.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.16055", "abs": "https://arxiv.org/abs/2510.16055", "authors": ["Norman Zadeh"], "title": "Is Zadeh's Least-Entered Pivot Rule Exponential?", "comment": "8 pages, 1 figure", "summary": "In 2011, Friedmann [F 7] claimed to have proved that pathological linear\nprograms existed for which the Simplex method using Zadeh's least-entered rule\n[Z 14] would take an exponential number of pivots. In 2019, Disser and Hopp [DH\n5] argued that there were errors in Friedmann's 2011 construction. In 2020,\nDisser, Friedmann, and Hopp [DFH 3,4] again contended that the least-entered\nrule was exponential. We show that their arguments contain multiple flaws. In\nother words, the worst-case behavior of the least-entered rule has not been\nestablished. Neither [F 7] nor [DFH 3,4] provides pathological linear programs\nthat can be tested. Instead, the authors contend that their pathological linear\nprograms are of the form (P) as shown on page 12 of [DFH 3]. The authors\ncontend that the constraints of (P) ensure that the probability of entering a\nvertex u is equal to the probability of exiting u. In fact, we note that the\nauthors' constraints (P) are flawed in at least three ways: a) they require the\nprobability of exiting u to exceed the probability of entering u, b) they\nrequire the probability of exiting some nodes to exceed 1, and c) they overlook\nflows from decision nodes to decision nodes. At my request, in August of 2025,\nDisser, Friedmann, and Hopp provided me with their first ten purportedly\npathological LPs and the graph of their first purportedly pathological Markov\nDecision Process (MDP1). It is shown that: a) their first two pathological LPs\nare infeasible if the variables are supposed to be probabilities, as the\nauthors contend, and b) their first purportedly pathological LP does not match\nup with their first purportedly pathological MDP. In other words, the authors\nhave not come close to providing counterexamples to the least-entered rule."}
{"id": "2510.15871", "categories": ["cs.IT", "cs.AI", "cs.LG", "math.IT", "math.PR", "94A17, 94A15, 68T05, 62F15, 68P30, 68T27, 68T50, 30B42", "H.1.1; I.1.2; I.2.6; I.2.8; I.2.4; E.4; G.1.6"], "pdf": "https://arxiv.org/pdf/2510.15871", "abs": "https://arxiv.org/abs/2510.15871", "authors": ["Chenguang Lu"], "title": "A Semantic Generalization of Shannon's Information Theory and Applications", "comment": "45 pages, 18 Figures, a review paper", "summary": "Does semantic communication require a semantic information theory parallel to\nShannon's information theory, or can Shannon's work be generalized for semantic\ncommunication? This paper advocates for the latter and introduces a semantic\ngeneralization of Shannon's information theory (G theory for short). The core\nidea is to replace the distortion constraint with the semantic constraint,\nachieved by utilizing a set of truth functions as a semantic channel. These\ntruth functions enable the expressions of semantic distortion, semantic\ninformation measures, and semantic information loss. Notably, the maximum\nsemantic information criterion is equivalent to the maximum likelihood\ncriterion and similar to the Regularized Least Squares criterion. This paper\nshows G theory's applications to daily and electronic semantic communication,\nmachine learning, constraint control, Bayesian confirmation, portfolio theory,\nand information value. The improvements in machine learning methods involve\nmultilabel learning and classification, maximum mutual information\nclassification, mixture models, and solving latent variables. Furthermore,\ninsights from statistical physics are discussed: Shannon information is similar\nto free energy; semantic information to free energy in local equilibrium\nsystems; and information efficiency to the efficiency of free energy in\nperforming work. The paper also proposes refining Friston's minimum free energy\nprinciple into the maximum information efficiency principle. Lastly, it\ncompares G theory with other semantic information theories and discusses its\nlimitation in representing the semantics of complex data."}
{"id": "2510.16334", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16334", "abs": "https://arxiv.org/abs/2510.16334", "authors": ["Eden Shaveet", "Crystal Su", "Daniel Hsu", "Luis Gravano"], "title": "Investigating the Association Between Text-Based Indications of Foodborne Illness from Yelp Reviews and New York City Health Inspection Outcomes (2023)", "comment": "Presented as a poster at Data Science Day 2024", "summary": "Foodborne illnesses are gastrointestinal conditions caused by consuming\ncontaminated food. Restaurants are critical venues to investigate outbreaks\nbecause they share sourcing, preparation, and distribution of foods. Public\nreporting of illness via formal channels is limited, whereas social media\nplatforms host abundant user-generated content that can provide timely public\nhealth signals. This paper analyzes signals from Yelp reviews produced by a\nHierarchical Sigmoid Attention Network (HSAN) classifier and compares them with\nofficial restaurant inspection outcomes issued by the New York City Department\nof Health and Mental Hygiene (NYC DOHMH) in 2023. We evaluate correlations at\nthe Census tract level, compare distributions of HSAN scores by prevalence of\nC-graded restaurants, and map spatial patterns across NYC. We find minimal\ncorrelation between HSAN signals and inspection scores at the tract level and\nno significant differences by number of C-graded restaurants. We discuss\nimplications and outline next steps toward address-level analyses."}
{"id": "2510.17326", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.17326", "abs": "https://arxiv.org/abs/2510.17326", "authors": ["Kun Yu", "Jiabao Jin", "Xiaoyao Zhong", "Peng Cheng", "Lei Chen", "Zhitao Shen", "Jingkuan Song", "Hengtao Shen", "Xuemin Lin"], "title": "Approximate Nearest Neighbor Search of Large Scale Vectors on Distributed Storage", "comment": null, "summary": "Approximate Nearest Neighbor Search (ANNS) in high-dimensional space is an\nessential operator in many online services, such as information retrieval and\nrecommendation. Indices constructed by the state-of-the-art ANNS algorithms\nmust be stored in single machine's memory or disk for high recall rate and\nthroughput, suffering from substantial storage cost, constraint of limited\nscale and single point of failure. While distributed storage can provide a\ncost-effective and robust solution, there is no efficient and effective\nalgorithms for indexing vectors in distributed storage scenarios. In this\npaper, we present a new graph-cluster hybrid indexing and search system which\nsupports Distributed Storage Approximate Nearest Neighbor Search, called DSANN.\nDSANN can efficiently index, store, search billion-scale vector database in\ndistributed storage and guarantee the high availability of index service. DSANN\nemploys the concurrent index construction method to significantly reduces the\ncomplexity of index building. Then, DSANN applies Point Aggregation Graph to\nleverage the structural information of graph to aggregate similar vectors,\noptimizing storage efficiency and improving query throughput via asynchronous\nI/O in distributed storage. Through extensive experiments, we demonstrate DSANN\ncan efficiently and effectively index, store and search large-scale vector\ndatasets in distributed storage scenarios."}
{"id": "2510.16869", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.16869", "abs": "https://arxiv.org/abs/2510.16869", "authors": ["Yuan Deng", "Yilin Li", "Wei Tang", "Hanrui Zhang"], "title": "No-Regret Online Autobidding Algorithms in First-price Auctions", "comment": "12 pages (main); appendix included. Conference version to appear in\n  the proceeding of the 39th Conference on Neural Information Processing\n  Systems (NeurIPS'25)", "summary": "Automated bidding to optimize online advertising with various constraints,\ne.g. ROI constraints and budget constraints, is widely adopted by advertisers.\nA key challenge lies in designing algorithms for non-truthful mechanisms with\nROI constraints. While prior work has addressed truthful auctions or\nnon-truthful auctions with weaker benchmarks, this paper provides a significant\nimprovement: We develop online bidding algorithms for repeated first-price\nauctions with ROI constraints, benchmarking against the optimal randomized\nstrategy in hindsight. In the full feedback setting, where the maximum\ncompeting bid is observed, our algorithm achieves a near-optimal\n$\\widetilde{O}(\\sqrt{T})$ regret bound, and in the bandit feedback setting\n(where the bidder only observes whether the bidder wins each auction), our\nalgorithm attains $\\widetilde{O}(T^{3/4})$ regret bound."}
{"id": "2510.16330", "categories": ["cs.DS", "cs.DM"], "pdf": "https://arxiv.org/pdf/2510.16330", "abs": "https://arxiv.org/abs/2510.16330", "authors": ["Daniel Paul-Pena", "C. Seshadhri"], "title": "Near-linear time subhypergraph counting in bounded degeneracy hypergraphs", "comment": null, "summary": "Counting small patterns in a large dataset is a fundamental algorithmic task.\nThe most common version of this task is subgraph/homomorphism counting, wherein\nwe count the number of occurrences of a small pattern graph $H$ in an input\ngraph $G$. The study of this problem is a field in and of itself. Recently,\nboth in theory and practice, there has been an interest in \\emph{hypergraph}\nalgorithms, where $G = (V,E)$ is a hypergraph. One can view $G$ as a set system\nwhere hyperedges are subsets of the universe $V$.\n  Counting patterns $H$ in hypergraphs is less studied, although there are many\napplications in network science and database algorithms. Inspired by advances\nin the graph literature, we study when linear time algorithms are possible.\n  We focus on input hypergraphs $G$ that have bounded \\emph{degeneracy}, a\nwell-studied concept for graph algorithms. We give a spectrum of definitions\nfor hypergraph degeneracy that cover all existing notions. For each such\ndefinition, we give a precise characterization of the patterns $H$ that can be\ncounted in (near) linear time. Specifically, we discover a set of ``obstruction\npatterns\". If $H$ does not contain an obstruction, then the number of\n$H$-subhypergraphs can be counted exactly in $O(n\\log n)$ time (where $n$ is\nthe number of vertices in $G$). If $H$ contains an obstruction, then (assuming\nhypergraph variants of fine-grained complexity conjectures), there is a\nconstant $\\gamma > 0$, such that there is no $o(n^{1+\\gamma})$ time algorithm\nfor counting $H$-subhypergraphs. These sets of obstructions can be defined for\nall notions of hypergraph degeneracy."}
{"id": "2510.16432", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.16432", "abs": "https://arxiv.org/abs/2510.16432", "authors": ["Zahra Mobini", "Ahmet Hasim Gokceoglu", "Li Wang", "Gunnar Peters", "Hyundong Shin", "Hien Quoc Ngo"], "title": "Cluster-wise processing in fronthaul-aware cell-free massive MIMO systems", "comment": null, "summary": "We exploit a general cluster-based network architecture for a\nfronthaul-limited user-centric cell-free massive multiple-input multiple-output\n(CF-mMIMO) system under different degrees of cooperation among the access\npoints (APs) to achieve scalable implementation. In particular, we consider a\nCF-mMIMO system wherein the available APs are grouped into multiple processing\nclusters (PCs) to share channel state information (CSI), ensuring that they\nhave knowledge of the CSI for all users assigned to the given cluster for the\npurposes of designing resource allocation and precoding. We utilize the sum\npseudo-SE metric, which accounts for intra-cluster interference and\nintercluster-leakage, providing a close approximation to the true sum\nachievable SE. For a given PC, we formulate two optimization problems to\nmaximize the cluster-wise weighted sum pseudo-SE under fronthaul constraints,\nrelying solely on local CSI. These optimization problems are associated with\ndifferent computational complexity requirements. The first optimization problem\njointly designs precoding, user association, and power allocation, and is\nperformed at the small-scale fading time scale. The second optimization problem\noptimizes user association and power allocation at the large-scale fading time\nscale. Accordingly, we develop a novel application of modified weighted minimum\nmean square error (WMMSE)-based approach to solve the challenging formulated\nnon-convex mixed-integer problems."}
{"id": "2510.16393", "categories": ["cs.IR", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.16393", "abs": "https://arxiv.org/abs/2510.16393", "authors": ["Franco Maria Nardini", "Raffaele Perego", "Nicola Tonellotto", "Salvatore Trani"], "title": "Blending Learning to Rank and Dense Representations for Efficient and Effective Cascades", "comment": null, "summary": "We investigate the exploitation of both lexical and neural relevance signals\nfor ad-hoc passage retrieval. Our exploration involves a large-scale training\ndataset in which dense neural representations of MS-MARCO queries and passages\nare complemented and integrated with 253 hand-crafted lexical features\nextracted from the same corpus. Blending of the relevance signals from the two\ndifferent groups of features is learned by a classical Learning-to-Rank (LTR)\nmodel based on a forest of decision trees. To evaluate our solution, we employ\na pipelined architecture where a dense neural retriever serves as the first\nstage and performs a nearest-neighbor search over the neural representations of\nthe documents. Our LTR model acts instead as the second stage that re-ranks the\nset of candidates retrieved by the first stage to enhance effectiveness. The\nresults of reproducible experiments conducted with state-of-the-art dense\nretrievers on publicly available resources show that the proposed solution\nsignificantly enhances the end-to-end ranking performance while relatively\nminimally impacting efficiency. Specifically, we achieve a boost in nDCG@10 of\nup to 11% with an increase in average query latency of only 4.3%. This confirms\nthe advantage of seamlessly combining two distinct families of signals that\nmutually contribute to retrieval effectiveness."}
{"id": "2510.17586", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.17586", "abs": "https://arxiv.org/abs/2510.17586", "authors": ["Boyan Li", "Chong Chen", "Zhujun Xue", "Yinan Mei", "Yuyu Luo"], "title": "DeepEye-SQL: A Software-Engineering-Inspired Text-to-SQL Framework", "comment": null, "summary": "Large language models (LLMs) have advanced Text-to-SQL, yet existing\nsolutions still fall short of system-level reliability. The limitation is not\nmerely in individual modules - e.g., schema linking, reasoning, and\nverification - but more critically in the lack of structured orchestration that\nenforces correctness across the entire workflow. This gap motivates a paradigm\nshift: treating Text-to-SQL not as free-form language generation but as a\nsoftware-engineering problem that demands structured, verifiable orchestration.\nWe present DeepEye-SQL, a software-engineering-inspired framework that reframes\nText-to-SQL as the development of a small software program, executed through a\nverifiable process guided by the Software Development Life Cycle (SDLC).\nDeepEye-SQL integrates four synergistic stages: it grounds ambiguous user\nintent through semantic value retrieval and robust schema linking; enhances\nfault tolerance with N-version SQL generation using diverse reasoning\nparadigms; ensures deterministic verification via a tool-chain of unit tests\nand targeted LLM-guided revision; and introduces confidence-aware selection\nthat clusters execution results to estimate confidence and then takes a\nhigh-confidence shortcut or runs unbalanced pairwise adjudication in\nlow-confidence cases, yielding a calibrated, quality-gated output. This\nSDLC-aligned workflow transforms ad hoc query generation into a disciplined\nengineering process. Using ~30B open-source LLMs without any fine-tuning,\nDeepEye-SQL achieves 73.5% execution accuracy on BIRD-Dev and 89.8% on\nSpider-Test, outperforming state-of-the-art solutions. This highlights that\nprincipled orchestration, rather than LLM scaling alone, is key to achieving\nsystem-level reliability in Text-to-SQL."}
{"id": "2510.17067", "categories": ["cs.GT", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.17067", "abs": "https://arxiv.org/abs/2510.17067", "authors": ["Ioannis Anagnostides", "Emanuel Tewolde", "Brian Hu Zhang", "Ioannis Panageas", "Vincent Conitzer", "Tuomas Sandholm"], "title": "Convergence of Regret Matching in Potential Games and Constrained Optimization", "comment": null, "summary": "Regret matching (RM} -- and its modern variants -- is a foundational online\nalgorithm that has been at the heart of many AI breakthrough results in solving\nbenchmark zero-sum games, such as poker. Yet, surprisingly little is known so\nfar in theory about its convergence beyond two-player zero-sum games. For\nexample, whether regret matching converges to Nash equilibria in potential\ngames has been an open problem for two decades. Even beyond games, one could\ntry to use RM variants for general constrained optimization problems. Recent\nempirical evidence suggests that they -- particularly regret matching$^+$\n(RM$^+$) -- attain strong performance on benchmark constrained optimization\nproblems, outperforming traditional gradient descent-type algorithms.\n  We show that alternating RM$^+$ converges to an $\\epsilon$-KKT point after\n$O_\\epsilon(1/\\epsilon^4)$ iterations, establishing for the first time that it\nis a sound and fast first-order optimizer. Our argument relates the KKT gap to\nthe accumulated regret, two quantities that are entirely disparate in general\nbut interact in an intriguing way in our setting, so much so that when regrets\nare bounded, our complexity bound improves all the way to\n$O_\\epsilon(1/\\epsilon^2)$. From a technical standpoint, while RM$^+$ does not\nhave the usual one-step improvement property in general, we show that it does\nin a certain region that the algorithm will quickly reach and remain in\nthereafter. In sharp contrast, our second main result establishes a lower\nbound: RM, with or without alternation, can take an exponential number of\niterations to reach a crude approximate solution even in two-player potential\ngames. This represents the first worst-case separation between RM and RM$^+$.\nOur lower bound shows that convergence to coarse correlated equilibria in\npotential games is exponentially faster than convergence to Nash equilibria."}
{"id": "2510.16336", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.16336", "abs": "https://arxiv.org/abs/2510.16336", "authors": ["Pachara Sawettamalya", "Huacheng Yu"], "title": "A (Very) Nearly Optimal Sketch for $k$-Edge Connectivity Certificates", "comment": null, "summary": "In this note, we present a simple algorithm for computing a\n\\emph{$k$-connectivity certificate} in dynamic graph streams. Our algorithm\nuses $O(n \\log^2 n \\cdot \\max\\{k, \\log n \\log k\\})$ bits of space which\nimproves upon the $O(kn \\log^3 n)$-space algorithm of Ahn, Guha, and McGregor\n(SODA'12). For the values of $k$ that are truly sublinear, our space usage\n\\emph{very nearly} matches the known lower bound $\\Omega(n \\log^2 n \\cdot\n\\max\\{k, \\log n\\})$ established by Nelson and Yu (SODA'19; implicit) and\nRobinson (DISC'24). In particular, our algorithm fully settles the space\ncomplexity at $\\Theta(kn \\log^2{n})$ for $k = \\Omega(\\log n \\log \\log n)$, and\nbridges the gap down to only a doubly-logarithmic factor of $O(\\log \\log n)$\nfor a smaller range of $k = o(\\log n \\log \\log n)$."}
{"id": "2510.16539", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.16539", "abs": "https://arxiv.org/abs/2510.16539", "authors": ["Zhaowei Guan", "Wenkun Wen", "Peiran Wu", "Chen Wang", "Minghua Xia"], "title": "Hybrid CNN-Transformer Based Sparse Channel Prediction for High-Mobility OTFS Systems", "comment": "5 pages, 9 figures. To appear in IEEE Wireless Communications Letters", "summary": "High-mobility scenarios in next-generation wireless networks, such as those\ninvolving vehicular communications, require ultra-reliable and low-latency\ncommunications (URLLC). However, rapidly time-varying channels pose significant\nchallenges to traditional OFDM-based systems due to the Doppler effect and\nchannel aging. Orthogonal time frequency space (OTFS) modulation offers\nresilience by representing channels in the quasi-static delay-Doppler (DD)\ndomain. This letter proposes a novel channel prediction framework for OTFS\nsystems using a hybrid convolutional neural network and transformer\n(CNN-Transformer) architecture. The CNN extracts compact features that exploit\nthe DD-domain sparsity of the channel matrices, while the transformer models\ntemporal dependencies with causal masking for consistency. Simulation\nexperiments under extreme $500$ \\si{km/h} mobility conditions demonstrate that\nthe proposed method outperforms state-of-the-art baselines, reducing the root\nmean square error and mean absolute error by $12.2\\%$ and $9.4\\%$,\nrespectively. These results demonstrate the effectiveness of DD-domain\nrepresentations and the proposed model in accurately predicting channels in\nhigh-mobility scenarios, thereby supporting the stringent URLLC requirements in\nfuture wireless systems."}
{"id": "2510.16597", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16597", "abs": "https://arxiv.org/abs/2510.16597", "authors": ["Qiyao Peng", "Chen Wang", "Yinghui Wang", "Hongtao Liu", "Xuan Guo", "Wenjun Wang"], "title": "FRONTIER-RevRec: A Large-scale Dataset for Reviewer Recommendation", "comment": null, "summary": "Reviewer recommendation is a critical task for enhancing the efficiency of\nacademic publishing workflows. However, research in this area has been\npersistently hindered by the lack of high-quality benchmark datasets, which are\noften limited in scale, disciplinary scope, and comparative analyses of\ndifferent methodologies. To address this gap, we introduce FRONTIER-RevRec, a\nlarge-scale dataset constructed from authentic peer review records (2007-2025)\nfrom the Frontiers open-access publishing platform\nhttps://www.frontiersin.org/. The dataset contains 177941 distinct reviewers\nand 478379 papers across 209 journals spanning multiple disciplines including\nclinical medicine, biology, psychology, engineering, and social sciences. Our\ncomprehensive evaluation on this dataset reveals that content-based methods\nsignificantly outperform collaborative filtering. This finding is explained by\nour structural analysis, which uncovers fundamental differences between\nacademic recommendation and commercial domains. Notably, approaches leveraging\nlanguage models are particularly effective at capturing the semantic alignment\nbetween a paper's content and a reviewer's expertise. Furthermore, our\nexperiments identify optimal aggregation strategies to enhance the\nrecommendation pipeline. FRONTIER-RevRec is intended to serve as a\ncomprehensive benchmark to advance research in reviewer recommendation and\nfacilitate the development of more effective academic peer review systems. The\nFRONTIER-RevRec dataset is available at:\nhttps://anonymous.4open.science/r/FRONTIER-RevRec-5D05."}
{"id": "2510.17748", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.17748", "abs": "https://arxiv.org/abs/2510.17748", "authors": ["William Zhang", "Wan Shen Lim", "Andrew Pavlo"], "title": "This is Going to Sound Crazy, But What If We Used Large Language Models to Boost Automatic Database Tuning Algorithms By Leveraging Prior History? We Will Find Better Configurations More Quickly Than Retraining From Scratch!", "comment": "Accepted to SIGMOD2026", "summary": "Tuning database management systems (DBMSs) is challenging due to trillions of\npossible configurations and evolving workloads. Recent advances in tuning have\nled to breakthroughs in optimizing over the possible configurations. However,\ndue to their design and inability to leverage query-level historical insights,\nexisting automated tuners struggle to adapt and re-optimize the DBMS when the\nenvironment changes (e.g., workload drift, schema transfer).\n  This paper presents the Booster framework that assists existing tuners in\nadapting to environment changes (e.g., drift, cross-schema transfer). Booster\nstructures historical artifacts into query-configuration contexts, prompts\nlarge language models (LLMs) to suggest configurations for each query based on\nrelevant contexts, and then composes the query-level suggestions into a\nholistic configuration with beam search. With multiple OLAP workloads, we\nevaluate Booster's ability to assist different state-of-the-art tuners (e.g.,\ncost-/machine learning-/LLM-based) in adapting to environment changes. By\ncomposing recommendations derived from query-level insights, Booster assists\ntuners in discovering configurations that are up to 74% better and in up to\n4.7x less time than the alternative approach of continuing to tune from\nhistorical configurations."}
{"id": "2510.17285", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.17285", "abs": "https://arxiv.org/abs/2510.17285", "authors": ["Leo Landolt", "Anna Maddux", "Andreas Schlaginhaufen", "Saurabh Vaishampayan", "Maryam Kamgarpour"], "title": "Eliciting Truthful Feedback for Preference-Based Learning via the VCG Mechanism", "comment": null, "summary": "We study resource allocation problems in which a central planner allocates\nresources among strategic agents with private cost functions in order to\nminimize a social cost, defined as an aggregate of the agents' costs. This\nsetting poses two main challenges: (i) the agents' cost functions may be\nunknown to them or difficult to specify explicitly, and (ii) agents may\nmisreport their costs strategically. To address these challenges, we propose an\nalgorithm that combines preference-based learning with Vickrey-Clarke-Groves\n(VCG) payments to incentivize truthful reporting. Our algorithm selects\ninformative preference queries via D-optimal design, estimates cost parameters\nthrough maximum likelihood, and computes VCG allocations and payments based on\nthese estimates. In a one-shot setting, we prove that the mechanism is\napproximately truthful, individually rational, and efficient up to an error of\n$\\tilde{\\mathcal O}(K^{-1/2})$ for $K$ preference queries per agent. In an\nonline setting, these guarantees hold asymptotically with sublinear regret at a\nrate of $\\tilde{\\mathcal O}(T^{2/3})$ after $T$ rounds. Finally, we validate\nour approach through a numerical case study on demand response in local\nelectricity markets."}
{"id": "2510.16346", "categories": ["cs.DS", "cs.CG"], "pdf": "https://arxiv.org/pdf/2510.16346", "abs": "https://arxiv.org/abs/2510.16346", "authors": ["Timothy M. Chan", "Hsien-Chih Chang", "Jie Gao", "Sándor Kisfaludi-Bak", "Hung Le", "Da Wei Zheng"], "title": "Truly Subquadratic Time Algorithms for Diameter and Related Problems in Graphs of Bounded VC-dimension", "comment": "FOCS 2025", "summary": "We give the first truly subquadratic time algorithm, with $O^*(n^{2-1/18})$\nrunning time, for computing the diameter of an $n$-vertex unit-disk graph,\nresolving a central open problem in the literature. Our result is obtained as\nan instance of a general framework, applicable to different graph families and\ndistance problems. Surprisingly, our framework completely bypasses sublinear\nseparators (or $r$-divisions) which were used in all previous algorithms.\nInstead, we use low-diameter decompositions in their most elementary form. We\nalso exploit bounded VC-dimension of set systems associated with the input\ngraph, as well as new ideas on geometric data structures. Among the numerous\napplications of the general framework, we obtain:\n  1. An $\\tilde{O}(mn^{1-1/(2d)})$ time algorithm for computing the diameter of\n$m$-edge sparse unweighted graphs with constant VC-dimension $d$. The\npreviously known algorithms by Ducoffe, Habib, and Viennot [SODA 2019] and\nDuraj, Konieczny, and Pot\\c{e}pa [ESA 2024] are truly subquadratic only when\nthe diameter is a small polynomial. Our result thus generalizes truly\nsubquadratic time algorithms known for planar and minor-free graphs (in fact,\nit slightly improves the previous time bound for minor-free graphs).\n  2. An $\\tilde{O}(n^{2-1/12})$ time algorithm for computing the diameter of\nintersection graphs of axis-aligned squares with arbitrary size. The best-known\nalgorithm by Duraj, Konieczny, and Pot\\c{e}pa [ESA 2024] only works for unit\nsquares and is only truly subquadratic in the low-diameter regime.\n  3. The first algorithms with truly subquadratic complexity for other\ndistance-related problems, including all-vertex eccentricities, Wiener index,\nand exact distance oracles. (... truncated to meet the arXiv abstract\nrequirement.)"}
{"id": "2510.16576", "categories": ["cs.IT", "cs.IR", "cs.SY", "eess.SP", "eess.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.16576", "abs": "https://arxiv.org/abs/2510.16576", "authors": ["Zijian Zhang", "Mingyao Cui"], "title": "Enhancing Channel Estimation in RIS-aided Systems via Observation Matrix Design", "comment": "5 pages, 2 figures", "summary": "Reconfigurable intelligent surfaces (RISs) have emerged as a promising\ntechnology for enhancing wireless communications through dense antenna arrays.\nAccurate channel estimation is critical to unlocking their full performance\npotential. To enhance RIS channel estimators, this paper proposes a novel\nobservation matrix design scheme. Bayesian optimization framework is adopted to\ngenerate observation matrices that maximize the mutual information between\nreceived pilot signals and RIS channels. To solve the formulated problem\nefficiently, we develop an alternating Riemannian manifold optimization (ARMO)\nalgorithm to alternately update the receiver combiners and RIS phase-shift\nmatrices. An adaptive kernel training strategy is further introduced to\niteratively refine the channel covariance matrix without requiring additional\npilot resources. Simulation results demonstrate that the proposed ARMO-enhanced\nestimator achieves substantial gains in estimation accuracy over\nstate-of-the-art methods."}
{"id": "2510.16715", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16715", "abs": "https://arxiv.org/abs/2510.16715", "authors": ["Zulun Zhu", "Haoyu Liu", "Mengke He", "Siqiang Luo"], "title": "Right Answer at the Right Time - Temporal Retrieval-Augmented Generation via Graph Summarization", "comment": null, "summary": "Question answering in temporal knowledge graphs requires retrieval that is\nboth time-consistent and efficient. Existing RAG methods are largely semantic\nand typically neglect explicit temporal constraints, which leads to\ntime-inconsistent answers and inflated token usage. We propose STAR-RAG, a\ntemporal GraphRAG framework that relies on two key ideas: building a\ntime-aligned rule graph and conducting propagation on this graph to narrow the\nsearch space and prioritize semantically relevant, time-consistent evidence.\nThis design enforces temporal proximity during retrieval, reduces the candidate\nset of retrieval results, and lowers token consumption without sacrificing\naccuracy. Compared with existing temporal RAG approaches, STAR-RAG eliminates\nthe need for heavy model training and fine-tuning, thereby reducing\ncomputational cost and significantly simplifying deployment.Extensive\nexperiments on real-world temporal KG datasets show that our method achieves\nimproved answer accuracy while consuming fewer tokens than strong GraphRAG\nbaselines."}
{"id": "2510.16516", "categories": ["cs.DS", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.16516", "abs": "https://arxiv.org/abs/2510.16516", "authors": ["Yossi Azar", "Niv Buchbinder", "Roie Levin", "Or Vardi"], "title": "Trading Prophets with Initial Capital", "comment": null, "summary": "Correa et al. [EC' 2023] introduced the following trading prophets problem. A\ntrader observes a sequence of stochastic prices for a stock, each drawn from a\nknown distribution, and at each time must decide whether to buy or sell.\nUnfortunately, they observed that in this setting it is impossible to compete\nwith a prophet who knows all future stock prices.\n  In this paper, we explore the trading prophets problem when we are given\ninitial capital with which to start trading. We show that initial capital is\nenough to bypass the impossibility result and obtain a competitive ratio of $3$\nwith respect to a prophet who knows all future prices (and who also starts with\ncapital), and we show that this competitive ratio is best possible. We further\nstudy a more realistic model in which the trader must pay multiplicative and/or\nadditive transaction costs for trading which model dynamics such as bid-ask\nspreads and broker fees."}
{"id": "2510.16351", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.16351", "abs": "https://arxiv.org/abs/2510.16351", "authors": ["Amir Azarmehr", "Soheil Behnezhad", "Mohammad Roghani", "Aviad Rubinstein"], "title": "Tight Pair Query Lower Bounds for Matching and Earth Mover's Distance", "comment": null, "summary": "How many adjacency matrix queries (also known as pair queries) are required\nto estimate the size of a maximum matching in an $n$-vertex graph $G$? We study\nthis fundamental question in this paper.\n  On the upper bound side, an algorithm of Bhattacharya, Kiss, and Saranurak\n[FOCS'23] gives an estimate that is within $\\epsilon n$ of the right bound with\n$n^{2-\\Omega_\\epsilon(1)}$ queries, which is subquadratic in $n$ (and thus\nsublinear in the matrix size) for any fixed $\\epsilon > 0$. On the lower bound\nside, while there has been a lot of progress in the adjacency list model, no\nnon-trivial lower bound has been established for algorithms with adjacency\nmatrix query access. In particular, the only known lower bound is a folklore\nbound of $\\Omega(n)$, leaving a huge gap.\n  In this paper, we present the first superlinear in $n$ lower bound for this\nproblem. In fact, we close the gap mentioned above entirely by showing that the\nalgorithm of [BKS'23] is optimal. Formally, we prove that for any fixed $\\delta\n> 0$, there is a fixed $\\epsilon > 0$ such that an estimate that is within\n$\\epsilon n$ of the true bound requires $\\Omega(n^{2-\\delta})$ adjacency matrix\nqueries.\n  Our lower bound also has strong implications for estimating the earth mover's\ndistance between distributions. For this problem, Beretta and Rubinstein\n[STOC'24] gave an $n^{2-\\Omega_\\epsilon(1)}$ time algorithm that obtains an\nadditive $\\epsilon$-approximation and works for any distance function. Whether\nthis can be improved generally, or even for metric spaces, had remained open.\nOur lower bound rules out the possibility of any improvements over this bound,\neven under the strong assumption that the underlying distances are in a (1,\n2)-metric."}
{"id": "2510.16620", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.16620", "abs": "https://arxiv.org/abs/2510.16620", "authors": ["Yingyao Zhou", "Natasha Devroye", "Onur Günlü"], "title": "Feedback Lunch: Deep Feedback Codes for Wiretap Channels", "comment": "submitted to IEEE COMMUNICATIONS LETTERS", "summary": "We consider reversely-degraded wiretap channels, for which the secrecy\ncapacity is zero if there is no channel feedback. This work focuses on a seeded\nmodular code design for the Gaussian wiretap channel with channel output\nfeedback, combining universal hash functions for security and learned\nfeedback-based codes for reliability to achieve positive secrecy rates. We\nstudy the trade-off between communication reliability and information leakage,\nillustrating that feedback enables agreeing on a secret key shared between\nlegitimate parties, overcoming the security advantage of the wiretapper. Our\nfindings also motivate code designs for sensing-assisted secure communication,\nto be used in next-generation integrated sensing and communication methods."}
{"id": "2510.16736", "categories": ["cs.IR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16736", "abs": "https://arxiv.org/abs/2510.16736", "authors": ["Patrizio Dazzi", "William Guglielmo", "Franco Maria Nardini", "Raffaele Perego", "Salvatore Trani"], "title": "Exact Nearest-Neighbor Search on Energy-Efficient FPGA Devices", "comment": null, "summary": "This paper investigates the usage of FPGA devices for energy-efficient exact\nkNN search in high-dimension latent spaces. This work intercepts a relevant\ntrend that tries to support the increasing popularity of learned\nrepresentations based on neural encoder models by making their large-scale\nadoption greener and more inclusive. The paper proposes two different\nenergy-efficient solutions adopting the same FPGA low-level configuration. The\nfirst solution maximizes system throughput by processing the queries of a batch\nin parallel over a streamed dataset not fitting into the FPGA memory. The\nsecond minimizes latency by processing each kNN incoming query in parallel over\nan in-memory dataset. Reproducible experiments on publicly available image and\ntext datasets show that our solution outperforms state-of-the-art CPU-based\ncompetitors regarding throughput, latency, and energy consumption.\nSpecifically, experiments show that the proposed FPGA solutions achieve the\nbest throughput in terms of queries per second and the best-observed latency\nwith scale-up factors of up to 16.6X. Similar considerations can be made\nregarding energy efficiency, where results show that our solutions can achieve\nup to 11.9X energy saving w.r.t. strong CPU-based competitors."}
{"id": "2510.16454", "categories": ["cs.DS", "F.2.2"], "pdf": "https://arxiv.org/pdf/2510.16454", "abs": "https://arxiv.org/abs/2510.16454", "authors": ["Gregory Kucherov", "Yakov Nekrich"], "title": "Online computation of normalized substring complexity", "comment": "15 pages, 1 figure", "summary": "The normalized substring complexity $\\delta$ of a string is defined as\n$\\max_k \\{c[k]/k\\}$, where $c[k]$ is the number of \\textit{distinct} substrings\nof length $k$. This simply defined measure has recently attracted attention due\nto its established relationship to popular string compression algorithms. We\nconsider the problem of computing $\\delta$ online, when the string is provided\nfrom a stream. We present two algorithms solving the problem: one working in\n$O(\\log n)$ amortized time per character, and the other in $O(\\log^3 n)$\nworst-case time per character. To our knowledge, this is the first polylog-time\nonline solution to this problem."}
{"id": "2510.16792", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.16792", "abs": "https://arxiv.org/abs/2510.16792", "authors": ["Zhi Gu", "Wai Ho Mow"], "title": "Non-Orthogonal Pilot Sequence Design for Multi-Cells Interference Networks", "comment": null, "summary": "In wireless communications, the performance of non-orthogonal sequence sets\nsignificantly affects the level of multi-user interference when the number of\nusers surpasses the sequence length. The design of non-orthogonal sequences\nplays a crucial role in both the non-orthogonality of the pilots in multi-cell\nsystems and the signature sequences in overloaded code-division multiple-access\n(CDMA) systems. In multi-cell systems, considering the strength disparity\nbetween channels originating from the home cell and the neighboring cells, the\nextended total squared correlation (ETSC) is proposed as a new sequence design\ncriterion, which is defined as the sum of squares of the weighted correlations\namong sequences. In this paper, we derive a closed-form expression for the\nlower bound of ETSC for multi-cell systems with a given sequence length $\\tau$,\nwhere $\\tau \\leq K$ and $K$ is the number of users per cell. This can be\nregarded as a generalization of the well-known Welch bound (Welch, 1974, IEEE\nTIT) and the extended Welch bound (Wang et al., 2021, IEEE TWC). Additionally,\nfrom the necessary conditions of the bound, the optimal sequence set can be\neasily obtained when the interference power factor matrix is positive definite.\nOn the other hand, to address the lack of sequence generation methods under\ncertain parameter conditions, we propose the ETSC-MM algorithm, which generates\nsequence sets with low ETSC based on a Majorization-Minimization (MM)\noptimization framework."}
{"id": "2510.16803", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16803", "abs": "https://arxiv.org/abs/2510.16803", "authors": ["Zishuai Zhang", "Sihao Yu", "Wenyi Xie", "Ying Nie", "Junfeng Wang", "Zhiming Zheng", "Dawei Yin", "Hainan Zhang"], "title": "An Efficient Framework for Whole-Page Reranking via Single-Modal Supervision", "comment": null, "summary": "The whole-page reranking plays a critical role in shaping the user experience\nof search engines, which integrates retrieval results from multiple modalities,\nsuch as documents, images, videos, and LLM outputs. Existing methods mainly\nrely on large-scale human-annotated data, which is costly to obtain and\ntime-consuming. This is because whole-page annotation is far more complex than\nsingle-modal: it requires assessing the entire result page while accounting for\ncross-modal relevance differences. Thus, how to improve whole-page reranking\nperformance while reducing annotation costs is still a key challenge in\noptimizing search engine result pages(SERP). In this paper, we propose SMAR, a\nnovel whole-page reranking framework that leverages strong Single-modal rankers\nto guide Modal-wise relevance Alignment for effective Reranking, using only\nlimited whole-page annotation to outperform fully-annotated reranking models.\nSpecifically, high-quality single-modal rankers are first trained on data\nspecific to their respective modalities. Then, for each query, we select a\nsubset of their outputs to construct candidate pages and perform human\nannotation at the page level. Finally, we train the whole-page reranker using\nthese limited annotations and enforcing consistency with single-modal\npreferences to maintain ranking quality within each modality. Experiments on\nthe Qilin and Baidu datasets demonstrate that SMAR reduces annotation costs by\nabout 70-90\\% while achieving significant ranking improvements compared to\nbaselines. Further offline and online A/B testing on Baidu APPs also shows\nnotable gains in standard ranking metrics as well as user experience\nindicators, fully validating the effectiveness and practical value of our\napproach in real-world search scenarios."}
{"id": "2510.16516", "categories": ["cs.DS", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.16516", "abs": "https://arxiv.org/abs/2510.16516", "authors": ["Yossi Azar", "Niv Buchbinder", "Roie Levin", "Or Vardi"], "title": "Trading Prophets with Initial Capital", "comment": null, "summary": "Correa et al. [EC' 2023] introduced the following trading prophets problem. A\ntrader observes a sequence of stochastic prices for a stock, each drawn from a\nknown distribution, and at each time must decide whether to buy or sell.\nUnfortunately, they observed that in this setting it is impossible to compete\nwith a prophet who knows all future stock prices.\n  In this paper, we explore the trading prophets problem when we are given\ninitial capital with which to start trading. We show that initial capital is\nenough to bypass the impossibility result and obtain a competitive ratio of $3$\nwith respect to a prophet who knows all future prices (and who also starts with\ncapital), and we show that this competitive ratio is best possible. We further\nstudy a more realistic model in which the trader must pay multiplicative and/or\nadditive transaction costs for trading which model dynamics such as bid-ask\nspreads and broker fees."}
{"id": "2510.16948", "categories": ["cs.IT", "cs.CV", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.16948", "abs": "https://arxiv.org/abs/2510.16948", "authors": ["Ruiming Guo", "Ayush Bhandari"], "title": "Unlocking Off-the-Grid Sparse Recovery with Unlimited Sensing: Simultaneous Super-Resolution in Time and Amplitude", "comment": "28 Pages, 10 figures. To appear in IEEE Journal of Selected Topics in\n  Signal Processing", "summary": "The recovery of Dirac impulses, or spikes, from filtered measurements is a\nclassical problem in signal processing. As the spikes lie in the continuous\ndomain while measurements are discrete, this task is known as super-resolution\nor off-the-grid sparse recovery. Despite significant theoretical and\nalgorithmic advances over the past decade, these developments often overlook\ncritical challenges at the analog-digital interface. In particular, when spikes\nexhibit strong-weak amplitude disparity, conventional digital acquisition may\nresult in clipping of strong components or loss of weak ones beneath the\nquantization noise floor. This motivates a broader perspective:\nsuper-resolution must simultaneously resolve both amplitude and temporal\nstructure. Under a fixed bit budget, such information loss is unavoidable. In\ncontrast, the emerging theory and practice of the Unlimited Sensing Framework\n(USF) demonstrate that these fundamental limitations can be overcome. Building\non this foundation, we demonstrate that modulo encoding within USF enables\ndigital super-resolution by enhancing measurement precision, thereby unlocking\ntemporal super-resolution beyond conventional limits. We develop new\ntheoretical results that extend to non-bandlimited kernels commonly encountered\nin practice and introduce a robust algorithm for off-the-grid sparse recovery.\nTo demonstrate practical impact, we instantiate our framework in the context of\ntime-of-flight imaging. Both numerical simulations and hardware experiments\nvalidate the effectiveness of our approach under low-bit quantization, enabling\nsuper-resolution in amplitude and time."}
{"id": "2510.16804", "categories": ["cs.IR", "H.3.3"], "pdf": "https://arxiv.org/pdf/2510.16804", "abs": "https://arxiv.org/abs/2510.16804", "authors": ["Xiaokai Wei", "Jiajun Wu", "Daiyao Yi", "Reza Shirkavand", "Michelle Gong"], "title": "The Layout Is the Model: On Action-Item Coupling in Generative Recommendation", "comment": null, "summary": "Generative Recommendation (GR) models treat a user's interaction history as a\nsequence to be autoregressively predicted. When both items and actions (e.g.,\nwatch time, purchase, comment) are modeled, the layout-the ordering and\nvisibility of item/action tokens-critically determines what information the\nmodel can use and how it generalizes. We present a unified study of token\nlayouts for GR grounded in first principles: (P1) maximize item/action signal\nin both input/output space, (P2) preserve the conditioning relationship \"action\ngiven item\" and (P3) no information leakage.\n  While interleaved layout (where item and action occupy separate tokens)\nnaturally satisfies these principles, it also bloats sequence length with\nlarger training/inference cost. On the non-interleaved front, we design a novel\nand effective approach, Lagged Action Conditioning (LAC), which appears strange\non the surface but aligns well with the design principles to yield strong\naccuracy. Comprehensive experiments on public datasets and large-scale\nproduction logs evaluate different layout options and empirically verifies the\ndesign principles. Our proposed non-interleaved method, LAC, achieves\ncompetitive or superior quality at substantially lower FLOPs than interleaving.\nOur findings offer actionable guidance for assembling GR systems that are both\naccurate and efficient."}
{"id": "2510.16663", "categories": ["cs.DS", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16663", "abs": "https://arxiv.org/abs/2510.16663", "authors": ["Yiding Feng", "Vahideh Manshadi", "Rad Niazadeh", "Saba Neyshabouri"], "title": "Robust Dynamic Staffing with Predictions", "comment": null, "summary": "We consider a natural dynamic staffing problem in which a decision-maker\nsequentially hires workers over a finite horizon to meet an unknown demand\nrevealed at the end. Predictions about demand arrive over time and become\nincreasingly accurate, while worker availability decreases. This creates a\nfundamental trade-off between hiring early to avoid understaffing (when workers\nare more available but forecasts are less reliable) and hiring late to avoid\noverstaffing (when forecasts are more accurate but availability is lower). This\nproblem is motivated by last-mile delivery operations, where companies such as\nAmazon rely on gig-economy workers whose availability declines closer to the\noperating day.\n  To address practical limitations of Bayesian models (in particular, to remain\nagnostic to the underlying forecasting method), we study this problem under\nadversarial predictions. In this model, sequential predictions are\nadversarially chosen uncertainty intervals that (approximately) contain the\ntrue demand. The objective is to minimize worst-case staffing imbalance cost.\nOur main result is a simple and computationally efficient online algorithm that\nis minimax optimal. We first characterize the minimax cost against a restricted\nadversary via a polynomial-size linear program, then show how to emulate this\nsolution in the general case. While our base model focuses on a single demand,\nwe extend the framework to multiple demands (with egalitarian/utilitarian\nobjectives), to settings with costly reversals of hiring decisions, and to\ninconsistent prediction intervals. We also introduce a practical \"re-solving\"\nvariant of our algorithm, which we prove is also minimax optimal. Finally we\nconduct numerical experiments showing that our algorithms outperform Bayesian\nheuristics in both cost and speed, and are competitive with (approximate or\nexact) Bayesian-optimal policies when those can be computed."}
{"id": "2510.17093", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.17093", "abs": "https://arxiv.org/abs/2510.17093", "authors": ["Yunfeng Wen", "Fang Yang", "Jian Song", "Zhu Han"], "title": "Channel Capacity for FMCW-based Optical Wireless Integrated Sensing and Communication: Asymptotic Analysis and Envelope Design", "comment": "This work has been submitted to the IEEE for possible publication. 13\n  pages, 7 figures", "summary": "Optical wireless integrated sensing and communication (OW-ISAC) is rapidly\nburgeoning as a complement and augmentation to its radio-frequency counterpart.\nIn this paper, the channel capacity is analyzed to guide the design of a\ncoherent OW-ISAC system based on frequency-modulated continuous wave (FMCW).\nFirstly, the system model of FMCW-based OW-ISAC is recast into an\ninformation-theoretic formulation, where an additional harmonic-mean constraint\nis imposed to ensure the sensing performance. Subsequently, both lower and\nupper bounds for channel capacity are derived under the imposed sensing\nconstraint, based on which asymptotic expressions for channel capacity are\npresented for both low and high signal-to-noise-ratio regions. Moreover, the\nanalysis of channel capacity provides guidance for the envelope design based on\npulse amplitude modulation, whose capacity-achieving capabilities are\ndemonstrated by numerical results. Furthermore, simulations reveal the\ntrade-off between communication and sensing functionalities. In summary, the\nanalysis of channel capacity under the sensing constraint provides insights\ninto both the optimality and the practicality of OW-ISAC design."}
{"id": "2510.16925", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16925", "abs": "https://arxiv.org/abs/2510.16925", "authors": ["Zhiding Liu", "Ben Chen", "Mingyue Cheng", "Enchong Chen", "Li Li", "Chenyi Lei", "Wenwu Ou", "Han Li", "Kun Gai"], "title": "Towards Context-aware Reasoning-enhanced Generative Searching in E-commerce", "comment": null, "summary": "Search-based recommendation is one of the most critical application scenarios\nin e-commerce platforms. Users' complex search contexts--such as spatiotemporal\nfactors, historical interactions, and current query's information--constitute\nan essential part of their decision-making, reflecting implicit preferences\nthat complement explicit query terms. Modeling such rich contextual signals and\ntheir intricate associations with candidate items remains a key challenge.\nAlthough numerous efforts have been devoted to building more effective search\nmethods, existing approaches still show limitations in integrating contextual\ninformation, which hinders their ability to fully capture user intent.\n  To address these challenges, we propose a context-aware reasoning-enhanced\ngenerative search framework for better \\textbf{understanding the complicated\ncontext}. Specifically, the framework first unifies heterogeneous user and item\ncontexts into textual representations or text-based semantic identifiers and\naligns them. To overcome the lack of explicit reasoning trajectories, we\nintroduce a self-evolving post-training paradigm that iteratively combines\nsupervised fine-tuning and reinforcement learning to progressively enhance the\nmodel's reasoning capability. In addition, we identify potential biases in\nexisting RL algorithms when applied to search scenarios and present a debiased\nvariant of GRPO to improve ranking performance. Extensive experiments on search\nlog data collected from a real-world e-commerce platform demonstrate that our\napproach achieves superior performance compared with strong baselines,\nvalidating its effectiveness for search-based recommendation."}
{"id": "2510.16678", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.16678", "abs": "https://arxiv.org/abs/2510.16678", "authors": ["Feyza Duman Keles", "Lisa Hellerstein", "Kunal Marwaha", "Christopher Musco", "Xinchen Yang"], "title": "An Exact Algorithm for the Unanimous Vote Problem", "comment": "1+23+31 pages, 5 figures", "summary": "Consider $n$ independent, biased coins, each with a known probability of\nheads. Presented with an ordering of these coins, flip (i.e., toss) each coin\nonce, in that order, until we have observed both a *head* and a *tail*, or\nflipped all coins. The Unanimous Vote problem asks us to find the ordering that\nminimizes the expected number of flips. Gkenosis et al. [arXiv:1806.10660] gave\na polynomial-time $\\phi$-approximation algorithm for this problem, where $\\phi\n\\approx 1.618$ is the golden ratio. They left open whether the problem was\nNP-hard. We answer this question by giving an exact algorithm that runs in time\n$O(n \\log n)$. The Unanimous Vote problem is an instance of the more general\nStochastic Boolean Function Evaluation problem: it thus becomes one of the only\nsuch problems known to be solvable in polynomial time. Our proof uses simple\ninterchange arguments to show that the optimal ordering must be close to the\nordering produced by a natural greedy algorithm. Beyond our main result, we\ncompare the optimal ordering with the best adaptive strategy, proving a tight\nadaptivity gap of $1.2\\pm o(1)$ for the Unanimous Vote problem."}
{"id": "2510.17466", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.17466", "abs": "https://arxiv.org/abs/2510.17466", "authors": ["Fathima Jesbin", "Ananthanarayanan Chockalingam"], "title": "Delay-Doppler Pulse Shaping in Zak-OTFS Using Hermite Basis Functions", "comment": "Submitted to IEEE journal for possible publication", "summary": "The performance of Zak-OTFS modulation is critically dependent on the choice\nof the delay-Doppler (DD) domain pulse shaping filter. The design of pulses for\n$L^2(\\mathbb{R})$ is constrained by the Balian-Low Theorem, which imposes an\ninescapable trade-off between time-frequency localization and orthogonality for\nspectrally efficient systems. In Zak-OTFS, this trade-off requires balancing\nthe need for localization for input/output (I/O) relation estimation with the\nneed for orthogonality for reliable data detection when operating without time\nor bandwidth expansion. The well-known sinc and Gaussian pulse shapes represent\nthe canonical extremes of this trade-off, while composite constructions such as\nthe Gaussian-sinc (GS) pulse shape offer a good compromise. In this work, we\npropose a systematic DD pulse design framework for Zak-OTFS that expresses the\npulse as a linear combination of Hermite basis functions. We obtain the optimal\ncoefficients for the Hermite basis functions that minimize the inter-symbol\ninterference (ISI) energy at the DD sampling points by solving a constrained\noptimization problem via singular value decomposition. For the proposed class\nof Hermite pulses, we derive closed-form expressions for the I/O relation and\nnoise covariance in Zak-OTFS. Simulation results of Zak-OTFS with embedded\npilot and model-free I/O relation estimation in Vehicular-A channels with\nfractional DDs demonstrate that the optimized pulse shape achieves a bit error\nrate performance that is significantly superior compared to those of the\ncanonical sinc and Gaussian pulses and is on par with that of the\nstate-of-the-art GS pulse, validating the proposed framework which provides\ngreater design flexibility in terms of control of ISI and sidelobe energies."}
{"id": "2510.17228", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17228", "abs": "https://arxiv.org/abs/2510.17228", "authors": ["Qing Shi", "Jing He", "Qiaosheng Chen", "Gong Cheng"], "title": "DSEBench: A Test Collection for Explainable Dataset Search with Examples", "comment": "34 pages, 5 figures, submitted to Knowledge-Based Systems", "summary": "Dataset search has been an established information retrieval task. Current\nparadigms either retrieve datasets that are relevant to a keyword query or find\ndatasets that are similar to an input target dataset. To allow for their\ncombined specification of information needs, in this article, we investigate\nthe more generalized task of Dataset Search with Examples (DSE) and further\nextend it to Explainable DSE that requires identifying the metadata and content\nfields of a dataset that indicate its relevance to the query and similarity to\nthe target datasets. To facilitate this research, we construct DSEBench, a test\ncollection that provides high-quality dataset- and field-level annotations to\nenable the evaluation of explainable DSE. We also employ a large language model\nto generate numerous annotations to be used for training. We establish\nextensive baselines on DSEBench by adapting and evaluating a variety of sparse,\ndense, and LLM-based retrieval, reranking, and explanation methods."}
{"id": "2510.16741", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.16741", "abs": "https://arxiv.org/abs/2510.16741", "authors": ["Yotam Kenneth-Mordoch", "Robert Krauthgamer"], "title": "All-Pairs Minimum Cut using $\\tilde{O}(n^{7/4})$ Cut Queries", "comment": null, "summary": "We present the first non-trivial algorithm for the all-pairs minimum cut\nproblem in the cut-query model. Given cut-query access to an unweighted graph\n$G=(V,E)$ with $n$ vertices, our randomized algorithm constructs a Gomory-Hu\ntree of $G$, and thus solves the all-pairs minimum cut problem, using\n$\\tilde{O}(n^{7/4})$ cut queries."}
{"id": "2510.17544", "categories": ["cs.IT", "cs.FL", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.17544", "abs": "https://arxiv.org/abs/2510.17544", "authors": ["Neil Lutz"], "title": "Multihead Finite-State Compression", "comment": null, "summary": "This paper develops multihead finite-state compression, a generalization of\nfinite-state compression, complementary to the multihead finite-state\ndimensions of Huang, Li, Lutz, and Lutz (2025). In this model, an infinite\nsequence of symbols is compressed by a compressor that produces outputs\naccording to finite-state rules, based on the symbols read by a constant number\nof finite-state read heads moving forward obliviously through the sequence. The\nmain theorem of this work establishes that for every sequence and every\npositive integer $h$, the infimum of the compression ratios achieved by\n$h$-head finite-state information-lossless compressors equals the $h$-head\nfinite-state predimension of the sequence. As an immediate corollary, the\ninfimum of these ratios over all $h$ is the multihead finite-state dimension of\nthe sequence."}
{"id": "2510.17245", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17245", "abs": "https://arxiv.org/abs/2510.17245", "authors": ["Wenyu Mao", "Jiancan Wu", "Guoqing Hu", "Wei Ji", "Xiang Wang"], "title": "On Efficiency-Effectiveness Trade-off of Diffusion-based Recommenders", "comment": null, "summary": "Diffusion models have emerged as a powerful paradigm for generative\nsequential recommendation, which typically generate next items to recommend\nguided by user interaction histories with a multi-step denoising process.\nHowever, the multi-step process relies on discrete approximations, introducing\ndiscretization error that creates a trade-off between computational efficiency\nand recommendation effectiveness. To address this trade-off, we propose TA-Rec,\na two-stage framework that achieves one-step generation by smoothing the\ndenoising function during pretraining while alleviating trajectory deviation by\naligning with user preferences during fine-tuning. Specifically, to improve the\nefficiency without sacrificing the recommendation performance, TA-Rec pretrains\nthe denoising model with Temporal Consistency Regularization (TCR), enforcing\nthe consistency between the denoising results across adjacent steps. Thus, we\ncan smooth the denoising function to map the noise as oracle items in one step\nwith bounded error. To further enhance effectiveness, TA-Rec introduces\nAdaptive Preference Alignment (APA) that aligns the denoising process with user\npreference adaptively based on preference pair similarity and timesteps.\nExtensive experiments prove that TA-Rec's two-stage objective effectively\nmitigates the discretization errors-induced trade-off, enhancing both\nefficiency and effectiveness of diffusion-based recommenders."}
{"id": "2510.17182", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.17182", "abs": "https://arxiv.org/abs/2510.17182", "authors": ["Aaron Bernstein", "Joakim Blikstad", "Jason Li", "Thatchaphol Saranurak", "Ta-Wei Tu"], "title": "Combinatorial Maximum Flow via Weighted Push-Relabel on Shortcut Graphs", "comment": null, "summary": "We give a combinatorial algorithm for computing exact maximum flows in\ndirected graphs with $n$ vertices and edge capacities from $\\{1,\\dots,U\\}$ in\n$\\tilde{O}(n^{2}\\log U)$ time, which is near-optimal on dense graphs. This\nshaves an $n^{o(1)}$ factor from the recent result of\n[Bernstein-Blikstad-Saranurak-Tu FOCS'24] and, more importantly, greatly\nsimplifies their algorithm. We believe that ours is by a significant margin the\nsimplest of all algorithms that go beyond $\\tilde{O}(m\\sqrt{n})$ time in\ngeneral graphs. To highlight this relative simplicity, we provide a full\nimplementation of the algorithm in C++.\n  The only randomized component of our work is the cut-matching game. Via\nexisting tools, we show how to derandomize it for vertex-capacitated max flow\nand obtain a deterministic $\\tilde{O}(n^2)$ time algorithm. This marks the\nfirst deterministic near-linear time algorithm for this problem (or even for\nthe special case of bipartite matching) in any density regime."}
{"id": "2510.17613", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.17613", "abs": "https://arxiv.org/abs/2510.17613", "authors": ["MohammadHossein Alishahi", "Ming Zeng", "Paul Fortier", "Ji Wang", "Nian Xia", "Gongpu Wang"], "title": "Mode Switching-based STAR-RIS with Discrete Phase Shifters", "comment": "accepted by IEEE WCL", "summary": "The increasing demand for cost-effective, high-speed Internet of Things (IoT)\napplications in the coming sixth-generation (6G) networks has driven research\ntoward maximizing spectral efficiency and simplifying hardware designs. In this\ncontext, we investigate the sum rate maximization problem for a mode-switching\ndiscrete-phase shifters simultaneously transmitting and reflecting\nreconfigurable intelligent surface (STAR-RIS)-aided multi-antenna access point\nnetwork, emphasizing hardware efficiency and reduced cost. A mixed-integer\nnonlinear optimization framework is formulated for joint optimization of the\nactive beamforming matrix, user power allocation, and STAR-RIS phase shift\nvectors, including binary transmission/reflection amplitudes and discrete phase\nshifters. To solve the formulated problem, we employ a block coordinate descent\nmethod, dividing it into three subproblems tackled using difference-of-concave\nprogramming and combinatorial optimization techniques. Numerical results\nvalidate the effectiveness of the proposed joint optimization approach,\nconsistently achieving superior sum rate performance compared to partial\noptimization methods, thereby underscoring its potential for efficient and\nscalable 6G IoT systems."}
{"id": "2510.17535", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17535", "abs": "https://arxiv.org/abs/2510.17535", "authors": ["Yumeng Wang", "Jirui Qi", "Catherine Chen", "Panagiotis Eustratiadis", "Suzan Verberne"], "title": "How role-play shapes relevance judgment in zero-shot LLM rankers", "comment": null, "summary": "Large Language Models (LLMs) have emerged as promising zero-shot rankers, but\ntheir performance is highly sensitive to prompt formulation. In particular,\nrole-play prompts, where the model is assigned a functional role or identity,\noften give more robust and accurate relevance rankings. However, the mechanisms\nand diversity of role-play effects remain underexplored, limiting both\neffective use and interpretability. In this work, we systematically examine how\nrole-play variations influence zero-shot LLM rankers. We employ causal\nintervention techniques from mechanistic interpretability to trace how\nrole-play information shapes relevance judgments in LLMs. Our analysis reveals\nthat (1) careful formulation of role descriptions have a large effect on the\nranking quality of the LLM; (2) role-play signals are predominantly encoded in\nearly layers and communicate with task instructions in middle layers, while\nreceiving limited interaction with query or document representations.\nSpecifically, we identify a group of attention heads that encode information\ncritical for role-conditioned relevance. These findings not only shed light on\nthe inner workings of role-play in LLM ranking but also offer guidance for\ndesigning more effective prompts in IR and beyond, pointing toward broader\nopportunities for leveraging role-play in zero-shot applications."}
{"id": "2510.17262", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.17262", "abs": "https://arxiv.org/abs/2510.17262", "authors": ["Chuhan Qi"], "title": "Finding 4-Additive Spanners: Faster, Stronger, and Simpler", "comment": null, "summary": "Additive spanners are fundamental graph structures with wide applications in\nnetwork design, graph sparsification, and distance approximation. In\nparticular, a $4$-additive spanner is a subgraph that preserves all pairwise\ndistances up to an additive error of $4$. In this paper, we present a new\ndeterministic algorithm for constructing $4$-additive spanners that matches the\nbest known edge bound of $\\tilde{O}(n^{7/5})$ (up to polylogarithmic factors),\nwhile improving the running time to $\\tilde{O}(\\min\\{mn^{3/5}, n^{11/5}\\})$,\ncompared to the previous $\\tilde{O}(mn^{3/5})$ randomized construction. Our\nalgorithm is not only faster in the dense regime but also fully deterministic,\nconceptually simpler, and easier to implement and analyze."}
{"id": "2510.17625", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.17625", "abs": "https://arxiv.org/abs/2510.17625", "authors": ["Jaehyup Seong", "Byungju Lee", "Aryan Kaushik", "Wonjae Shin"], "title": "Space-Time Rate-Splitting Multiple Access for Multibeam LEO Satellite Networks", "comment": "17 pages, 3 figures, accepted for publication in IEEE Transactions on\n  Vehicular Technology", "summary": "This paper proposes a novel space-time rate-splitting multiple access\n(ST-RSMA) framework for multibeam low Earth orbit (LEO) satellite\ncommunications (SATCOM) systems, where space-time coding is integrated into the\ncommon stream transmission. This design enables full diversity gain in the\ncommon stream transmission for all users, regardless of the uncertainty of the\nchannel state information (CSI) and network load conditions, thereby overcoming\nthe performance limitations of conventional RSMA that employs a single\nbeamforming vector for all users. To further enhance performance, we develop a\nweighted minimum mean square error (WMMSE)-based algorithm tailored to ST-RSMA\nthat jointly optimizes the power allocation for the common stream and the\npower/beamforming vectors for private streams, aiming to maximize the minimum\nuser rate. Numerical results show that ST-RSMA significantly outperforms\nconventional RSMA and other multiple access techniques, offering a robust and\nscalable solution for LEO SATCOM."}
{"id": "2510.16576", "categories": ["cs.IT", "cs.IR", "cs.SY", "eess.SP", "eess.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.16576", "abs": "https://arxiv.org/abs/2510.16576", "authors": ["Zijian Zhang", "Mingyao Cui"], "title": "Enhancing Channel Estimation in RIS-aided Systems via Observation Matrix Design", "comment": "5 pages, 2 figures", "summary": "Reconfigurable intelligent surfaces (RISs) have emerged as a promising\ntechnology for enhancing wireless communications through dense antenna arrays.\nAccurate channel estimation is critical to unlocking their full performance\npotential. To enhance RIS channel estimators, this paper proposes a novel\nobservation matrix design scheme. Bayesian optimization framework is adopted to\ngenerate observation matrices that maximize the mutual information between\nreceived pilot signals and RIS channels. To solve the formulated problem\nefficiently, we develop an alternating Riemannian manifold optimization (ARMO)\nalgorithm to alternately update the receiver combiners and RIS phase-shift\nmatrices. An adaptive kernel training strategy is further introduced to\niteratively refine the channel covariance matrix without requiring additional\npilot resources. Simulation results demonstrate that the proposed ARMO-enhanced\nestimator achieves substantial gains in estimation accuracy over\nstate-of-the-art methods."}
{"id": "2510.17344", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.17344", "abs": "https://arxiv.org/abs/2510.17344", "authors": ["Nicolas Bousquet", "Amer E. Mouawad", "Stephanie Maaz", "Naomi Nishimura", "Sebastian Siebertz"], "title": "On Algorithmic Meta-Theorems for Solution Discovery: Tractability and Barriers", "comment": null, "summary": "Solution discovery asks whether a given (infeasible) starting configuration\nto a problem can be transformed into a feasible solution using a limited number\nof transformation steps. This paper investigates meta-theorems for solution\ndiscovery for graph problems definable in monadic second-order logic (MSO$_1$\nand MSO$_2$) and first-order logic (FO) where the transformation step is to\nslide a token to an adjacent vertex, focusing on parameterized complexity and\nstructural graph parameters that do not involve the transformation budget $b$.\nWe present both positive and negative results. On the algorithmic side, we\nprove that MSO$_2$-Discovery is in XP when parameterized by treewidth and that\nMSO$_1$-Discovery is fixed-parameter tractable when parameterized by\nneighborhood diversity. On the hardness side, we establish that FO-Discovery is\nW[1]-hard when parameterized by modulator to stars, modulator to paths, as well\nas twin cover, numbers. Additionally, we prove that MSO$_1$-Discovery is\nW[1]-hard when parameterized by bandwidth. These results complement the\nstraightforward observation that solution discovery for the studied problems is\nfixed-parameter tractable when the budget $b$ is included in the parameter (in\nparticular, parameterized by cliquewidth$+b$, where the cliquewidth of a graph\nis at most any of the studied parameters), and provide a near-complete\n(fixed-parameter tractability) meta-theorems investigation for solution\ndiscovery problems for MSO- and FO-definable graph problems and structural\nparameters larger than cliquewidth."}
{"id": "2510.17781", "categories": ["cs.IT", "math.IT", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.17781", "abs": "https://arxiv.org/abs/2510.17781", "authors": ["Hua Sun", "Syed A. Jafar"], "title": "On the Capacity of Erasure-prone Quantum Storage with Erasure-prone Entanglement Assistance", "comment": null, "summary": "A quantum message is encoded into $N$ storage nodes (quantum systems\n$Q_1\\dots Q_N$) with assistance from $N_B$ maximally entangled bi-partite\nquantum systems $A_1B_1, \\dots, A_{N_B}B_{N_B}$, that are prepared in advance\nsuch that $B_1\\dots B_{N_B}$ are stored separately as entanglement assistance\n(EA) nodes, while $A_1\\dots A_{N_B}$ are made available to the encoder. Both\nthe storage nodes and EA nodes are erasure-prone. The quantum message must be\nrecoverable given any $K$ of the $N$ storage nodes along with any $K_B$ of the\n$N_B$ EA nodes. The capacity for this setting is the maximum size of the\nquantum message, given that the size of each EA node is $\\lambda_B$. All node\nsizes are relative to the size of a storage node, which is normalized to unity.\nThe exact capacity is characterized as a function of $N,K,N_B,K_B, \\lambda_B$\nin all cases, with one exception. The capacity remains open for an intermediate\nrange of $\\lambda_B$ values when a strict majority of the $N$ storage nodes,\nand a strict non-zero minority of the $N_B$ EA nodes, are erased. As a key\nstepping stone, an analogous classical storage (with shared-randomness\nassistance) problem is introduced. A set of constraints is identified for the\nclassical problem, such that classical linear code constructions translate to\nquantum storage codes, and the converse bounds for the two settings utilize\nsimilar insights. In particular, the capacity characterizations for the\nclassical and quantum settings are shown to be identical in all cases where the\ncapacity is settled."}
{"id": "2510.17595", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.17595", "abs": "https://arxiv.org/abs/2510.17595", "authors": ["Manuel Christalla", "Luise Puhlmann", "Vera Traub"], "title": "Approximating Asymmetric A Priori TSP beyond the Adaptivity Gap", "comment": null, "summary": "In Asymmetric A Priori TSP (with independent activation probabilities) we are\ngiven an instance of the Asymmetric Traveling Salesman Problem together with an\nactivation probability for each vertex. The task is to compute a tour that\nminimizes the expected length after short-cutting to the randomly sampled set\nof active vertices.\n  We prove a polynomial lower bound on the adaptivity gap for Asymmetric A\nPriori TSP. Moreover, we show that a poly-logarithmic approximation ratio, and\nhence an approximation ratio below the adaptivity gap, can be achieved by a\nrandomized algorithm with quasi-polynomial running time.\n  To achieve this, we provide a series of polynomial-time reductions. First we\nreduce to a novel generalization of the Asymmetric Traveling Salesman Problem,\ncalled Hop-ATSP. Next, we use directed low-diameter decompositions to obtain\nstructured instances, for which we then provide a reduction to a covering\nproblem. Eventually, we obtain a polynomial-time reduction of Asymmetric A\nPriori TSP to a problem of finding a path in an acyclic digraph minimizing a\nparticular objective function, for which we give an O(log n)-approximation\nalgorithm in quasi-polynomial time."}
{"id": "2510.17645", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.17645", "abs": "https://arxiv.org/abs/2510.17645", "authors": ["Ce Jin", "Tomasz Kociumaka"], "title": "Near-Optimal Property Testers for Pattern Matching", "comment": "To appear at FOCS 2025. Abstract shortened to meet arXiv requirements", "summary": "The classic exact pattern matching problem, given two strings -- a pattern\n$P$ of length $m$ and a text $T$ of length $n$ -- asks whether $P$ occurs as a\nsubstring of $T$. A property tester for the problem needs to distinguish (with\nhigh probability) the following two cases for some threshold $k$: the YES case,\nwhere $P$ occurs as a substring of $T$, and the NO case, where $P$ has Hamming\ndistance greater than $k$ from every substring of $T$, that is, $P$ has no\n$k$-mismatch occurrence in $T$.\n  In this work, we provide adaptive and non-adaptive property testers for the\nexact pattern matching problem, jointly covering the whole spectrum of\nparameters. We further establish unconditional lower bounds demonstrating that\nthe time and query complexities of our algorithms are optimal, up to\n$\\mathrm{polylog}\\, n$ factors hidden within the $\\tilde O(\\cdot)$ notation\nbelow.\n  In the most studied regime of $n=m+\\Theta(m)$, our non-adaptive property\ntester has the time complexity of $\\tilde O(n/\\sqrt{k})$, and a matching lower\nbound remains valid for the query complexity of adaptive algorithms. This\nimproves both upon a folklore solution that attains the optimal query\ncomplexity but requires $\\Omega(n)$ time, and upon the only previously known\nsublinear-time property tester, by Chan, Golan, Kociumaka, Kopelowitz, and\nPorat [STOC 2020], with time complexity $\\tilde O(n/\\sqrt[3]{k})$. The\naforementioned results remain valid for $n=m+\\Omega(m)$, where our optimal\nrunning time $\\tilde O(\\sqrt{nm/k}+n/k)$ improves upon the previously best time\ncomplexity of $\\tilde O(\\sqrt[3]{n^2m/k}+n/k)$. In the regime of $n=m+o(m)$,\nwhich has not been targeted in any previous work, we establish a surprising\nseparation between adaptive and non-adaptive algorithms, whose optimal time and\nquery complexities are $\\tilde O(\\sqrt{(n-m+1)m/k}+n/k)$ and $\\tilde\nO(\\min(n\\sqrt{n-m+1}/k,\\sqrt{nm/k}+n/k))$, respectively."}
{"id": "2510.17714", "categories": ["cs.DS", "cs.LG", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2510.17714", "abs": "https://arxiv.org/abs/2510.17714", "authors": ["Atticus McWhorter", "Daryl DeFord"], "title": "The Marked Edge Walk: A Novel MCMC Algorithm for Sampling of Graph Partitions", "comment": null, "summary": "Novel Markov Chain Monte Carlo (MCMC) methods have enabled the generation of\nlarge ensembles of redistricting plans through graph partitioning. However,\nexisting algorithms such as Reversible Recombination (RevReCom) and\nMetropolized Forest Recombination (MFR) are constrained to sampling from\ndistributions related to spanning trees. We introduce the marked edge walk\n(MEW), a novel MCMC algorithm for sampling from the space of graph partitions\nunder a tunable distribution. The walk operates on the space of spanning trees\nwith marked edges, allowing for calculable transition probabilities for use in\nthe Metropolis-Hastings algorithm. Empirical results on real-world dual graphs\nshow convergence under target distributions unrelated to spanning trees. For\nthis reason, MEW represents an advancement in flexible ensemble generation."}
{"id": "2510.17740", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.17740", "abs": "https://arxiv.org/abs/2510.17740", "authors": ["Shunhua Jiang", "Michael Kapralov", "Lawrence Li", "Aaron Sidford"], "title": "Generalized Flow in Nearly-linear Time on Moderately Dense Graphs", "comment": "65 pages. FOCS 2025", "summary": "In this paper we consider generalized flow problems where there is an\n$m$-edge $n$-node directed graph $G = (V,E)$ and each edge $e \\in E$ has a loss\nfactor $\\gamma_e >0$ governing whether the flow is increased or decreased as it\ncrosses edge $e$. We provide a randomized $\\tilde{O}( (m + n^{1.5}) \\cdot\n\\mathrm{polylog}(\\frac{W}{\\delta}))$ time algorithm for solving the generalized\nmaximum flow and generalized minimum cost flow problems in this setting where\n$\\delta$ is the target accuracy and $W$ is the maximum of all costs,\ncapacities, and loss factors and their inverses. This improves upon the\nprevious state-of-the-art $\\tilde{O}(m \\sqrt{n} \\cdot \\log^2(\\frac{W}{\\delta})\n)$ time algorithm, obtained by combining the algorithm of [Daitch-Spielman,\n2008] with techniques from [Lee-Sidford, 2014]. To obtain this result we\nprovide new dynamic data structures and spectral results regarding the matrices\nassociated to generalized flows and apply them through the interior point\nmethod framework of [Brand-Lee-Liu-Saranurak-Sidford-Song-Wang, 2021]."}
{"id": "2510.17752", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.17752", "abs": "https://arxiv.org/abs/2510.17752", "authors": ["Panagiotis Charalampopoulos", "Tomasz Kociumaka", "Philip Wellnitz"], "title": "Pattern Matching under Weighted Edit Distance", "comment": "96 pages + bibliography + index of results, 8 figures. Sections 7 and\n  8 of this article generalize and heavily draw from our earlier works\n  arXiv:2004.08350 and arXiv:2204.03087", "summary": "In Pattern Matching with Weighted Edits (PMWED), we are given a pattern $P$\nof length $m$, a text $T$ of length $n$, a positive threshold $k$, and oracle\naccess to a weight function that specifies the costs of edits (depending on the\ninvolved characters, and normalized so that the cost of each edit is at least\n$1$). The goal is to compute the starting positions of all fragments of $T$\nthat can be obtained from $P$ with edits of total cost at most $k$. PMWED\ncaptures typical real-world applications more accurately than its unweighted\nvariant (PMED), where all edits have unit costs.\n  We obtain three main results:\n  (a) a conceptually simple $\\tilde{O}(nk)$-time algorithm for PMWED, very\ndifferent from that of Landau and Vishkin for PMED;\n  (b) a significantly more complicated $\\tilde{O}(n+k^{3.5} \\cdot W^4\\cdot\nn/m)$-time algorithm for PMWED under the assumption that the weight function is\na metric with integer values between $0$ and $W$; and\n  (c) an $\\tilde{O}(n+k^4 \\cdot n/m)$-time algorithm for PMWED for the case of\narbitrary weights.\n  In the setting of metrics with small integer values, we nearly match the\nstate of the art for PMED where $W=1$."}
{"id": "2510.17799", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.17799", "abs": "https://arxiv.org/abs/2510.17799", "authors": ["Debarati Das", "Jacob Gilbert", "MohammadTaghi Hajiaghayi", "Tomasz Kociumaka", "Barna Saha"], "title": "Dynamic Dyck and Tree Edit Distance: Decompositions and Reductions to String Edit Distance", "comment": "Full version of a FOCS 2025 paper", "summary": "We present the first dynamic algorithms for Dyck and tree edit distances with\nsubpolynomial update times. Dyck edit distance measures how far a parenthesis\nstring is from a well-parenthesized expression, while tree edit distance\nquantifies the minimum number of node insertions, deletions, and substitutions\nrequired to transform one rooted, ordered, labeled tree into another. Despite\nextensive study, no prior work has addressed efficient dynamic algorithms for\nthese problems, which naturally arise in evolving structured data such as LaTeX\ndocuments, JSON or XML files, and RNA secondary structures.\n  Our main contribution is a set of reductions and decompositions that\ntransform Dyck and tree edit distance instances into efficiently maintainable\nstring edit distance instances, which can be approximated within a $n^{o(1)}$\nfactor in $n^{o(1)}$ update time. For Dyck edit distance, our reduction incurs\nonly polylogarithmic overheads in approximation and update time, yielding an\n$n^{o(1)}$-approximation with $n^{o(1)}$ updates. For tree edit distance, we\nintroduce a new static reduction that improves the best-known approximation\nratio from $n^{3/4}$ to $\\tilde{O}(\\sqrt{n})$ and removes the restriction to\nconstant-degree trees. Extending this reduction dynamically achieves\n$n^{1/2+o(1)}$ approximation with $n^{o(1)}$ update time.\n  A key component is a dynamic maintenance algorithm for history-independent\nheavy-light decompositions, of independent interest. We also provide a novel\nstatic and dynamic decomposition achieving an $O(k \\log n)$-approximation when\nthe tree edit distance is at most $k$. Combined with the trivial bound $k \\le\nn$, this yields a dynamic deterministic $O(\\sqrt{n \\log n})$-approximation. In\nthe static setting, our algorithm runs in near-linear time; dynamically, it\nrequires only polylogarithmic updates, improving on prior linear-time static\n$O(\\sqrt{n})$-approximation."}
