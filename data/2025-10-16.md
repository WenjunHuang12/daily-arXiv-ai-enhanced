<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 7]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IT](#cs.IT) [Total: 8]
- [cs.IR](#cs.IR) [Total: 11]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Finding a Nash equilibrium of a random win-lose game in expected polynomial time](https://arxiv.org/abs/2510.12846)
*Andrea Collevecchio,Gabor Lugosi,Adrian Vetta,Rui-Ray Zhang*

Main category: cs.GT

TL;DR: 该论文研究了随机赢-输博弈中纳什均衡的计算问题，证明了对于几乎所有的参数p(n)，存在期望多项式时间算法来找到纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 解决算法博弈论中长期存在的开放问题：是否存在多项式时间算法计算随机双矩阵博弈中的纳什均衡。

Method: 研究随机赢-输博弈，其中n×n支付矩阵的条目是参数为p=p(n)的独立同分布伯努利随机变量。分析不同参数范围下的算法效率。

Result: 当p∼cn^{-a}时，对于a∉{1/2,1}的所有情况，存在期望多项式时间算法。对于a=1/2和a=1的特殊情况，给出了具体的参数范围条件。

Conclusion: 该研究为随机赢-输博弈中纳什均衡的计算提供了重要的理论进展，几乎解决了这一长期开放问题。

Abstract: A long-standing open problem in algorithmic game theory asks whether or not
there is a polynomial time algorithm to compute a Nash equilibrium in a random
bimatrix game. We study random win-lose games, where the entries of the
$n\times n$ payoff matrices are independent and identically distributed
(i.i.d.) Bernoulli random variables with parameter $p=p(n)$. We prove that, for
nearly all values of the parameter $p=p(n)$, there is an expected
polynomial-time algorithm to find a Nash equilibrium in a random win-lose game.
More precisely, if $p\sim cn^{-a}$ for some parameters $a,c\ge 0$, then there
is an expected polynomial-time algorithm whenever $a\not\in \{1/2, 1\}$. In
addition, if $a = 1/2$ there is an efficient algorithm if either $c \le e^{-52}
2^{-8} $ or $c\ge 0.977$. If $a=1$, then there is an expected polynomial-time
algorithm if either $c\le 0.3849$ or $c\ge \log^9 n$.

</details>


### [2] [Equilibria in routing games with connected autonomous vehicles will not be strong, as exclusive clubs may form](https://arxiv.org/abs/2510.12862)
*Rafał Kucharski,Anastasia Psarou,Natello Descormier*

Main category: cs.GT

TL;DR: 该论文首次展示了联网自动驾驶车辆(CAVs)可以形成路由联盟，偏离用户均衡状态，使联盟成员受益但损害其他用户和系统效率，这对城市交通公平性有深远影响。


<details>
  <summary>Details</summary>
Motivation: 随着联网自动驾驶车辆(CAVs)的出现，车辆能够通信和协作形成路由联盟，这改变了传统人类驾驶无法形成联盟的情况，需要研究这种新能力对交通系统的影响。

Method: 通过精心设计的玩具网络示例，展示由三辆自动驾驶车辆组成的'俱乐部'如何联合决定偏离用户均衡状态，并分析这种联盟形成对其他用户和系统的影响。

Result: 研究发现CAV联盟能够通过偏离用户均衡获得更快的到达时间，但这对未被邀请加入联盟的其他用户造成负面影响，导致他们旅行时间延长，并使系统处于次优和失衡状态。

Conclusion: 如果不加以预防，CAV运营商可能故意使交通系统偏离经典纳什均衡，为其用户谋利而将成本转嫁给他人，可能导致出现排他性的CAV精英阶层，损害公共道路网络的公平性。

Abstract: User Equilibrium is the standard representation of the so-called routing game
in which drivers adjust their route choices to arrive at their destinations as
fast as possible. Asking whether this Equilibrium is strong or not was
meaningless for human drivers who did not form coalitions due to technical and
behavioral constraints. This is no longer the case for connected autonomous
vehicles (CAVs), which will be able to communicate and collaborate to jointly
form routing coalitions.
  We demonstrate this for the first time on a carefully designed toy-network
example, where a `club` of three autonomous vehicles jointly decides to deviate
from the user equilibrium and benefit (arrive faster). The formation of such a
club has negative consequences for other users, who are not invited to join it
and now travel longer, and for the system, making it suboptimal and
disequilibrated, which triggers adaptation dynamics.
  This discovery has profound implications for the future of our cities. We
demonstrate that, if not prevented, CAV operators may intentionally
disequilibrate traffic systems from their classic Nash equilibria, benefiting
their own users and imposing costs on others. These findings suggest the
possible emergence of an exclusive CAV elite, from which human-driven vehicles
and non-coalition members may be excluded, potentially leading to
systematically longer travel times for those outside the coalition, which would
be harmful for the equity of public road networks.

</details>


### [3] [Efficiency of Constant Log Utility Market Makers](https://arxiv.org/abs/2510.12952)
*Maneesha Papireddygari,Xintong Wang,Bo Waggoner,David M. Pennock*

Main category: cs.GT

TL;DR: 本文分析了恒定对数效用做市商(CLUM)的计算复杂性，证明了其定价问题是#P-难的，并提出了一种高概率近似算法来使CLUM更实用。


<details>
  <summary>Details</summary>
Motivation: 传统对数市场评分规则(LMSR)做市商在最坏情况下的损失随结果数量增长，而CLUM具有恒定最坏情况损失，允许动态添加结果甚至处理可数无限结果，但需要解决其计算复杂性问题。

Method: 通过从2-SAT模型计数问题归约证明CLUM定价的#P-难性；提出基于oracle的近似算法，该oracle能确定任何结果的最大购买份额和具有该最大数量的结果总数；在区间证券情况下展示了oracle的多项式时间实现。

Result: 证明了CLUM证券定价是#P-难的；开发了高概率近似算法；在区间证券场景下实现了多项式时间的oracle。

Conclusion: CLUM虽然具有优于LMSR的损失特性，但其定价计算复杂；提出的近似算法为实际应用提供了可行方案，特别是在区间证券等特定场景下。

Abstract: Automated Market Makers (AMMs) are used to provide liquidity for
combinatorial prediction markets that would otherwise be too thinly traded.
They offer both buy and sell prices for any of the doubly exponential many
possible securities that the market can offer. The problem of setting those
prices is known to be #P-hard for the original and most well-known AMM, the
logarithmic market scoring rule (LMSR) market maker [Chen et al., 2008]. We
focus on another natural AMM, the Constant Log Utility Market Maker (CLUM).
Unlike LMSR, whose worst-case loss bound grows with the number of outcomes,
CLUM has constant worst-case loss, allowing the market to add outcomes on the
fly and even operate over countably infinite many outcomes, among other
features. Simpler versions of CLUM underpin several Decentralized Finance
(DeFi) mechanisms including the Uniswap protocol that handles billions of
dollars of cryptocurrency trades daily. We first establish the computational
complexity of the problem: we prove that pricing securities is #P-hard for
CLUM, via a reduction from the model counting 2-SAT problem. In order to make
CLUM more practically viable, we propose an approximation algorithm for pricing
securities that works with high probability. This algorithm assumes access to
an oracle capable of determining the maximum shares purchased of any one
outcome and the total number of outcomes that has that maximum amount
purchased. We then show that this oracle can be implemented in polynomial time
when restricted to interval securities, which are used in designing financial
options.

</details>


### [4] [Repeated Sales with Heterogeneous Buyer Sophistication](https://arxiv.org/abs/2510.13088)
*Rishi Patel,Emmanouil Pountourakis,Samuel Taggart*

Main category: cs.GT

TL;DR: 研究行为定价歧视在重复销售非耐用品给长期买家时的动态，分析精明和天真买家混合对卖家学习和收入的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨在卖家缺乏承诺能力的情况下，混合精明和天真买家群体如何影响卖家通过行为定价进行学习和收入获取。

Method: 使用两期模型分析短期互动，无限期模型分析长期互动，比较不同时间跨度下买家策略性需求减少的影响。

Result: 短期：天真买家加剧整体需求减少，损害卖家收入；长期：天真买家存在时，极端需求减少现象消失，卖家能有效学习并获得接近承诺能力下的收入。

Conclusion: 买家类型混合对行为定价的影响因时间跨度而异，长期互动中即使少量天真买家也能显著改善卖家收入表现。

Abstract: This paper considers behavior-based price discrimination in the repeated sale
of a non-durable good to a single long-lived buyer, by a seller without
commitment power. We assume that there is a mixed population of forward-looking
``sophisticated'' buyers and myopic ``naive'' buyers. We investigate the impact
of these dynamics on the seller's ability to learn about the buyer and exploit
this learning for revenue. We obtain conclusions that differ dramatically with
the time horizon of the interactions. To understand short time horizons, we
analyze a two-period model, and find that the strategic demand reduction
observed with fully sophisticated buyers is robust to the introduction of naive
types. In fact, despite the inability of naive buyers to game the pricing
algorithm, their introduction can further harm the seller's revenue, due to
more intense demand reduction overall. For long horizons, we consider an
infinite-horizon model with time discounting. We find that the extreme demand
reduction predicted by previous work does not survive the introduction of naive
buyers. Instead, we observe equilibria where the seller learns meaningfully
despite the sophisticated buyers' demand reduction. We prove that for a natural
family of such equilibria, the seller's revenue is not just high, but
approximates the revenue attainable with commitment power, even when the
fraction of naive types is vanishingly small.

</details>


### [5] [A Ratio-Based Shapley Value for Collaborative Machine Learning - Extended Version](https://arxiv.org/abs/2510.13261)
*Björn Filter,Ralf Möller,Özgür Lütfü Özçep*

Main category: cs.GT

TL;DR: 本文提出了一种基于比率的Shapley值方法，用于协作机器学习中的激励分配，替代传统的加法形式，强调相对贡献而非绝对差异。


<details>
  <summary>Details</summary>
Motivation: 协作机器学习中确保激励兼容性和基于贡献的公平奖励分配是一个关键挑战。现有方法使用基于信息增益的Shapley值分配模型奖励，但本文认为在某些情境下，贡献者之间的比例关系比加法差异更有意义。

Method: 引入比率型Shapley值，将标准加法公式替换为相对贡献度量。保持与Sim等人相同的奖励框架，但使用根本不同的价值函数。

Result: 比率型价值函数诱导出不同的模型奖励分布，并提供了分析激励特性的新视角。该方法满足与加法公式相同的激励条件，包括公平性、个体理性和稳定性。

Conclusion: 本文提供了一个数学上严谨的加法Shapley框架替代方案，在贡献者比例关系比加法差异更有意义的情境下可能更适用。

Abstract: Collaborative machine learning enables multiple data owners to jointly train
models for improved predictive performance. However, ensuring incentive
compatibility and fair contribution-based rewards remains a critical challenge.
Prior work by Sim and colleagues (Rachel Hwee Ling Sim et al: Collaborative
machine learning with incentive-aware model rewards. In: International
conference on machine learning. PMLR. 2020, pp. 8927-8963) addressed this by
allocating model rewards, which are non-monetary and freely replicable, based
on the Shapley value of each party's data contribution, measured via
information gain. In this paper, we introduce a ratio-based Shapley value that
replaces the standard additive formulation with a relative contribution
measure. While our overall reward framework, including the incentive
definitions and model-reward setting, remains aligned with that of Sim and
colleagues, the underlying value function is fundamentally different. Our
alternative valuation induces a different distribution of model rewards and
offers a new lens through which to analyze incentive properties. We formally
define the ratio-based value and prove that it satisfies the same set of
incentive conditions as the additive formulation, including adapted versions of
fairness, individual rationality, and stability. Like the original approach,
our method faces the same fundamental trade-offs between these incentives. Our
contribution is a mathematically grounded alternative to the additive Shapley
framework, potentially better suited to contexts where proportionality among
contributors is more meaningful than additive differences.

</details>


### [6] [Nash Flows Over Time with Tolls](https://arxiv.org/abs/2510.13518)
*Shaul Rosner,Marc Schröder,Laura Vargas Koch*

Main category: cs.GT

TL;DR: 研究带有收费的动态路由博弈，分析收费对交通流均衡的影响，并提供计算稳态的方法。


<details>
  <summary>Details</summary>
Motivation: 在Vickrey瓶颈模型基础上引入收费机制，研究收费如何影响动态均衡的唯一性和稳态特性。

Method: 基于Vickrey瓶颈模型，引入边收费成本，考虑非原子均衡，分析流粒子在收费和旅行时间总和最小路径上的行为。

Result: 发现带收费的动态均衡在成本方面不唯一，且不一定达到稳态；提供了计算带收费模型稳态的程序。

Conclusion: 收费显著改变了动态路由博弈的特性，需要新的方法来分析和计算稳态均衡。

Abstract: We study a dynamic routing game motivated by traffic flows. The base model
for an edge is the Vickrey bottleneck model. That is, edges are equipped with a
free flow transit time and a capacity. When the inflow into an edge exceeds its
capacity, a queue forms and the following particles experience a waiting time.
In this paper, we enhance the model by introducing tolls, i.e., a cost each
flow particle must pay for traversing an edge. In this setting we consider
non-atomic equilibria, which means flows over time in which every particle is
on a cheapest path, when summing up toll and travel time. We first show that
unlike in the non-tolled version of this model, dynamic equilibria are not
unique in terms of costs and do not necessarily reach a steady state. As a main
result, we provide a procedure to compute steady states in the model with
tolls.

</details>


### [7] [Online Fair Division With Subsidy: When Do Envy-Free Allocations Exist, and at What Cost?](https://arxiv.org/abs/2510.13633)
*Pooja Kulkarni,Ruta Mehta,Vishnu V. Narayan,Tomasz Ponitka*

Main category: cs.GT

TL;DR: 研究在线环境下不可分割物品的公平分配问题，探讨如何维持嫉妒可消除性(EF)并确保低补贴。发现在线设置下即使对加性估值，最小补贴也可能高达Ω(mn)，远高于离线设置的O(n)界限。


<details>
  <summary>Details</summary>
Motivation: 将离线环境下的嫉妒可消除分配研究扩展到在线设置，解决物品逐个到达且必须立即分配的挑战，探索在线算法能否维持嫉妒可消除性以及所需的补贴量。

Method: 设计在线算法来维持嫉妒可消除性，分析不同估值类别下的补贴需求，包括加性估值、k-demand、SPLC估值等，并识别补贴需求较小的特殊估值类别。

Result: 对于子模或超模估值，即使二元边际也无法在线维持嫉妒可消除性；对于加性估值及其超类，设计了能维持嫉妒可消除性的在线算法；在线设置下最小补贴可达Ω(mn)，但某些特殊估值类别补贴需求较小。

Conclusion: 在线公平分配面临比离线更严峻的挑战，特别是补贴需求显著增加，但在特定估值类别下仍可实现低补贴的嫉妒可消除分配。

Abstract: We study the problem of fairly allocating $m$ indivisible items arriving
online, among $n$ (offline) agents. Although envy-freeness has emerged as the
archetypal fairness notion, envy-free (EF) allocations need not exist with
indivisible items. To bypass this, a prominent line of research demonstrates
that there exist allocations that can be made envy-free by allowing a subsidy.
Extensive work in the offline setting has focused on finding such envy-freeable
allocations with bounded subsidy. We extend this literature to an online
setting where items arrive one at a time and must be immediately and
irrevocably allocated. Our contributions are two-fold:
  1. Maintaining EF Online: We show that envy-freeability cannot always be
preserved online when the valuations are submodular or supermodular, even with
binary marginals. In contrast, we design online algorithms that maintain
envy-freeability at every step for the class of additive valuations, and for
its superclasses including $k$-demand and SPLC valuations.
  2. Ensuring Low Subsidy: We investigate the quantity of subsidy required to
guarantee envy-freeness online. Surprisingly, even for additive valuations, the
minimum subsidy may be as large as $\Omega(mn)$, in contrast to the offline
setting, where the bound is $O(n)$. On the positive side, we identify valuation
classes where the minimum subsidy is small (i.e., does not depend on $m$),
including $k$-valued, rank-one, restricted additive, and identical valuations,
and we obtain (mostly) tight subsidy bounds for these classes.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [8] [A faster algorithm for efficient longest common substring calculation for non-parametric entropy estimation in sequential data](https://arxiv.org/abs/2510.13330)
*Bridget Smart,Max Ward,Matthew Roughan*

Main category: cs.DS

TL;DR: 提出LCSFinder算法，将最长公共子串计算的最坏情况性能从三次方时间提升到对数线性时间，显著加速序列数据的非参数熵估计


<details>
  <summary>Details</summary>
Motivation: 基于最长公共子串的非参数熵估计方法在信号处理中很重要，但现有方法效率低下，限制了在真实世界数据上的应用

Method: 使用排序后缀数组和持久化二叉搜索树等标准算法构建，通过精心设计实现动态增长序列的匹配需求

Result: 在真实和模拟数据上相比现有实现获得显著加速，使熵估计在之前不可行的规模上变得可行

Conclusion: LCSFinder算法实现了从三次方到对数线性时间的性能提升，为实际信号处理中的大规模熵估计提供了可行方案

Abstract: Non-parametric entropy estimation on sequential data is a fundamental tool in
signal processing, capturing information flow within or between processes to
measure predictability, redundancy, or similarity. Methods based on longest
common substrings (LCS) provide a non-parametric estimate of typical set size
but are often inefficient, limiting use on real-world data. We introduce
LCSFinder, a new algorithm that improves the worst-case performance of LCS
calculations from cubic to log-linear time. Although built on standard
algorithmic constructs - including sorted suffix arrays and persistent binary
search trees - the details require care to provide the matches required for
entropy estimation on dynamically growing sequences. We demonstrate that
LCSFinder achieves dramatic speedups over existing implementations on real and
simulated data, enabling entropy estimation at scales previously infeasible in
practical signal processing.

</details>


### [9] [Tight Parameterized (In)tractability of Layered Crossing Minimization: Subexponential Algorithms and Kernelization](https://arxiv.org/abs/2510.13335)
*Fedor V. Fomin,Petr A. Golovach,Tanmay Inamdar,Saket Saurabh,Meirav Zehavi*

Main category: cs.DS

TL;DR: 本文解决了2层交叉最小化的十年开放问题，给出了首个亚指数固定参数算法，并研究了多层交叉最小化问题的复杂性边界。


<details>
  <summary>Details</summary>
Motivation: 解决2层交叉最小化问题的亚指数参数化复杂性这一十年开放问题，并探索多层交叉最小化问题的复杂性边界。

Method: 设计了新的多项式核，并基于这些核构建亚指数固定参数算法。对于2层和3层问题分别给出了不同的算法复杂度。

Result: 成功解决了2层交叉最小化问题，给出了2^O(√k log k) + n·k^O(1)时间的算法；对于3层问题给出了2^O(k^{2/3} log k) + n·k^O(1)时间的算法；证明了对于h≥5层，除非指数时间假设失败，否则不存在2^o(k/log k)·n^O(1)时间的算法。

Conclusion: 交叉最小化问题在2层和3层具有亚指数固定参数算法，但对于5层及以上则不存在这样的算法，除非指数时间假设失败。同时，对于h≥4层，多项式核的存在会导致多项式层次结构坍塌。

Abstract: The starting point of our work is a decade-old open question concerning the
subexponential parameterized complexity of \textsc{2-Layer Crossing
Minimization}. In this problem, the input is an $n$-vertex graph $G$ whose
vertices are partitioned into two independent sets $V_1$ and $V_2$, and a
non-negative integer $k$. The question is whether $G$ admits a 2-layered
drawing with at most $k$ crossings, where each $V_i$ lies on a distinct line
parallel to the $x$-axis, and all edges are straight lines. We resolve this
open question by giving the first subexponential fixed-parameter algorithm for
this problem, running in time $2^{O(\sqrt{k}\log k)} + n \cdot k^{O(1)}$.
  We then ask whether the subexponential phenomenon extends beyond two layers.
In the general $h$-Layer Crossing Minimization problem, the vertex set is
partitioned into $h$ independent sets $V_1, \ldots, V_h$, and the goal is to
decide whether an $h$-layered drawing with at most $k$ crossings exists. We
present a subexponential FPT algorithm for three layers with running time
$2^{O(k^{2/3}\log k)} + n \cdot k^{O(1)}$ for $h = 3$ layers. In contrast, we
show that for all $h \ge 5$, no algorithm with running time $2^{o(k/\log k)}
\cdot n^{O(1)}$ exists unless the Exponential-Time Hypothesis fails.
  Finally, we address polynomial kernelization. While a polynomial kernel was
already known for $h=2$, we design a new polynomial kernel for $h=3$. These
kernels are essential ingredients in our subexponential algorithms. Finally, we
rule out polynomial kernels for all $h \ge 4$ unless the polynomial hierarchy
collapses.

</details>


### [10] [Chromatic correlation clustering via cluster LP](https://arxiv.org/abs/2510.13446)
*Fateme Abbasi,Hyung-Chan An,Jarosław Byrka,Changyeol Lee,Yongho Shin*

Main category: cs.DS

TL;DR: 提出了一种使用色聚类LP的(2+ε)-近似算法来解决色相关聚类问题


<details>
  <summary>Details</summary>
Motivation: 鉴于聚类LP在相关聚类中的成功应用，研究者想知道是否可以将聚类LP扩展到色相关聚类问题中

Method: 使用色聚类LP来设计近似算法

Result: 开发出了(2+ε)-近似算法

Conclusion: 证实了聚类LP确实可以用于色相关聚类问题，并取得了良好的近似比

Abstract: Correlation Clustering is a fundamental clustering problem, and there has
been a line of work on improving the approximation ratio for this problem in
recent years. A key algorithmic component in these works is the cluster LP.
Chromatic Correlation Clustering is an interesting generalization that has also
been intensively studied. In light of success of the cluster LP in Correlation
Clustering, it would be an interesting question whether the cluster LP can be
used in Chromatic Correlation Clustering. We answer this question with
affirmatives by presenting a $(2+\varepsilon)$-approximation algorithm for
Chromatic Correlation Clustering using a chromatic cluster LP.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [11] [Experiments \& Analysis of Privacy-Preserving SQL Query Sanitization Systems](https://arxiv.org/abs/2510.13528)
*Loïs Ecoffet,Veronika Rehn-Sonigo,Jean-François Couchot,Catuscia Palamidessi*

Main category: cs.DB

TL;DR: 本文对SQL查询消毒系统进行了系统分类和性能评估，分析了隐私保护数据库技术的现状和局限性。


<details>
  <summary>Details</summary>
Motivation: 分析型SQL查询在提取数据库洞察的同时带来显著的隐私风险，现有查询消毒系统设计复杂多样，需要系统化的分类和性能评估来指导研究和实践。

Method: 基于隐私模型、保护单元和软件架构等定性标准对SQL消毒系统进行分类，并对主要系统进行定量分析，测量数据效用、查询执行开销和隐私保证之间的权衡。

Result: 提供了SQL消毒系统的结构化概览和性能评估，阐明了当前隐私保护数据库技术的能力和局限性。

Conclusion: 这项工作为研究人员和从业者提供了清晰的框架来理解和选择适合的隐私保护数据库技术。

Abstract: Analytical SQL queries are essential for extracting insights from relational
databases but concurrently introduce significant privacy risks by potentially
exposing sensitive information. To mitigate these risks, numerous query
sanitization systems have been developed, employing diverse approaches that
create a complex landscape for both researchers and practitioners. These
systems vary fundamentally in their design, including the underlying privacy
model, such as k-anonymity or Differential Privacy; the protected privacy unit,
whether at the tuple- or user-level; and the software architecture, which can
be proxy-based or integrated. This paper provides a systematic classification
of state-of-the-art SQL sanitization systems based on these qualitative
criteria and the scope of queries they support. Furthermore, we present a
quantitative analysis of leading systems, empirically measuring the trade-offs
between data utility, query execution overhead, and privacy guarantees across a
range of analytical queries. This work offers a structured overview and
performance assessment intended to clarify the capabilities and limitations of
current privacy-preserving database technologies.

</details>


### [12] [The Past Still Matters: A Temporally-Valid Data Discovery System](https://arxiv.org/abs/2510.13662)
*Mahdi Esmailoghli,Matthias Weidlich*

Main category: cs.DB

TL;DR: 提出了一个考虑时间维度的数据发现系统愿景，解决现有方法忽略数据相关性随时间变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有数据发现方法主要忽略时间维度，特别是当缺少明确日期/时间元数据时，而数据相关性是随时间变化的。

Method: 定义了时间有效数据发现问题，提出了版本发现、时间谱系推断、变更日志合成和时间感知数据发现等技术，并设计了系统架构。

Result: 为新一代数据发现系统奠定了基础，将改变我们与演进数据湖的交互方式。

Conclusion: 时间维度是数据发现的关键方面，需要开发专门技术来支持时间感知的数据发现，这代表了该领域的重要发展方向。

Abstract: Over the past decade, the proliferation of public and enterprise data lakes
has fueled intensive research into data discovery, aiming to identify the most
relevant data from vast and complex corpora to support diverse user tasks.
Significant progress has been made through the development of innovative index
structures, similarity measures, and querying infrastructures. Despite these
advances, a critical aspect remains overlooked: relevance is time-varying.
Existing discovery methods largely ignore this temporal dimension, especially
when explicit date/time metadata is missing. To fill this gap, we outline a
vision for a data discovery system that incorporates the temporal dimension of
data. Specifically, we define the problem of temporally-valid data discovery
and argue that addressing it requires techniques for version discovery,
temporal lineage inference, change log synthesis, and time-aware data
discovery. We then present a system architecture to deliver these techniques,
before we summarize research challenges and opportunities. As such, we lay the
foundation for a new class of data discovery systems, transforming how we
interact with evolving data lakes.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [13] [On the performance of Active STAR-RIS-Assisted Cell-Free Massive MIMO Systems with Phase Errors and Channel Aging](https://arxiv.org/abs/2510.13171)
*Jun Qian,Ross Murch,Khaled B. Letaief*

Main category: cs.IT

TL;DR: 分析了主动可重构智能表面在cell-free大规模MIMO系统中的相位误差和信道老化影响，提出了资源块长度设计指南，发现增加接入点和STAR-RIS元素可以缓解性能下降。


<details>
  <summary>Details</summary>
Motivation: 主动STAR-RIS使用放大来克服级联链路衰减，但面临相位误差和信道老化问题，需要分析这些因素对系统性能的影响。

Method: 利用空间相关瑞利衰落模型，推导基于最小均方误差估计的信道估计，并制定下行链路频谱效率的闭式表达式。

Result: 主动STAR-RIS能有效补偿相位误差和信道老化的不利影响，增加APs和STAR-RIS元素以及更大的放大因子可以缓解性能下降。

Conclusion: 主动STAR-RIS在cell-free大规模MIMO系统中能有效应对相位误差和信道老化挑战，提出了实用的资源块长度设计指南。

Abstract: Active reconfigurable intelligent surfaces (RISs) employ amplification to
overcome attenuation caused by the RIS cascaded link. In this paper, we analyze
the effects of phase errors and channel aging in active simultaneously
transmitting and reflecting (STAR) RIS-assisted cell-free massive
multiple-input multiple-output (MIMO) systems. By leveraging a spatially
correlated Rayleigh fading model, this paper derives minimum mean square error
estimate-based channel estimates and formulates closed-form expressions for
downlink spectral efficiency. This analytical framework enables a comprehensive
evaluation of the effects of channel aging and uniformly distributed phase
errors on system performance. The results demonstrate that active STAR-RISs can
effectively compensate for the adverse effects of phase errors and channel
aging. To counteract the impact of channel aging, we propose practical
guidelines for resource-block-length design. Also, an increase in APs and
STAR-RIS elements, along with a larger amplification factor, can alleviate
performance degradation.

</details>


### [14] [A Dimension-Keeping Semi-Tensor Product Framework for Compressed Sensing](https://arxiv.org/abs/2510.13180)
*Qi Qi,Abdelhamid Tayebi,Daizhan Cheng,Jun-e Feng*

Main category: cs.IT

TL;DR: 提出了一种名为DK-STP-CS的新方法，通过利用组内相关性同时保持组间不相干性来增强测量矩阵设计，在图像压缩和重建任务中实现了显著的噪声抑制和视觉保真度提升。


<details>
  <summary>Details</summary>
Motivation: 传统压缩感知框架依赖测量矩阵列的不相干性来保证重建性能，但未能充分利用信号的结构特性。本文旨在通过利用组内相关性同时保持组间不相干性来改进测量矩阵设计。

Method: 将DK-STP算法集成到感知矩阵设计中，在保持信号恢复能力的同时实现降维。该方法利用组内相关性并保持组间不相干性来增强测量矩阵。

Result: 实验结果表明，DK-STP-CS在图像压缩和重建任务中显著优于传统CS和STP-CS方法，重建图像与原图像之间的峰值信噪比(PSNR)值更高。在噪声条件和不同采样率下都表现出鲁棒性。

Conclusion: DK-STP-CS方法在资源受限环境中具有实际应用潜力，能够显著提升压缩感知性能，特别是在噪声抑制和视觉保真度方面表现优异。

Abstract: In compressed sensing (CS), sparse signals can be reconstructed from
significantly fewer samples than required by the Nyquist-Shannon sampling
theorem. While non-sparse signals can be sparsely represented in appropriate
transformation domains, conventional CS frameworks rely on the incoherence of
the measurement matrix columns to guarantee reconstruction performance. This
paper proposes a novel method termed Dimension-Keeping Semi-Tensor Product
Compressed Sensing (DK-STP-CS), which leverages intra-group correlations while
maintaining inter-group incoherence to enhance the measurement matrix design.
Specifically, the DK-STP algorithm is integrated into the design of the sensing
matrix, enabling dimensionality reduction while preserving signal recovery
capability. For image compression and reconstruction tasks, the proposed method
achieves notable noise suppression and improves visual fidelity. Experimental
results demonstrate that DK-STP-CS significantly outperforms traditional CS and
STP-CS approaches, as evidenced by higher Peak Signal-to-Noise Ratio (PSNR)
values between the reconstructed and original images. The robustness of
DK-STP-CS is further validated under noisy conditions and varying sampling
rates, highlighting its potential for practical applications in
resource-constrained environments.

</details>


### [15] [Movable and Reconfigurable Antennas for 6G: Unlocking Electromagnetic-Domain Design and Optimization](https://arxiv.org/abs/2510.13209)
*Lipeng Zhu,Haobin Mao,Ge Yan,Wenyan Ma,Zhenyu Xiao,Rui Zhang*

Main category: cs.IT

TL;DR: 本文综述了可移动天线和可重构天线在6G通信中的应用、硬件架构和设计方法，展示了它们相比传统天线的性能优势。


<details>
  <summary>Details</summary>
Motivation: 6G移动通信网络日益增长的需求需要先进的天线技术，可移动天线和可重构天线能够动态控制天线参数，为无线系统设计引入丰富的电磁域自由度。

Method: 通过综述应用场景、硬件架构和设计方法，结合现场测试和仿真结果进行分析。

Result: 现场测试和仿真结果表明，可移动天线和可重构天线相比传统固定/不可重构天线具有显著的性能优势。

Conclusion: 可移动天线和可重构天线为6G通信系统提供了重要的性能增强潜力，是未来无线通信发展的关键技术方向。

Abstract: The growing demands of 6G mobile communication networks necessitate advanced
antenna technologies. Movable antennas (MAs) and reconfigurable antennas (RAs)
enable dynamic control over antenna's position, orientation, radiation,
polarization, and frequency response, introducing rich electromagnetic-domain
degrees of freedom for the design and performance enhancement of wireless
systems. This article overviews their application scenarios, hardware
architectures, and design methods. Field test and simulation results highlight
their performance benefits over conventional fixed/non-reconfigurable antennas.

</details>


### [16] [Non-Linear Precoding via Dirty Paper Coding for Near-Field Downlink MISO Communications](https://arxiv.org/abs/2510.13485)
*Akash Kulkarni,Rajshekhar V Bhat*

Main category: cs.IT

TL;DR: 提出基于Dirty Paper Coding的非线性预编码框架，用于6G近场通信系统，相比传统线性预编码（如ZF）能显著提升和速率性能


<details>
  <summary>Details</summary>
Motivation: 6G系统中大规模天线阵列使近场通信成为可能，但现有线性预编码技术因高发射功率需求导致性能下降，需要更有效的干扰管理方法

Method: 采用Dirty Paper Coding非线性预编码框架，预消除已知干扰，并推导了DPC和ZF方案的最优功率分配策略

Result: 仿真表明DPC在各种近场配置下相比ZF实现了显著的和速率增益，特别是对紧密排列用户改进最为明显

Conclusion: DPC非线性预编码是6G近场通信系统中提升多用户性能的有效方法，特别适用于用户间距较近的场景

Abstract: In 6G systems, extremely large-scale antenna arrays operating at terahertz
frequencies extend the near-field region to typical user distances from the
base station, enabling near-field communication (NFC) with fine spatial
resolution through beamfocusing. Existing multiuser NFC systems predominantly
employ linear precoding techniques such as zero-forcing (ZF), which suffer from
performance degradation due to the high transmit power required to suppress
interference. This paper proposes a nonlinear precoding framework based on
Dirty Paper Coding (DPC), which pre-cancels known interference to maximize the
sum-rate performance. We formulate and solve the corresponding sum-rate
maximization problems, deriving optimal power allocation strategies for both
DPC and ZF schemes. Extensive simulations demonstrate that DPC achieves
substantial sum-rate gains over ZF across various near-field configurations,
with the most pronounced improvements observed for closely spaced users.

</details>


### [17] [Simulating Mediumband Wireless Communication Systems: A Concise Description](https://arxiv.org/abs/2510.13532)
*Dushyantha A Basnayaka*

Main category: cs.IT

TL;DR: 本文详细描述了在MATLAB中准确模拟中频数字无线通信系统的必要步骤，特别关注物理层操作的细节模拟，包括脉冲整形、上变频、混频、载波同步和符号定时同步等关键子系统。


<details>
  <summary>Details</summary>
Motivation: 现有文献通常假设在离散时间复基带域模拟数字无线通信系统，忽略了脉冲整形、上变频等物理层操作细节。这些假设在大多数情况下足够，但要准确捕捉中频通信的本质，需要对这些PHY操作进行详细模拟。

Method: 在MATLAB中从单个发射器到单个接收器模拟中频无线通信场景，详细阐述关键PHY子系统的操作，确保模拟系统能够捕捉中频无线通信的微妙动态。

Result: 所描述的方法能够准确模拟中频无线通信系统，包括深度衰落避免等效应，为初学者和专家提供了准确的模拟框架。

Conclusion: 通过详细模拟物理层操作，可以更准确地捕捉中频无线通信系统的动态特性，这对于理解和设计实际通信系统具有重要意义。

Abstract: In this paper, we describe the necessary procedures for accurately simulating
digital wireless communication systems operating in the mediumband, aimed at
both beginners and experts. In the research literature, digital wireless
communication systems are typically simulated in the discrete-time complex
baseband domain, where pulse shaping, upconversion, mixing, carrier
synchronization, and symbol timing synchronization are often ignored. These
assumptions are indeed sufficient in most cases, but to capture the essence of
communication in the mediumband, certain physical layer (PHY) operations should
be simulated in detail. In this paper, we concisely describe how to simulate a
mediumband wireless communication scenario from a single transmitter (TX) to a
single receiver (RX) in MATLAB, elaborating the operation of key PHY
subsystems. The approach described here ensures that the simulated system
captures the delicate dynamics of mediumband wireless communication, including
the effect of deep fading avoidance.

</details>


### [18] [Local Information-Theoretic Security via Euclidean Geometry](https://arxiv.org/abs/2510.13661)
*Emmanouil M. Athanasakos,Nicholas Kalouptsidis,Hariprasad Manjunath*

Main category: cs.IT

TL;DR: 本文提出了一种基于欧几里得信息论的方法，用于研究离散无记忆窃听信道上安全通信的局部特性。通过将非凸优化问题转化为可处理的二次规划结构，推导出近似局部保密容量的解析公式，并定义了新的秘密局部收缩系数来量化信道的固有局部泄漏效率。


<details>
  <summary>Details</summary>
Motivation: 研究离散无记忆窃听信道上安全通信的局部特性，旨在在限制信息泄漏给窃听者和编码秘密消息的信息成本的同时，最大化合法用户的信息速率。

Method: 基于欧几里得信息论，将非凸优化问题转化为可处理的二次规划结构，通过求解线性程序找到最优拉格朗日乘子，约束条件来自KKT条件并以信道矩阵的广义特征值表示。

Result: 推导出近似局部保密容量的解析公式，定义了新的秘密局部收缩系数，这些系数作为矩阵铅笔的最大广义特征值，量化了近似效用与近似泄漏的最大可实现比率。

Conclusion: 所提出的框架通过详细的数值分析证明了其有效性，建立了局部系数与全局对应物之间的界限，为一般多模式信道和典型二进制对称窃听信道提供了分析工具。

Abstract: This paper introduces a methodology based on Euclidean information theory to
investigate local properties of secure communication over discrete memoryless
wiretap channels. We formulate a constrained optimization problem that
maximizes a legitimate user's information rate while imposing explicit upper
bounds on both the information leakage to an eavesdropper and the informational
cost of encoding the secret message. By leveraging local geometric
approximations, this inherently non-convex problem is transformed into a
tractable quadratic programming structure. It is demonstrated that the optimal
Lagrange multipliers governing this approximated problem can be found by
solving a linear program. The constraints of this linear program are derived
from Karush-Kuhn-Tucker conditions and are expressed in terms of the
generalized eigenvalues of channel-derived matrices. This framework facilitates
the derivation of an analytical formula for an approximate local secrecy
capacity. Furthermore, we define and analyze a new class of secret local
contraction coefficients. These coefficients, characterized as the largest
generalized eigenvalues of a matrix pencil, quantify the maximum achievable
ratio of approximate utility to approximate leakage, thus measuring the
intrinsic local leakage efficiency of the channel. We establish bounds
connecting these local coefficients to their global counterparts defined over
true mutual information measures. The efficacy of the proposed framework is
demonstrated through detailed analysis and numerical illustrations for both
general multi-mode channels and the canonical binary symmetric wiretap channel.

</details>


### [19] [Combinatorial Bounds for List Recovery via Discrete Brascamp--Lieb Inequalities](https://arxiv.org/abs/2510.13775)
*Joshua Brakensiek,Yeyuan Chen,Manik Dhar,Zihan Zhang*

Main category: cs.IT

TL;DR: 本文提出了列表恢复问题的新组合界，证明当接近容量时，列表大小L最多为(ℓ/(R+ε))^{O(R/ε)}，解决了L是否可以用ℓ的多项式界定的长期开放问题。


<details>
  <summary>Details</summary>
Motivation: 解决列表恢复中最大列表大小L的边界问题，特别是当ρ接近容量时L是否可以用ℓ的多项式界定的长期开放问题。

Method: 应用离散熵Brascamp-Lieb不等式到列表恢复问题，将每个坐标的局部结构与恢复列表的全局结构联系起来。

Result: 对于各种线性码和折叠线性码，当ρ=1-R-ε接近容量时，列表大小L最多为(ℓ/(R+ε))^{O(R/ε)}，在零错误情况下与已知下界完美匹配。

Conclusion: 本文为列表恢复问题提供了多项式边界，解决了长期开放问题，并将Chen和Zhang的结果推广为新的Brascamp-Lieb型不等式。

Abstract: In coding theory, the problem of list recovery asks one to find all codewords
$c$ of a given code $C$ which such that at least $1-\rho$ fraction of the
symbols of $c$ lie in some predetermined set of $\ell$ symbols for each
coordinate of the code. A key question is bounding the maximum possible list
size $L$ of such codewords for the given code $C$.
  In this paper, we give novel combinatorial bounds on the list recoverability
of various families of linear and folded linear codes, including random linear
codes, random Reed--Solomon codes, explicit folded Reed--Solomon codes, and
explicit univariate multiplicity codes. Our main result is that in all of these
settings, we show that for code of rate $R$, when $\rho = 1 - R - \epsilon$
approaches capacity, the list size $L$ is at most
$(\ell/(R+\epsilon))^{O(R/\epsilon)}$. These results also apply in the
average-radius regime. Our result resolves a long-standing open question on
whether $L$ can be bounded by a polynomial in $\ell$. In the zero-error regime,
our bound on $L$ perfectly matches known lower bounds.
  The primary technique is a novel application of a discrete entropic
Brascamp--Lieb inequality to the problem of list recovery, allowing us to
relate the local structure of each coordinate with the global structure of the
recovered list. As a result of independent interest, we show that a recent
result by Chen and Zhang (STOC 2025) on the list decodability of folded
Reed--Solomon codes can be generalized into a novel Brascamp--Lieb type
inequality.

</details>


### [20] [From Random to Explicit via Subspace Designs With Applications to Local Properties and Matroids](https://arxiv.org/abs/2510.13777)
*Joshua Brakensiek,Yeyuan Chen,Manik Dhar,Zihan Zhang*

Main category: cs.IT

TL;DR: 本文扩展了Levi等人的框架，研究子空间可设计码的局部性质，证明了随机线性码与最优子空间设计码的局部等价性，并应用于拟阵理论中的可纠正擦除模式识别问题。


<details>
  <summary>Details</summary>
Motivation: 扩展局部性质阈值率计算框架到子空间可设计码，包括显式折叠Reed-Solomon码和单变量重数码，建立随机线性码与子空间设计码的局部等价关系。

Method: 扩展Levi等人的统一框架，研究子空间可设计码的局部性质，通过局部等价性证明和拟阵理论应用。

Result: 证明了随机线性码与最优子空间设计码的局部等价性；给出了首个同时具备随机线性码所有局部性质的显式折叠线性码构造；在拟阵理论中改进了可纠正擦除模式识别的复杂度结果。

Conclusion: 建立了随机线性码与子空间设计码的局部等价性框架，为编码理论和拟阵理论提供了新的联系和应用。

Abstract: In coding theory, a common question is to understand the threshold rates of
various local properties of codes, such as their list decodability and list
recoverability. A recent work Levi, Mosheiff, and Shagrithaya (FOCS 2025) gave
a novel unified framework for calculating the threshold rates of local
properties for random linear and random Reed--Solomon codes.
  In this paper, we extend their framework to studying the local properties of
subspace designable codes, including explicit folded Reed-Solomon and
univariate multiplicity codes. Our first main result is a local equivalence
between random linear codes and (nearly) optimal subspace design codes up to an
arbitrarily small rate decrease. We show any local property of random linear
codes applies to all subspace design codes. As such, we give the first explicit
construction of folded linear codes that simultaneously attain all local
properties of random linear codes. Conversely, we show that any local property
which applies to all subspace design codes also applies to random linear codes.
  Our second main result is an application to matroid theory. We show that the
correctable erasure patterns in a maximally recoverable tensor code can be
identified in deterministic polynomial time, assuming a positive answer to a
matroid-theoretic question due to Mason (1981). This improves on a result of
Jackson and Tanigawa (JCTB 2024) who gave a complexity characterization of
$\mathsf{RP} \cap \mathsf{coNP}$ assuming a stronger conjecture. Our result
also applies to the generic bipartite rigidity and matrix completion matroids.
  As a result of additional interest, we study the existence and limitations of
subspace designs. In particular, we tighten the analysis of family of subspace
designs constructioned by Guruswami and Kopparty (Combinatorica 2016) and show
that better subspace designs do not exist over algebraically closed fields.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [21] [Energy-Guided Diffusion Sampling for Long-Term User Behavior Prediction in Reinforcement Learning-based Recommendation](https://arxiv.org/abs/2510.12815)
*Xiaocong Chen,Siyu Wang,Lina Yao*

Main category: cs.IR

TL;DR: 提出DAC4Rec框架，将扩散过程与强化学习结合，解决离线推荐系统中数据效率低和长期用户偏好建模困难的问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习推荐系统面临数据效率低、依赖预收集轨迹、处理噪声数据困难以及难以捕捉长期用户偏好的挑战。

Method: 集成扩散过程与强化学习，利用扩散模型的去噪能力增强鲁棒性，采用Q值引导的策略优化处理次优轨迹，并引入基于能量的采样策略减少推荐生成随机性。

Result: 在六个真实世界离线数据集和在线模拟环境中验证了有效性，能够优化长期用户偏好，且可无缝集成到其他RL算法中。

Conclusion: DAC4Rec框架通过扩散增强的actor-critic方法有效解决了离线RL推荐系统的关键挑战，具有广泛适用性。

Abstract: Reinforcement learning-based recommender systems (RL4RS) have gained
attention for their ability to adapt to dynamic user preferences. However,
these systems face challenges, particularly in offline settings, where data
inefficiency and reliance on pre-collected trajectories limit their broader
applicability. While offline reinforcement learning methods leverage extensive
datasets to address these issues, they often struggle with noisy data and fail
to capture long-term user preferences, resulting in suboptimal recommendation
policies. To overcome these limitations, we propose Diffusion-enhanced
Actor-Critic for Offline RL4RS (DAC4Rec), a novel framework that integrates
diffusion processes with reinforcement learning to model complex user
preferences more effectively. DAC4Rec leverages the denoising capabilities of
diffusion models to enhance the robustness of offline RL algorithms and
incorporates a Q-value-guided policy optimization strategy to better handle
suboptimal trajectories. Additionally, we introduce an energy-based sampling
strategy to reduce randomness during recommendation generation, ensuring more
targeted and reliable outcomes. We validate the effectiveness of DAC4Rec
through extensive experiments on six real-world offline datasets and in an
online simulation environment, demonstrating its ability to optimize long-term
user preferences. Furthermore, we show that the proposed diffusion policy can
be seamlessly integrated into other commonly used RL algorithms in RL4RS,
highlighting its versatility and wide applicability.

</details>


### [22] [Maximum In-Support Return Modeling for Dynamic Recommendation with Language Model Prior](https://arxiv.org/abs/2510.12816)
*Xiaocong Chen,Siyu Wang,Lina Yao*

Main category: cs.IR

TL;DR: MDT4Rec是一个基于决策变换器的离线强化学习推荐系统框架，通过将轨迹拼接从训练阶段转移到动作推断，并使用预训练LLM和LoRA微调来解决次优历史和复杂用户-物品交互问题。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中RLRS面临的挑战：用户反馈数据可能次优或稀疏，难以从次优历史中学习，以及表示复杂用户-物品交互的困难。

Method: 1) 将轨迹拼接从训练阶段转移到动作推断，允许系统在必要时缩短历史上下文；2) 使用预训练LLM初始化决策变换器进行知识迁移；3) 用MLP替换线性嵌入层以获得更灵活表示；4) 采用LoRA高效微调少量参数。

Result: 在五个公共数据集和在线仿真环境中评估，MDT4Rec优于现有方法。

Conclusion: MDT4Rec通过创新的轨迹拼接策略和高效的LLM微调方法，有效解决了RLRS在现实环境中的关键挑战，展现出优越性能。

Abstract: Reinforcement Learning-based recommender systems (RLRS) offer an effective
way to handle sequential recommendation tasks but often face difficulties in
real-world settings, where user feedback data can be sub-optimal or sparse. In
this paper, we introduce MDT4Rec, an offline RLRS framework that builds on the
Decision Transformer (DT) to address two major challenges: learning from
sub-optimal histories and representing complex user-item interactions. First,
MDT4Rec shifts the trajectory stitching procedure from the training phase to
action inference, allowing the system to shorten its historical context when
necessary and thereby ignore negative or unsuccessful past experiences. Second,
MDT4Rec initializes DT with a pre-trained large language model (LLM) for
knowledge transfer, replaces linear embedding layers with Multi-Layer
Perceptrons (MLPs) for more flexible representations, and employs Low-Rank
Adaptation (LoRA) to efficiently fine-tune only a small subset of parameters.
We evaluate MDT4Rec on five public datasets and in an online simulation
environment, demonstrating that it outperforms existing methods.

</details>


### [23] [Post-hoc Popularity Bias Correction in GNN-based Collaborative Filtering](https://arxiv.org/abs/2510.12959)
*Md Aminul Islam,Elena Zheleva,Ren Wang*

Main category: cs.IR

TL;DR: 提出一种后处理流行度去偏方法(PPD)，直接在预训练嵌入上修正GNN推荐系统中的流行度偏差，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: GNN推荐系统在训练过程中会传播和放大流行度偏差，现有方法通常修改训练目标但无法直接对抗GNN聚合过程中的偏差传播。

Method: 通过估计交互级别的流行度，使用流行度方向向量从节点表示中移除流行度成分，同时保留用户偏好。

Result: 实验结果表明该方法在GNN推荐系统的流行度偏差修正方面优于现有最先进方法。

Conclusion: PPD方法能有效减少GNN推荐系统中的流行度偏差，同时保持个性化推荐质量。

Abstract: User historical interaction data is the primary signal for learning user
preferences in collaborative filtering (CF). However, the training data often
exhibits a long-tailed distribution, where only a few items have the majority
of interactions. CF models trained directly on such imbalanced data are prone
to learning popularity bias, which reduces personalization and leads to
suboptimal recommendation quality. Graph Neural Networks (GNNs), while
effective for CF due to their message passing mechanism, can further propagate
and amplify popularity bias through their aggregation process. Existing
approaches typically address popularity bias by modifying training objectives
but fail to directly counteract the bias propagated during GNN's neighborhood
aggregation. Applying weights to interactions during aggregation can help
alleviate this problem, yet it risks distorting model learning due to unstable
node representations in the early stages of training. In this paper, we propose
a Post-hoc Popularity Debiasing (PPD) method that corrects for popularity bias
in GNN-based CF and operates directly on pre-trained embeddings without
requiring retraining. By estimating interaction-level popularity and removing
popularity components from node representations via a popularity direction
vector, PPD reduces bias while preserving user preferences. Experimental
results show that our method outperforms state-of-the-art approaches for
popularity bias correction in GNN-based CF.

</details>


### [24] [Retrieval-in-the-Chain: Bootstrapping Large Language Models for Generative Retrieval](https://arxiv.org/abs/2510.13095)
*Yingchen zhang,Ruqing zhang,Jiafeng Guo,Wenjun Peng,Sen Li,Fuyu Lv*

Main category: cs.IR

TL;DR: 提出了R4R框架，通过结构化推理增强生成式检索，将自由形式的思维链转换为紧凑的结构化格式，并在检索过程中迭代优化推理。


<details>
  <summary>Details</summary>
Motivation: 现有生成式检索方法主要关注LLMs的生成能力，而忽视了其推理能力可能带来的益处。初步研究发现自由形式思维链推理虽然能提升性能，但存在冗长且与文档标识空间对齐不佳的问题。

Method: R4R框架使用单个推理能力强的LLM，首先生成结构化推理，然后在推理生成和约束解码生成文档标识之间交替迭代，基于检索结果更新推理以改进下一轮检索。

Result: 在Natural Questions、MS MARCO和真实世界物品搜索基准上的广泛实验验证了R4R的有效性。

Conclusion: R4R证明了显式推理能够显著提升生成式检索性能，且无需额外模型或训练，单个LLM即可同时担任推理生成器和检索器。

Abstract: Generative retrieval (GR) is an emerging paradigm that leverages large
language models (LLMs) to autoregressively generate document identifiers
(docids) relevant to a given query. Prior works have focused on leveraging the
generative capabilities of LLMs to improve GR, while overlooking that their
reasoning capabilities could likewise help. This raises a key question: Can
explicit reasoning benefit GR? To investigate, we first conduct a preliminary
study where an LLM is prompted to generate free-form chain-of-thought (CoT)
reasoning before performing constrained docid decoding. Although this method
outperforms standard GR, the generated reasoning tends to be verbose and poorly
aligned with the docid space. These limitations motivate the development of a
reasoning mechanism better tailored to GR.
  Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented
framework for GR that converts free-form CoT reasoning into a compact,
structured format, and iteratively refines the reasoning during the retrieval
process. R4R augments an existing GR method by leveraging a reasoning-capable
LLM that has been instruction-tuned for GR. At inference time, R4R first uses
the LLM to generate an initial structured reasoning; then the same LLM
alternates between (i) constrained decoding with the chosen GR method to
produce candidate docids and (ii) updating the reasoning based on retrieval
results to improve the next round. R4R does not require additional models or
training, and instead a single LLM serves as both the reasoning generator and
the retriever. Extensive experiments on Natural Questions, MS MARCO, and a
real-world item-search benchmark validate the effectiveness of R4R.

</details>


### [25] [ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient RAG](https://arxiv.org/abs/2510.13193)
*Yikuan Hu,Jifeng Zhu,Lanrui Tang,Chen Huang*

Main category: cs.IR

TL;DR: REMINDRAG是一个基于知识图谱的检索增强生成系统，通过LLM引导的图遍历方法，结合节点探索、节点利用和记忆回放机制，在提升系统效果的同时保持成本效率。


<details>
  <summary>Details</summary>
Motivation: 现有的KG-RAG系统在系统效果和成本效率之间难以达到有效协同，要么性能不理想，要么需要过多的LLM提示词和推理时间。

Method: 采用LLM引导的图遍历方法，包含节点探索、节点利用和记忆回放三个关键组件。通过将遍历经验存储在KG边嵌入中，以无训练的方式实现类似LLM参数化记忆世界知识的效果。

Result: 理论和实验验证了REMINDRAG的有效性，在多个基准数据集和LLM骨干网络上均优于现有基线方法。

Conclusion: REMINDRAG成功解决了KG-RAG系统中效果与成本效率的平衡问题，为知识图谱增强的检索生成系统提供了有效的解决方案。

Abstract: Knowledge graphs (KGs), with their structured representation capabilities,
offer promising avenue for enhancing Retrieval Augmented Generation (RAG)
systems, leading to the development of KG-RAG systems. Nevertheless, existing
methods often struggle to achieve effective synergy between system
effectiveness and cost efficiency, leading to neither unsatisfying performance
nor excessive LLM prompt tokens and inference time. To this end, this paper
proposes REMINDRAG, which employs an LLM-guided graph traversal featuring node
exploration, node exploitation, and, most notably, memory replay, to improve
both system effectiveness and cost efficiency. Specifically, REMINDRAG
memorizes traversal experience within KG edge embeddings, mirroring the way
LLMs "memorize" world knowledge within their parameters, but in a train-free
manner. We theoretically and experimentally confirm the effectiveness of
REMINDRAG, demonstrating its superiority over existing baselines across various
benchmark datasets and LLM backbones. Our code is available at
https://github.com/kilgrims/ReMindRAG.

</details>


### [26] [LLM-guided Hierarchical Retrieval](https://arxiv.org/abs/2510.13217)
*Nilesh Gupta,Wei-Cheng Chang,Ngot Bui,Cho-Jui Hsieh,Inderjit S. Dhillon*

Main category: cs.IR

TL;DR: LATTICE是一个分层检索框架，通过语义树结构组织语料库，使LLM能够以对数搜索复杂度在大规模语料中进行推理和导航。


<details>
  <summary>Details</summary>
Motivation: 解决传统检索-重排范式的局限性、参数化生成方法难以更新新信息、以及长上下文方法在大规模文档集合中计算不可行的问题。

Method: 包含离线阶段（通过自底向上聚合或自顶向下分割策略构建语义层次结构）和在线遍历阶段（搜索LLM导航树结构），并提出基于校准潜在相关性分数的遍历算法。

Result: 在BRIGHT基准测试中实现最先进的零样本性能，Recall@100提升9%，nDCG@10提升5%，与微调SOTA方法DIVER-v2在静态语料评估上获得相当结果。

Conclusion: LATTICE框架有效解决了复杂多面查询的深度推理需求，通过层次化检索实现了高效的大规模语料导航。

Abstract: Modern IR systems are increasingly tasked with answering complex,
multi-faceted queries that require deep reasoning rather than simple keyword or
semantic matching. While LLM-based IR has shown great promise, the prevailing
retrieve-then-rerank paradigm inherits the limitations of embedding-based
retrieval; parametric generative approaches are difficult to update with new
information; and long-context methods that place the entire corpus in context
are computationally infeasible for large document collections. To address these
challenges, we introduce LATTICE, a hierarchical retrieval framework that
enables an LLM to reason over and navigate large corpora with logarithmic
search complexity by imposing a semantic tree structure on the corpus. Our
approach consists of two stages: (1) an offline phase that organizes the corpus
into a semantic hierarchy via either a bottom-up agglomerative strategy or a
top-down divisive strategy using multi-level summaries and (2) an online
traversal phase where a search LLM navigates this tree. A central challenge in
such LLM-guided search is that the model's relevance judgments are noisy,
context-dependent, and unaware of the hierarchy, making cross-branch and
cross-level comparisons difficult. To overcome this, we propose a traversal
algorithm that estimates calibrated latent relevance scores from local LLM
outputs and aggregates them into a global path relevance metric. Our
training-free framework achieves state-of-the-art zero-shot performance on the
reasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in
Recall@100 and 5% in nDCG@10 over the next best zero-shot baseline.
Furthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains
comparable results on BRIGHT subsets that use a static corpus for evaluation.

</details>


### [27] [Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning for Recommendation](https://arxiv.org/abs/2510.13229)
*Yi Zhang,Lili Xie,Ruihong Qiu,Jiajun Liu,Sen Wang*

Main category: cs.IR

TL;DR: 提出一种基于离线强化学习的新框架，利用LLM生成的轨迹进行模仿学习，避免直接部署LLM带来的延迟和幻觉问题，实现高效且可解释的推荐系统。


<details>
  <summary>Details</summary>
Motivation: LLM在推荐系统中具有强大的泛化能力和上下文理解，但直接作为推荐策略存在延迟、幻觉和偏见等问题，需要更高效的部署方案。

Method: 使用逆强化学习从LLM演示中提取鲁棒奖励模型，通过离线强化学习策略学习累积奖励，无需微调LLM，降低计算开销。

Result: 在两个基准数据集上的实验验证了方法的有效性，相比最先进的基于RL和上下文学习的基线方法表现出更优性能。

Conclusion: 该框架成功将LLM的语义洞察转移到高效的RL策略中，为LLM在推荐系统中的实际部署提供了可行解决方案。

Abstract: Recommender systems (RecSys) have become critical tools for enhancing user
engagement by delivering personalized content across diverse digital platforms.
Recent advancements in large language models (LLMs) demonstrate significant
potential for improving RecSys, primarily due to their exceptional
generalization capabilities and sophisticated contextual understanding, which
facilitate the generation of flexible and interpretable recommendations.
However, the direct deployment of LLMs as primary recommendation policies
presents notable challenges, including persistent latency issues stemming from
frequent API calls and inherent model limitations such as hallucinations and
biases. To address these issues, this paper proposes a novel offline
reinforcement learning (RL) framework that leverages imitation learning from
LLM-generated trajectories. Specifically, inverse reinforcement learning is
employed to extract robust reward models from LLM demonstrations. This approach
negates the need for LLM fine-tuning, thereby substantially reducing
computational overhead. Simultaneously, the RL policy is guided by the
cumulative rewards derived from these demonstrations, effectively transferring
the semantic insights captured by the LLM. Comprehensive experiments conducted
on two benchmark datasets validate the effectiveness of the proposed method,
demonstrating superior performance when compared against state-of-the-art
RL-based and in-context learning baselines. The code can be found at
https://github.com/ArronDZhang/IL-Rec.

</details>


### [28] [Improving Visual Recommendation on E-commerce Platforms Using Vision-Language Models](https://arxiv.org/abs/2510.13359)
*Yuki Yada,Sho Akiyama,Ryo Watanabe,Yuta Ueno,Yusuke Shido,Andre Rusli*

Main category: cs.IR

TL;DR: 在Mercari电商平台上应用视觉语言模型SigLIP进行产品推荐，通过微调100万产品图像-标题对，在离线评估中nDCG@5提升9.1%，在线A/B测试中点击率提升50%，转化率提升14%。


<details>
  <summary>Details</summary>
Motivation: 在大规模电商平台上，推荐视觉相似产品对帮助用户高效发现符合偏好的商品至关重要，特别是在拥有超过2000万月活用户的Mercari平台上。

Method: 微调基于sigmoid对比损失的视觉语言模型SigLIP，使用三个月内收集的100万产品图像-标题对，开发用于推荐系统的图像编码器生成项目嵌入。

Result: 离线分析显示nDCG@5提升9.1%；在线A/B测试中点击率提升50%，转化率提升14%。

Conclusion: 基于视觉语言模型的编码器在电商产品推荐中具有显著效果，为开发基于视觉相似性的推荐系统提供了实用见解。

Abstract: On large-scale e-commerce platforms with tens of millions of active monthly
users, recommending visually similar products is essential for enabling users
to efficiently discover items that align with their preferences. This study
presents the application of a vision-language model (VLM) -- which has
demonstrated strong performance in image recognition and image-text retrieval
tasks -- to product recommendations on Mercari, a major consumer-to-consumer
marketplace used by more than 20 million monthly users in Japan. Specifically,
we fine-tuned SigLIP, a VLM employing a sigmoid-based contrastive loss, using
one million product image-title pairs from Mercari collected over a three-month
period, and developed an image encoder for generating item embeddings used in
the recommendation system. Our evaluation comprised an offline analysis of
historical interaction logs and an online A/B test in a production environment.
In offline analysis, the model achieved a 9.1% improvement in nDCG@5 compared
with the baseline. In the online A/B test, the click-through rate improved by
50% whereas the conversion rate improved by 14% compared with the existing
model. These results demonstrate the effectiveness of VLM-based encoders for
e-commerce product recommendations and provide practical insights into the
development of visual similarity-based recommendation systems.

</details>


### [29] [MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive Recommendation](https://arxiv.org/abs/2510.13371)
*Jiin Park,Misuk Kim*

Main category: cs.IR

TL;DR: MADRec是一个基于LLM的多方面驱动推荐系统，通过无监督提取评论中的多方面信息构建用户和物品画像，支持直接推荐、序列推荐和解释生成，在多个领域优于传统和LLM基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐系统大多局限于简单文本生成或静态提示推理，无法捕捉用户偏好和真实交互的复杂性，需要更智能的推荐方法。

Method: 通过基于方面类别的总结生成结构化画像，应用重排序构建高密度输入，当输出中缺少真实物品时使用自反馈机制动态调整推理标准。

Result: 在多个领域的实验中，MADRec在准确性和可解释性方面均优于传统和LLM基线方法，人工评估进一步证实了生成解释的说服力。

Conclusion: MADRec展示了LLM代理在构建复杂推荐系统方面的潜力，能够有效处理用户偏好和交互复杂性，提供准确且可解释的推荐。

Abstract: Recent attempts to integrate large language models (LLMs) into recommender
systems have gained momentum, but most remain limited to simple text generation
or static prompt-based inference, failing to capture the complexity of user
preferences and real-world interactions. This study proposes the Multi-Aspect
Driven LLM Agent MADRec, an autonomous LLM-based recommender that constructs
user and item profiles by unsupervised extraction of multi-aspect information
from reviews and performs direct recommendation, sequential recommendation, and
explanation generation. MADRec generates structured profiles via
aspect-category-based summarization and applies Re-Ranking to construct
high-density inputs. When the ground-truth item is missing from the output, the
Self-Feedback mechanism dynamically adjusts the inference criteria. Experiments
across multiple domains show that MADRec outperforms traditional and LLM-based
baselines in both precision and explainability, with human evaluation further
confirming the persuasiveness of the generated explanations.

</details>


### [30] [RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for Evolving Knowledge](https://arxiv.org/abs/2510.13590)
*Jiale Han,Austin Cheung,Yubai Wei,Zheng Yu,Xusheng Wang,Bing Zhu,Yi Yang*

Main category: cs.IR

TL;DR: 提出了Temporal GraphRAG（TG-RAG）方法，通过构建双层时序图来使RAG系统具备时间感知能力，解决了现有RAG方法在处理时序知识时的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统主要忽略知识的时序特性，存在两个关键挑战：缺乏有效的时间感知表示方法，以及评估方法假设静态语料库而忽视了知识演化带来的更新成本和检索稳定性问题。

Method: 构建双层时序图：包含带时间戳关系的时序知识图和层次化时间图；为每个时间节点生成多粒度时序摘要；支持增量更新，从新语料中提取时序事实并合并到现有图中；在推理时动态检索查询时间和语义范围内的子图。

Result: 大量实验表明TG-RAG显著优于现有基线方法，证明了该方法在处理时序知识和增量更新方面的有效性。

Conclusion: TG-RAG通过时序图表示有效解决了RAG系统中的时间感知问题，提出的ECT-QA数据集和评估协议为时间敏感问答提供了全面评估框架。

Abstract: Knowledge is inherently time-sensitive and continuously evolves over time.
Although current Retrieval-Augmented Generation (RAG) systems enrich LLMs with
external knowledge, they largely ignore this temporal nature. This raises two
challenges for RAG. First, current RAG methods lack effective time-aware
representations. Same facts of different time are difficult to distinguish with
vector embeddings or conventional knowledge graphs. Second, most RAG
evaluations assume a static corpus, leaving a blind spot regarding update costs
and retrieval stability as knowledge evolves. To make RAG time-aware, we
propose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level
temporal graph consisting of a temporal knowledge graph with timestamped
relations and a hierarchical time graph. Multi-granularity temporal summaries
are generated for each time node to capture both key events and broader trends
at that time. The design supports incremental updates by extracting new
temporal facts from the incoming corpus and merging them into the existing
graph. The temporal graph explicitly represents identical facts at different
times as distinct edges to avoid ambiguity, and the time hierarchy graph allows
only generating reports for new leaf time nodes and their ancestors, ensuring
effective and efficient updates. During inference, TG-RAG dynamically retrieves
a subgraph within the temporal and semantic scope of the query, enabling
precise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive
question-answering dataset featuring both specific and abstract queries, along
with a comprehensive evaluation protocol designed to assess incremental update
capabilities of RAG systems. Extensive experiments show that TG-RAG
significantly outperforms existing baselines, demonstrating the effectiveness
of our method in handling temporal knowledge and incremental updates.

</details>


### [31] [HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based Sequential Recommendation](https://arxiv.org/abs/2510.13738)
*Jingyi Zhou,Cheng Chen,Kai Zuo,Manjie Xu,Zhendong Fu,Yibo Chen,Xu Tang,Yao Hu*

Main category: cs.IR

TL;DR: 提出HyMiRec混合多兴趣序列推荐框架，解决LLM在推荐系统中长序列建模和兴趣多样性不足的问题


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐方法受限于推理延迟和特征获取带宽，只能截断用户行为序列，丢失长期偏好信号；且依赖单预测嵌入，忽略了用户兴趣的多面性，限制了推荐多样性

Method: 使用轻量级推荐器从长序列提取粗粒度兴趣嵌入，LLM推荐器捕获细粒度兴趣嵌入；引入基于余弦相似度的残差码本压缩用户历史嵌入；设计解耦多兴趣学习模块自适应学习多个兴趣信号

Result: 在基准数据集和工业数据集上的实验显示优于现有SOTA方法，在线A/B测试证明在真实推荐系统中带来持续改进

Conclusion: HyMiRec通过混合架构有效解决了LLM推荐中的长序列建模和兴趣多样性问题，在真实场景中表现优异

Abstract: Large language models (LLMs) have recently demonstrated strong potential for
sequential recommendation. However, current LLM-based approaches face critical
limitations in modeling users' long-term and diverse interests. First, due to
inference latency and feature fetching bandwidth constraints, existing methods
typically truncate user behavior sequences to include only the most recent
interactions, resulting in the loss of valuable long-range preference signals.
Second, most current methods rely on next-item prediction with a single
predicted embedding, overlooking the multifaceted nature of user interests and
limiting recommendation diversity. To address these challenges, we propose
HyMiRec, a hybrid multi-interest sequential recommendation framework, which
leverages a lightweight recommender to extracts coarse interest embeddings from
long user sequences and an LLM-based recommender to captures refined interest
embeddings. To alleviate the overhead of fetching features, we introduce a
residual codebook based on cosine similarity, enabling efficient compression
and reuse of user history embeddings. To model the diverse preferences of
users, we design a disentangled multi-interest learning module, which leverages
multiple interest queries to learn disentangles multiple interest signals
adaptively, allowing the model to capture different facets of user intent.
Extensive experiments are conducted on both benchmark datasets and a collected
industrial dataset, demonstrating our effectiveness over existing
state-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec
brings consistent improvements in real-world recommendation systems.

</details>
