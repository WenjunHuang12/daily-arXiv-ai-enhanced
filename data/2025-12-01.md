<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 9]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.MM](#cs.MM) [Total: 8]
- [cs.IT](#cs.IT) [Total: 14]
- [cs.DS](#cs.DS) [Total: 14]
- [cs.GT](#cs.GT) [Total: 13]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Selecting User Histories to Generate LLM Users for Cold-Start Item Recommendation](https://arxiv.org/abs/2511.21989)
*Nachiket Subbaraman,Jaskinder Sarai,Aniruddh Nath,Lichan Hong,Lukasz Heldt,Li Wei,Zhe Zhao*

Main category: cs.IR

TL;DR: 提出基于强化学习的用户选择框架，利用LLM模拟用户行为，为冷启动物品生成增强数据，显著提升推荐性能


<details>
  <summary>Details</summary>
Motivation: 现有方法使用LLM为冷启动物品生成增强数据时存在两个问题：1) 仅使用部分用户历史，无法让LLM完全模拟用户行为；2) 随机选择用户进行增强不是最优策略。需要更智能的用户选择方法来优化冷启动物品性能

Method: 开发强化学习框架，训练策略模型基于用户行为特征和历史记录选择用户进行数据增强。使用LLM作为用户模拟器，采用策略梯度方法更新策略，选择能带来高奖励（冷启动物品性能提升）的用户

Result: 在Amazon产品评论数据集上的实验显示，该方法在冷启动物品召回率方面取得显著提升，证明了其作为可扩展、服务效率高的增强策略的有效性

Conclusion: 提出的强化学习框架能够智能选择用户进行数据增强，有效解决冷启动物品问题，为现代推荐系统提供了一种高效的数据增强策略

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in reasoning, generalization, and simulating human-like behavior across a wide range of tasks. These strengths present new opportunities to enhance traditional recommendation systems (RS), especially in the cold-start item scenario where newly introduced items lack interactions. Existing works have used LLMs to address cold-start issues in traditional RS through data augmentation, but they have limitations. One recent work directly addresses this issue by prompting LLMs to generate augmented interaction data between randomly sampled users and cold-start items. Then, they train the traditional RS with augmented data, incorporating collaborative signals for cold-start items. Although they use LLMs to provide cold-start items with feedback, they use partial user histories, which does not allow the LLM to fully emulate the user. Furthermore, randomly selecting users is not optimal for augmentation. To address these challenges, we leverage the LLM as a user and develop a reinforcement learning (RL) framework that trains a policy to select users for augmentation, optimizing for cold-start item performance after augmented training. The policy model learns to select users for cold-start item data augmentation based on their behavioral features and histories. To optimize user selection for cold-start item performance, we employ a policy gradient method that updates the policy in the direction of actions that lead to high rewards. Experiments on Amazon Product Review datasets show substantial gains in cold-start item recall, demonstrating the effectiveness of our method as a scalable, serving-efficient augmentation strategy for modern RS.

</details>


### [2] [Evaluating Embedding Models and Pipeline Optimization for AI Search Quality](https://arxiv.org/abs/2511.22240)
*Philip Zhong,Kent Chen,Don Wang*

Main category: cs.IR

TL;DR: 评估文本嵌入模型和检索管道配置对AI搜索系统性能的影响，发现高维嵌入、神经重排序和细粒度分块能显著提升检索准确率。


<details>
  <summary>Details</summary>
Motivation: 研究不同文本嵌入模型和检索管道配置对AI搜索系统性能的影响，为构建高效的文档检索系统提供实证指导。

Method: 使用本地LLM从美国市议会会议记录合成11,975个查询-文本块对作为评估数据集，比较不同嵌入模型（All-MPNet、BGE、GTE、Qwen）、维度、索引方法（Milvus HNSW/IVF）和分块策略，采用Top-K准确率和NDCG作为评估指标。

Result: 高维嵌入显著提升搜索质量（Qwen3-Embedding-8B/4096的Top-3准确率达0.571，GTE-large/1024为0.412）；神经重排序器（如BGE交叉编码器）进一步提升排序准确率（Top-3达0.527）；细粒度分块（512字符vs2000字符）也提高准确率。

Conclusion: 嵌入维度、重排序和分块粒度是影响检索系统性能的关键因素，未来研究方向包括管道自动化和评估方法的改进。

Abstract: We evaluate the performance of various text embedding models and pipeline configurations for AI-driven search systems. We compare sentence-transformer and generative embedding models (e.g., All-MPNet, BGE, GTE, and Qwen) at different dimensions, indexing methods (Milvus HNSW/IVF), and chunking strategies. A custom evaluation dataset of 11,975 query-chunk pairs was synthesized from US City Council meeting transcripts using a local large language model (LLM). The data pipeline includes preprocessing, automated question generation per chunk, manual validation, and continuous integration/continuous deployment (CI/CD) integration. We measure retrieval accuracy using reference-based metrics: Top-K Accuracy and Normalized Discounted Cumulative Gain (NDCG). Our results demonstrate that higher-dimensional embeddings significantly boost search quality (e.g., Qwen3-Embedding-8B/4096 achieves Top-3 accuracy about 0.571 versus 0.412 for GTE-large/1024), and that neural re-rankers (e.g., a BGE cross-encoder) further improve ranking accuracy (Top-3 up to 0.527). Finer-grained chunking (512 characters versus 2000 characters) also improves accuracy. We discuss the impact of these factors and outline future directions for pipeline automation and evaluation.

</details>


### [3] [FIGROTD: A Friendly-to-Handle Dataset for Image Guided Retrieval with Optional Text](https://arxiv.org/abs/2511.22247)
*Hoang-Bao Le,Allie Tran,Binh T. Nguyen,Liting Zhou,Cathal Gurrin*

Main category: cs.IR

TL;DR: IGROT统一视觉检索和组合检索，但缺乏可访问基准。作者提出轻量级数据集FIGROTD和方差引导特征掩码方法VaGFeM，在多个基准上取得竞争性结果。


<details>
  <summary>Details</summary>
Motivation: IGROT（图像引导检索与可选文本）统一了视觉检索和组合检索，在Google Image、Bing等应用中很重要，但进展受限于缺乏可访问的基准和平衡各子任务性能的方法。现有大规模数据集计算成本高，现有模型往往偏向视觉或组合查询。

Method: 1) 引入FIGROTD数据集：轻量级高质量的IGROT数据集，包含16,474个训练三元组和1,262个测试三元组，涵盖CIR、SBIR和CSTBIR任务。2) 提出方差引导特征掩码(VaGFeM)：基于方差统计选择性增强判别性维度以减少冗余。3) 采用双损失设计：结合InfoNCE和三元组损失来改进组合推理。

Result: 在FIGROTD上训练的VaGFeM在九个基准测试中取得竞争性结果：在CIRCO上达到34.8 mAP@10，在Sketchy上达到75.7 mAP@200，尽管使用更少的三元组，仍优于更强的基线方法。

Conclusion: FIGROTD数据集和VaGFeM方法有效解决了IGROT任务中的基准和方法平衡问题，在轻量级设置下实现了竞争性性能，为图像引导检索与可选文本任务提供了实用解决方案。

Abstract: Image-Guided Retrieval with Optional Text (IGROT) unifies visual retrieval (without text) and composed retrieval (with text). Despite its relevance in applications like Google Image and Bing, progress has been limited by the lack of an accessible benchmark and methods that balance performance across subtasks. Large-scale datasets such as MagicLens are comprehensive but computationally prohibitive, while existing models often favor either visual or compositional queries. We introduce FIGROTD, a lightweight yet high-quality IGROT dataset with 16,474 training triplets and 1,262 test triplets across CIR, SBIR, and CSTBIR. To reduce redundancy, we propose the Variance Guided Feature Mask (VaGFeM), which selectively enhances discriminative dimensions based on variance statistics. We further adopt a dual-loss design (InfoNCE + Triplet) to improve compositional reasoning. Trained on FIGROTD, VaGFeM achieves competitive results on nine benchmarks, reaching 34.8 mAP@10 on CIRCO and 75.7 mAP@200 on Sketchy, outperforming stronger baselines despite fewer triplets.

</details>


### [4] [UNION: A Lightweight Target Representation for Efficient Zero-Shot Image-Guided Retrieval with Optional Textual Queries](https://arxiv.org/abs/2511.22253)
*Hoang-Bao Le,Allie Tran,Binh T. Nguyen,Liting Zhou,Cathal Gurrin*

Main category: cs.IR

TL;DR: IGROT统一了CIR和SBIR任务，提出UNION方法通过融合图像嵌入和空文本提示，在低数据监督下实现图像引导检索的通用目标表示。


<details>
  <summary>Details</summary>
Motivation: 现有图像检索方法通常针对特定任务设计，缺乏统一的框架。IGROT（图像引导检索与可选文本）旨在统一组合图像检索（CIR）和基于草图的图像检索（SBIR）两大任务，解决低数据监督下的通用检索问题。

Method: 提出UNION方法：轻量级、可泛化的目标表示，将图像嵌入与空文本提示融合。无需修改预训练视觉语言模型架构，仅需少量训练样本（5000个），通过增强多模态查询的语义对齐来提升检索性能。

Result: 在仅5000个训练样本下，UNION在CIRCO基准上达到mAP@50 38.5，在Sketchy基准上达到mAP@200 82.7，超越了多个需要大量监督的基线方法，展现了在低数据条件下的竞争力和泛化能力。

Conclusion: UNION方法证明了在低数据监督下，通过融合图像嵌入和空文本提示可以构建通用的目标表示，有效桥接视觉和语言模态，为统一的图像检索框架提供了高效解决方案。

Abstract: Image-Guided Retrieval with Optional Text (IGROT) is a general retrieval setting where a query consists of an anchor image, with or without accompanying text, aiming to retrieve semantically relevant target images. This formulation unifies two major tasks: Composed Image Retrieval (CIR) and Sketch-Based Image Retrieval (SBIR). In this work, we address IGROT under low-data supervision by introducing UNION, a lightweight and generalisable target representation that fuses the image embedding with a null-text prompt. Unlike traditional approaches that rely on fixed target features, UNION enhances semantic alignment with multimodal queries while requiring no architectural modifications to pretrained vision-language models. With only 5,000 training samples - from LlavaSCo for CIR and Training-Sketchy for SBIR - our method achieves competitive results across benchmarks, including CIRCO mAP@50 of 38.5 and Sketchy mAP@200 of 82.7, surpassing many heavily supervised baselines. This demonstrates the robustness and efficiency of UNION in bridging vision and language across diverse query types.

</details>


### [5] [Efficiency and Effectiveness of SPLADE Models on Billion-Scale Web Document Title](https://arxiv.org/abs/2511.22263)
*Taeryun Won,Tae Kwan Lee,Hiun Kim,Hyemin Lee*

Main category: cs.IR

TL;DR: 本文全面比较了BM25、SPLADE和Expanded-SPLADE模型在大规模网页文档检索中的表现，发现稀疏表示模型性能更优但计算成本更高，通过剪枝策略可在保持性能的同时提升效率。


<details>
  <summary>Details</summary>
Motivation: 在大规模网页文档检索场景中，需要评估不同检索模型的有效性和效率，特别是稀疏表示模型与经典BM25方法的对比，以及如何平衡检索性能与计算成本。

Method: 在从数千万到数十亿规模的网页文档标题数据集上，比较BM25、SPLADE和Expanded-SPLADE模型；引入文档中心剪枝、top-k查询词选择、布尔查询加词项阈值等剪枝策略来优化计算效率。

Result: SPLADE和Expanded-SPLADE在检索性能上优于BM25，特别是对复杂查询；但计算成本更高；通过剪枝策略可显著提升效率而不明显牺牲性能；Expanded-SPLADE在效果和效率之间达到最佳平衡。

Conclusion: Expanded-SPLADE在大规模数据集上表现出最佳的效果-效率平衡，为稀疏检索模型在大型搜索引擎中的部署提供了有价值的指导。

Abstract: This paper presents a comprehensive comparison of BM25, SPLADE, and Expanded-SPLADE models in the context of large-scale web document retrieval. We evaluate the effectiveness and efficiency of these models on datasets spanning from tens of millions to billions of web document titles. SPLADE and Expanded-SPLADE, which utilize sparse lexical representations, demonstrate superior retrieval performance compared to BM25, especially for complex queries. However, these models incur higher computational costs. We introduce pruning strategies, including document-centric pruning and top-k query term selection, boolean query with term threshold to mitigate these costs and improve the models' efficiency without significantly sacrificing retrieval performance. The results show that Expanded-SPLADE strikes the best balance between effectiveness and efficiency, particularly when handling large datasets. Our findings offer valuable insights for deploying sparse retrieval models in large-scale search engines.

</details>


### [6] [CoFiRec: Coarse-to-Fine Tokenization for Generative Recommendation](https://arxiv.org/abs/2511.22707)
*Tianxin Wei,Xuying Ning,Xuxing Chen,Ruizhong Qiu,Yupeng Hou,Yan Xie,Shuang Yang,Zhigang Hua,Jingrui He*

Main category: cs.IR

TL;DR: CoFiRec提出了一种新的生成式推荐框架，通过粗到细的语义层次化tokenization来建模用户意图的渐进演变，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐器忽略了用户偏好从浏览宽泛类别到探索具体项目的自然细化过程，将异构属性压缩到单一嵌入中会扁平化项目的语义层次结构，无法捕捉Web交互中用户意图的渐进演变。

Method: CoFiRec将项目信息分解为多个语义层次（从高级类别到详细描述和协同过滤信号），通过CoFiRec Tokenizer独立tokenize每个层次并保持结构顺序，在自回归解码时指导语言模型从粗到细生成项目token。

Result: 在多个公共基准测试和骨干网络上，CoFiRec优于现有方法，理论证明结构化tokenization能降低生成项目与真实项目之间的差异度。

Conclusion: CoFiRec通过显式建模项目语义的粗到细特性，为生成式推荐提供了新视角，能更好地捕捉用户意图的渐进演变过程。

Abstract: In web environments, user preferences are often refined progressively as users move from browsing broad categories to exploring specific items. However, existing generative recommenders overlook this natural refinement process. Generative recommendation formulates next-item prediction as autoregressive generation over tokenized user histories, where each item is represented as a sequence of discrete tokens. Prior models typically fuse heterogeneous attributes such as ID, category, title, and description into a single embedding before quantization, which flattens the inherent semantic hierarchy of items and fails to capture the gradual evolution of user intent during web interactions. To address this limitation, we propose CoFiRec, a novel generative recommendation framework that explicitly incorporates the Coarse-to-Fine nature of item semantics into the tokenization process. Instead of compressing all attributes into a single latent space, CoFiRec decomposes item information into multiple semantic levels, ranging from high-level categories to detailed descriptions and collaborative filtering signals. Based on this design, we introduce the CoFiRec Tokenizer, which tokenizes each level independently while preserving structural order. During autoregressive decoding, the language model is instructed to generate item tokens from coarse to fine, progressively modeling user intent from general interests to specific item-level interests. Experiments across multiple public benchmarks and backbones demonstrate that CoFiRec outperforms existing methods, offering a new perspective for generative recommendation. Theoretically, we prove that structured tokenization leads to lower dissimilarity between generated and ground truth items, supporting its effectiveness in generative recommendation. Our code is available at https://github.com/YennNing/CoFiRec.

</details>


### [7] [Two-Stage Distributionally Robust Optimization Framework for Secure Communications in Aerial-RIS Systems](https://arxiv.org/abs/2511.22855)
*Zhongming Feng,Qiling Gao,Zeping Sui,Yun Lin,Michail Matthaiou*

Main category: cs.IR

TL;DR: 提出两阶段分布鲁棒优化框架，用于空中可重构智能表面辅助毫米波系统的安全部署和波束成形，应对多时间尺度不确定性。


<details>
  <summary>Details</summary>
Motivation: 解决空中可重构智能表面辅助毫米波系统中，由用户移动性、不完美信道状态信息和硬件损伤引起的多时间尺度不确定性挑战，确保系统安全部署和性能鲁棒性。

Method: 采用两阶段分布鲁棒优化框架：1）长期无人机部署与实时波束成形解耦；2）使用条件风险价值作为分布无关风险度量；3）结合代理模型进行高效部署和交替优化方案进行鲁棒实时波束成形。

Result: 仿真结果表明，所提出的DRO-CVaR框架显著提高了尾部保密频谱效率，在严重不确定性条件下相比基准方案保持了更低的中断概率。

Conclusion: 该框架为空中可重构智能表面辅助毫米波系统提供了一种有效的鲁棒优化方法，能够应对多时间尺度不确定性，提升系统安全性和可靠性。

Abstract: This letter proposes a two-stage distributionally robust optimization (DRO) framework for secure deployment and beamforming in an aerial reconfigurable intelligent surface (A-RIS) assisted millimeter-wave system. To account for multi-timescale uncertainties arising from user mobility, imperfect channel state information (CSI), and hardware impairments, our approach decouples the long-term unmanned aerial vehicle (UAV) placement from the per-slot beamforming design. By employing the conditional value-at-risk (CVaR) as a distribution-free risk metric, a low-complexity algorithm is developed, which combines a surrogate model for efficient deployment with an alternating optimization (AO) scheme for robust real-time beamforming. Simulation results validate that the proposed DRO-CVaR framework significantly enhances the tail-end secrecy spectral efficiency and maintains a lower outage probability compared to benchmark schemes, especially under severe uncertainty conditions.

</details>


### [8] [FedAU2: Attribute Unlearning for User-Level Federated Recommender Systems with Adaptive and Robust Adversarial Training](https://arxiv.org/abs/2511.22872)
*Yuyuan Li,Junjie Fang,Fengyuan Yu,Xichun Sheng,Tianyu Du,Xuyang Teng,Shaowei Jiang,Linbo Jiang,Jianan Lin,Chaochao Chen*

Main category: cs.IR

TL;DR: FedAU2：一种针对用户级联邦推荐系统的属性遗忘方法，通过自适应对抗训练和双随机变分自编码器解决训练不稳定性和梯度信息泄露问题


<details>
  <summary>Details</summary>
Motivation: 联邦推荐系统虽然保护用户隐私，但用户嵌入仍包含敏感属性信息，易受属性推断攻击。现有属性遗忘方法在用户级联邦推荐系统中面临挑战，需要解决用户数据异质性导致的训练不稳定性和梯度信息泄露问题。

Method: 提出FedAU2方法：1）自适应对抗训练策略，根据本地优化行为动态调整训练过程；2）双随机变分自编码器扰动对抗模型，防止基于梯度的信息泄露。

Result: 在三个真实世界数据集上的实验表明，FedAU2在属性遗忘效果和推荐性能方面均优于现有基线方法。

Conclusion: FedAU2有效解决了用户级联邦推荐系统中的属性遗忘问题，通过创新的自适应对抗训练和梯度保护机制，在保护用户隐私的同时保持了推荐质量。

Abstract: Federated Recommender Systems (FedRecs) leverage federated learning to protect user privacy by retaining data locally. However, user embeddings in FedRecs often encode sensitive attribute information, rendering them vulnerable to attribute inference attacks. Attribute unlearning has emerged as a promising approach to mitigate this issue. In this paper, we focus on user-level FedRecs, which is a more practical yet challenging setting compared to group-level FedRecs. Adversarial training emerges as the most feasible approach within this context. We identify two key challenges in implementing adversarial training-based attribute unlearning for user-level FedRecs: i) mitigating training instability caused by user data heterogeneity, and ii) preventing attribute information leakage through gradients. To address these challenges, we propose FedAU2, an attribute unlearning method for user-level FedRecs. For CH1, we propose an adaptive adversarial training strategy, where the training dynamics are adjusted in response to local optimization behavior. For CH2, we propose a dual-stochastic variational autoencoder to perturb the adversarial model, effectively preventing gradient-based information leakage. Extensive experiments on three real-world datasets demonstrate that our proposed FedAU2 achieves superior performance in unlearning effectiveness and recommendation performance compared to existing baselines.

</details>


### [9] [Do LLM-judges Align with Human Relevance in Cranfield-style Recommender Evaluation?](https://arxiv.org/abs/2511.23312)
*Gustavo Penha,Aleksandr V. Petrov,Claudia Hauff,Enrico Palumbo,Ali Vardasbi,Edoardo D'Amico,Francesco Fabbri,Alice Wang,Praveen Chandar,Henrik Lindstrom,Hugues Bouchard,Mounia Lalmas*

Main category: cs.IR

TL;DR: LLM可以作为推荐系统的可靠自动评估器，解决了传统评估方法因人工标注成本高而难以扩展的问题。


<details>
  <summary>Details</summary>
Motivation: 推荐系统评估面临长期挑战：基于历史交互的离线方法存在曝光偏差、流行度偏差等问题，结果不稳定；而类似文档检索的Cranfield式人工标注评估方法成本高昂，难以扩展。

Method: 使用ML-32M-ext电影推荐数据集，首先分析现有评估方法的局限性，然后探究LLM评估器与人工标注的相关性对齐程度，最后在播客推荐领域进行工业案例研究。

Result: 加入更丰富的物品元数据和更长的用户历史记录能提升对齐效果；LLM评估器与人工标注的排名一致性很高（Kendall's tau = 0.87）；工业案例证明LLM评估器可用于模型选择。

Conclusion: LLM评估器是推荐系统评估的可行且可扩展的方法，能够解决传统评估方法的扩展性限制。

Abstract: Evaluating recommender systems remains a long-standing challenge, as offline methods based on historical user interactions and train-test splits often yield unstable and inconsistent results due to exposure bias, popularity bias, sampled evaluations, and missing-not-at-random patterns. In contrast, textual document retrieval benefits from robust, standardized evaluation via Cranfield-style test collections, which combine pooled relevance judgments with controlled setups. While recent work shows that adapting this methodology to recommender systems is feasible, constructing such collections remains costly due to the need for manual relevance judgments, thus limiting scalability. This paper investigates whether Large Language Models (LLMs) can serve as reliable automatic judges to address these scalability challenges. Using the ML-32M-ext Cranfield-style movie recommendation collection, we first examine the limitations of existing evaluation methodologies. Then we explore the alignment and the recommender systems ranking agreement between the LLM-judge and human provided relevance labels. We find that incorporating richer item metadata and longer user histories improves alignment, and that LLM-judge yields high agreement with human-based rankings (Kendall's tau = 0.87). Finally, an industrial case study in the podcast recommendation domain demonstrates the practical value of LLM-judge for model selection. Overall, our results show that LLM-judge is a viable and scalable approach for evaluating recommender systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [10] [A Conceptual Model for Context Awareness in Ethical Data Management](https://arxiv.org/abs/2511.21942)
*Elisa Quintarelli,Fabio Alberto Schreiber,Kostas Stefanidis,Letizia Tanca,Barbara Oliboni*

Main category: cs.DB

TL;DR: 提出一个双部分概念模型，包含描述可能情境的上下文维度树(CDT)和表示伦理规则的伦理需求树(ERT)，用于根据情境调整数据集以满足伦理要求。


<details>
  <summary>Details</summary>
Motivation: 信息管理领域越来越关注伦理问题，算法和数据都需要满足伦理规则以避免产生不道德行为。然而，这些伦理规则会随着应用程序运行的具体情境而变化，因此需要一种能够根据情境调整伦理要求的方法。

Method: 提出了一个双部分概念模型：1) 上下文维度树(CDT) - 描述可能的情境；2) 伦理需求树(ERT) - 表示在不同情境下为数据分析和学习系统准备数据集所需的伦理规则。通过这两个工具的结合，可以根据具体情境定制和预处理数据集。

Result: 开发了一个系统性的概念框架，能够将伦理要求与具体情境相关联，为数据管理和分析提供伦理指导。提供了使用这些概念工具的具体示例和建议。

Conclusion: 提出的双部分概念模型为解决信息管理中的伦理问题提供了一个结构化方法，使伦理规则能够根据具体情境进行适应性调整，确保数据分析和学习系统在不同情境下都能满足伦理要求。

Abstract: Ethics has become a major concern to the information management community, as both algorithms and data should satisfy ethical rules that guarantee not to generate dishonourable behaviours when they are used. However, these ethical rules may vary according to the situation-the context-in which the application programs must work. In this paper, after reviewing the basic ethical concepts and their possible influence on data management, we propose a bipartite conceptual model, composed of the Context Dimensions Tree (CDT), which describes the possible contexts, and the Ethical Requirements Tree (ERT), representing the ethical rules necessary to tailor and preprocess the datasets that should be fed to Data Analysis and Learning Systems in each possible context. We provide some examples and suggestions on how these conceptual tools can be used.

</details>


### [11] [Relation-Stratified Sampling for Shapley Values Estimation in Relational Databases](https://arxiv.org/abs/2511.22035)
*Amirhossein Alizad,Mostafa Milani*

Main category: cs.DB

TL;DR: 提出关系分层抽样(RSS)和自适应变体(ARSS)来高效计算关系查询中元组的Shapley值，相比传统方法显著降低误差和方差


<details>
  <summary>Details</summary>
Motivation: Shapley值等博弈论指标可用于量化关系查询中单个元组对查询结果的贡献，但精确计算需要指数级排列组合，计算不可行。现有采样方法未充分利用关系数据库的模式和连接结构

Method: 提出关系分层抽样(RSS)：按关系计数向量（记录从每个关系抽取的元组数量）对样本空间分层，而非仅按联盟大小分层。这种连接感知的分层将样本集中在结构有效且信息丰富的联盟上。进一步开发自适应变体ARSS，在采样过程中根据方差估计重新分配预算，提高估计器效率而不增加总样本数

Result: 在TPCH工作负载的多样化查询（多关系连接和聚合）中，RSS和ARSS始终优于经典蒙特卡洛抽样(MCS)和基于大小的分层抽样(SS)，以更少样本获得更低误差和方差。消融实验表明关系感知分层和自适应分配具有互补增益

Conclusion: ARSS是一个简单、有效、随时可用的数据库中心Shapley归因估计器，关系感知分层和自适应预算分配的结合显著提高了关系查询中元组贡献量化的效率

Abstract: Shapley-like values, including the Shapley and Banzhaf values, provide a principled way to quantify how individual tuples contribute to a query result. Their exact computation, however, is intractable because it requires aggregating marginal contributions over exponentially many permutations or subsets. While sampling-based estimators have been studied in cooperative game theory, their direct use for relational query answering remains underexplored and often ignores the structure of schemas and joins.
  We study tuple-level attribution for relational queries through sampling and introduce Relation-Stratified Sampling (RSS). Instead of stratifying coalitions only by size, RSS partitions the sample space by a relation-wise count vector that records how many tuples are drawn from each relation. This join-aware stratification concentrates samples on structurally valid and informative coalitions and avoids strata that cannot satisfy query conditions. We further develop an adaptive variant, ARSS, that reallocates budget across strata using variance estimates obtained during sampling, improving estimator efficiency without increasing the total number of samples. We analyze these estimators, describe a practical implementation that reuses compiled views to reduce per-sample query cost, and evaluate them on TPCH workloads.
  Across diverse queries with multi-relation joins and aggregates, RSS and ARSS consistently outperform classical Monte Carlo (MCS) and size-based Stratified Sampling (SS), yielding lower error and variance with fewer samples. An ablation shows that relation-aware stratification and adaptive allocation contribute complementary gains, making ARSS a simple, effective, and anytime estimator for database-centric Shapley attribution.

</details>


### [12] [Performant Synchronization in Geo-Distributed Databases](https://arxiv.org/abs/2511.22444)
*Duling Xu,Tong Li,Zegang Sun,Zheng Chen,Weixing Zhou,Yanfeng Zhang,Wei Lu,Xiaoyong Du*

Main category: cs.DB

TL;DR: GeoCoCo是一个用于跨区域分布式数据库的同步加速框架，通过分组重调度、数据过滤和一致性保证传输来优化同步成本，在真实部署中降低带宽使用达40.3%，提升吞吐量14.1%。


<details>
  <summary>Details</summary>
Motivation: 分布式数据库在广域网中面临高延迟问题，主要瓶颈是跨节点数据一致性的同步成本。研究发现网络聚类现象、传输三角不等式违规和冗余数据传输等优化机会。

Method: 提出GeoCoCo框架：1) 适应实时网络条件的组重调度策略最大化WAN传输效率；2) 任务保留数据过滤方法减少WAN传输数据量；3) 集成分组和剪枝的一致性保证传输框架。

Result: 在跟踪驱动模拟和真实部署评估中，GeoCoCo将同步成本（主要通过降低WAN带宽使用）降低达40.3%，在GeoGauss中将系统吞吐量提升达14.1%。

Conclusion: GeoCoCo通过优化同步成本有效解决了分布式数据库在广域网中的性能瓶颈，显著提升了系统效率和资源利用率。

Abstract: The deployment of databases across geographically distributed regions has become increasingly critical for ensuring data reliability and scalability. Recent studies indicate that distributed databases exhibit significantly higher latency than single-node databases, primarily due to consensus protocols maintaining data consistency across multiple nodes. We argue that synchronization cost constitutes the primary bottleneck for distributed databases, which is particularly pronounced in wide-area networks (WAN). Fortunately, we identify opportunities to optimize synchronization costs in real production environments: (1) network clustering phenomena, (2) triangle inequality violations in transmission, and (3) redundant data transfers. Based on these observations, we propose GeoCoCo, a synchronization acceleration framework for cross-region distributed databases. First, GeoCoCo presents a group rescheduling strategy that adapts to real-time network conditions to maximize WAN transmission efficiency. Second, GeoCoCo introduces a task-preserving data filtering method that reduces data volume transmitted over the WAN. Finally, GeoCoCo develops a consistency-guaranteed transmission framework integrating grouping and pruning. Extensive evaluations in both trace-driven simulations and real-world deployments demonstrate that GeoCoCo reduces synchronization cost-primarily by lowering WAN bandwidth usage-by up to 40.3%, and increases system throughput by up to 14.1% in GeoGauss.

</details>


### [13] [Structured Multi-Step Reasoning for Entity Matching Using Large Language Model](https://arxiv.org/abs/2511.22832)
*Rohan Bopardikar,Jin Wang,Jia Zou*

Main category: cs.DB

TL;DR: 本文提出了一种基于大语言模型的多步推理实体匹配框架，通过分解匹配过程为三个阶段来提升匹配准确性，并探索了基于辩论的策略来增强决策鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的实体匹配方法主要依赖单步提示，缺乏对结构化推理策略的深入探索。为了提升实体匹配的准确性，需要研究如何将匹配过程分解为多个明确的推理阶段。

Method: 提出了一个三步框架：1) 识别两条记录之间匹配和不匹配的token；2) 确定对匹配决策影响最大的属性；3) 预测记录是否指向同一现实世界实体。此外，还探索了基于辩论的策略，通过对比支持和反对论点来提高决策鲁棒性。

Result: 在多个真实世界实体匹配基准数据集上的实验结果表明，结构化多步推理在多个案例中能够提升匹配性能，同时也突显了现有方法的挑战和进一步优化推理引导的大语言模型方法的机遇。

Conclusion: 结构化多步推理能够提升基于大语言模型的实体匹配性能，但仍有挑战需要解决，为未来推理引导的大语言模型方法提供了进一步优化的方向。

Abstract: Entity matching is a fundamental task in data cleaning and data integration. With the rapid adoption of large language models (LLMs), recent studies have explored zero-shot and few-shot prompting to improve entity matching accuracy. However, most existing approaches rely on single-step prompting and offer limited investigation into structured reasoning strategies. In this work, we investigate how to enhance LLM-based entity matching by decomposing the matching process into multiple explicit reasoning stages. We propose a three-step framework that first identifies matched and unmatched tokens between two records, then determines the attributes most influential to the matching decision, and finally predicts whether the records refer to the same real-world entity. In addition, we explore a debate-based strategy that contrasts supporting and opposing arguments to improve decision robustness. We evaluate our approaches against multiple existing baselines on several real-world entity matching benchmark datasets. Experimental results demonstrate that structured multi-step reasoning can improve matching performance in several cases, while also highlighting remaining challenges and opportunities for further refinement of reasoning-guided LLM approaches.

</details>


### [14] [Extended Serial Safety Net: A Refined Serializability Criterion for Multiversion Concurrency Control](https://arxiv.org/abs/2511.22956)
*Atsushi Kitazawa,Chihaya Ito,Yuta Yoshida,Takamitsu Shioi*

Main category: cs.DB

TL;DR: ESSN是SSN的改进版本，通过放宽排除条件允许更多事务安全提交，保持多版本可串行化，严格包含SSN，并在提交时检查，工作负载线性于读写数量。


<details>
  <summary>Details</summary>
Motivation: 现有并发控制协议（如SSN）依赖单一串行化点（开始或提交），与快照隔离不兼容，且SSN在提交时检查较为保守。需要更灵活、高效的协议来减少事务中止率。

Method: ESSN基于多版本串行化图（MVSG）准则，引入已知总序（KTO）进行推理。通过提交时检查保持版本链单调性，消除链遍历。协议基于直接串行化图（DSG），提交时工作量线性于读写数量。

Result: ESSN严格包含SSN，保持多版本可串行化。在提交有序KTO下，使用开始快照读取可将长事务中止率降低约0.25绝对值（约50%相对值）。

Conclusion: ESSN是SSN的通用化改进，通过放宽排除条件提高并发性，减少事务中止，同时保持高效性，适用于混合工作负载。

Abstract: A long line of concurrency-control (CC) protocols argues correctness via a single serialization point (begin or commit), an assumption that is incompatible with snapshot isolation (SI), where read-write anti-dependencies arise. Serial Safety Net (SSN) offers a lightweight commit-time test but is conservative and effectively anchored on commit time as the sole point. We present ESSN, a principled generalization of SSN that relaxes the exclusion condition to allow more transactions to commit safely, and we prove that this preserves multiversion serializability (MVSR) and that it strictly subsumes SSN. ESSN states an MVSG (Multiversion Serialization Graph)-based criterion and introduces a known total order over transactions (KTO; e.g., begin-ordered or commit-ordered) for reasoning about the graph's serializability. With a single commit-time check under invariant-based semantics, ESSN's exclusion condition preserves monotonicity along per-item version chains, and eliminates chain traversal. The protocol is Direct Serialization Graph (DSG)-based with commit-time work linear in the number of reads and writes, matching SSN's per-version footprint. We also make mixed workloads explicit by defining a Long transaction via strict interval containment of Short transactions, and we evaluate ESSN on reproducible workloads. Under a commit-ordered KTO, using begin-snapshot reads reduces the long-transaction abort rate by up to approximately 0.25 absolute (about 50% relative) compared with SSN.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [15] [Designing a Multimodal Viewer for Piano Performance Analysis -- a Pedagogy-First Approach](https://arxiv.org/abs/2511.21693)
*Joonhyung Bae,Hyeyoon Cho,Kirak Kim,Dawon Park,Taegyun Kwon,Yoon-Seok Choi,Hyeon Hur,Shigeru Kai,Yohei Wada,Satoshi Obata,Akira Maezawa,Jaebum Park,Jonghwa Park,Juhan Nam*

Main category: cs.MM

TL;DR: 开发了一个基于网络的钢琴教学仪表板原型，通过视频、动作捕捉和乐谱集成，将抽象的口头指导转化为具体的视觉反馈，解决了钢琴教学中抽象指令解释不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 钢琴教育中的抽象指令（如"抬起手腕"、"放松紧张"）在不同学习者中产生不同解释，导致教师无法有效传达教学意图，需要更具体的反馈方式。

Method: 1. 对拥有18年教学经验的钢琴教授进行系统访谈；2. 两位研究人员通过交叉验证推导出七个核心需求组；3. 开发基于网络的仪表板原型，集成视频、动作捕捉和乐谱；4. 使用109个表演数据集验证技术可行性。

Result: 成功开发了原型系统，能够将抽象的口头指导转化为具体的视觉反馈，技术可行性通过109个数据集得到验证，为钢琴教学提供了更有效的指导工具。

Conclusion: 该研究提出的集成视频、动作捕捉和乐谱的仪表板原型，能够帮助钢琴教师提供更具体、可视化的反馈，解决了抽象指令解释不一致的问题，具有实际应用价值。

Abstract: Abstract instructions in piano education, such as "raise your wrist" and "relax your tension," lead to varying interpretations among learners, preventing instructors from effectively conveying their intended pedagogical guidance. To address this problem, this study conducted systematic interviews with a piano professor with 18 years teaching experience, and two researchers derived seven core need groups through cross-validation. Based on these findings, we developed a web-based dashboard prototype integrating video, motion capture, and musical scores, enabling instructors to provide concrete, visual feedback instead of relying solely on abstract verbal instructions. Technical feasibility was validated through 109 performance datasets.

</details>


### [16] [A Survey of Information Disorder on Video-Sharing Platforms](https://arxiv.org/abs/2511.21694)
*Meiyu Li,Wei Ai,Naeemul Hassan*

Main category: cs.MM

TL;DR: 这篇论文综述了视频分享平台上的信息失序现象，从信息失序类型、研究方法论和平台特征三个维度进行分析，并指出了未来研究的关键挑战和开放性问题。


<details>
  <summary>Details</summary>
Motivation: 视频分享平台已成为核心信息枢纽，但也助长了信息失序的传播，包括误导性叙事和捏造内容。需要系统性地研究这些平台的多媒体生态系统，以理解信息失序的机制和影响。

Method: 采用文献综述方法，从三个维度综合现有研究：(1) 信息失序类型，(2) 方法论方法，(3) 平台特征。通过系统性地梳理相关文献，构建分析框架。

Result: 论文综合了视频分享平台上信息失序研究的现状，识别了不同类型的信息失序现象，总结了常用的研究方法，分析了平台特征对信息传播的影响，为理解这一复杂生态系统提供了系统性的视角。

Conclusion: 视频分享平台的信息失序问题需要跨学科的综合研究。未来研究应关注平台治理、算法透明度、用户行为干预等关键挑战，并解决如何有效检测和缓解多媒体内容中的信息失序这一开放性问题。

Abstract: Video sharing platforms (VSPs) have become central information hubs but also facilitate the spread of information disorder, from misleading narratives to fabricated content. This survey synthesizes research on VSPs' multimedia ecosystems across three dimensions: (1) types of information disorder, (2) methodological approaches, and (3) platform features. We conclude by identifying key challenges and open questions for future research.

</details>


### [17] [TIP and Polish: Text-Image-Prototype Guided Multi-Modal Generation via Commonality-Discrepancy Modeling and Refinement](https://arxiv.org/abs/2511.21698)
*Zhiyong Ma,Jiahao Chen,Qingyuan Chuai,Zhengping Li*

Main category: cs.MM

TL;DR: TIPPo框架通过显式建模文本、图像和原型信号，结合双重对齐注意力、差异算子模块和PolishPPO强化学习，解决多模态生成中的主题一致性和风格一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态生成方法存在跨模态不匹配问题，缺乏对共性和差异的显式建模，细粒度训练方法难以平衡语义精度与写作风格一致性，导致生成质量不理想。

Method: TIPPo框架使用多模态编码器和适配器提取文本和图像特征，测量视觉原型，通过双重对齐注意力和差异算子模块处理文本、图像和原型信号，最后通过语言模型解码。采用PolishPPO强化风格一致性，SFT阶段使用无监督对比学习防止表示坍塌。

Result: 实验结果显示TIPPo在自动评估和基于LLM的创造力、语义一致性标准上表现出色，验证了其有效性。

Conclusion: TIPPo通过显式输入建模和综合优化目标，有效解决了多模态生成中的主题一致性和风格一致性问题，是一个简单而有效的框架。

Abstract: Multi-modal generation struggles to ensure thematic coherence and style consistency. Semantically, existing methods suffer from cross-modal mismatch and lack explicit modeling of commonality and discrepancy. Methods that rely on fine-grained training fail to balance semantic precision with writing style consistency. These shortcomings lead to suboptimal generation quality. To tackle these issues, we propose \textbf{\textit{TIPPo}}, a simple yet effective framework with explicit input modeling and comprehensive optimization objectives. It extracts the input text and images via multi-modal encoder and adapters, then measures the visual prototype. \textbf{T}extual, \textbf{I}mage, and \textbf{P}rototype signals are then fed to our proposed Dual Alignment Attention and Difference Operator modules before language model decoding. The proposed \textbf{Po}lishPPO reinforces the style consistency, while the unsupervised contrastive learning during SFT mitigates inter-sample representation collapse. Experimental results demonstrate the promising performance of \textbf{\textit{TIPPo}} in automatic evaluation and LLM-based criteria for creativity and semantic consistency.

</details>


### [18] [3MDiT: Unified Tri-Modal Diffusion Transformer for Text-Driven Synchronized Audio-Video Generation](https://arxiv.org/abs/2511.21780)
*Yaoru Li,Heyu Si,Federico Landi,Pilar Oplustil Gallegos,Ioannis Koutsoumpas,O. Ricardo Cortez Vazquez,Ruiju Fu,Qi Guo,Xin Jin,Shunyu Liu,Mingli Song*

Main category: cs.MM

TL;DR: 3MDiT是一个统一的三模态扩散Transformer，用于文本驱动的同步音视频生成，通过联合建模视频、音频和文本流，支持从头训练或适配预训练T2V模型。


<details>
  <summary>Details</summary>
Motivation: 现有音视频生成系统通常采用级联方法，导致跨模态误差累积；而联合生成器通常使用双塔架构和静态文本条件，难以重用T2V主干网络并处理跨模态的时间交互。

Method: 提出3MDiT统一框架：1）同构音频分支镜像T2V主干；2）三模态全向块执行特征级融合；3）可选动态文本条件机制根据音视频证据更新文本表示。支持从头训练或正交适配预训练T2V模型。

Result: 实验表明该方法能生成高质量视频和真实音频，在音频-视频同步性和三模态对齐方面持续改进，在一系列定量指标上表现优异。

Conclusion: 3MDiT通过统一的三模态扩散Transformer架构，有效解决了现有音视频生成系统的局限性，实现了更好的跨模态对齐和同步，同时保持了与现有T2V模型的兼容性。

Abstract: Text-to-video (T2V) diffusion models have recently achieved impressive visual quality, yet most systems still generate silent clips and treat audio as a secondary concern. Existing audio-video generation pipelines typically decompose the task into cascaded stages, which accumulate errors across modalities and are trained under separate objectives. Recent joint audio-video generators alleviate this issue but often rely on dual-tower architectures with ad-hoc cross-modal bridges and static, single-shot text conditioning, making it difficult to both reuse T2V backbones and to reason about how audio, video and language interact over time. To address these challenges, we propose 3MDiT, a unified tri-modal diffusion transformer for text-driven synchronized audio-video generation. Our framework models video, audio and text as jointly evolving streams: an isomorphic audio branch mirrors a T2V backbone, tri-modal omni-blocks perform feature-level fusion across the three modalities, and an optional dynamic text conditioning mechanism updates the text representation as audio and video evidence co-evolve. The design supports two regimes: training from scratch on audio-video data, and orthogonally adapting a pretrained T2V model without modifying its backbone. Experiments show that our approach generates high-quality videos and realistic audio while consistently improving audio-video synchronization and tri-modal alignment across a range of quantitative metrics.

</details>


### [19] [VSpeechLM: A Visual Speech Language Model for Visual Text-to-Speech Task](https://arxiv.org/abs/2511.22229)
*Yuyue Wang,Xin Cheng,Yihan Wu,Xihua Wang,Jinchuan Tian,Ruihua Song*

Main category: cs.MM

TL;DR: VSpeechLM：基于语音大语言模型的新型视觉语音语言模型，通过文本-视频对齐器学习音素与唇部运动的细粒度对齐，生成高质量且唇部同步的语音。


<details>
  <summary>Details</summary>
Motivation: 现有VisualTTS模型通常采用轻量级架构和专门模块，但由于模型容量有限和VisualTTS数据不足，语音质量不理想。虽然语音大语言模型能生成高质量语音，但很少有工作能很好地利用视频输入的时间线索来生成唇部同步的语音。

Method: 提出基于SpeechLLM的VSpeechLM模型：1）文本-视频对齐器学习音素与唇部运动的细粒度对齐，输出包含唇部同步线索的扩展音素序列；2）基于SpeechLLM的解码器以扩展音素序列为输入，学习生成唇部同步的语音。

Result: 大量实验表明，VSpeechLM在整体质量、说话人相似性和同步指标方面显著优于之前的VisualTTS方法。

Conclusion: 提出的VSpeechLM模型通过结合SpeechLLM的强大生成能力和文本-视频对齐技术，成功实现了高质量且唇部同步的语音生成，为VisualTTS任务提供了有效的解决方案。

Abstract: The task of Visual Text-to-Speech (VisualTTS), also known as video dubbing, aims to generate speech synchronized with the lip movements in an input video, in additional to being consistent with the content of input text and cloning the timbre of a reference speech. Existing VisualTTS models typically adopt lightweight architectures and design specialized modules to achieve the above goals respectively, yet the speech quality is not satisfied due to the model capacity and the limited data in VisualTTS. Recently, speech large language models (SpeechLLM) show the robust ability to generate high-quality speech. But few work has been done to well leverage temporal cues from video input in generating lip-synchronized speech. To generate both high-quality and lip-synchronized speech in VisualTTS tasks, we propose a novel Visual Speech Language Model called VSpeechLM based upon a SpeechLLM. To capture the synchronization relationship between text and video, we propose a text-video aligner. It first learns fine-grained alignment between phonemes and lip movements, and then outputs an expanded phoneme sequence containing lip-synchronization cues. Next, our proposed SpeechLLM based decoders take the expanded phoneme sequence as input and learns to generate lip-synchronized speech. Extensive experiments demonstrate that our VSpeechLM significantly outperforms previous VisualTTS methods in terms of overall quality, speaker similarity, and synchronization metrics.

</details>


### [20] [Angle-Optimized Partial Disentanglement for Multimodal Emotion Recognition in Conversation](https://arxiv.org/abs/2511.22447)
*Xinyi Che,Wenbo Wang,Yuanbo Hou,Mingjie Xie,Qijun Zhao,Jian Guan*

Main category: cs.MM

TL;DR: 本文提出AO-FL框架，通过角度优化特征学习实现多模态情感对话中共享特征与模态特定特征的部分解耦，提升情感识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有MERC方法主要关注跨模态共享特征，忽略了模态特定特征（如微表情、韵律变化、讽刺等细微情感线索）。虽然已有研究尝试解耦共享和特定特征，但通常使用刚性正交约束实现完全解耦，忽视了特征类型之间的互补性，可能限制识别性能。

Method: 提出角度优化特征学习（AO-FL）框架：1）通过自适应角度优化实现每个模态内共享特征和特定特征的部分解耦；2）对齐跨模态共享特征以确保语义一致性；3）在每个模态内自适应建模共享特征与模态特定特征之间的角度关系，保持区分性和互补性；4）正交投影细化去除特定特征中的冗余，并用上下文信息丰富共享特征。

Result: 大量实验证实AO-FL在MERC任务中的有效性，表现出优于现有最先进方法的性能。该框架可无缝集成各种单模态特征提取器，并可扩展到其他多模态融合任务（如MER），显示出超越MERC的强泛化能力。

Conclusion: AO-FL框架通过角度优化实现部分特征解耦，有效平衡了共享特征与模态特定特征的区分性和互补性，在多模态情感对话识别中取得了优越性能，并具有良好的可扩展性和泛化能力。

Abstract: Multimodal Emotion Recognition in Conversation (MERC) aims to enhance emotion understanding by integrating complementary cues from text, audio, and visual modalities. Existing MERC approaches predominantly focus on cross-modal shared features, often overlooking modality-specific features that capture subtle yet critical emotional cues such as micro-expressions, prosodic variations, and sarcasm. Although related work in multimodal emotion recognition (MER) has explored disentangling shared and modality-specific features, these methods typically employ rigid orthogonal constraints to achieve full disentanglement, which neglects the inherent complementarity between feature types and may limit recognition performance. To address these challenges, we propose Angle-Optimized Feature Learning (AO-FL), a framework tailored for MERC that achieves partial disentanglement of shared and specific features within each modality through adaptive angular optimization. Specifically, AO-FL aligns shared features across modalities to ensure semantic consistency, and within each modality it adaptively models the angular relationship between its shared and modality-specific features to preserve both distinctiveness and complementarity. An orthogonal projection refinement further removes redundancy in specific features and enriches shared features with contextual information, yielding more discriminative multimodal representations. Extensive experiments confirm the effectiveness of AO-FL for MERC, demonstrating superior performance over state-of-the-art approaches. Moreover, AO-FL can be seamlessly integrated with various unimodal feature extractors and extended to other multimodal fusion tasks, such as MER, thereby highlighting its strong generalization beyond MERC.

</details>


### [21] [Orthogonal Disentanglement with Projected Feature Alignment for Multimodal Emotion Recognition in Conversation](https://arxiv.org/abs/2511.22463)
*Xinyi Che,Wenbo Wang,Jian Guan,Qijun Zhao*

Main category: cs.MM

TL;DR: 提出OD-PFA框架，通过正交解耦和投影特征对齐，同时捕捉跨模态共享语义和模态特定情感线索，在多模态情感对话识别任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感识别方法通常使用对比学习和跨注意力机制对齐跨模态情感语义，但忽略了模态特定的情感细微差别（如微表情、语调变化、讽刺语言），需要同时捕捉共享语义和模态特定线索。

Method: 提出正交解耦与投影特征对齐（OD-PFA）框架：1）将单模态特征解耦为共享和模态特定组件；2）使用正交解耦策略强制分离这些组件，并通过重建损失保持各模态关键情感信息；3）投影特征对齐策略将共享特征映射到公共潜在空间，应用跨模态一致性对齐损失增强语义连贯性。

Result: 在IEMOCAP和MELD基准数据集上的广泛评估表明，OD-PFA在多模态情感识别任务上优于最先进方法。

Conclusion: OD-PFA框架能有效捕捉跨模态共享语义和模态特定情感线索，在多模态情感对话识别中表现出优越性能，解决了现有方法忽略模态特定细微差别的问题。

Abstract: Multimodal Emotion Recognition in Conversation (MERC) significantly enhances emotion recognition performance by integrating complementary emotional cues from text, audio, and visual modalities. While existing methods commonly utilize techniques such as contrastive learning and cross-attention mechanisms to align cross-modal emotional semantics, they typically overlook modality-specific emotional nuances like micro-expressions, tone variations, and sarcastic language. To overcome these limitations, we propose Orthogonal Disentanglement with Projected Feature Alignment (OD-PFA), a novel framework designed explicitly to capture both shared semantics and modality-specific emotional cues. Our approach first decouples unimodal features into shared and modality-specific components. An orthogonal disentanglement strategy (OD) enforces effective separation between these components, aided by a reconstruction loss to maintain critical emotional information from each modality. Additionally, a projected feature alignment strategy (PFA) maps shared features across modalities into a common latent space and applies a cross-modal consistency alignment loss to enhance semantic coherence. Extensive evaluations on widely-used benchmark datasets, IEMOCAP and MELD, demonstrate effectiveness of our proposed OD-PFA multimodal emotion recognition tasks, as compared with the state-of-the-art approaches.

</details>


### [22] [A Progressive Evaluation Framework for Multicultural Analysis of Story Visualization](https://arxiv.org/abs/2511.22576)
*Janak Kapuriya,Ali Hatami,Paul Buitelaar*

Main category: cs.MM

TL;DR: 该研究对文本到图像生成模型在故事可视化中的文化维度进行了多文化分析，发现现有模型存在文化偏见，英语文化表现最佳，印地语表现最差，并提出了新的评估框架和指标。


<details>
  <summary>Details</summary>
Motivation: 当前的故事可视化模型往往忽视文化维度，导致生成的视觉内容缺乏真实性和文化保真度。需要全面评估文本到图像模型在多语言环境下的文化表现，以促进生成更具文化包容性的视觉故事。

Method: 1. 使用FlintstonesSV和VIST两个数据集进行多文化分析；2. 提出渐进式多文化评估框架；3. 引入五个新的故事可视化评估指标：文化适宜性、视觉美学、连贯性、语义一致性和对象存在性；4. 开发MLLM-as-Jury框架自动化评估，近似人类判断。

Result: 1. 模型在真实世界数据集上比动画数据集生成更连贯、视觉吸引力更强、文化更适宜的故事；2. 生成的故事在所有指标（除连贯性外）都与英语文化更一致；3. 中文在连贯性指标上表现更好；4. 印地语在所有指标（除视觉美学外）都排名最低，反映了模型中嵌入的现实世界文化偏见。

Conclusion: 该多文化分析为未来研究奠定了基础，旨在为不同语言和文化环境生成文化适宜且包容的视觉故事。研究表明当前模型存在显著的文化偏见，需要进一步改进以实现真正的文化包容性。

Abstract: Recent advancements in text-to-image generative models have improved narrative consistency in story visualization. However, current story visualization models often overlook cultural dimensions, resulting in visuals that lack authenticity and cultural fidelity. In this study, we conduct a comprehensive multicultural analysis of story visualization using current text-to-image models across multilingual settings on two datasets: FlintstonesSV and VIST. To assess cultural dimensions rigorously, we propose a Progressive Multicultural Evaluation Framework and introduce five story visualization metrics, Cultural Appropriateness, Visual Aesthetics, Cohesion, Semantic Consistency, and Object Presence, that are not addressed by existing metrics. We further automate assessment through an MLLM-as-Jury framework that approximates human judgment. Human evaluations show that models generate more coherent, visually appealing, and culturally appropriate stories for real-world datasets than for animated ones. The generated stories exhibit a stronger alignment with English-speaking cultures across all metrics except Cohesion, where Chinese performs better. In contrast, Hindi ranks lowest on all metrics except Visual Aesthetics, reflecting real-world cultural biases embedded in current models. This multicultural analysis provides a foundation for future research aimed at generating culturally appropriate and inclusive visual stories across diverse linguistic and cultural settings.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [23] [Fluid Antenna System-Enabled UAV Communications in the Finite Blocklength Regime](https://arxiv.org/abs/2511.21834)
*Xusheng Zhu,Kai-Kit Wong,Hanjiang Hong,Han Xiao,Hao Xu,Tuo Wu,Chan-Byoung Chae*

Main category: cs.IT

TL;DR: 本文为有限块长体制下的流体天线系统(FAS)无人机中继网络建立了性能分析框架，推导了块错误率的闭式表达式，分析了能量效率优化问题，并提出了分层算法进行参数联合优化。


<details>
  <summary>Details</summary>
Motivation: 研究流体天线系统(FAS)在无人机中继网络中的性能，特别是在有限块长体制下的可靠性分析。现有研究往往忽略FAS端口选择过程中的时间和能量开销，本文旨在建立更现实的性能分析框架。

Method: 采用基于特征值的可处理近似方法对空间相关的无人机-用户链路建模，使用Nakagami-m衰落模型。针对LOS主导的农村场景和概率性NLoS城市场景进行分析。提出分层算法联合优化系统参数。

Result: 推导了块错误率(BLER)的闭式表达式，揭示了无人机-用户链路的基本分集阶数。数值结果表明FAS能带来显著功率增益，但操作开销引入了重要权衡，导致农村和城市环境中存在不同的最优端口数量和无人机部署策略。

Conclusion: 本文为FAS使能的无人机通信提供了基础分析和实用设计指南，强调了在考虑实际操作开销时，农村和城市环境需要不同的优化策略，FAS的性能增益与操作开销之间存在重要权衡。

Abstract: This paper develops a comprehensive framework for the performance analysis of fluid antenna system (FAS)-enabled unmanned aerial vehicle (UAV) relaying networks operating in the finite blocklength regime. Our contribution lies in establishing a rigorous methodology for characterizing system reliability under diverse propagation environments. Closed-form expressions for the block error rate (BLER) are derived by employing a tractable eigenvalue-based approximation of the spatially correlated UAV-to-user link, whose underlying independent diversity components are modeled as Nakagami-$m$ fading. This approach addresses both line-of-sight (LoS) dominant rural and probabilistic non-line-of-sight (NLoS) urban scenarios. Furthermore, a high signal-to-noise ratio (SNR) asymptotic analysis is developed, revealing the fundamental diversity order of the UAV-to-user link. Based on this, we further address the practical issue of energy efficiency. A realistic energy efficiency maximization problem is formulated, which explicitly accounts for the time and energy overhead inherent in the FAS port selection process, a factor often omitted in idealized models. An efficient hierarchical algorithm is then proposed to jointly optimize the key system parameters. Extensive numerical results validate the analysis and illustrate that while FASs can yield substantial power gains, the operational overhead introduces a non-trivial trade-off. This trade-off leads to an optimal number of ports and fundamentally different UAV deployment strategies in rural versus urban environments. This work provides both foundational analysis and practical design guidelines for FAS-enabled UAV communications.

</details>


### [24] [Spectrum-Aware IRS Configuration Techniques for Ultrawideband Signals](https://arxiv.org/abs/2511.21927)
*Alessandro Nordio,Alberto Tarable,Francisco J. Escribano*

Main category: cs.IT

TL;DR: 论文提出两种针对超宽带下行场景的高效智能反射面配置技术，利用波束分裂效应并考虑发射信号频谱形状，通过局部优化提升接收信号功率。


<details>
  <summary>Details</summary>
Motivation: 随着无线频谱资源日益紧张，需要开发更高频段，但高频无线场景面临视距阻塞问题。智能反射面是解决该问题的方案之一，但现有技术难以实现能够重定向大带宽信号的实际设计，容易受到波束分裂色散效应的影响。

Method: 提出两种高效的配置技术：1）基于智能反射面表面的局部优化，而非全局优化；2）利用波束分裂效应，同时考虑发射信号频谱形状。这些技术专门针对超宽带下行场景设计。

Result: 在不同几何设置和信号频谱的仿真中，所提出的技术相比传统的基于窄带的解决方案或对整个智能反射面表面进行全局优化的技术，能够保证接收端获得更高的信号功率。

Conclusion: 通过局部优化和利用波束分裂效应，所提出的智能反射面配置技术能够有效应对超宽带场景中的信号色散问题，显著提升接收信号质量，为高频无线通信提供了实用解决方案。

Abstract: Intelligent reflecting surfaces (IRS) have become the subject of many current research efforts, as the ongoing wireless spectrum crunch has made the need to open higher frequency bands a priority. IRS are one of the alternatives proposed to overcome the problem of line-of-sight blocking in very high frequency wireless scenarios. The current state-of-the-art shows the difficulty of implementing practical IRS designs able to redirect large signal bandwidths, prone to the so-called beam split (BS) dispersion effect. In this work, we propose two highly efficient configuration techniques, adapted to ultrawideband downlink scenarios, based on localized optimization over the IRS surface. Such techniques exploit the BS effect while taking into account for the shape of the transmitted signal spectrum. Simulations considering different geometrical setups and different signal spectra show how the proposed techniques are able to guarantee an increased signal power at the receiver with respect to classical narrowband-based solutions or techniques that perform a global optimization over the entire IRS surface.

</details>


### [25] [Four classes of optimal p-ary cyclic codes](https://arxiv.org/abs/2511.22086)
*Jinmei Fan,Jingyao Feng,Yuhan Men,Yanhai Zhang*

Main category: cs.IT

TL;DR: 本文研究了p元最优循环码的构造，通过放宽码字汉明重量为3的充要条件，并分析有限域上特定方程的解，提出了四类由(p^m+1)/2导出的参数为[p^m-1,p^m-2m-2,4]的最优p元循环码，其中三类是无限的。


<details>
  <summary>Details</summary>
Motivation: 对于奇素数p>3和正整数m，参数为[p^m-1,p^m-2m-2,4]的最优p元循环码的研究进展有限，需要新的构造方法。

Method: 通过放宽循环码具有汉明重量为3的码字的充要条件，并分析有限域上特定方程的解，构造了四类由(p^m+1)/2导出的最优p元循环码。

Result: 提出了四类参数为[p^m-1,p^m-2m-2,4]的最优p元循环码，其中三类是无限的。许多已知的五元最优循环码是本文构造的特殊情况。

Conclusion: 本文成功构造了新的最优p元循环码类，扩展了最优循环码的理论体系，为相关编码理论提供了新的构造方法。

Abstract: Let p>3 be an odd prime and m be a positive integer. Little progress on the study of optimal p-ary cyclic codes with parameters [p^m-1,p^m-2m-2,4] has been made.In this paper, by weakening the necessary and sufficient conditions on cyclic codes to have codewords of Hamming weight 3 and analyzing the solutions of certain equations over finite fields, four classes of optimal p-ary cyclic codes deduced by p^m+1/2 with parameters [p^m-1,p^m-2m-2,4] are presented.Wherein three classes of optimal p-ary cyclic codes are infinite.Many classes of known optimal quinary cyclic codes with parameters [5^m-1,5^m-2m-2,4] are special cases of the codes constructed in this paper.

</details>


### [26] [Fluid Antenna-Enhanced Flexible Beamforming](https://arxiv.org/abs/2511.22163)
*Jingyuan Xu,Zhentian Zhang,Jian Dang,Hao Jiang,Zaichen Zhang*

Main category: cs.IT

TL;DR: 本文提出了一种基于二维平面流体天线阵列的灵活波束成形框架，将任意波束模式合成转化为稀疏回归问题，并通过定制压缩感知算法和迭代FFT相位恢复方法实现高效波束重构。


<details>
  <summary>Details</summary>
Motivation: 流体天线系统能够在固定物理孔径内提供高空间分辨率，对于下一代无线部署具有吸引力。实际通信网络中需要窄波束和宽波束模式，因此实现流体天线的灵活波束成形成为重要研究方向。

Method: 建立统一框架将任意波束模式合成与流体天线端口选择连接，将波束模式重构转化为稀疏回归问题，采用定制压缩感知算法结合FFT高效求解，并引入迭代FFT相位恢复方法确保物理一致的相位建模。

Result: 仿真结果表明，与传统固定阵列架构相比，流体天线在波束模式重构精度方面显著提升，突显了其在未来无线系统中实现高分辨率和自适应波束成形的潜力。

Conclusion: 提出的灵活波束成形框架有效解决了流体天线系统的波束成形问题，通过压缩感知和相位恢复方法实现了高效准确的波束模式重构，为未来无线系统的高分辨率自适应波束成形提供了有前景的解决方案。

Abstract: Fluid antenna systems encompass a broad class of reconfigurable antenna technologies that offer substantial spatial diversity for various optimization objectives and communication tasks. Their capability to enhance spatial resolution within a fixed physical aperture makes fluid antennas particularly attractive for next-generation wireless deployments. In this work, we focus on the beamforming problem using a two-dimensional planar fluid antenna array. Since both narrow-beam and broad-beam patterns are essential in practical communication networks, enabling flexible beamforming through fluid antennas becomes an important and interesting research direction. We establish a unified and flexible framework that connects arbitrary beam-pattern synthesis with fluid-antenna port selection. The resulting formulation transforms beam-pattern reconstruction into a sparse regression problem, which is addressed using a tailored compressive sensing algorithm designed to operate efficiently with the fast Fourier transform (FFT). Furthermore, to ensure physically consistent phase modeling in the desired beam, we introduce an iterative FFT-based phase retrieval method. Owing to its structure, the proposed phase-refinement procedure exhibits low computational complexity and rapid convergence, requiring only one FFT and one inverse FFT per iteration. Simulation results demonstrate the effectiveness of the proposed flexible beamforming framework. Compared with conventional fixed-array architectures, fluid antennas exhibit significantly improved beam-pattern reconstruction accuracy, highlighting their potential for high-resolution and adaptive beamforming in future wireless systems.

</details>


### [27] [Constructions of block MDS LDPC codes from punctured circulant matrices](https://arxiv.org/abs/2511.22183)
*Hongwei Zhu,Xuantai Wu,Jingjie Lv,Qinshan Zhang,Shu-Tao Xia*

Main category: cs.IT

TL;DR: 本文构造了同时具有块MDS特性和Tanner图中无4环的码，称为块MDS LDPC码，在二进制域上实现，兼具随机错误和突发错误纠正能力。


<details>
  <summary>Details</summary>
Motivation: LDPC码在迭代译码中性能优异但通常不是MDS码，而MDS阵列码（如BR码）适合纠正突发错误但存在4环问题。需要构造同时具备块MDS特性和无4环Tanner图的码，即块MDS LDPC码。

Method: 1. 从穿孔循环置换矩阵构造二进制块MDS码；2. 从列重大于1的循环矩阵(CM(t))构造块MDS LDPC码；3. 提出CM(t)的Moore行列式公式和避免4环的充分条件；4. 证明二进制块MDS CPM-QC LDPC码不存在性。

Result: 构造的块MDS LDPC码在相似码长和码率下，相比现有工作增强了随机错误纠正能力，同时作为阵列码能有效对抗突发错误，且适用于二进制域。

Conclusion: 成功构造了二进制块MDS LDPC码，兼具LDPC码的迭代译码优势和MDS阵列码的突发错误纠正能力，填补了现有码型之间的空白。

Abstract: Low density parity check (LDPC) codes, initially discovered by Gallager, exhibit excellent performance in iterative decoding, approaching the Shannon limit. MDS array codes, with favorable algebraic structures, are codes suitable for decoding large burst errors. The Blaum-Roth (BR) code, an MDS array code similar to the Reed-Solomon (RS) code but has a parity-check matrix prone to $4$-cycles. Fossorier proposed constructing quasi-cyclic LDPC codes from circulant permutation matrices but are not MDS array codes. This paper aims to construct codes that possess both the block MDS property and have no $4$-cycles in the Tanner graph of their parity-check matrices, namely the so-called block MDS LDPC codes. Non-binary block MDS QC codes were first constructed by [Tauz {\it et al. }IEEE ITW, 2025] using circulant shift matrices. We first generate a family of block MDS codes over $\F_2$ from punctured circulant permutation matrices. Second, we construct a family of block MDS LDPC codes from circulant matrices with column weight $> 1$ (CM$(t)$). Additionally, we present the Moore determinant formula for CM$(t)$s and a sufficient condition to avoid $4$-cycles in CM\((t)\)-QC LDPC codes' Tanner graphs for $t> 1$. We also point out the non-existence of binary block MDS CPM-QC LDPC codes. Compared to the codes constructed in [Li {\it et al. }IEEE TIT, 2023] and [Xiao {\it et al. }IEEE TCOM, 2021], our block MDS LDPC codes show enhanced random-error-correction at a similar code length and rate. Meanwhile, these codes can effectively combat burst errors when considered as array codes. Both of our two types of constructions for block MDS LDPC codes are applicable to the scenario of the binary field.

</details>


### [28] [Maximum Entropy and Bayesian Conditioning Under Extended Space](https://arxiv.org/abs/2511.22375)
*Boning Yu*

Main category: cs.IT

TL;DR: 本文探讨贝叶斯条件化与最大熵原理在何种条件下一致，重点处理新信息不在原概率空间事件中的情况，需要扩展概率空间。作者回应了Skyrms和Seidenfeld的争论，认为Friedman-Shimony结果要么是Skyrms方法的良性推论，要么对所有空间扩展方法构成普遍挑战。


<details>
  <summary>Details</summary>
Motivation: 研究贝叶斯条件化与最大熵原理的一致性条件，特别是当新信息不属于原概率空间事件时如何处理。这涉及概率空间扩展的必要性，以及不同学者（Skyrms vs Seidenfeld）对此问题的争议。

Method: 通过分析Skyrms（1985）和Seidenfeld（1986）的论点，特别是Seidenfeld引用Friedman和Shimony（1971）的结果来批评Skyrms的方法。作者提出两种可能性：要么Friedman-Shimony结果是Skyrms方法的良性推论，要么它对所有空间扩展方法构成普遍挑战。

Result: 作者论证了Friedman-Shimony结果要么是Skyrms方法的必然结果，要么意味着贝叶斯条件化根本无法处理超出原结果空间概率空间的信息。接受后者将严重限制贝叶斯方法的应用范围。

Conclusion: 贝叶斯条件化与最大熵原理的一致性问题是概率论基础的重要议题。作者认为必须澄清Friedman-Shimony结果的含义：它要么支持Skyrms的方法，要么对所有概率空间扩展方法构成根本性挑战，后者将意味着贝叶斯条件化在处理新信息类型时存在严重局限性。

Abstract: This paper examines the conditions under which Bayesian conditioning aligns with Maximum Entropy. Specifically, I address cases in which newly learned information does not correspond to an event in the probability space defined on the sample space of outcomes. To facilitate Bayesian conditioning in such cases, one must therefore extend the probability space so that the new information becomes an event in this expanded space. Skyrms (1985) argues that Bayesian conditioning in an extended probability space on a product space of outcomes aligns precisely with the solution from Maximum Entropy. In contrast, Seidenfeld (1986) uses Friedman and Shimony's (1971) result to criticize Skyrms' approach as trivial, suggesting that alignment holds only under a degenerate probability model. Here, I argue that Friedman and Shimony's result must either (1) be a benign consequence of Skyrms' approach, or (2) pose a universal challenge to any method of extending spaces. Accepting (2) would imply that Bayesian conditioning is incapable of accommodating information beyond the probability space defined on the original outcome space.

</details>


### [29] [On the SER Performance of ZF and MMSE Receivers in Pilot-Aided Simultaneous Communication and Localization](https://arxiv.org/abs/2511.22418)
*Shuaishuai Han,Emad Alsusa,Arafat Al-Dweik*

Main category: cs.IT

TL;DR: 该论文分析了定位误差对ZF和MMSE均衡器通信性能的影响，推导了紧密近似的SER表达式，揭示了定位误差对不同均衡器性能的差异化影响。


<details>
  <summary>Details</summary>
Motivation: 在无人机通信与定位一体化系统中，定位误差会影响信道矩阵的重构，进而影响通信性能。需要量化分析定位不准确性对ZF和MMSE均衡器符号错误率的影响。

Method: 采用PASCAL系统模型，通过诺伊曼近似和泰勒近似的混合方法，分别推导了ZF和MMSE均衡器的紧密近似SER表达式。

Result: 分析发现：1）所有无人机的定位误差都会影响单个无人机的SER；2）ZF不受距离估计误差影响，而MMSE受影响；3）角度估计误差对两种均衡器影响最大；4）ZF对定位误差高度敏感，在某些大误差条件下甚至不如MRC。

Conclusion: 定位误差对ZF和MMSE均衡器性能有显著影响，ZF对定位误差更敏感。研究结果为无人机通信定位一体化系统设计提供了重要指导。

Abstract: In this paper, a symbol error rate (SER) analysis is provided to evaluate the impact of localization inaccuracy on the communication performance under Zero-Forcing (ZF) and Minimum Mean-Square Error (MMSE) equalizers. Specifically, we adopt a pilot-aided simultaneous communication and localization (PASCAL) system, in which multiple drones actively transmit signals towards the base station (BS). Upon receiving the signal, the BS estimates the drones' location parameters to reconstruct the channel matrix, which is then utilized for ZF and MMSE equalization. As the channel matrix is characterized by the estimated parameters associated with the target's location and the matrix inversion involved in ZF and MMSE further complicates the analysis, obtaining a closed-form SER expression becomes intractable. Thus, a tightly approximated SER expression is respectively derived for ZF and MMSE by using a hybrid approximation method incorporating Neumann approximation and Taylor approximation. Our analysis reveals several important design insights: first, the average SER of drone $k$ for both ZF and MMSE can be affected by the localization errors from all drones including drone $k$; second, the average SER of ZF is unaffected by the estimation inaccuracy of range, whereas the average SER of MMSE is influenced by it; third, ZF and MMSE is the most susceptible to the influence of angle estimation errors compared to the other localization errors; fourth, ZF is highly sensitive to localization errors and may be even worse than maximal ratio combining (MRC) under some conditions of significant estimation errors. Numerical simulation results verify our findings and also validate the accuracy of the analysis across a wide range of system parameters.

</details>


### [30] [TransCoder: A Neural-Enhancement Framework for Channel Codes](https://arxiv.org/abs/2511.22539)
*Anastasiia Kurmukova,Selim F. Yilmaz,Emre Ozfatura,Deniz Gunduz*

Main category: cs.IT

TL;DR: TransCoder：基于Transformer的神经传输方案，通过迭代解码增强现有纠错码性能，计算复杂度与传统解码器相当，特别适用于长码和低码率场景。


<details>
  <summary>Details</summary>
Motivation: 神经网络解码器虽能提升纠错码可靠性，但计算复杂度高阻碍实际部署。需要设计一种既能提升性能又保持合理复杂度的解决方案。

Method: 提出TransCoder框架，使用Transformer架构作为码自适应神经模块，可灵活部署在发射端、接收端或两端。采用迭代解码过程，通过块注意力机制高效处理信道噪声信息和传统解码器更新。

Result: 在多种传统码（LDPC、BCH、Polar、Turbo）和广泛信道条件下，TransCoder显著改善块错误率性能，计算复杂度与传统解码器相当。特别在长码（块长>64）和低码率场景表现优异。

Conclusion: TransCoder为资源受限无线设备提供了一种实用的可靠通信解决方案，在保持合理计算复杂度的同时显著提升现有纠错码性能。

Abstract: Reliable communication over noisy channels requires the design of specialized error-correcting codes (ECCs) tailored to specific system requirements. Recently, neural network-based decoders have emerged as promising tools for enhancing ECC reliability, yet their high computational complexity prevents their potential practical deployment. In this paper, we take a different approach and design a neural transmission scheme that employs the transformer architecture in order to improve the reliability of existing ECCs. We call this approach TransCoder, alluding both to its function and architecture. TransCoder operates as a code-adaptive neural module aimed at performance enhancement that can be implemented flexibly at either the transmitter, receiver, or both. The framework employs an iterative decoding procedure, where both noisy information from the channel and updates from the conventional ECC decoder are processed by a neural decoder block, utilizing a block attention mechanism for efficiency. Through extensive simulations with various conventional codes (LDPC, BCH, Polar, and Turbo) and across a wide range of channel conditions, we demonstrate that TransCoder significantly improves block error rate (BLER) performance while maintaining computational complexity comparable to traditional decoders. Notably, our approach is particularly effective for longer codes (block length >64) and at lower code rates, scenarios in which existing neural decoders often struggle (despite their formidable computational complexity). The results establish TransCoder as a promising practical solution for reliable communication among resource-constrained wireless devices.

</details>


### [31] [Maximum Spectral Efficiency With Adaptive MQAM Transmissions Over Terrestrial Coherent FSO Links](https://arxiv.org/abs/2511.22682)
*Himani Verma,Kamal Singh,Ranjan K. Mallik*

Main category: cs.IT

TL;DR: 自适应MQAM在自由空间光通信中的性能分析，仅用6种调制格式即可接近理论极限


<details>
  <summary>Details</summary>
Motivation: 自由空间光通信是下一代无线网络超高速前传和回传的关键技术，但自适应MQAM在陆地FSO信道上的理论分析仍然有限

Method: 首先推导了在gamma-gamma湍流和指向误差下自适应无约束MQAM的频谱效率极限，然后分析仅使用6种方形MQAM星座的自适应传输性能

Result: 仅使用6种方形MQAM的自适应传输在广泛的信噪比和信道条件下，性能接近理论极限（差距仅0.10-0.12 bits/s/Hz）

Conclusion: 自适应MQAM传输在实际FSO系统中具有可行性，仅需少量调制格式即可实现接近理论极限的性能

Abstract: Coherent free-space optical (FSO) communication is recognized as a key enabler for ultra-high-capacity fronthaul and backhaul links in next-generation wireless networks. Spectrally efficient $M$-ary quadrature amplitude modulation (MQAM) formats are well-suited for these links. However, theoretical analyses of adaptive MQAM transmissions over terrestrial FSO channels remain limited. In this letter, we first derive the spectral efficiency limit of adaptive unconstrained MQAM over gamma-gamma turbulence with pointing error. We then show that adaptive transmissions using only six square MQAM constellations performs close to the theoretical limit (within $0.10$-$0.12$ bits/s/Hz) across a wide range of signal-to-noise ratios and channel conditions.

</details>


### [32] [On Information Theoretic Fairness With A Bounded Point-Wise Statistical Parity Constraint: An Information Geometric Approach](https://arxiv.org/abs/2511.22683)
*Amirreza Zamani,Ayfer Özgür,Mikael Skoglund*

Main category: cs.IT

TL;DR: 提出一种在点式统计公平性约束下设计公平表示的信息论方法，通过最大化任务信息同时满足压缩率约束


<details>
  <summary>Details</summary>
Motivation: 研究在无法直接访问敏感属性和任务变量的情况下，如何设计满足点式统计公平性约束的表示，以平衡隐私保护与任务效用

Method: 使用信息几何方法近似KL散度和互信息，将复杂的公平设计问题转化为二次优化问题，通过矩阵最大奇异值和奇异向量求解

Result: 在某些约束下获得闭式解，其他情况提供低计算复杂度的下界，数值实验显示与最优解性能接近

Conclusion: 提出了一种高效的点式统计公平表示设计方法，在保护敏感信息的同时最大化任务相关信息，具有实际应用价值

Abstract: In this paper, we study an information-theoretic problem of designing a fair representation under a bounded point-wise statistical (demographic) parity constraint. More specifically, an agent uses some useful data (database) $X$ to solve a task $T$. Since both $X$ and $T$ are correlated with some latent sensitive attribute or secret $S$, the agent designs a representation $Y$ that satisfies a bounded point-wise statistical parity, that is, such that for all realizations of the representation $y\in\cal Y$, we have $χ^2(P_{S|y};P_S)\leq ε$. In contrast to our previous work, here we use the point-wise measure instead of a bounded mutual information, and we assume that the agent has no direct access to $S$ and $T$; hence, the Markov chains $S - X - Y$ and $T - X - Y$ hold. In this work, we design $Y$ that maximizes the mutual information $I(Y;T)$ about the task while satisfying a bounded compression rate constraint, that is, ensuring that $I(Y;X) \leq r$. Finally, $Y$ satisfies the point-wise bounded statistical parity constraint $χ^2(P_{S|y};P_S)\leq ε$. When $ε$ is small, concepts from information geometry allow us to locally approximate the KL-divergence and mutual information. To design the representation $Y$, we utilize this approximation and show that the main complex fairness design problem can be rewritten as a quadratic optimization problem that has simple closed-form solution under certain constraints. For the cases where the closed-form solution is not obtained we obtain lower bounds with low computational complexity. Here, we provide simple fairness designs with low complexity which are based on finding the maximum singular value and singular vector of a matrix. Finally, in a numerical example we compare our obtained results with the optimal solution.

</details>


### [33] [Leveraging Channel Knowledge Map for Multi-User Hierarchical Beam Training Under Position Uncertainty](https://arxiv.org/abs/2511.22902)
*Xu Shi,Haohan Wang,Yashuai Cao,Hengyu Zhang,Sufang Yang,Jintao Wang*

Main category: cs.IT

TL;DR: 提出一种基于信道知识地图(CKM)的波束训练框架，通过分层搜索和位置剪枝策略解决位置不确定性和多用户干扰问题，显著降低训练开销。


<details>
  <summary>Details</summary>
Motivation: CKM在实际波束训练中面临两大挑战：1) 用户精确位置在训练前通常未知，限制了CKM的效用；2) CKM、实时观测和训练策略之间的复杂交互尚未深入研究，导致性能次优和实施困难。

Method: 针对单用户场景提出奖励驱动的波束-潜力分层策略，将用户位置不确定性建模为剪枝二叉树，通过评估潜在码字的权重和奖励推导最小开销的最优分层搜索策略，并设计低复杂度的双层前瞻方案。针对多用户场景提出相关性驱动的位置剪枝训练方案，利用用户间干扰的旁瓣增益提供额外侧信息以减少开销。

Result: 仿真验证了所提方法在推进6G波束训练方面的优越性能，显著降低了训练开销并提高了效率。

Conclusion: 提出的CKM辅助波束训练框架有效解决了位置不确定性和多用户干扰问题，为6G波束训练提供了高效实用的解决方案。

Abstract: Channel knowledge map (CKM) emerges as a promising framework to acquire location-specific channel information without consuming wireless resources, creating new horizons for advanced wireless network design and optimization. Despite its potential, the practical application of CKM in beam training faces several challenges. On one hand, the user's precise location is typically unavailable prior to beam training, which limits the utility of CKM since its effectiveness relies heavily on accurate input of position data. On the other hand, the intricate interplay among CKM, real-time observations, and training strategies has not been thoroughly studied, leading to suboptimal performance and difficulties in practical implementation. In this paper, we present a framework for CKM-aided beam training that addresses these limitations. For single-user scenario, we propose a reward-motivated beam-potential hierarchical strategy which integrates partial position information and CKM. This strategy models the user equipment (UE) position uncertainty and formulates the hierarchical searching process as a pruned binary search tree. An optimal hierarchical searching strategy with minimal overhead is derived by evaluating the weights and rewards of potential codewords. Furthermore, a low-complexity two-layer lookahead scheme is designed to balance overhead and computational demands. For multi-user scenario, we develop a correlation-driven position-pruning training scheme, where sidelobe gains from inter-user interference are exploited to provide additional side information for overhead reduction, allowing all users to be simultaneously assigned their respective supportive beams. Simulations validate the superior performances of proposed approaches in advancing 6G beam training.

</details>


### [34] [Decoding Trombetti-Zhou codes: a new syndrome-based decoding approach](https://arxiv.org/abs/2511.23202)
*Chunlei Li,Angelica Piccirillo,Olga Polverino,Ferdinando Zullo*

Main category: cs.IT

TL;DR: 本文为Trombetti-Zhou提出的$\mathbb{F}_{q^n}$-线性最大秩距离码设计了一种新的基于伴随式的译码算法，通过引入$\mathbb{F}_{q^n}$-生成矩阵和$\mathbb{F}_{q^n}$-校验矩阵的概念，将译码问题转化为Gabidulin码的译码问题。


<details>
  <summary>Details</summary>
Motivation: Trombetti-Zhou码是2019年提出的$\mathbb{F}_{q^n}$-线性最大秩距离码，但只在其子域$\mathbb{F}_{q^n}$上线性，缺乏完整的线性结构。传统的基于伴随式的译码算法依赖于线性码的校验矩阵，因此需要为这类$\mathbb{F}_{q^n}$-线性秩度量码开发新的译码方法。

Method: 1. 引入$\mathbb{F}_{q^n}$-生成矩阵和$\mathbb{F}_{q^n}$-校验矩阵的概念，类比线性码中的生成矩阵和校验矩阵；2. 为Trombetti-Zhou码构造具体的$\mathbb{F}_{q^n}$-生成矩阵和$\mathbb{F}_{q^n}$-校验矩阵，使用迹几乎对偶基；3. 当错误向量的秩权重$t < \frac{d-1}{2}$时，将译码转化为维度大1的Gabidulin码的译码；4. 当$t = \frac{d-1}{2}$时，将译码简化为确定某个矩阵的秩。

Result: 成功为Trombetti-Zhou码设计了基于伴随式的译码算法，建立了$\mathbb{F}_{q^n}$-线性秩度量码的代数框架，并分析了算法的复杂度。当$t < \frac{d-1}{2}$时，译码可转化为Gabidulin码译码；当$t = \frac{d-1}{2}$时，只需确定矩阵的秩。

Conclusion: 本文为Trombetti-Zhou码提供了一种有效的基于伴随式的译码算法，通过引入$\mathbb{F}_{q^n}$-生成矩阵和$\mathbb{F}_{q^n}$-校验矩阵的概念，解决了这类$\mathbb{F}_{q^n}$-线性但非$\mathbb{F}_{q^{2n}}$-线性码的译码问题，为类似结构的秩度量码的译码提供了理论框架。

Abstract: In 2019, Trombetti and Zhou introduced a new family of $\mathbb{F}_{q^n}$-linear Maximum Rank Distance (MRD) codes over $\mathbb{F}_{q^{2n}}$. For such codes we propose a new syndrome-based decoding algorithm. It is well known that a syndrome-based decoding approach relies heavily on a parity-check matrix of a linear code. Nonetheless, Trombetti-Zhou codes are not linear over the entire field $\mathbb{F}_{q^{2n}}$, but only over its subfield $\mathbb{F}_{q^{n}}$. Due to this lack of linearity, we introduce the notions of $\mathbb{F}_{q^{n}}$-generator matrix and $\mathbb{F}_{q^{n}}$-parity-check matrix for a generic $\mathbb{F}_{q^{n}}$-linear rank-metric code over $\mathbb{F}_{q^{rn}}$ in analogy with the roles that generator and parity-check matrices play in the context of linear codes. Accordingly, we present an $\mathbb{F}_{q^n}$-generator matrix and $\mathbb{F}_{q^n}$-parity-check matrix for Trombetti-Zhou codes as evaluation codes over an $\mathbb{F}_q$-basis of $\mathbb{F}_{q^{2n}}$. This relies on the choice of a particular basis called \emph{trace almost dual basis}. Subsequently, denoting by $d$ the minimum distance of the code, we show that if the rank weight $t$ of the error vector is strictly smaller than $\frac{d-1}{2}$, the syndrome-based decoding of Trombetti-Zhou codes can be converted to the decoding of Gabidulin codes of dimension one larger. On the other hand, when $t=\frac{d-1}{2}$, we reduce the decoding to determining the rank of a certain matrix. The complexity of the proposed decoding for Trombetti-Zhou codes is also discussed.

</details>


### [35] [Efficient Estimation of Sum-Parameters for Multi-Component Complex Exponential Signals with Theoretical Cramer-Rao Bound Analysis](https://arxiv.org/abs/2511.23318)
*Huiguang Zhang*

Main category: cs.IT

TL;DR: 提出基于低维和参数的新框架，用于多分量复指数信号参数估计，避免传统方法的排列模糊性和计算复杂度问题，实现接近理论性能边界的统计效率。


<details>
  <summary>Details</summary>
Motivation: 传统多分量复指数信号参数估计方法面临三大挑战：1) 分量数量大时的排列模糊性；2) 高维Fisher信息矩阵求逆带来的计算复杂度；3) 模型阶数选择问题。需要一种能避免这些困难的新方法。

Method: 提出基于低维和参数的新框架，包括：振幅和、功率加权频率、相位相关和。这些参数具有清晰的物理解释，完全避免排列模糊性。开发了高效全局估计方法(EGEM)，推导了确定性和随机信号模型下的精确闭式Cramer-Rao界。

Result: 频率和参数实现了与单分量估计器相当的统计效率，同时自动受益于所有信号分量的功率池化。EGEM在宽信噪比范围内表现出渐近效率，在长样本和短样本情况下均显著优于Zoom-Interpolated FFT和Root-MUSIC等现有技术。250个观测值的小样本情况下仍能接近理论性能边界。

Conclusion: 基于和参数的框架为多分量复指数信号参数估计提供了有效解决方案，避免了传统方法的根本性困难，实现了接近理论最优的性能，特别适用于分量数量大的复杂场景。

Abstract: This paper addresses the challenging problem of parameter estimation for multicomponent complex exponential signals, commonly known as sums of cisoids. Traditional approaches that estimate individual component parameters face significant difficulties when the number of components is large, including permutation ambiguity, computational complexity from high-dimensional Fisher information matrix inversion, and model order selection issues. We introduce a novel framework based on low-dimensional sum-parameters that capture essential global characteristics of the signal ensemble. These parameters include the sum of amplitudes, the power-weighted frequency, and the phase-related sum. These quantities possess clear physical interpretations representing total signal strength, power-weighted average frequency, and composite phase information, while completely avoiding permutation ambiguities. We derive exact closed-form Cramer-Rao bounds for these sum-parameters under both deterministic and stochastic signal models. Our analysis reveals that the frequency sumparameter achieves statistical efficiency comparable to single-component estimators while automatically benefiting from power pooling across all signal components. The proposed Efficient Global Estimation Method (EGEM) demonstrates asymptotic efficiency across a wide range of signal-to-noise ratios, significantly outperforming established techniques such as Zoom-Interpolated FFT and Root-MUSIC in both long- and short-sample regimes. Extensive numerical simulations involving 2000 Monte-Carlo trials confirm that EGEM closely approaches the theoretical performance bounds even with relatively small sample sizes of 250 observations.

</details>


### [36] [Quantum Private Distributed Matrix Multiplication With Degree Tables](https://arxiv.org/abs/2511.23406)
*Mohamed Nomeir,Alptug Aytekin,Lei Hu,Sennur Ulukus*

Main category: cs.IT

TL;DR: 量子资源用于提升私有分布式矩阵乘法速率，针对高隐私和低隐私两种机制，分别开发了量子可行性条件和新的编码方案。


<details>
  <summary>Details</summary>
Motivation: 传统私有分布式矩阵乘法需要大量服务器，量子资源（纠缠态和量子信道）有望减少所需服务器数量，提升计算效率。

Method: 1) 高隐私机制：为GASP编码定义量子可行性条件，开发新编码族；2) 低隐私机制：将可行性条件扩展到CAT和DOG编码，提出新编码方案。

Result: 建立了量子可行性条件与矩阵维度、隐私要求的关系，开发了适用于量子设置的新编码方案，提升了私有分布式矩阵乘法的效率。

Conclusion: 量子资源能有效减少私有分布式矩阵乘法所需服务器数量，针对不同隐私机制开发的编码方案为量子计算在分布式计算中的应用提供了新途径。

Abstract: In this paper, we explore how quantum resources can be used to increase the rate of private distributed matrix multiplication (PDMM). In PDMM, a user who has two high-dimensional matrices, $A$ and $B$, and lacks the computational capabilities to apply matrix multiplication locally, divides the matrices $A$ and $B$ into $K$ and $L$ sub-blocks, respectively. Then, the user sends them to $N$ servers to apply the required multiplication privately from any $T$ servers. The goal is to reduce the number of servers needed to perform the required matrix multiplication. In the quantum setting, we allow the servers to share an entangled state and respond over quantum channels. Upon receiving the qudits, the user applies measurements to obtain the required multiplication. There are two main regimes in the PDMM literature: The high-privacy regime and the low-privacy regime where $T$ is less than $K$ and $L$.
  First, in the high-privacy regime, the state-of-the-art classical code is called the gap additive secure polynomial (GASP) code. We define a feasibility requirement in the quantum setting for the GASP code such that the highest performance is achieved when it is satisfied. When it is not satisfied, we address two main concerns. The first is to find a relation between the minimum privacy requirement and the dimensions of the two matrices needed for the feasibility condition to be satisfied. Second, we develop a new family of codes that can work in the quantum setting.
  Second, since GASP does not work efficiently in the low-privacy regimes compared to cyclic-addition degree tables (CAT) and discretely optimized GASP (DOG), we show that the feasibility condition developed for GASP can be adopted for both CAT and DOG codes as well. In addition, we propose another set of codes that can be used in the low privacy regime in the quantum setting when the feasibility requirement is not satisfied.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [37] [A Combinatorial Characterization of Constant Mixing Time](https://arxiv.org/abs/2511.21868)
*Lap Chi Lau,Raymond Liu*

Main category: cs.DS

TL;DR: 该论文提出了具有常数混合时间图的组合特征，基于小集二分密度条件，该条件比具有接近最优谱半径更弱，但比具有接近最优小集顶点扩展更强。


<details>
  <summary>Details</summary>
Motivation: 经典谱图理论描述了具有对数混合时间的图，但缺乏对具有常数混合时间图的组合特征。本文旨在填补这一空白，为常数混合时间图提供组合特征。

Method: 提出基于小集二分密度条件的组合特征方法。该条件介于谱半径条件和小集顶点扩展条件之间，为常数混合时间提供了更精细的组合刻画。

Result: 建立了具有常数混合时间图的组合特征，证明了小集二分密度条件与常数混合时间之间的等价关系，该条件比谱半径条件更弱但比顶点扩展条件更强。

Conclusion: 成功为常数混合时间图提供了组合特征，小集二分密度条件是一个合适的组合特征，填补了经典谱图理论在常数混合时间图特征方面的空白。

Abstract: Classical spectral graph theory characterizes graphs with logarithmic mixing time. In this work, we present a combinatorial characterization of graphs with constant mixing time. The combinatorial characterization is based on the small-set bipartite density condition, which is weaker than having near-optimal spectral radius and is stronger than having near-optimal small-set vertex expansion.

</details>


### [38] [Differential privacy from axioms](https://arxiv.org/abs/2511.21876)
*Guy Blanc,William Pires,Toniann Pitassi*

Main category: cs.DS

TL;DR: 该论文证明任何满足四个核心公理的隐私度量都等价于差分隐私，表明在统计设置中无法找到比DP更弱但仍合理的隐私概念。


<details>
  <summary>Details</summary>
Motivation: 探索是否存在比差分隐私更弱的隐私概念，这些概念能否在保持基本隐私保护的同时，更高效地实现隐私保护或适用于更广泛的任务。

Method: 提出四个核心公理：预处理不变性、禁止公然非隐私、强组合性和线性可扩展性。证明任何满足这些公理的隐私度量都等价于差分隐私（在样本复杂度上相差多项式因子）。

Result: 主要定理表明：在统计设置中，任何合理的隐私度量（满足四个公理）都等价于差分隐私。同时证明这些公理是最小化的，移除任何一个都会导致病态的隐私度量。

Conclusion: 即使考虑平均情况放松，也无法找到比差分隐私更弱但仍合理的隐私概念。差分隐私在满足基本隐私要求方面具有根本性的地位，任何合理的隐私度量最终都会归结为DP。

Abstract: Differential privacy (DP) is the de facto notion of privacy both in theory and in practice. However, despite its popularity, DP imposes strict requirements which guard against strong worst-case scenarios. For example, it guards against seemingly unrealistic scenarios where an attacker has full information about all but one point in the data set, and still nothing can be learned about the remaining point. While preventing such a strong attack is desirable, many works have explored whether average-case relaxations of DP are easier to satisfy [HWR13,WLF16,BF16,LWX23].
  In this work, we are motivated by the question of whether alternate, weaker notions of privacy are possible: can a weakened privacy notion still guarantee some basic level of privacy, and on the other hand, achieve privacy more efficiently and/or for a substantially broader set of tasks? Our main result shows the answer is no: even in the statistical setting, any reasonable measure of privacy satisfying nontrivial composition is equivalent to DP. To prove this, we identify a core set of four axioms or desiderata: pre-processing invariance, prohibition of blatant non-privacy, strong composition, and linear scalability. Our main theorem shows that any privacy measure satisfying our axioms is equivalent to DP, up to polynomial factors in sample complexity. We complement this result by showing our axioms are minimal: removing any one of our axioms enables ill-behaved measures of privacy.

</details>


### [39] [Identifying all snarls and superbubbles in linear-time, via a unified SPQR-tree framework](https://arxiv.org/abs/2511.21919)
*Francisco Sena,Aleksandr Politov,Corentin Moumard,Manuel Cáceres,Sebastian Schmidt,Juha Harviainen,Alexandru I. Tomescu*

Main category: cs.DS

TL;DR: 提出首个线性时间算法识别所有snarls，并基于统一框架提供新的线性时间算法寻找superbubbles，利用SPQR树分解高效发现pangenomics中的气泡状结构。


<details>
  <summary>Details</summary>
Motivation: Snarls和superbubbles是pangenomics中的基本分解结构，用于捕获变异位点，支持结构变异基因分型、距离索引、单倍型采样和变异注释等关键任务。现有方法要么无法在线性时间内找到所有snarls，要么superbubbles算法过于专门化。

Method: 基于所有气泡状结构都被两个顶点与图的其余部分分离的观察（即端点构成底层无向图的2-分离器），利用SPQR树分解来编码所有2-分离器，通过遍历高效发现这些结构。提出了新的snarls表示方法，大小与输入图成线性关系。

Result: 实现了首个线性时间识别所有snarls的算法，比vg快2倍；新的superbubbles算法比BubbleGun快50倍。在多种pangenomic数据集上评估，性能优于或与现有方法相当。

Conclusion: SPQR树框架为pangenomics中的气泡状结构提供了统一视角，并为高效发现其他气泡状结构提供了模板，解决了长期存在的snarls线性时间识别问题。

Abstract: Snarls and superbubbles are fundamental pangenome decompositions capturing variant sites. These bubble-like structures underpin key tasks in computational pangenomics, including structural-variant genotyping, distance indexing, haplotype sampling, and variant annotation. Snarls can be quadratically-many in the size of the graph, and since their introduction in 2018 with the vg toolkit, there has been no work on identifying all snarls in linear time. Moreover, while it is known how to find superbubbles in linear time, this result is a highly specialized solution only achieved after a long series of papers.
  We present the first algorithm identifying all snarls in linear time. This is based on a new representation of all snarls, of size linear in the input graph size, and which can be computed in linear time. Our algorithm is based on a unified framework that also provides a new linear-time algorithm for finding superbubbles. An observation behind our results is that all such structures are separated from the rest of the graph by two vertices (except for cases which are trivially computable), i.e. their endpoints are a 2-separator of the underlying undirected graph. Based on this, we employ the well-known SPQR tree decomposition, which encodes all 2-separators, to guide a traversal that finds the bubble-like structures efficiently.
  We implemented our algorithms in C++ (available at https://github.com/algbio/BubbleFinder) and evaluated them on various pangenomic datasets. Our algorithms outcompete or they are on the same level of existing methods. For snarls, we are up to two times faster than vg, while identifying all snarls. When computing superbubbles, we are up to 50 times faster than BubbleGun. Our SPQR tree framework provides a unifying perspective on bubble-like structures in pangenomics, together with a template for finding other bubble-like structures efficiently.

</details>


### [40] [MagnifierSketch: Quantile Estimation Centered at One Point](https://arxiv.org/abs/2511.22070)
*Jiarui Guo,Qiushi Lyu,Yuhan Wu,Haoyu Li,Zhaoqian Yao,Yuqi Dong,Xiaolin Wang,Bin Cui,Tong Yang*

Main category: cs.DS

TL;DR: MagnifierSketch：一种针对数据流中单点分位数（如0.95分位数）估计的高效算法，支持单键和每键分位数估计，相比现有方法显著提高精度和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有分位数估计算法未专门针对单点分位数（如尾延迟测量中的0.95分位数）进行优化，在每键分位数估计场景下精度不足，且吞吐量无法满足高速数据流处理需求。

Method: 提出MagnifierSketch算法，采用Value Focus（值聚焦）、Distribution Calibration（分布校准）和Double Filtration（双重过滤）三项关键技术，支持单键和每键分位数估计，并提供严格的数学证明。

Result: 实验结果显示MagnifierSketch的平均误差显著低于现有最优方法，在单键和每键场景下均表现优异；在RocksDB数据库中的实现有效降低了分位数查询延迟。

Conclusion: MagnifierSketch为解决数据流中单点分位数估计问题提供了高效解决方案，在精度和吞吐量方面均优于现有方法，并已在实际数据库系统中验证其有效性。

Abstract: In this paper, we take into consideration quantile estimation in data stream models, where every item in the data stream is a key-value pair. Researchers sometimes aim to estimate per-key quantiles (i.e. quantile estimation for every distinct key), and some popular use cases, such as tail latency measurement, recline on a predefined single quantile (e.g. 0.95- or 0.99- quantile) rather than demanding arbitrary quantile estimation. However, existing algorithms are not specially designed for per-key estimation centered at one point. They cannot achieve high accuracy in our problem setting, and their throughput are not satisfactory to handle high-speed items in data streams. To solve this problem, we propose MagnifierSketch for point-quantile estimation. MagnifierSketch supports both single-key and per-key quantile estimation, and its key techniques are named Value Focus, Distribution Calibration and Double Filtration. We provide strict mathematical derivations to prove the unbiasedness of MagnifierSketch and show its space and time complexity. Our experimental results show that the Average Error (AE) of MagnifierSketch is significantly lower than the state-of-the-art in both single-key and per-key situations. We also implement MagnifierSketch on RocksDB database to reduce quantile query latency in real databases. All related codes of MagnifierSketch are open-sourced and available at GitHub.

</details>


### [41] [How fast are algorithms reducing the demands on memory? A survey of progress in space complexity](https://arxiv.org/abs/2511.22084)
*Hayden Rome,Jayson Lynch,Jeffery Li,Chirag Falor,Neil Thompson*

Main category: cs.DS

TL;DR: 本文首次对算法研究中内存使用（空间复杂度）的改进进行了广泛调查，分析了118个最重要的计算机科学算法问题及其800多个算法，发现空间复杂度在近年变得越来越重要，20%的情况下算法进步比硬件进步更能减少内存访问延迟。


<details>
  <summary>Details</summary>
Motivation: 传统算法研究主要关注处理器操作次数（时间复杂度），但对于许多问题，运行时间和能耗主要由内存访问决定。随着"内存墙"问题的出现，需要了解算法进步如何改善内存使用效率。

Method: 对118个最重要的计算机科学算法问题进行了系统性调查，分析了800多个解决这些问题的算法，评估了空间复杂度的改进情况，并创建了算法维基参考网站。

Result: 研究发现：1）空间复杂度在近年变得越来越重要；2）在20%的情况下，对于大规模问题（n=10亿），空间复杂度的改进超过了DRAM访问速度的改进，表明算法进步比硬件进步更能减少内存访问延迟；3）出现了算法帕累托前沿，即改善时间复杂度会导致空间复杂度变差，反之亦然。

Conclusion: 程序员需要越来越多地考虑多种算法选项，理解哪种最适合特定问题。为此，研究者创建了https://algorithm-wiki.csail.mit.edu参考网站，帮助理论家和实践者权衡这些折衷。

Abstract: Algorithm research focuses primarily on how many operations processors need to do (time complexity). But for many problems, both the runtime and energy used are dominated by memory accesses. In this paper, we present the first broad survey of how algorithmic progress has improved memory usage (space complexity). We analyze 118 of the most important algorithm problems in computer science, reviewing the 800+ algorithms used to solve them.
  Our results show that space complexity has become much more important in recent years as worries have arisen about memory access bottle-necking performance (the ``memory wall''). In 20% of cases we find that space complexity improvements for large problems (n=1 billion) outpaced improvements in DRAM access speed, suggesting that for these problems algorithmic progress played a larger role than hardware progress in minimizing memory access delays. Increasingly, we also see the emergence of algorithmic Pareto frontiers, where getting better asymptotic time complexity for a problem requires getting worse asymptotic space complexity, and vice-versa. This tension implies that programmers will increasingly need to consider multiple algorithmic options to understand which is best for their particular problem. To help theorists and practitioners alike consider these trade-offs, we have created a reference for them at https://algorithm-wiki.csail.mit.edu.

</details>


### [42] [Balancing Two-Dimensional Straight-Line Programs](https://arxiv.org/abs/2511.22212)
*Itai Boneh,Estéban Gabory,Paweł Gawrychowski,Adam Górkiewicz*

Main category: cs.DS

TL;DR: 本文研究二维字符串的SLP平衡问题，证明无法在保持线性大小的同时获得对数深度，但通过引入"带洞SLP"可以实现对数深度平衡，从而支持高效随机访问。


<details>
  <summary>Details</summary>
Motivation: 在一维字符串中，已知可以通过构建深度为O(log N)的等价SLP来实现O(g)大小和O(log N)时间的随机访问。本文探讨二维字符串是否也能实现类似的结果，即能否构建大小相近且深度较小的等价SLP。

Method: 1. 证明下界：构造无限族二维字符串，证明任何深度为O(log N)的2D SLP必须具有Ω(g·N/log³N)的大小；2. 提供上界：展示如何构建大小为O(g·N)的2D SLP；3. 引入"带洞2D SLP"概念，利用已知平衡定理构建深度O(log N)、大小O(g)的结构；4. 扩展结构以获得更优的时间复杂度。

Result: 1. 对于二维字符串，无法在保持线性大小的同时获得对数深度；2. 通过带洞2D SLP可以实现深度O(log N)和大小O(g)的平衡；3. 可以构建大小为O(g)的结构支持O(log N)时间的随机访问；4. 进一步可构建大小为O(g log^ε N)的结构支持O(log N/log log N)时间的随机访问。

Conclusion: 二维字符串的SLP平衡问题比一维更复杂，无法直接获得线性大小和对数深度的等价SLP，但通过引入带洞SLP的概念，可以实现有效的平衡，从而支持高效的随机访问查询。

Abstract: We consider building, given a straight-line program (SLP) consisting of $g$ productions deriving a two-dimensional string $T$ of size $N\times N$, a structure capable of providing random access to any character of $T$. For one-dimensional strings, it is now known how to build a structure of size $\mathcal{O}(g)$ that provides random access in $\mathcal{O}(\log N)$ time. In fact, it is known that this can be obtained by building an equivalent SLP of size $\mathcal{O}(g)$ and depth $\mathcal{O}(\log N)$ [Ganardi, Jeż, Lohrey, JACM 2021]. We consider the analogous question for two-dimensional strings: can we build an equivalent SLP of roughly the same size and small depth?
  We show that the answer is negative: there exists an infinite family of two-dimensional strings of size $N\times N$ described by a 2D SLP of size $g$ such that any 2D SLP describing the same string of depth $\mathcal{O}(\log N)$ must be of size $Ω(g\cdot N/\log^{3}N)$. We complement this with an upper bound showing how to construct such a 2D SLP of size $\mathcal{O}(g\cdot N)$. Next, we observe that one can naturally define a generalization of 2D SLP, which we call 2D SLP with holes. We show that a known general balancing theorem by [Ganardi, Jeż, Lohrey, JACM 2021] immediately implies that, given a 2D SLP of size $g$ deriving a string of size $N\times N$, we can construct a 2D SLP with holes of depth $\mathcal{O}(\log N)$ and size $\mathcal{O}(g)$. This allows us to conclude that there is a structure of size $\mathcal{O}(g)$ providing random access in $\mathcal{O}(\log N)$ time for such a 2D SLP. Further, this can be extended (analogously as for a 1D SLP) to obtain a structure of size $\mathcal{O}(g \log^εN)$ providing random access in $\mathcal{O}(\log N/\log \log N)$ time, for any $ε>0$.

</details>


### [43] [Efficient Trace Frequency Queries in Sparse Graphs](https://arxiv.org/abs/2511.22289)
*Christine Awofeso,Pål Grønås Drange,Patrick Greaves,Oded Lachish,Felix Reidl*

Main category: cs.DS

TL;DR: 提出基于强2-着色数的高效迹查询数据结构，在稀疏图上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 在图中高效计算顶点集X的迹（外部顶点与X的交集）及其频率是图分析的基础任务。现有方法在d-退化图上查询时间随|X|指数增长，不实用。

Method: 使用比退化度更强的参数——强2-着色数s₂，构建在O(d·||G||)时间内初始化的数据结构，查询时间为O((d² + s₂^{d+2})|X|)。提出简单启发式算法计算小强2-着色数的排序。

Result: 在217个真实网络（最多110万边）上测试，新方法在几乎所有设置中都优于简单基线。强2-着色数计算在实践中可行。

Conclusion: 基于强2-着色数的数据结构在稀疏图上高效实用，解决了迹查询的指数时间瓶颈，为图分析提供了新工具。

Abstract: Understanding how a vertex relates to a set of vertices is a fundamental task in graph analysis. Given a graph $G$ and a vertex set $X \subseteq V(G)$, consider the collection of subsets of the form $N(u) \cap X$ where $u$ ranges over all vertices outside $X$. These intersections, which we call the traces of $X$, capture all ways vertices in $G$ connect to $X$, and in this paper we consider the problem of listing these traces efficiently, and the related problem of recording the multiplicity (frequency) of each trace. For a given query set $X$, both problems have obvious algorithms with running time $O(|N(X)| \cdot |X|)$ and conditional lower bounds suggest that, on general graphs, one cannot expect better. However, in certain sparse graph classes, more efficient algorithms are possible: Drange \etal (IPEC 2023) used a data structure that answers trace queries in $d$-degenerate graphs with linear initialisation time and query time that only depends on the query set $X$ and $d$. However, the query time is exponential in $|X|$, which makes this approach impractical. By using a stronger parameter than degeneracy, namely the strong $2$-colouring number $s_2$, we construct a data structure in $O(d \cdot \|G\|)$ time, which answers subsequent trace frequency queries in time $O\big((d^2 + s_2^{d+2})|X|\big)$, where $\|G\|$ is the number of edges of $G$, $s_2$ is the strong $2$-colouring number and $d$ the degeneracy of a suitable ordering of $G$. We demonstrate that this data structure is indeed practical and that it beats the simple, obvious alternative in almost all tested settings, using a collection of 217 real-world networks with up to 1.1M edges. As part of this effort, we demonstrate that computing an ordering with a small strong $2$-colouring number is feasible with a simple heuristic.

</details>


### [44] [Improved exploration of temporal graphs](https://arxiv.org/abs/2511.22604)
*Paul Bastide,Carla Groenland,Lukas Michel,Clément Rambaud*

Main category: cs.DS

TL;DR: 本文改进了时间图探索问题的上界，从O(n^{7/4})提升到O(n^{3/2}√(logn))，并引入平均时间最大度D，得到更一般的O(n^{3/2}√(D logn))上界。


<details>
  <summary>Details</summary>
Motivation: 时间图探索问题是计算访问所有顶点的最短序列长度。先前研究在每时刻图连通且有界最大度的情况下，已有O(n^{7/4})上界，但仍有改进空间。

Method: 引入平均时间最大度D的概念，定义为所有顶点在各时刻图中最大度的平均值。基于此提出更一般的分析方法，将探索时间与D关联。

Result: 证明存在O(n^{3/2}√(D logn))时间步的探索序列。特别地，当D为常数时，得到O(n^{3/2}√(logn))上界，显著改进先前O(n^{7/4})结果。

Conclusion: 本文显著改进了时间图探索问题的上界，引入的平均时间最大度概念为平面图、有界树宽图等特殊情形提供了统一分析框架，当D=o(n/logn)时即获得次二次上界。

Abstract: A temporal graph $G$ is a sequence $(G_t)_{t \in I}$ of graphs on the same vertex set of size $n$. The \emph{temporal exploration problem} asks for the length of the shortest sequence of vertices that starts at a given vertex, visits every vertex, and at each time step $t$ either stays at the current vertex or moves to an adjacent vertex in $G_t$. Bounds on the length of a shortest temporal exploration have been investigated extensively. Perhaps the most fundamental case is when each graph $G_t$ is connected and has bounded maximum degree. In this setting, Erlebach, Kammer, Luo, Sajenko, and Spooner [ICALP 2019] showed that there exists an exploration of $G$ in $\mathcal{O}(n^{7/4})$ time steps. We significantly improve this bound by showing that $\mathcal{O}(n^{3/2} \sqrt{\log n})$ time steps suffice.
  In fact, we deduce this result from a much more general statement. Let the \emph{average temporal maximum degree} $D$ of $G$ be the average of $\max_{t \in I} d_{G_t}(v)$ over all vertices $v \in V(G)$, where $d_{G_t}(v)$ denotes the degree of $v$ in $G_t$. If each graph $G_t$ is connected, we show that there exists an exploration of $G$ in $\mathcal{O}(n^{3/2} \sqrt{D \log n})$ time steps. In particular, this gives the first subquadratic upper bound when the underlying graph has bounded average degree. As a special case, this also improves the previous best bounds when the underlying graph is planar or has bounded treewidth and provides a unified approach for all of these settings. Our bound is subquadratic already when $D=o(n/\log n)$.

</details>


### [45] [Sublinear Edge Fault Tolerant Spanners for Hypergraphs](https://arxiv.org/abs/2511.22803)
*Jialin He,Nicholas Popescu,Chunjiang Zhu*

Main category: cs.DS

TL;DR: 本文首次研究超图中的容错生成器，提出基于聚类的算法构建边容错超生成器，获得次线性规模，并建立了下界证明存在多项式差距。


<details>
  <summary>Details</summary>
Motivation: 容错生成器在网络故障下保持近似距离，在网络设计和分布式系统中有重要应用。虽然经典（无故障）生成器可以轻松扩展到超图，但在容错设置中，简单方法只能获得与故障数f线性相关的规模，而最优规模应该是次线性的。

Method: 采用基于聚类的算法，结合容错聚类技术。对于n节点m边的超图（秩为r，参数k），算法以高概率构建拉伸为2k-1的边容错超生成器，规模为O(k²f¹⁻¹/(rk)n¹⁺¹/k log n)，时间复杂度为Õ(mr³+fn)。还通过结合乘法边容错超生成器和加法超生成器来构建加法边容错超生成器。

Result: 算法实现了次线性规模边界O(k²f¹⁻¹/(rk)n¹⁺¹/k log n)，建立了下界Ω(f¹⁻¹/r⁻¹/rk n¹⁺¹/k⁻o(1))，两者之间存在poly(k)f¹/r的差距。算法时间复杂度为Õ(mr³+fn)。

Conclusion: 本文首次系统研究超图中的容错生成器问题，揭示了容错设置与无故障设置的显著差异，提出了有效的聚类算法获得次线性规模，建立了理论下界，为未来开发最优容错超图生成器奠定了基础。

Abstract: We initiate the study on fault-tolerant spanners in hypergraphs and develop fast algorithms for their constructions. A fault-tolerant (FT) spanner preserves approximate distances under network failures, often used in applications like network design and distributed systems. While classic (fault-free) spanners are believed to be easily extended to hypergraphs such as by the method of associated graphs, we reveal that this is not the case in the fault-tolerant setting: simple methods can only get a linear size in the maximum number of faults $f$. In contrast, all known optimal size of FT spanners are sublinear in $f$. Inspired by the FT clustering technique, we propose a clustering based algorithm that achieves an improved sublinear size bound. For an $n$-node $m$-edge hypergraph with rank $r$ and a sketch parameter $k$, our algorithm constructs edge FT (EFT) hyperspanners of stretch $2k-1$ and size $O(k^2f^{1-1/(rk)}n^{1+1/k}\log n)$ with high probability in time $\widetilde{O}(mr^3+fn)$. We also establish a lower bound of $Ω(f^{1-1/r-1/rk}n^{1+1/k-o(1)})$ edges for EFT hyperspanners, which leaves a gap of poly$(k)f^{1/r}$. Finally, we provide an algorithm for constructing additive EFT hyperspanners by combining multiplicative EFT hyperspanners with additive hyperspanners. We believe that our work will spark interest in developing optimal FT spanners for hypergraphs.

</details>


### [46] [Spanning Trees with a Small Vertex Cover: the Complexity on Specific Graph Classes](https://arxiv.org/abs/2511.22912)
*Toranosuke Kokai,Akira Suzuki,Takahiro Suzuki,Yuma Tamura,Xiao Zhou*

Main category: cs.DS

TL;DR: MCST问题（最小覆盖生成树）在直径≤2或P5-free图上等价于支配集问题，证明了这些图上的难解性及P5-free子类的可解性。对二分平面图（最大度4）和单位圆盘图证明了NP完全性，解决了开放问题。给出了基于团宽的FPT算法和区间图的线性时间算法。


<details>
  <summary>Details</summary>
Motivation: 研究具有理想性质的生成树是算法理论中的重要课题。本文关注MCST问题，即判断图是否存在顶点覆盖大小不超过k的生成树。该问题在先前研究中存在一些开放问题需要解决。

Method: 1. 证明MCST在直径≤2或P5-free图上等价于支配集问题；2. 分析该等价关系带来的难解性和可解性结果；3. 证明MCST在二分平面图（最大度4）和单位圆盘图上的NP完全性；4. 设计基于团宽参数的FPT算法；5. 为区间图设计线性时间算法。

Result: 1. 揭示了MCST与支配集问题的等价关系；2. 解决了先前研究中的开放问题：证明了MCST在二分平面图（最大度4）和单位圆盘图上的NP完全性；3. 提供了P5-free图子类的可解性结果；4. 设计了基于团宽的FPT算法和区间图的线性时间算法。

Conclusion: MCST问题在多种图类中具有丰富的复杂性结果：在直径≤2或P5-free图上等价于支配集，在二分平面图和单位圆盘图上为NP完全，但在某些P5-free子类中可解。参数化算法和特殊图类的线性算法为问题求解提供了实用工具。

Abstract: In the context of algorithm theory, various studies have been conducted on spanning trees with desirable properties. In this paper, we consider the \textsc{Minimum Cover Spanning Tree} problem (MCST for short). Given a graph $G$ and a positive integer $k$, the problem determines whether $G$ has a spanning tree with a vertex cover of size at most $k$. We reveal the equivalence between \mcst\ and the \textsc{Dominating Set} problem when $G$ is of diameter at most~$2$ or $P_5$-free. This provides the intractability for these graphs and the tractability for several subclasses of $P_5$-free graphs. We also show that \mcst\ is NP-complete for bipartite planar graphs of maximum degree~$4$ and unit disk graphs. These hardness results resolve open questions posed in prior research. Finally, we present an FPT algorithm for {\mcst} parameterized by clique-width and a linear-time algorithm for interval graphs.

</details>


### [47] [Towards an algebraic approach to the reconfiguration CSP](https://arxiv.org/abs/2511.22914)
*Kei Kimura*

Main category: cs.DS

TL;DR: 该论文研究约束满足问题（CSP）的重配置变体，称为重配置CSP（RCSP），提出了一种基于部分操作的新代数方法，用于分析RCSP的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: RCSP在理论计算机科学中受到越来越多的关注，但现有方法主要基于完全操作，难以处理涉及等式约束的归约。需要一种新的代数框架来扩展复杂度结果到更一般的领域。

Method: 提出了一种新颖的代数方法，使用部分操作而非传统的完全操作，特别设计用于捕获涉及等式约束的归约。这种方法受到经典CSP复杂度分析技术的启发。

Result: 该框架能够将复杂度结果从布尔域扩展到更一般的设置，展示了部分操作在识别可处理的RCSP实例方面的多功能性。

Conclusion: 部分操作提供了一个强大的代数框架来分析RCSP的复杂度，特别是在处理涉及等式约束的归约时，为更广泛的领域设置中的复杂度分析开辟了新途径。

Abstract: This paper investigates the reconfiguration variant of the Constraint Satisfaction Problem (CSP), referred to as the Reconfiguration CSP (RCSP). Given a CSP instance and two of its solutions, RCSP asks whether one solution can be transformed into the other via a sequence of intermediate solutions, each differing by the assignment of a single variable. RCSP has attracted growing interest in theoretical computer science, and when the variable domain is Boolean, the computational complexity of RCSP exhibits a dichotomy depending on the allowed constraint types. A notable special case is the reconfiguration of graph homomorphisms -- also known as graph recoloring -- which has been studied using topological methods. We propose a novel algebraic approach to RCSP, inspired by techniques used in classical CSP complexity analysis. Unlike traditional methods based on total operations, our framework employs partial operations to capture a reduction involving equality constraints. This perspective facilitates the extension of complexity results from Boolean domains to more general settings, demonstrating the versatility of partial operations in identifying tractable RCSP instances.

</details>


### [48] [Solution Discovery for Vertex Cover, Independent Set, Dominating Set, and Feedback Vertex Set](https://arxiv.org/abs/2511.23012)
*Rin Saito,Anouk Sommer,Tatsuhiro Suga,Takahiro Suzuki,Yuma Tamura*

Main category: cs.DS

TL;DR: 论文研究了图论中顶点子集问题（顶点覆盖、独立集、支配集、反馈顶点集）的解决方案发现问题，分析了不同参数下的计算复杂度，包括团宽、图直径等。


<details>
  <summary>Details</summary>
Motivation: 研究图论中经典顶点子集问题的解决方案发现问题，即给定初始标记配置，能否通过少量修改转化为可行解。探索这些问题的计算复杂度在不同图类和参数下的表现。

Method: 1. 针对团宽参数设计了所有四个问题的XP算法；2. 证明了在弦图和直径2的图上，顶点覆盖、独立集和反馈顶点集发现问题都是NP完全的；3. 在分裂图上设计了多项式时间算法；4. 针对标记数量参数设计了反馈顶点集发现问题的FPT算法。

Result: 1. 所有四个问题在团宽参数下都有XP算法；2. 顶点覆盖、独立集和反馈顶点集发现在弦图和直径2图上是NP完全的；3. 这三个问题在分裂图上有多项式时间算法；4. 反馈顶点集发现在标记数量参数下有FPT算法。

Conclusion: 解决方案发现问题在图论顶点子集问题中表现出复杂的计算复杂度特性，不同参数和图类导致不同的可解性，为参数化算法设计提供了新的视角。

Abstract: In the solution discovery problem for a search problem on graphs, we are given an initial placement of $k$ tokens on the vertices of a graph and asked whether this placement can be transformed into a feasible solution by applying a small number of modifications. In this paper, we study the computational complexity of solution discovery for several fundamental vertex-subset problems on graphs, namely Vertex Cover Discovery, Independent Set Discovery, Dominating Set Discovery, and Feedback Vertex Set Discovery. We first present XP algorithms for all four problems parameterized by clique-width. We then prove that Vertex Cover Discovery, Independent Set Discovery, and Feedback Vertex Set Discovery are NP-complete for chordal graphs and graphs of diameter 2, which have unbounded clique-width. In contrast to these hardness results, we show that all three problems can be solved in polynomial time on split graphs. Furthermore, we design an FPT algorithm for Feedback Vertex Set Discovery parameterized by the number of tokens.

</details>


### [49] [Improved and Parameterized Algorithms for Online Multi-level Aggregation: A Memory-based Approach](https://arxiv.org/abs/2511.23211)
*Alexander Turoczy,Young-San Lin*

Main category: cs.DS

TL;DR: 提出了在线多层聚合问题(MLAP-D)的改进算法，基于树的深度D和毛虫维度H分别给出了e(D+1)和e(4H+2)竞争比算法，后者在H较小时优于现有最优算法。


<details>
  <summary>Details</summary>
Motivation: 现有的在线MLAP-D算法要么基于树的深度(D)给出6(D+1)竞争比，要么基于顶点数给出O(log|V|)竞争比。作者希望开发基于更精细结构参数的算法，特别是毛虫维度H，该参数对于许多简单但丰富的树类（如线图、毛虫图、龙虾图）保持常数，从而在这些情况下获得更好的竞争比。

Method: 提出了一个简单框架，直接适用于任意结构的树，而不像先前框架需要将问题约简到特定结构的树。该框架基于毛虫维度这一参数，该参数衡量树结构相对于毛虫图的复杂度。算法设计考虑了请求的聚合服务，通过传输子树来更经济地服务多个请求。

Result: 1. 提出了e(D+1)-竞争算法，其中D为树深度；2. 提出了e(4H+2)-竞争算法，其中H为毛虫维度，满足H≤D且H≤log₂|V|。当H=o(min{D,log₂|V|})时，新框架优于现有最优算法（6(D+1)和O(log|V|)）。

Conclusion: 本文首次提出了基于毛虫维度这一优于深度的度量的在线算法，对于许多简单但丰富的树类（线图、毛虫图、龙虾图）能获得常数竞争比，显著改进了现有结果。提出的简单框架直接适用于任意树结构，具有更好的参数化性能。

Abstract: We study the online multi-level aggregation problem with deadlines (MLAP-D) introduced by Bienkowski et al. (ESA 2016, OR 2020). In this problem, requests arrive over time at the vertices of a given vertex-weighted tree, and each request has a deadline that it must be served by. The cost of serving a request equals the cost of a path from the root to the vertex where the request resides. Instead of serving each request individually, requests can be aggregated and served by transmitting a subtree from the root that spans the vertices on which the requests reside, to potentially be more cost-effective. The aggregated cost is the weight of the transmission subtree. The goal of MLAP-D is to find an aggregation solution that minimizes the total cost while serving all requests.
  We present improved and parameterized algorithms for MLAP-D. Our result is twofold. First, we present an $e(D+1)$-competitive algorithm where $D$ is the depth of the tree. Second, we present an $e(4H+2)$-competitive algorithm where $H$ is the caterpillar dimension of the tree. Here, $H \le D$ and $H \le \log_2 |V|$ where $|V|$ is the number of vertices in the given tree. The caterpillar dimension remains constant for rich but simple classes of trees, such as line graphs ($H=1$), caterpillar graphs ($H=2$), and lobster graphs ($H=3$). To the best of our knowledge, this is the first online algorithm parameterized on a measure better than depth. The state-of-the-art online algorithms are $6(D+1)$-competitive by Buchbinder, Feldman, Naor, and Talmon (SODA 2017) and $O(\log |V|)$-competitive by Azar and Touitou (FOCS 2020). Our framework outperforms the state-of-the-art ratios when $H = o(\min\{D,\log_2 |V|\})$. Our simple framework directly applies to trees with any structure and differs from the previous frameworks that reduce the problem to trees with specific structures.

</details>


### [50] [Homomorphism Testing with Resilience to Online Manipulations](https://arxiv.org/abs/2511.23363)
*Esty Kelman,Uri Meir,Debanuj Nayak,Sofya Raskhodnikova*

Main category: cs.DS

TL;DR: 提出了一种在线操纵模型下的群同态测试器，查询复杂度为O(1/ε+log t)，其中ε是距离参数，t是每个查询中对手可擦除或损坏的函数值数量。


<details>
  <summary>Details</summary>
Motivation: 传统代数性质测试假设无限制、无噪声的函数访问，这在对抗性或动态环境中不成立。需要研究在线操纵模型下的群同态测试，其中对手可以根据测试者的历史查询擦除或损坏查询响应。

Method: 提出随机符号测试器，通过引入更多随机性将已知的F2^n→F2的操纵弹性线性测试器推广到一般群域和陪域。不是验证随机元素和的同态条件，而是使用随机元素的加法和减法，为每个元素随机选择符号。

Result: 获得了最优测试器，查询复杂度为O(1/ε+log t)，恢复了标准属性测试模型中O(1/ε)的经典界限。对于关键群族获得了改进的群特定查询界限。

Conclusion: 该工作首次研究了在线操纵模型下的群同态测试，提出了最优的随机符号测试器，将操纵弹性测试扩展到一般群结构，为对抗环境中的代数性质验证提供了新工具。

Abstract: A central challenge in property testing is verifying algebraic structure with minimal access to data. A landmark result addressing this challenge, the linearity test of Blum, Luby, and Rubinfeld (JCSS `93), spurred a rich body of work on testing algebraic properties such as linearity and its generalizations to low-degree polynomials and group homomorphisms. However, classical tests for these properties assume unrestricted, noise-free access to the input function--an assumption that breaks down in adversarial or dynamic settings. To address this, Kalemaj, Raskhodnikova, and Varma (Theory of Computing `23) introduced the online manipulation model, where an adversary may erase or corrupt query responses over time, based on the tester's past queries.
  We initiate the study of {manipulation-resilient} testing for {group homomorphism} in this online model. Our main result is an {optimal} tester that makes $O(1/\varepsilon+\log t)$ queries, where $\varepsilon$ is the distance parameter and $t$ is the number of function values the adversary can erase or corrupt per query. Our result recovers the celebrated $O(1/\varepsilon)$ bound by Ben-Or, Coppersmith, Luby, and Rubinfeld (Random Struct.\ Algorithms `08) for homomorphism testing in the standard property testing model, albeit with a different tester. Our tester, $\mathsf{Random\ Signs\ Test}$, {lifts} known manipulation-resilient linearity testers for $\mathbb{F}_2^n\to \mathbb{F}_2$ to general group domains and codomains by introducing more randomness: instead of verifying the homomorphism condition for a sum of random elements, it uses additions and subtractions of random elements, randomly selecting a sign for each element. We also obtain improved group-specific query bounds for key families of groups.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [51] [Tacit Bidder-Side Collusion: Artificial Intelligence in Dynamic Auctions](https://arxiv.org/abs/2511.21802)
*Sriram Tolety*

Main category: cs.GT

TL;DR: 研究大型语言模型作为自主竞标者是否能在重复荷兰式拍卖中通过协调接受平台支付的时间进行默契合谋，无需任何沟通。


<details>
  <summary>Details</summary>
Motivation: 探索AI代理在拍卖环境中的战略行为，特别是它们是否能够像人类一样形成默契合谋，这对理解AI在市场竞争中的潜在风险具有重要意义。

Method: 建立最小化重复拍卖模型，推导出简单的激励相容条件和默契合谋可持续性的闭式阈值。在受控模拟中使用多个语言模型进行实验，观察不同市场结构下的行为模式。

Result: 在小规模拍卖环境中观察到系统性超竞争价格，随着竞标者数量增加，行为回归竞争性。发现LLMs使用多种机制促进默契协调，如焦点接受时机与耐心策略。这是首次发现LLMs作为竞标者能够形成默契合谋的证据。

Conclusion: 市场结构调节比能力限制更有效地缓解AI代理的默契合谋风险，为监管和政策制定提供了重要启示。

Abstract: We study whether large language models acting as autonomous bidders can tacitly collude by coordinating when to accept platform posted payouts in repeated Dutch auctions, without any communication. We present a minimal repeated auction model that yields a simple incentive compatibility condition and a closed form threshold for sustainable collusion for subgame-perfect Nash equilibria. In controlled simulations with multiple language models, we observe systematic supra-competitive prices in small auction settings and a return to competitive behavior as the number of bidders in the market increases, consistent with the theoretical model. We also find LLMs use various mechanisms to facilitate tacit coordination, such as focal point acceptance timing versus patient strategies that track the theoretical incentives. The results provide, to our knowledge, the first evidence of bidder side tacit collusion by LLMs and show that market structure levers can be more effective than capability limits for mitigation.

</details>


### [52] [The Evolution of Trust under Institutional Moral Hazard](https://arxiv.org/abs/2511.21875)
*Hiroaki Chiba-Okabe,Joshua B. Plotkin*

Main category: cs.GT

TL;DR: 平台有动机通过操纵声誉评级来增加佣金收入，但需要平衡评级准确性以维持市场信任，形成平台激励与社会效用之间的复杂张力。


<details>
  <summary>Details</summary>
Motivation: 研究营利性平台在维护声誉系统时的激励问题，揭示平台为增加佣金收入而操纵声誉评级的道德风险，以及这种操纵如何影响市场参与者的信任和交易行为。

Method: 建立理论模型，其中买家和卖家在平台上匹配交易，存在道德风险（卖家可能不交付商品）。平台维护概率性二元声誉信号系统，买家基于声誉信号做出购买决策，卖家根据收益调整行为类型，平台从交易佣金中获利。

Result: 分析表明：1) 平台有评级通胀的内在激励；2) 这种操纵受限于平台需要维持足够准确性以保持可信卖家在市场中；3) 平台最优策略可能是投资降低信号准确性；4) 当平台可自由设定佣金时，最大利润可能通过投资准确声誉系统实现。

Conclusion: 平台声誉管理存在双重道德风险：卖家交付风险和平台操纵风险。平台激励与社会效用之间存在复杂张力，平台需要在短期利润和长期市场信任之间权衡，自由设定佣金时可能激励平台投资准确声誉系统。

Abstract: We study the behavior of for-profit institutions that broadcast reputations to foster trust among market participants. We develop a theoretical model in which buyers and sellers are matched on a platform to engage in transactions involving a moral hazard: sellers can either faithfully deliver goods after receiving payment, or not. Although the buyer does not know a seller's true type, the platform maintains a reputation system that probabilistically assigns binary reputation signals. Buyers make purchase decisions based on reputation signals, which influence the payoffs to sellers who then adapt their type over time. These market dynamics ultimately shape the platform's profit from commissions on sales. Our analysis reveals that platforms inherently have an incentive for rating inflation, driven by the desire to increase commission. This introduces a second layer of moral hazard: the platform's incentive to distort reputations for its own profit. Such distortion is self-limited by the platform's need to maintain enough accuracy that trustworthy sellers remain in the market, without which rational buyers would refrain from purchases altogether. Nonetheless, the optimal strategy for the platform can be to invest in order to reduce signal accuracy. When the platform can freely set commission fees, however, maximum profit may be achieved by costly investment in an accurate reputation system. These findings highlight the intricate tensions between platform incentives and resulting social utility for marketplace participants.

</details>


### [53] [Aligning with Human Values to Enhance Interaction: An eHMI-Mediated Lane-Changing Negotiation Strategy Using Bayesian Inference](https://arxiv.org/abs/2511.22061)
*Boyao Peng,Linkun Liu*

Main category: cs.GT

TL;DR: 本文提出了一种基于博弈论和贝叶斯推理的车道变换模型，研究自动驾驶系统中善意欺骗对交互效率和安全性的影响，发现善意欺骗能提升效率但可能导致信任崩溃。


<details>
  <summary>Details</summary>
Motivation: 现有研究强调AI遵循诚实伦理原则，但忽视了善意欺骗在最大化整体收益方面的潜在价值。自动驾驶系统需要与人类价值观对齐以确保稳定性和安全性。

Method: 提出博弈论模型分析车道变换场景，结合贝叶斯推理捕捉外部人机界面信息下人类信任的动态变化，通过案例研究验证模型。

Result: 善意欺骗在59.4%场景中提升交互效率，在52.7%场景中改善安全性，但在最极端情况下导致36.9%驾驶员信任崩溃，暴露伦理设计漏洞。

Conclusion: 自动驾驶系统应与全面的人类伦理价值观对齐，包括有条件使用善意欺骗以增强人机交互，但信任崩溃风险是未来开发必须解决的主要伦理漏洞。

Abstract: As autonomous driving technology evolves, ensuring the stability and safety of Autonomous Driving Systems (ADS) through alignment with human values becomes increasingly crucial. While existing research emphasizes the adherence of AI to honest ethical principles, it overlooks the potential benefits of benevolent deception, which maximize overall payoffs. This study proposes a game-theoretic model for lane-changing scenarios, incorporating Bayesian inference to capture dynamic changes in human trust during interactions under external Human-Machine Interface (eHMI) disclosed information. Case studies reveal that benevolent deception can enhance the efficiency of interaction in up to 59.4% of scenarios and improve safety in up to 52.7%. However, in the most pronounced cases, deception also led to trust collapse in up to 36.9% of drivers, exposing a critical vulnerability in the ethical design of ADS. The findings suggest that aligning ADS with comprehensive human ethical values, including the conditional use of benevolent deception, can enhance human-machine interaction. Additionally, the risk of trust collapse remains a major ethical loophole that must be addressed in future ADS development.

</details>


### [54] [On Computing the Shapley Value in Bankruptcy Games -llustrated by Rectified Linear Function Game-](https://arxiv.org/abs/2511.22208)
*Shunta Yamazaki,Tomomi Matsui*

Main category: cs.GT

TL;DR: 本文证明破产博弈中夏普利值的计算是NP完全问题，提出了基于动态规划和递归算法的精确计算方法，并设计了基于蒙特卡洛采样的FPRAS近似算法。


<details>
  <summary>Details</summary>
Motivation: 研究破产博弈中夏普利值的计算复杂性，解决大规模实例的计算难题，建立破产博弈与加权投票博弈之间的联系。

Method: 1) 证明计算复杂性：证明破产博弈中夏普利值计算是NP完全问题；2) 建立联系：分析破产博弈夏普利值与加权投票博弈夏普利-舒比克指数的关系；3) 精确算法：提出基于动态规划的技术和两种递归算法（O'Neill递归完成法和基于对偶博弈的新方法）；4) 近似算法：设计基于蒙特卡洛采样的FPRAS。

Result: 1) 证明了破产博弈夏普利值计算是NP完全问题；2) 建立了破产博弈与加权投票博弈的理论联系；3) 开发了高效的精确计算算法；4) 提出了适用于大规模实例的FPRAS近似方案。

Conclusion: 破产博弈中夏普利值的精确计算是计算困难的，但通过理论联系、递归算法和近似方法可以有效处理，为实际应用提供了理论和计算基础。

Abstract: In this research, we discuss a problem of calculating the Shapley value in bankruptcy games. We show that the decision problem of computing the Shapley value in bankruptcy games is NP-complete. We also investigate the relationship between the Shapley value of bankruptcy games and the Shapley-Shubik index in weighted voting games. The relation naturally implies a dynamic programming technique for calculating the Shapley value. We also present two recursive algorithms for computing the Shapley value: the first is the recursive completion method originally proposed by O'Neill, and the second is our novel contribution based on the dual game formulation. These recursive approaches offer conceptual clarity and computational efficiency, especially when combined with memoisation technique. Finally, we propose a Fully Polynomial-Time Randomized Approximation Scheme (FPRAS) based on Monte Carlo sampling, providing an efficient approximation method for large-scale instances.

</details>


### [55] [Mechanism Design under Unawareness -- Extended Abstract](https://arxiv.org/abs/2511.22369)
*Kym Pram,Burkhard C. Schipper*

Main category: cs.GT

TL;DR: 研究在不对称认知和信息下的机制设计，提出动态VCG机制，在无需规划者完全认知的情况下实现效用主义效率，并探讨预算平衡和参与约束问题。


<details>
  <summary>Details</summary>
Motivation: 机制设计者在面对认知不对称时无法承诺特定的社会选择函数，但可以承诺某些属性（如给定事后认知的效率）。研究如何在认知不对称和信息不对称的情况下设计有效机制。

Method: 开发新颖的动态版本Vickrey-Clarke-Groves机制，其中真实类型在认知水平提高时逐步揭示和细化。提出动态细化反向第二价格拍卖用于复杂不完全指定项目的采购。

Result: 可以在条件占优策略中实现一个在合并认知下效用主义效率的社会选择函数，无需社会规划者事前完全认知。事前未预见的事件不能作为赤字的借口。

Conclusion: 通过动态VCG机制可以在认知不对称环境下实现效率，同时满足预算平衡和参与约束，为复杂不完全指定项目的采购提供了有效解决方案。

Abstract: We study the design of mechanisms under asymmetric awareness and information. While the mechanism designer cannot necessarily commit to a particular social choice function in the face of unawareness, she can at least commit to properties of social choice functions such as efficiency given ex post awareness. Assuming quasi-linear utilities and private values, we show that we can implement in conditional dominant strategies a social choice function that is utilitarian ex post efficient under pooled awareness without the need of the social planner being fully aware ex ante. To this end, we develop novel dynamic versions of Vickrey-Clarke-Groves mechanisms in which true types are revealed and subsequently elaborated at endogenous higher awareness levels. We explore how asymmetric awareness affects budget balance and participation constraints. We show that ex ante unforeseen contingencies are no excuse for deficits. Finally, we propose a dynamic elaboration reverse second price auction for efficient procurement of complex incompletely specified projects with budget balance and participation constraints.

</details>


### [56] [Solving Four Open Problems about Core Stability in Altruistic Hedonic Games](https://arxiv.org/abs/2511.22370)
*Jörg Rothe,Ildikó Schlotter*

Main category: cs.GT

TL;DR: 本文解决了利他主义享乐博弈中四种变体的核心稳定性验证问题的计算复杂度，证明这些问题都是coNP完全的。


<details>
  <summary>Details</summary>
Motivation: 享乐博弈是合作博弈论和计算社会选择的交叉领域，其中玩家的效用不仅取决于自身对联盟的评价，还受到朋友评价的影响。Kerkmann等人引入了利他主义享乐博弈，但其中四种变体（基于平均和最小值的"平等对待"和"利他对待"偏好）的核心稳定性验证问题的复杂度一直未解决。

Method: 通过构建复杂的网络朋友关系图作为归约工具，将已知的coNP完全问题归约到利他主义享乐博弈的核心稳定性验证问题。

Result: 证明了四种变体的利他主义享乐博弈的核心稳定性验证问题都是coNP完全的，解决了这些开放性问题。

Conclusion: 利他主义享乐博弈中基于平均和最小值的平等对待与利他对待偏好的核心稳定性验证在计算上都是困难的（coNP完全），这为理解这类博弈的计算复杂性提供了重要结论。

Abstract: Hedonic games -- at the interface of cooperative game theory and computational social choice -- are coalition formation games in which the players have preferences over the coalitions they can join. Kerkmann et al. [13] introduced altruistic hedonic games where the players' utilities depend not only on their own but also on their friends' valuations of coalitions. The complexity of the verification problem for core stability has remained open in four variants of altruistic hedonic games: namely, for the variants with average- and minimum-based "equal-treatment" and "altruistic-treatment" preferences. We solve these four open questions by proving the corresponding problems coNP-complete; our reductions rely on rather intricate gadgets in the related networks of friends.

</details>


### [57] [Common $p$-Belief with Plausibility Measures: Extended Abstract](https://arxiv.org/abs/2511.22372)
*Eric Pacuit,Leo Yang*

Main category: cs.GT

TL;DR: 该论文将Monderer-Samet-Neeman共识定理从经典概率测度扩展到更一般的似然测度框架，并应用于多种非经典信念模型。


<details>
  <summary>Details</summary>
Motivation: Aumann的"同意不同意"定理及其Monderer-Samet-Neeman推广在经典概率框架下建立了共识理论，但需要扩展到更一般的信念表示框架，特别是Halpern提出的似然测度框架，以统一多种形式信念模型。

Method: 1. 为经典Monderer-Samet-Neeman定理提供新证明；2. 基于原始证明和新证明，提出两种不同的定理推广到似然测度结构；3. 将推广结果应用于条件概率结构和词典概率结构等非经典信念模型。

Result: 成功将共识定理扩展到似然测度框架，并在多种非经典信念模型中验证了推广结果。当推广定理不适用时，原Monderer-Samet-Neeman定理也失效，表明该推广成功识别了信念模型满足共识定理的最小条件。

Conclusion: 该研究成功地将Monderer-Samet-Neeman共识定理从经典概率测度推广到更一般的似然测度框架，为多种非经典信念模型提供了统一的共识理论基础，并确定了满足共识定理的最小条件。

Abstract: Aumann's famous Agreeing to Disagree Theorem states that if a group of agents share a common prior, update their beliefs by Bayesian conditioning based on private information, and have common knowledge of their posterior beliefs regarding some event, these posteriors must be identical. There is an elegant generalization of this theorem by Monderer and Samet, later refined by Neeman: if a group of agents share a common prior, update their beliefs using Bayesian conditioning on private information, and have common p-belief of their posteriors, these posteriors must be close (i.e., they cannot differ by more than 1 - p). Here, common p-belief generalizes the concept of common knowledge to probabilistic beliefs: agents commonly p-believe an event E if everyone believes E to at least degree p, everyone believes to at least degree p that everyone believes E to at least degree p, and so on.
  This paper further extends the Monderer-Samet-Neeman Agreement Theorem from classical probability measures to plausibility measures -- a very general framework introduced by Halpern that unifies many formal models of belief. To facilitate this extension, we provide a new proof of the Monderer-Samet-Neeman theorem in the classical setting. Building upon both the original proof and our new proof, we offer two different generalizations of the theorem to plausibility-based structures.
  We then apply these generalized results to several non-classical belief models, including conditional probability structures and lexicographic probability structures. Moreover, we show that whenever our generalized theorems do not apply, the Monderer-Samet-Neeman Agreement Theorem fails. These findings suggest that our results successfully identify the minimal conditions required for a belief model to satisfy the Monderer-Samet-Neeman Agreement Theorem.

</details>


### [58] [Skating System Unveiled: Exploring Preference Aggregation in Ballroom Tournaments](https://arxiv.org/abs/2511.22384)
*Laryssa Horn,Paul Nüsken,Jörg Rothe,Tessa Seeger*

Main category: cs.GT

TL;DR: Skating System Single (SkS) 是一种源自舞蹈比赛评分系统的新投票系统，与 Bucklin 投票相似但有额外约束，研究了其公理性质、操纵攻击和选举控制问题。


<details>
  <summary>Details</summary>
Motivation: 将舞蹈比赛中的 Skating System 形式化为投票系统，研究其计算社会选择框架下的特性，并与 Bucklin 投票进行比较分析。

Method: 形式化定义 Skating System Single (SkS)，分析其公理性质（满足多数准则、正响应性、单调性等，违反孔多塞准则、一致性等），研究操纵问题的计算复杂度（构造性联盟加权操纵是 NP 完全的，破坏性操纵可在多项式时间解决），以及选举控制问题的计算复杂度。

Result: SkS 满足非独裁、多数准则、正响应性、单调性和公民主权，但违反孔多塞准则、强单调性、克隆独立性、一致性、参与性、决定性和策略防操纵性。构造性联盟加权操纵和选举控制（删除候选人、添加选民）是 NP 完全的，而破坏性操纵和破坏性控制添加选民可在多项式时间解决。

Conclusion: SkS 作为一种源自实际应用的投票系统，在公理性质上与 Bucklin 投票有相似之处但也有重要差异，其计算复杂度特性为实际应用中的操纵和选举控制提供了理论分析基础。

Abstract: The Skating System, which originated from the scrutineering system in dance sport tournaments, can be formulated as a voting system: We introduce and formalize the Skating System Single (SkS, for short), a new voting system embedded into the framework of computational social choice. Although SkS has similarities with Bucklin voting, it differs from it because it is subject to additional constraints when determining the election winners. Through an analysis of the axiomatic properties of SkS and of its vulnerability to manipulative and electoral control attacks, we compare SkS with Bucklin voting and provide insights into its potential strengths and weaknesses. In particular, we show that SkS satisfies nondictatorship as well as the majority criterion, positive responsiveness, monotonicity, and citizens' sovereignty but violates the Condorcet criterion, strong monotonicity, independence of clones, consistency, participation, resoluteness, and strategy-proofness. Further, we study manipulation, i.e., where (groups of) voters vote strategically to improve the outcome of an election in their favor, showing that the constructive coalitional weighted manipulation problem for SkS is NP-complete, while the destructive variant can be solved in polynomial time. Lastly, we initiate the study of electoral control, where an external agent attempts to change the election outcome by interfering with the structure of the election. Here, we show NP-completeness for constructive and destructive control by deleting candidates as well as for constructive control by adding voters, whereas we show that the problem of destructive control by adding voters can be solved in polynomial time.

</details>


### [59] [Prudent Rationalizability and the Best Rationalization Principle](https://arxiv.org/abs/2511.22388)
*Nicodemo De Vito*

Main category: cs.GT

TL;DR: 该论文提出了一种基于非标准概率测度的谨慎理性化定义，通过c-强信念概念，在完美回忆的有限序贯博弈中建立了与Heifetz等人原始定义的等价性，并展示了与迭代可容许性的算法联系。


<details>
  <summary>Details</summary>
Motivation: 研究有限序贯博弈中的谨慎推理，为Heifetz等人提出的谨慎理性化概念提供基于信念迭代缩减程序的形式化定义，建立与标准概率表示之间的等价关系。

Method: 使用条件非标准概率测度表示玩家信念，引入c-强信念作为Battigalli和Siniscalchi强信念概念的谨慎版本，构建基于"最佳理性化原则"的谨慎理性化迭代缩减程序。

Result: 证明了提出的非标准概率定义与Heifetz等人的标准概率定义等价，展示了谨慎理性化可以通过迭代可容许性进行算法刻画，并将方法扩展到具有无意识性的序贯博弈。

Conclusion: 成功建立了谨慎理性化的信念迭代缩减形式化，提供了与标准概率表示的等价证明，扩展了该方法在更广泛博弈情境中的应用范围。

Abstract: We study cautious reasoning in finite sequential games played by agents with perfect recall. Our contribution lies in formulating a definition of prudent rationalizability (Heifetz et al. 2021, BEJTE) as an iterative reduction procedure of beliefs. To this end, we represent the players' beliefs by systems of conditional non-standard probability measures. The key novelty is the notion of c-strong belief, a non-standard, "cautious" version of strong belief (Battigalli and Siniscalchi 2002, JET). Our formulation of prudent rationalizability embodies a "best rationalization principle" similar to the one that underlies the solution concept of strong rationalizability. The main results show the equivalence between the proposed definition with the one originally put forth by Heifetz et al. (2021) in terms of conditional beliefs represented by standard probabilities. In particular, it is shown that prudent rationalizability can be algorithmically characterized by iterated admissibility. Finally, our formulation can be extended to sequential games with unawareness.

</details>


### [60] [Beyond Last-Click: An Optimal Mechanism for Ad Attribution](https://arxiv.org/abs/2511.22918)
*Nan An,Weian Li,Qi Qi,Changyuan Yu,Liang Zhang*

Main category: cs.GT

TL;DR: 提出Peer-Validated Mechanism (PVM)作为最优的广告归因机制，相比传统Last-Click Mechanism (LCM)具有理论保证和更好性能


<details>
  <summary>Details</summary>
Motivation: 现有广告归因方法（如Last-Click Mechanism）依赖启发式规则，缺乏理论保证，在准确性和公平性方面表现不佳

Method: 提出理论模型，设计最优的dominant strategy incentive compatible (DSIC)机制，引入Peer-Validated Mechanism (PVM)，其归因仅依赖其他平台的报告

Result: 证明LCM不是DSIC且性能差；PVM在homogeneous setting下是最优DSIC机制；数值实验显示PVM在归因准确性和公平性上持续优于LCM

Conclusion: PVM为多平台广告归因提供了理论保证的解决方案，显著提升了准确性和公平性，是传统启发式方法的有效替代

Abstract: Accurate attribution for multiple platforms is critical for evaluating performance-based advertising. However, existing attribution methods rely heavily on the heuristic methods, e.g., Last-Click Mechanism (LCM) which always allocates the attribution to the platform with the latest report, lacking theoretical guarantees for attribution accuracy. In this work, we propose a novel theoretical model for the advertising attribution problem, in which we aim to design the optimal dominant strategy incentive compatible (DSIC) mechanisms and evaluate their performance. We first show that LCM is not DSIC and performs poorly in terms of accuracy and fairness. To address this limitation, we introduce the Peer-Validated Mechanism (PVM), a DSIC mechanism in which a platform's attribution depends solely on the reports of other platforms. We then examine the accuracy of PVM across both homogeneous and heterogeneous settings, and provide provable accuracy bounds for each case. Notably, we show that PVM is the optimal DSIC mechanism in the homogeneous setting. Finally, numerical experiments are conducted to show that PVM consistently outperforms LCM in terms of attribution accuracy and fairness.

</details>


### [61] [Merging Mechanisms for Ads and Organic Items in E-commerce Platforms](https://arxiv.org/abs/2511.22925)
*Nan An,Weian Li,Qi Qi,Liang Zhang*

Main category: cs.GT

TL;DR: 该论文提出了一种同时满足激励兼容性、个体理性、多槽位适应性、不可分割候选集成和避免广告与有机项目重复曝光等所有理想属性的电商平台搜索结果合并机制。


<details>
  <summary>Details</summary>
Motivation: 电商平台搜索结果页面包含广告项目和有机项目，分别由广告拍卖系统和推荐系统决定，两者优化目标不同，需要有效的合并机制。现有研究未能同时满足所有理想属性。

Method: 首先分析最优合并机制的必要条件，然后提出两种简单有效的机制：广义固定机制和广义变化机制，并理论证明它们在简单和一般设置下相对于最优机制具有保证的近似比。

Result: 提出的两种机制能够同时满足激励兼容性、个体理性、多槽位适应性、不可分割候选集成和避免重复曝光等所有理想属性。

Conclusion: 该研究成功设计了满足所有理想属性的电商平台搜索结果合并机制，为实际应用提供了理论保证的有效解决方案。

Abstract: In contemporary e-commerce platforms, search result pages display two types of items: ad items and organic items. Ad items are determined through an advertising auction system, while organic items are selected by a recommendation system. These systems have distinct optimization objectives, creating the challenge of effectively merging these two components. Recent research has explored merging mechanisms for e-commerce platforms, but none have simultaneously achieved all desirable properties: incentive compatibility, individual rationality, adaptability to multiple slots, integration of inseparable candidates, and avoidance of repeated exposure for ads and organic items. This paper addresses the design of a merging mechanism that satisfies all these properties. We first provide the necessary conditions for the optimal merging mechanisms. Next, we introduce two simple and effective mechanisms, termed the generalized fix mechanism and the generalized change mechanism. Finally, we theoretically prove that both mechanisms offer guaranteed approximation ratios compared to the optimal mechanism in both simplest and general settings.

</details>


### [62] [Fairness in the Multi-Secretary Problem](https://arxiv.org/abs/2511.23097)
*Georgios Papasotiropoulos,Zein Pishbin*

Main category: cs.GT

TL;DR: 该论文将多秘书问题与多赢家选举结合，从社会选择公平性视角研究在线决策，提出融合在线算法与社会选择规则的新机制


<details>
  <summary>Details</summary>
Motivation: 现有扩展合理代表(EJR)比例性概念在在线领域存在局限性，需要将在线决策与社会选择理论相结合，为多赢家选举提供公平的在线机制

Method: 提出融合在线算法与社会选择规则（如平等份额法和纳什规则）的机制，结合理论分析和大量实验评估

Result: 开发了新的在线公平机制，通过理论分析和实验验证了其有效性

Conclusion: 成功连接了在线决策与社会选择理论，为多赢家选举提供了公平的在线解决方案

Abstract: This paper bridges two perspectives: it studies the multi-secretary problem through the fairness lens of social choice, and examines multi-winner elections from the viewpoint of online decision making. After identifying the limitations of the prominent proportionality notion of Extended Justified Representation (EJR) in the online domain, the work proposes a set of mechanisms that merge techniques from online algorithms with rules from social choice -- such as the Method of Equal Shares and the Nash Rule -- and supports them through both theoretical analysis and extensive experimental evaluation.

</details>


### [63] [Designing Rules for Choosing a Winner in a Debate](https://arxiv.org/abs/2511.23454)
*Alexander Heckett,Vincent Conitzer*

Main category: cs.GT

TL;DR: 研究在信息不对称下，委托人如何设计规则从两个策略性代理人中选择更优行动，代理人掌握更多信息但只关心自己获胜，委托人需基于可验证的论证制定决策规则。


<details>
  <summary>Details</summary>
Motivation: 研究委托人面临信息劣势时如何设计决策机制：委托人不知道真实世界状态，但需要从两个掌握更多信息的代理人中选择更优行动。代理人只关心自己获胜，会策略性地提供论证。委托人需要设计规则来确保选择真正更好的行动。

Method: 建立形式化框架分析可验证论证环境下的决策问题。代理人基于真实世界状态提供可验证的论证，委托人设计选择规则。研究框架的基本性质，分析委托人策略评估和优化的计算问题，并提供关键误差界限。

Result: 提出了形式化分析框架，展示了框架的基本性质，研究了委托人策略评估和优化的计算问题，并提供了关键误差界限，为委托人设计决策规则提供了理论基础。

Conclusion: 在信息不对称环境下，委托人可以通过设计适当的决策规则从策略性代理人中选择更优行动。形式化框架为分析此类问题提供了基础，计算问题分析和误差界限为实际应用提供了指导。

Abstract: We consider settings where an uninformed principal must hear arguments from two better-informed agents, corresponding to two possible courses of action that they argue for. The arguments are verifiable in the sense that the true state of the world restricts the arguments that can be made by the agents. Each agent simply wants to be chosen as the winner and does so strategically based on the rule set by the principal. How should the principal design the rule to choose the better action? We provide a formal framework for answering this question, exhibit some basic properties of it, study the computational problems of evaluating and optimizing the principal's policy, and provide key error bounds.

</details>
