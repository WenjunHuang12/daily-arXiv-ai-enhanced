<div id=toc></div>

# Table of Contents

- [cs.DS](#cs.DS) [Total: 3]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.IT](#cs.IT) [Total: 10]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.IR](#cs.IR) [Total: 7]


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [1] [Scalable Fair Influence Blocking Maximization via Approximately Monotonic Submodular Optimization](https://arxiv.org/abs/2601.22584)
*Qiangpeng Fang,Jilong Shi,Xiaobin Rui,Jian Zhang,Zhixiao Wang*

Main category: cs.DS

TL;DR: 提出公平感知的影响力阻断最大化问题，设计兼顾公平性和阻断效果的算法CELF-R，实现高效优化并提供理论保证


<details>
  <summary>Details</summary>
Motivation: 现有影响力阻断最大化方法只关注阻断效果，忽视了不同社区间的公平性分配问题，且现有公平性约束计算成本高，难以在大规模网络中应用

Method: 1) 将人口统计公平性作为公平性度量；2) 设计保持近似单调子模结构的公平感知目标函数；3) 通过可调标量化整合公平性和阻断效果；4) 开发CELF-R算法，利用近似子模性消除冗余评估并支持帕累托前沿构建

Result: CELF-R在实验中持续优于现有基线方法，实现了(1-1/e-ψ)近似解，同时保持高效率，能够有效平衡公平性和阻断效果

Conclusion: 该研究首次将公平性考量引入影响力阻断最大化问题，提出的CELF-R算法为大规模网络中公平有效的负面影响力阻断提供了实用解决方案

Abstract: Influence Blocking Maximization (IBM) aims to select a positive seed set to suppress the spread of negative influence. However, existing IBM methods focus solely on maximizing blocking effectiveness, overlooking fairness across communities. To address this issue, we formalize fairness in IBM and justify Demographic Parity (DP) as a notion that is particularly well aligned with its semantics. Yet enforcing DP is computationally challenging: prior work typically formulates DP as a Linear Programming (LP) problem and relies on costly solvers, rendering them impractical for large-scale networks. In this paper, we propose a DP-aware objective while maintaining an approximately monotonic submodular structure, enabling efficient optimization with theoretical guarantees. We integrate this objective with blocking effectiveness through a tunable scalarization, yielding a principled fairness-effectiveness trade-offs. Building on this structure, we develop CELF-R, an accelerated seed selection algorithm that exploits approximate submodularity to eliminate redundant evaluations and naturally supports Pareto front construction. Extensive experiments demonstrate that CELF-R consistently outperforms state-of-the-art baselines, achieving a $(1-1/e-ψ)$-approximate solution while maintaining high efficiency.

</details>


### [2] [Competitive Non-Clairvoyant KV-Cache Scheduling for LLM Inference](https://arxiv.org/abs/2601.22996)
*Yiding Feng,Zonghan Yang,Yuhao Zhang*

Main category: cs.DS

TL;DR: 提出GSA算法，首个在离线批处理场景下实现常数竞争比的非预知性KV缓存调度策略


<details>
  <summary>Details</summary>
Motivation: LLM推理中的KV缓存调度面临独特挑战：作业内存占用随解码token数线性增长，而响应长度未知。现有方法要么假设预知长度，要么依赖预测，缺乏无先验知识下的鲁棒性能保证

Method: 提出几何切片算法(GSA)：通过几何相位结构周期性重启作业来限制内存暴露，结合交错流水线机制平滑聚合内存消耗实现高并发

Result: GSA在一般实例中竞争比≤61.92，大内存场景下提升至32；其预知版本GBA近似比为10.67(一般)和6.75(大内存)，显著优于先前超过9000的界限

Conclusion: GSA是首个在无先验知识下实现常数竞争比的非预知性KV缓存调度算法，实际请求追踪实验验证了其鲁棒性能

Abstract: Large Language Model (LLM) inference presents a unique scheduling challenge due to the Key-Value (KV) cache, where a job's memory footprint grows linearly with the number of decoded tokens. This growth couples scheduling decisions with feasibility: a scheduler must minimize latency under a hard memory budget, yet the response lengths of requests are inherently unknown. While recent works have explored this problem either assuming clairvoyance -- exact knowledge of response lengths -- or relying on machine-learned predictions, obtaining robust performance guarantees without any prior knowledge of job sizes remains a theoretically fundamental and practically important open problem.
  In this work, we propose the Geometric Slicing Algorithm (GSA), the non-clairvoyant policy to achieve the first constant competitive ratio for this problem in the offline batch setting. GSA manages uncertainty through a geometric phase structure that periodically restarts jobs to bound memory exposure, combined with a staggered pipeline mechanism that enables high concurrency by smoothing aggregate memory consumption. We prove that GSA achieves a competitive ratio of at most 61.92 for general instances, improving to 32 in the large-memory regime. Our algorithmic framework also yields a clairvoyant counterpart, the Geometric Batching Algorithm (GBA), which achieves an approximation ratio of 10.67 for general instances and 6.75 in the large-memory regime -- significantly improving upon the best previously known bound of over 9000. Numerical experiments on real request traces demonstrate that our algorithms perform robustly while preserving these worst-case guarantees.

</details>


### [3] [Compressed Set Representations based on Set Difference](https://arxiv.org/abs/2601.23240)
*Travis Gagie,Meng He,Gonzalo Navarro*

Main category: cs.DS

TL;DR: 提出一种压缩的集合集合表示方法，利用集合间的差异进行压缩，支持对数时间查询，并提供新的MST构建算法


<details>
  <summary>Details</summary>
Motivation: 传统集合集合表示方法可能效率不高，特别是当集合之间存在大量相似性时，需要一种能利用这种相似性的压缩表示方法

Method: 开发了一种压缩表示方法，利用集合间的差异进行压缩；提出了基于最小生成树(MST)的新构建算法

Result: 该表示支持对数时间的访问、成员查询、前驱和后继查询；MST构建算法优于标准算法

Conclusion: 提出的压缩表示方法能有效利用集合间的相似性，配合新的MST构建算法，在集合集合的表示和查询方面具有优势

Abstract: We introduce a compressed representation of sets of sets that exploits how much they differ from each other. Our representation supports access, membership, predecessor and successor queries on the sets within logarithmic time. In addition, we give a new MST-based construction algorithm for the representation that outperforms standard ones.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [4] [Tacit Coordination of Large Language Models](https://arxiv.org/abs/2601.22184)
*Ido Aharon,Emanuele La Malfa,Michael Wooldridge,Sarit Kraus*

Main category: cs.GT

TL;DR: LLMs在默契协调游戏中表现出色，能通过焦点协调，但在涉及数字或文化原型的常识协调上存在困难


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在默契协调游戏中的表现，探索焦点理论如何解释LLMs的协调行为，填补LLMs在协调能力评估方面的研究空白

Method: 在已有的人类实验基础上，比较LLMs在合作和竞争游戏中的协调能力；引入多种无学习策略来改进LLMs之间及与人类的协调；在Llama、Qwen、GPT-oss等开源模型上进行大规模评估

Result: LLMs展现出卓越的协调能力，通常优于人类，但在涉及数字或微妙文化原型的常识协调任务上表现不佳

Conclusion: 这是首次在焦点理论框架下对LLMs的默契协调能力进行大规模评估，揭示了LLMs在协调方面的优势和局限

Abstract: In tacit coordination games with multiple outcomes, purely rational solution concepts, such as Nash equilibria, provide no guidance for which equilibrium to choose. Shelling's theory explains how, in these settings, humans coordinate by relying on focal points: solutions or outcomes that naturally arise because they stand out in some way as salient or prominent to all players. This work studies Large Language Models (LLMs) as players in tacit coordination games, and addresses how, when, and why focal points emerge. We compare and quantify the coordination capabilities of LLMs in cooperative and competitive games for which human experiments are available. We also introduce several learning-free strategies to improve the coordination of LLMs, with themselves and with humans. On a selection of heterogeneous open-source models, including Llama, Qwen, and GPT-oss, we discover that LLMs have a remarkable capability to coordinate and often outperform humans, yet fail on common-sense coordination that involves numbers or nuanced cultural archetypes. This paper constitutes the first large-scale assessment of LLMs' tacit coordination within the theoretical and psychological framework of focal points.

</details>


### [5] [FAIRFORMER: A transformer architecture for discrete fair division](https://arxiv.org/abs/2601.22346)
*Chris Mascioli,Satyam Goyal,Mithun Chakraborty*

Main category: cs.GT

TL;DR: FairFormer：基于深度神经网络的不可分割物品分配方法，通过两塔Transformer架构平衡经济效率和嫉妒公平性，无需求解器监督即可实现接近最优的福利分配。


<details>
  <summary>Details</summary>
Motivation: 解决无货币转移情况下不可分割物品的分配问题，需要在经济效率（如Nash福利）和基于嫉妒的公平性之间取得平衡。传统方法需要复杂的求解器或启发式算法，FairFormer旨在通过深度学习实现端到端的优化分配。

Method: 提出FairFormer：基于两塔Transformer的架构，将物品和代理编码为无序token集合，通过自注意力机制和物品到代理的交叉注意力，在单次前向传播中生成每个物品的分配概率分布。模型端到端训练最大化期望对数Nash福利，无需求解器监督。测试时通过行向argmax离散化，并应用轻量级后处理消除违反"最多嫉妒一个物品"的公平性约束。

Result: FairFormer在均匀采样估值下实现了接近最优的福利：Nash福利达到96-97%，功利主义福利达到95-96%。在解决方案质量和/或运行时间方面优于强基线方法，且能泛化到训练范围之外的情况。

Conclusion: FairFormer展示了深度学习在资源分配问题中的有效性，通过端到端训练实现了经济效率和公平性的良好平衡，无需复杂的求解器或公平性标签，为不可分割物品分配提供了高效实用的解决方案。

Abstract: We propose a deep neural network-based solution to the problem of allocating indivisible goods under additive subjective valuations without monetary transfers, trading off economic efficiency with envy-based fairness. We introduce FairFormer, an amortized, permutation-equivariant two-tower transformer that encodes items and agents as unordered token sets, applies self-attention within each set, and uses item-to-agent cross-attention to produce per-item assignment distributions in a single forward pass. FairFormer is trained end-to-end to maximize expected log-Nash welfare on sampled instances, requiring no solver supervision, unrolled allocation procedures, or fairness labels. At test time, we discretize by row-wise $\arg\max$ and apply a lightweight post-processing routine that transfers items to eliminate violations of envy-freeness up to one item while prioritizing improvements in Nash welfare. Our approach generalizes beyond its training regime and achieves near-optimal welfare (e.g., for uniformly sampled valuations, $96$--$97\%$ for Nash welfare; $95$--$96\%$ for utilitarian welfare), outperforming strong baselines in solution quality and/or runtime.

</details>


### [6] [Dynamic Welfare-Maximizing Pooled Testing](https://arxiv.org/abs/2601.22419)
*Nicholas Lopez,Francisco Marmolejo-Cossío,Jose Roberto Tello Ayala,David C. Parkes*

Main category: cs.GT

TL;DR: 动态池化测试策略在有限测试资源下能显著提升社会福利，特别是简单贪心算法在低预算场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有福利最大化的池化测试研究多采用静态分配策略，而公共卫生筛查需要动态适应性的测试分配来最大化社会福利。

Method: 研究动态福利最大化池化测试策略，评估多种算法方法：精确动态优化、贪心启发式、混合整数规划松弛和学习策略，通过合成实验比较性能。

Result: 动态测试在低预算场景下相比静态基线带来显著福利提升，简单贪心策略能捕获大部分动态测试收益，计算效率高，而学习型方法未稳定优于启发式。

Conclusion: 动态池化测试为公共卫生筛查提供了原则性计算视角，阐明了动态分配在何时能实质性改善福利，简单贪心策略是实用有效的解决方案。

Abstract: Pooled testing is a common strategy for public health disease screening under limited testing resources, allowing multiple biological samples to be tested together with the resources of a single test, at the cost of reduced individual resolution. While dynamic and adaptive strategies have been extensively studied in the classical pooled testing literature, where the goal is to minimize the number of tests required for full diagnosis of a given population, much of the existing work on welfare-maximizing pooled testing adopts static formulations in which all tests are assigned in advance. In this paper, we study dynamic welfare-maximizing pooled testing strategies in which a limited number of tests are performed sequentially to maximize social welfare, defined as the aggregate utility of individuals who are confirmed to be healthy. We formally define the dynamic problem and study algorithmic approaches for sequential test assignment. Because exact dynamic optimization is computationally infeasible beyond small instances, we evaluate a range of strategies (including exact optimization baselines, greedy heuristics, mixed-integer programming relaxations, and learning-based policies) and empirically characterize their performance and tradeoffs using synthetic experiments. Our results show that dynamic testing can yield substantial welfare improvements over static baselines in low-budget regimes. We find that much of the benefit of dynamic testing is captured by simple greedy policies, which substantially outperform static approaches while remaining computationally efficient. Learning-based methods are included as flexible baselines, but in our experiments they do not reliably improve upon these heuristics. Overall, this work provides a principled computational perspective on dynamic pooled testing and clarifies when dynamic assignment meaningfully improves welfare in public health screening.

</details>


### [7] [Do AI Overviews Benefit Search Engines? An Ecosystem Perspective](https://arxiv.org/abs/2601.22493)
*Yihang Wu,Jiajun Tang,Jinfei Liu,Haifeng Xu,Fan Yao*

Main category: cs.GT

TL;DR: AI Overviews损害搜索长期利润，提出引用和补偿机制可改善


<details>
  <summary>Details</summary>
Motivation: AI Overviews提升用户体验但分流内容创作者流量，可能导致高质量内容减少和用户流失，损害搜索引擎长期利润

Method: 建立创作者竞争博弈模型，提出两种激励机制：引用机制（在AI Overview中标注来源）和补偿机制（提供金钱奖励），设计近似最优的利润最大化机制

Result: AI Overviews确实损害搜索引擎长期利润，但基于提出的干预机制可在多种现实场景中增加长期利润

Conclusion: 提出的激励机制能为AI增强搜索生态系统提供更可持续的发展路径

Abstract: The integration of AI Overviews into search engines enhances user experience but diverts traffic from content creators, potentially discouraging high-quality content creation and causing user attrition that undermines long-term search engine profit. To address this issue, we propose a game-theoretic model of creator competition with costly effort, characterize equilibrium behavior, and design two incentive mechanisms: a citation mechanism that references sources within an AI Overview, and a compensation mechanism that offers monetary rewards to creators. For both cases, we provide structural insights and near-optimal profit-maximizing mechanisms. Evaluations on real click data show that although AI Overviews harm long-term search engine profit, interventions based on our proposed mechanisms can increase long-term profit across a range of realistic scenarios, pointing toward a more sustainable trajectory for AI-enhanced search ecosystems.

</details>


### [8] [Greedy Routing Reachability Games](https://arxiv.org/abs/2601.23126)
*Pascal Lenzner,Paraskevi Machaira*

Main category: cs.GT

TL;DR: 研究自治智能体如何在度量空间中形成支持贪婪路由的网络，分析有向边和无向边两种模型下的均衡存在性、计算复杂性和性能保证。


<details>
  <summary>Details</summary>
Motivation: 现代网络由许多自治实体组成，每个实体只有局部视图，需要依赖本地路由协议。贪婪路由是常用解决方案，但缺乏对网络形成过程的理论理解。研究旨在理解自治智能体如何自组织形成支持贪婪路由的网络，而不对智能体分布或具体路由协议做假设。

Method: 建立博弈论模型：智能体位于度量空间中，每个智能体试图建立尽可能少的链接，同时确保通过贪婪路由能到达所有其他智能体。研究两种变体：有向边模型和无向边模型。分析均衡存在性、计算复杂性、价格无政府状态（PoA）以及近似均衡的计算。

Result: 有向边模型：均衡存在，总成本最优，在欧几里得度量中可高效找到均衡，但计算最优策略是NP难的。无向边模型（更具挑战性）：在2D欧几里得空间中，价格无政府状态在1.75-1.8之间；更高维度小于2。最佳响应动态可能循环，但在欧几里得空间中可在多项式时间内计算几乎最优的近似均衡。在2D欧几里得空间中，这些近似均衡优于著名的Delaunay三角剖分。

Conclusion: 该研究为自治智能体形成贪婪路由网络提供了理论基础。有向边模型具有良好的均衡性质，而无向边模型虽然更具挑战性，但仍能获得有界的性能保证和有效的近似算法。研究结果表明，在现实条件下，自治智能体可以自组织形成高效的贪婪路由网络。

Abstract: Today's networks consist of many autonomous entities that follow their own objectives, i.e., smart devices or parts of large AI systems, that are interconnected. Given the size and complexity of most communication networks, each entity typically only has a local view and thus must rely on a local routing protocol for sending and forwarding packets. A common solution for this is greedy routing, where packets are locally forwarded to a neighbor in the network that is closer to the packet's destination.
  In this paper we investigate a game-theoretic model with autonomous agents that aim at forming a network where greedy routing is enabled. The agents are positioned in a metric space and each agent tries to establish as few links as possible, while maintaining that it can reach every other agent via greedy routing. Thus, this model captures how greedy routing networks are formed without any assumption on the distribution of the agents or the specific employed greedy routing protocol. Hence, it distills the essence that makes greedy routing work.
  We study two variants of the model: with directed edges or with undirected edges. For the former, we show that equilibria exist, have optimal total cost, and that in Euclidean metrics they can be found efficiently. However, even for this simple setting computing optimal strategies is NP-hard. For the much more challenging setting with undirected edges, we show for the realistic setting with agents in 2D Euclidean space that the price of anarchy is between 1.75 and 1.8 and for higher dimensions it is less than 2. Also, we show that best response dynamics may cycle, but that in Euclidean space almost optimal approximate equilibria can be computed in polynomial time. Moreover, for 2D Euclidean space, these approximate equilibria outperform the well-known Delaunay triangulation.

</details>


### [9] [(Doubly) Exponential Lower Bounds for Follow the Regularized Leader in Potential Games](https://arxiv.org/abs/2601.23248)
*Ioannis Anagnostides,Ioannis Panageas,Nikolas Patris,Tuomas Sandholm*

Main category: cs.GT

TL;DR: FTRL算法在两人势博弈中收敛到纳什均衡需要指数时间，在多人势博弈中虚构博弈需要双指数时间，但交替执行的懒惰无遗憾动态有指数上界。


<details>
  <summary>Details</summary>
Motivation: 尽管FTRL是在线优化的主要算法，但几十年来对其在约束优化（特别是势博弈）中的收敛行为理解不足，需要明确其收敛时间界限。

Method: 分析FTRL在两人势博弈中的收敛时间，建立指数下界；研究交替执行的懒惰无遗憾动态，获得指数上界；分析多人势博弈中虚构博弈（FTRL的极端版本）的收敛时间。

Result: FTRL在两人势博弈中收敛到纳什均衡需要指数时间；交替执行的懒惰无遗憾动态有指数上界exp(O(1/ε²))；虚构博弈在多人势博弈中收敛需要双指数时间。

Conclusion: FTRL在势博弈中的收敛时间可能非常慢，特别是虚构博弈在多人博弈中需要双指数时间，这对博弈论中的基础学习算法有重要启示。

Abstract: Follow the regularized leader FTRL is the premier algorithm for online optimization. However, despite decades of research on its convergence in constrained optimization -- and potential games in particular -- its behavior remained hitherto poorly understood. In this paper, we establish that FTRL can take exponential time to converge to a Nash equilibrium in two-player potential games for any (permutation-invariant) regularizer and potentially vanishing learning rate. By known equivalences, this translates to an exponential lower bound for certain mirror descent counterparts, most notably multiplicative weights update. On the positive side, we establish the potential property for FTRL and obtain an exponential upper bound $\exp(O_ε(1/ε^2))$ for any no-regret dynamics executed in a lazy, alternating fashion, matching our lower bound up to factors in the exponent. Finally, in multi-player potential games, we show that fictitious play -- the extreme version of FTRL -- can take doubly exponential time to reach a Nash equilibrium. This constitutes an exponentially stronger lower bound for the foundational learning algorithm in games.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [10] [An Automatic Deep Learning Approach for Trailer Generation through Large Language Models](https://arxiv.org/abs/2601.23121)
*Roberto Balestri,Pasquale Cascarano,Mirko Degli Esposti,Guglielmo Pescatore*

Main category: cs.MM

TL;DR: 本文提出一个利用多模态策略和大型语言模型自动生成电影预告片的框架，相比现有方法能生成更具视觉吸引力的预告片。


<details>
  <summary>Details</summary>
Motivation: 传统预告片制作需要人工从电影中选取关键场景、对话和动作序列，并进行复杂的编辑工作。这个过程耗时且需要专业技能。需要一种自动化方法来高效生成高质量的预告片。

Method: 采用多模态策略和大型语言模型（LLM）的框架。LLM负责：1）选择与电影核心叙事相关的关键视觉序列；2）提取最具吸引力的台词，与预告片叙事对齐；3）协助创建音乐背景和旁白以增强观众参与度。

Result: 实验结果表明，该框架生成的预告片比之前最先进的竞争对手生成的预告片更具视觉吸引力。

Conclusion: 提出的多模态框架结合LLM能够自动生成高质量的预告片，不仅是对电影内容的总结，更是一种叙事体验本身，在视觉吸引力方面优于现有方法。

Abstract: Trailers are short promotional videos designed to provide audiences with a glimpse of a movie. The process of creating a trailer typically involves selecting key scenes, dialogues and action sequences from the main content and editing them together in a way that effectively conveys the tone, theme and overall appeal of the movie. This often includes adding music, sound effects, visual effects and text overlays to enhance the impact of the trailer. In this paper, we present a framework exploiting a comprehensive multimodal strategy for automated trailer production. Also, a Large Language Model (LLM) is adopted across various stages of the trailer creation. First, it selects main key visual sequences that are relevant to the movie's core narrative. Then, it extracts the most appealing quotes from the movie, aligning them with the trailer's narrative. Additionally, the LLM assists in creating music backgrounds and voiceovers to enrich the audience's engagement, thus contributing to make a trailer not just a summary of the movie's content but a narrative experience in itself. Results show that our framework generates trailers that are more visually appealing to viewers compared to those produced by previous state-of-the-art competitors.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [11] [Capacity of Two-User Wireless Systems Aided by Movable Signals](https://arxiv.org/abs/2601.22358)
*Matteo Nerini,Bruno Clerckx*

Main category: cs.IT

TL;DR: 可移动信号作为智能无线电环境的第三种方法，能够动态调整工作频率正交化用户信道，显著扩展多用户系统的容量区域，相比固定信号可提供高达45%的和速率增益。


<details>
  <summary>Details</summary>
Motivation: 可移动信号作为智能无线电环境的第三种新兴方法，需要研究其在多用户无线系统中的潜力，以补充可重构智能表面和灵活天线的不足。

Method: 研究两用户系统中的多址接入信道和广播信道的容量区域，通过可移动信号动态调整工作频率来正交化用户信道，并研究有限频率范围内的频率优化问题。

Result: 可移动信号能够显著扩展容量区域，在有限频率范围内优化时，相比固定信号可提供高达45%的和速率增益。

Conclusion: 可移动信号作为智能无线电环境的有力补充技术，通过动态频率调整能够有效正交化用户信道，显著提升多用户系统的性能。

Abstract: Movable signals have emerged as a third approach to enable smart radio environments (SREs), complementing reconfigurable intelligent surfaces (RISs) and flexible antennas. This paper investigates their potential to enhance multi-user wireless systems. Focusing on two-user systems, we characterize the capacity regions of the multiple access channel (MAC) and broadcast channel (BC). Interestingly, movable signals can dynamically adjust the operating frequency to orthogonalize the user channels, thereby significantly expanding the capacity regions. We also study frequency optimization, constraining it in a limited frequency range, and show that movable signals provide up to 45% sum rate gain over fixed signals.

</details>


### [12] [5G LDPC Codes as Root LDPC Codes via Diversity Alignment](https://arxiv.org/abs/2601.22470)
*Hyuntae Ahn,Inki Kim,Hee-Youl Kwak,Yongjune Kim,Chanki Kim,Sang-Hyo Kim*

Main category: cs.IT

TL;DR: 本文提出了一种基于布尔函数的多样性演化(DivE)分析方法，用于分析准循环LDPC码在非遍历块衰落信道下的多样性性能，并开发了贪婪块映射搜索算法来保证所有信息比特获得完全分集。


<details>
  <summary>Details</summary>
Motivation: 研究准循环LDPC码在非遍历块衰落信道下的分集性能，现有方法难以分析迭代译码过程中的分集演化，需要开发新的分析工具来优化码字设计。

Method: 提出多样性演化(DivE)分析方法，基于布尔函数跟踪迭代译码过程中消息的衰落依赖性演化；开发贪婪块映射搜索算法，将原图变量节点分配到衰落块中，使所有信息变量节点获得完全分集。

Result: 在5G新空口LDPC码上的数值结果表明，所提搜索算法找到的块映射能保证所有信息比特获得完全分集，无需修改基础图结构，相比随机映射具有更陡的高信噪比斜率和更低的误块率。

Conclusion: DivE方法为分析迭代译码下的分集性能提供了有效工具，贪婪块映射搜索算法能够优化码字在衰落信道下的性能，为实际系统设计提供了有价值的指导。

Abstract: This paper studies the diversity of protographbased quasi-cyclic low-density parity-check (QC-LDPC) codes over nonergodic block-fading channels under iterative beliefpropagation decoding. We introduce diversity evolution (DivE), a Boolean-function-based analysis method that tracks how the fading dependence of belief-propagation messages evolves across decoding iterations. Under a Boolean approximation of block fading, DivE derives a Boolean fading function for each variable node (VN) output (i.e., the a-posteriori reliability after iterative decoding), from which the VN diversity order can be directly determined. Building on this insight, we develop a greedy blockmapping search that assigns protograph VNs to fading blocks so that all information VNs achieve full diversity, while including the minimum additional parity VNs when full diversity is infeasible at the nominal rate. Numerical results on the 5G New Radio LDPC codes show that the proposed search finds block mappings that guarantee full diversity for all information bits without modifying the base-graph structure, yielding a markedly steeper high-SNR slope and lower BLER than random mappings.

</details>


### [13] [Successive Cancellation List Decoding of Extended Reed-Solomon Codes](https://arxiv.org/abs/2601.22482)
*Xiaoqian Ye,Jingyu Lin,Junjie Huang,Li Chen,Chang-An Zhao*

Main category: cs.IT

TL;DR: 提出了一种针对特征为2的有限域上扩展RS码的新列表译码方法，通过将eRS码转换为n个二进制极化码，利用SC和SCL译码实现。


<details>
  <summary>Details</summary>
Motivation: RS码作为重要的非二进制纠错码，在纠正突发错误方面表现优异，广泛应用于现代通信和数据存储系统。然而，需要开发更高效的译码方法来充分利用其MDS特性。

Method: 将扩展RS码转换为n个二进制极化码，然后使用连续消除（SC）译码及其列表版本（SCL译码）。通过预变换矩阵重新解释eRS码，并研究其列线性独立性以理论表征SC译码性能。

Result: 提出的译码方法通过数值实验得到验证，理论分析表明预变换矩阵的列线性独立性影响SC译码性能。

Conclusion: 成功开发了一种基于极化码转换的扩展RS码列表译码方法，为特征为2的有限域上的eRS码提供了新的高效译码解决方案。

Abstract: Reed-Solomon (RS) codes are an important class of non-binary error-correction codes. They are particularly competent in correcting burst errors, being widely applied in modern communications and data storage systems. This also thanks to their distance property of reaching the Singleton bound, being the maximum distance separable (MDS) codes. This paper proposes a new list decoding for extended RS (eRS) codes defined over a finite field of characteristic two, i.e., F_{2^n}. It is developed based on transforming an eRS code into n binary polar codes. Consequently, it can be decoded by the successive cancellation (SC) decoding and further their list decoding, i.e., the SCL decoding. A pre-transformed matrix is required for reinterpretating the eRS codes, which also determines their SC and SCL decoding performances. Its column linear independence property is studied, leading to theoretical characterization of their SC decoding performance. Our proposed decoding and analysis are validated numerically.

</details>


### [14] [Flexible FTN-OTFS for High-Mobility LEO Satellite-to-Ground Communication](https://arxiv.org/abs/2601.22526)
*Chaorong Zhang,Hui Xu,Benjamin K. Ng,Yue Liu,Chan-Tong Lam,Halim Yanikomeroglu*

Main category: cs.IT

TL;DR: 提出轻量级LEO卫星辅助的灵活FTN-OTFS方案，解决星上功耗限制和快速时变信道问题，通过SNR感知的灵活FTN策略和查找表优化时间压缩因子，实现频谱效率最大化与可靠性平衡。


<details>
  <summary>Details</summary>
Motivation: 针对非地面网络中星上功耗严格限制和快速时变信道严重影响的问题，需要设计既能提高频谱效率又能保持低复杂度的通信方案。

Method: 建立基于3GPP TDL信道模型的系统框架，引入SNR感知的灵活FTN策略，使用低复杂度查找表自适应优化时间压缩因子，平衡速率提升与干扰惩罚。

Result: 通过理论分析和仿真验证，该方案显著优于静态FTN基准，在吞吐量、能量效率和误码率方面表现优异，为下一代LEO通信提供了高吞吐与鲁棒性的平衡方案。

Conclusion: 提出的LEO-FFTN-OTFS方案有效解决了星上功耗和快速时变信道挑战，通过自适应FTN策略实现了频谱效率最大化与可靠性的最佳平衡，具有实际应用价值。

Abstract: In this paper, a lightweight LEO satellite-assisted flexible faster-than-Nyquist (FTN)-orthogonal time frequency space (OTFS) (LEO-FFTN-OTFS) scheme is proposed to address the stringent constraints on onboard power consumption and the severe impact of fast time-varying channels in non-terrestrial networks. A rigorous system framework incorporating realistic 3GPP Tapped Delay Line (TDL) channel models is established to accurately capture high-mobility propagation characteristics. To counteract channel aging effects while maintaining low computational complexity, an SNR-aware flexible FTN strategy is introduced, wherein a low-complexity Look-Up Table (LUT) is utilized to adaptively optimize the time-domain compression factor based on instantaneous channel responses. Through this mechanism, the trade-off between rate acceleration and interference penalty is effectively resolved, ensuring that spectral efficiency is maximized while strict reliability constraints are satisfied with minimal processing overhead. Moreover, a comprehensive theoretical analysis is provided, in which analytical expressions for effective throughput, energy efficiency, and bit error rate are derived. Finally, it is demonstrated by extensive simulations that the proposed scheme significantly outperforms static FTN benchmarks, offering a superior balance of high throughput and robustness for next-generation LEO communications.

</details>


### [15] [Quantum $(r,δ)$-Locally Recoverable BCH and Homothetic-BCH Codes](https://arxiv.org/abs/2601.22567)
*Carlos Galindo,Fernando Hernando,Ryutaroh Matsumoto*

Main category: cs.IT

TL;DR: 该论文研究如何从BCH码和同态BCH码构造量子(r,δ)-局部可恢复码，获得了满足Singleton-like界的优化量子LRCs


<details>
  <summary>Details</summary>
Motivation: 量子(r,δ)-局部可恢复码是经典(r,δ)-LRCs的量子版本，用于大规模分布式和云存储系统中的多故障恢复。需要从现有编码理论中寻找构造量子LRCs的方法。

Method: 从BCH码和同态BCH码出发，利用欧几里得或厄米特对偶包含的(r,δ)-LRCs构造量子(r,δ)-LRCs。通过分析这些经典码的特性来获得量子版本。

Result: 获得了纯量子(r,δ)-局部可恢复码，这些码对于Singleton-like界是最优的，即达到了理论上的性能极限。

Conclusion: 成功展示了从BCH码和同态BCH码构造量子(r,δ)-LRCs的有效方法，并获得了优化码，为量子存储系统的容错设计提供了理论支持。

Abstract: Quantum $(r,δ)$-locally recoverable codes ($(r,δ)$-LRCs) are the quantum version of classical $(r,δ)$-LRCs designed to recover multiple failures in large-scale distributed and cloud storage systems. A quantum $(r,δ)$-LRC, $Q(C)$, can be constructed from an $(r,δ)$-LRC, $C$, which is Euclidean or Hermitian dual-containing.
  This article is devoted to studying how to get quantum $(r,δ)$-LRCs from BCH and homothetic-BCH codes. As a consequence, we give pure quantum $(r,δ)$-LRCs which are optimal for the Singleton-like bound.

</details>


### [16] [Multi-target DoA estimation with a single Rydberg atomic receiver by spectral analysis of spatially-resolved fluorescence](https://arxiv.org/abs/2601.22704)
*Liangcheng Han,Haifan Yin,Mérouane Debbah*

Main category: cs.IT

TL;DR: 提出基于成像的谱估计方法，通过空间解析荧光剖面解决多目标DoA估计问题，消除单元长度限制，恢复传感器全宽带能力。


<details>
  <summary>Details</summary>
Motivation: 现有Rydberg DoA估计方法受限于接收器阵列复杂性和单接收器方法的单目标、窄带限制，需要解决多目标检测和宽带能力问题。

Method: 通过将入射信号与强本地振荡器叠加，将复杂原子吸收模式线性化为正弦波叠加，使空间频率直接映射到目标DoA，使用Prony方法进行谱估计（ISE方法）。

Result: 理论模型验证了方法的有效性，推导了CRLB性能基准，仿真表明能够有效解析多个目标，支持多目标检测并恢复全宽带能力。

Conclusion: ISE方法解决了Rydberg DoA估计的多目标限制，为多通道接收器和连续孔径传感（全息MIMO）的实现奠定了基础。

Abstract: Rydberg-based Direction-of-Arrival (DoA) estimation has been hampered by the complexity of receiver arrays and the single-target, narrow-band limitations of existing single-receiver methods. This paper introduces a novel approach that addresses these limitations. We demonstrate that by spatially resolving the fluorescence profile along the vapor cell, the multi-target problem can be effectively solved. Our approach hinges on the insight that by superimposing incoming signals with a strong local oscillator (LO), the complex atomic absorption pattern is linearized into a simple superposition of sinusoids. In this new representation, each spatial frequency uniquely and directly maps to the DoA of a target. This reduces the multi-target challenge into a spectral estimation problem, which we address using Prony's method. Our approach, termed Imaging-based Spectral Estimation (ISE), inherently supports multi-target detection and restores the full broadband capability of the sensor by removing the restrictive cell-length dependency. This development also shows potential for realizing multi-channel Rydberg receivers and the continuous-aperture sensing required for holographic multiple-input multiple-output (MIMO). We develop a comprehensive theoretical model, derive the Cramer-Rao Lower Bound (CRLB) as a performance benchmark, and present simulations validating the effectiveness of the approach to resolve multiple targets.

</details>


### [17] [Status Updating via Integrated Sensing and Communication: Freshness Optimisation](https://arxiv.org/abs/2601.22901)
*Touraj Soleymani,Mohamad Assaad,John S. Baras*

Main category: cs.IT

TL;DR: 论文研究集成感知与通信（ISAC）架构中远程导航代理状态更新的策略设计，通过马尔可夫决策过程优化信息新鲜度（AoI）与感知/通信开销的长期成本，证明最优策略具有单调阈值结构。


<details>
  <summary>Details</summary>
Motivation: 在集成感知与通信（ISAC）系统中，远程导航代理的状态更新需要同时考虑信息新鲜度（AoI）和资源开销。现有研究缺乏将信息新鲜度目标自然融入ISAC设计的框架，需要开发可解释且可实现的策略来平衡感知、通信成本与信息时效性。

Method: 将问题建模为折扣无限时域马尔可夫决策过程，状态空间为二维AoI（源端和基站端的信息新鲜度）。感知和通信操作分别以一定概率成功并产生不同成本。通过分析证明最优平稳策略具有单调阈值结构，表现为AoI状态空间中的非递减切换曲线。

Result: 理论证明最优平稳策略具有单调阈值结构，由AoI状态空间中的非递减切换曲线表征。数值分析展示了值函数和最优决策映射的结构，验证了理论结果。这表明基于信息新鲜度的目标可以自然地融入ISAC设计，同时产生可解释且可实施的策略。

Conclusion: 该研究成功将信息新鲜度（AoI）目标整合到ISAC系统设计中，通过马尔可夫决策过程框架得出了具有单调阈值结构的最优策略。这种结构化的策略不仅理论上有保证，而且在实际中易于解释和实施，为ISAC系统中状态更新的资源分配提供了有效解决方案。

Abstract: This paper studies strategic design in an integrated sensing and communication (ISAC) architecture for status updating of remotely navigating agents. We consider an ISAC-enabled base station that can sense the state of a remote source and communicate this information back to the source. Both sensing and communication succeed with given probabilities and incur distinct costs. The objective is to optimise a long-term cost that captures information freshness, measured by the age of information (AoI), at the source together with sensing and communication overheads. The resulting sequential decision problem is formulated as a discounted infinite-horizon Markov decision process with a two-dimensional AoI state, representing information freshness at the source and at the base station. We prove that the optimal stationary policy admits a monotone threshold structure characterised by a nondecreasing switching curve in the AoI state space. Our numerical analysis illustrates the structures of the value function and the optimal decision map. These results demonstrate that freshness-based objectives can be naturally integrated into ISAC design, while yielding interpretable and implementable strategies.

</details>


### [18] [Feedback Control via Integrated Sensing and Communication: Uncertainty Optimisation](https://arxiv.org/abs/2601.22912)
*Touraj Soleymani,Mohamad Assaad,John S. Baras*

Main category: cs.IT

TL;DR: 研究ISAC架构中用于网络物理系统反馈控制的策略设计，在伯努利传感和通信链路下，证明了最优切换策略基于阈值，最优控制策略是线性的。


<details>
  <summary>Details</summary>
Motivation: 研究集成传感与通信（ISAC）架构在网络物理系统反馈控制中的策略设计问题，解决基站如何在跟踪源状态和传输控制信息之间切换的优化问题。

Method: 针对高斯-马尔可夫源和独立同分布伯努利传感/通信链路，采用有限时域线性二次高斯成本框架，通过不确定性感知综合方法严格表征最优策略。

Result: 证明基站ISAC系统的最优切换策略基于源和基站估计协方差的阈值，源端执行器的最优控制策略是源状态估计的线性函数；阈值区域随源不确定性增加而扩大，随基站不确定性增加而收缩。

Conclusion: 在ISAC反馈控制系统中，最优策略具有阈值特性，为网络物理系统的集成传感与通信设计提供了理论指导。

Abstract: This paper studies strategic design in an integrated sensing and communication (ISAC) architecture for feedback control of cyber-physical systems. We focus on a setting in which the regulation of a physical process (i.e., remote source) is performed via an ISAC-enabled base station. The base station can alternate between tracking the state of the source and delivering control-relevant information back to the source. For a Gauss-Markov source subject to i.i.d. Bernoulli sensing and communication links, under a finite-horizon linear-quadratic-Gaussian cost, we rigorously characterise the optimal policies through an uncertainty-aware synthesis. We establish that the optimal switching policy, for the ISAC system at the base station, is threshold-based in terms of the source and base-station estimation covariances, while the optimal control policy, for the actuator at the source, is linear in the source state estimate. We show that the threshold region$\unicode{x2014}$defined as the set of estimation covariance pairs for which communication is preferred over sensing$\unicode{x2014}$expands with increasing source uncertainty and contracts with increasing base-station uncertainty.

</details>


### [19] [A complete characterisation of conditional entropies](https://arxiv.org/abs/2601.23213)
*Roberto Rubboli,Erkka Haapasalo,Marco Tomamichel*

Main category: cs.IT

TL;DR: 该论文完全表征了满足基本操作公理的广义条件熵，证明其最一般形式由Rényi熵的指数平均构成，并应用于量子热力学第二定律。


<details>
  <summary>Details</summary>
Motivation: 尽管Rényi熵族在信息论中作为最一般的熵度量已被充分表征，但条件熵的各种有用定义虽然存在，却缺乏一个满足自然操作公理的完整表征。条件熵通过考虑相关侧信息来量化不确定性，在信息论、统计学和定量科学中具有核心重要性，因此需要建立其公理基础。

Method: 作者通过一组基本操作公理来定义条件熵：独立随机变量的可加性、重标记不变性、以及条件混合信道下的单调性。基于这些公理，他们证明了最一般的条件熵形式由Rényi熵的条件分布的指数平均构成，参数化为一个实参数和一个正实数上的概率测度。

Result: 论文完全表征了满足操作公理的条件熵族，证明这些量决定了条件混合下的转换速率，并为能量本征基对角化的量子态提供了带侧信息的量子热力学第二定律集合。

Conclusion: 该工作为条件熵建立了完整的公理基础，揭示了其最一般形式，并将这些新定义应用于量子热力学，扩展了带侧信息的资源理论框架，为信息论和统计学的进一步发展提供了理论基础。

Abstract: Entropies are fundamental measures of uncertainty with central importance in information theory and statistics and applications across all the quantitative sciences. Under a natural set of operational axioms, the most general form of entropy is captured by the family of Rényi entropies, parameterized by a real number $α$. Conditional entropy extends the notion of entropy by quantifying uncertainty from the viewpoint of an observer with access to potentially correlated side information. However, despite their significance and the emergence of various useful definitions, a complete characterization of measures of conditional entropy that satisfy a natural set of operational axioms has remained elusive. In this work, we provide a complete characterization of conditional entropy, defined through a set of axioms that are essential for any operationally meaningful definition: additivity for independent random variables, invariance under relabeling, and monotonicity under conditional mixing channels. We prove that the most general form of conditional entropy is captured by a family of measures that are exponential averages of Rényi entropies of the conditioned distribution and parameterized by a real parameter and a probability measure on the positive reals. Finally, we show that these quantities determine the rate of transformation under conditional mixing and provide a set of second laws of quantum thermodynamics with side information for states diagonal in the energy eigenbasis.

</details>


### [20] [Secure Integrated Sensing and Communication against Communication and Sensing Eavesdropping](https://arxiv.org/abs/2601.23216)
*Sidong Guo,Matthieu R. Bloch*

Main category: cs.IT

TL;DR: 本文研究了安全集成感知与通信系统中，在存在被动对手的情况下，保密通信与隐私感知之间的基本权衡关系。


<details>
  <summary>Details</summary>
Motivation: 在对抗性无线环境中，隐私感知和通信保密性扮演着不同但相互关联的角色。在集成感知与通信系统中，同一波形同时服务于双重目的，这使得在单一物理层框架内捕捉这种相互作用特别具有挑战性。

Method: 研究一个安全的ISAC系统，其中单基地发射机同时向合法接收机发送保密消息并感知环境状态，而被动对手试图同时进行消息解码和状态估计。通过联合输入分布、反馈提取密钥、窃听码和可分辨性码等技术，部分表征了三个性能指标之间的基本权衡：发射机的保密率、检测指数和对手的检测指数。

Result: 推导了一个可达区域，并通过数值示例说明了由此产生的设计权衡。研究揭示了在存在被动对手的情况下，保密通信与隐私感知之间的基本权衡关系。

Conclusion: 本文为安全ISAC系统提供了一个理论框架，揭示了保密通信与隐私感知之间的基本权衡，这些权衡不仅受联合输入分布的影响，还受发射机通过反馈提取密钥以及通过窃听码和可分辨性码隐藏码字内容和结构的能力的影响。

Abstract: Sensing privacy and communication confidentiality play fundamentally different but interconnected roles in adversarial wireless environments. Capturing this interplay within a single physical-layer framework is particularly challenging in integrated sensing and communication (ISAC) systems, where the same waveform simultaneously serves dual purposes. We study a secure ISAC system in which a monostatic transmitter simultaneously sends a confidential message to a legitimate receiver and senses an environmental state, while a passive adversary attempts both message decoding and state estimation. We partially characterize the fundamental trade-offs among three performance measures: the transmitter's secrecy rate, its detection exponent, and the adversary's detection exponent. Beyond the joint input distribution that governs overall performance, the trade-offs are further shaped by the transmitter's ability to extract keys via feedback and hide both the content and structure of the codewords via wiretap and resolvability codes. We derive an achievable region, and illustrate the resulting design trade-offs through a numerical example.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [21] [An innovating approach to teaching applied to database design. Improvement of Action Learning in Lifelong Learning](https://arxiv.org/abs/2601.22175)
*Christophe Béchade*

Main category: cs.DB

TL;DR: 昂热大学通过行动学习模式，让企业员工参与数据库设计项目，结合职业教育、企业咨询和高校教学优势，实现理论与实践结合的职业培训。


<details>
  <summary>Details</summary>
Motivation: 传统数据库设计仅限专业技术人员掌握，企业员工虽有业务流程知识但缺乏数据处理能力。行动学习旨在将高校技术课程的成功因素、职业培训的适应性教学以及服务提供商的咨询经验相结合，为无数据处理背景的企业员工提供实用培训。

Method: 采用行动学习模式，教师担任项目管理的监督者角色，组织由专业机构资助的实际数据库设计项目。培训在昂热大学继续教育学院框架下进行，借鉴法国技术课程中已实施20多年的横向学期项目经验，让学员应用新学知识。

Result: 行动学习成功整合了法国技术课程的成功因素、职业培训的适应性教学以及服务提供商的咨询能力，实现了可评估且持久的教学与职业目标，促进了大学与企业之间的合作。

Conclusion: 行动学习模式结合了职业教育、企业实践和高校教学三方面的优势，是法国促进大学与企业合作政策的重要组成部分，能够实现长期可持续的教学与职业发展目标。

Abstract: For now 10 years, the Action Learning has allowed employees of University of Angers, private and public Companies to be initiated with the design of database, on projects financed by professional structures. These innovating training periods are carried out within the framework of the University College of Further Education of the University of Angers. Database design is a process initially reserved to the professional data processing specialists, coming from French Level-2 technological courses (2-year degrees) or Engineer Schools (Master). The pedagogical model of technological courses has integrated for more than 20 years transverse semester projects, in order to give the students the opportunity to apply newly acquired knowledge, coordinated by teachers. Action Learning requires teachers to assume the role of supervisors for the project management. The objective of Action Learning is to transmit not only knowledge from teachers, but also the experience of consultants to trainees having no competence in data processing, but who have the knowledge of their business process. The present paper shows that Action Learning puts together the factors for success of French technological courses, the adaptability of pedagogy provided to the vocational training, and finally the competence of service provider, Keeping the best parts of those three complementary approaches makes it possible for this kind of formation to achieve teaching and professional, assessable and long lasting goals. Action Learning belongs to the French policy that aims to improve the volume and the quality of the contracts between Universities and companies.

</details>


### [22] [Discovering High-utility Sequential Rules with Increasing Utility Ratio](https://arxiv.org/abs/2601.22178)
*Zhenqiang Ye,Wensheng Gan,Gengsen Huang,Tianlong Gu,Philip S. Yu*

Main category: cs.DB

TL;DR: 提出SRIU算法，用于挖掘具有递增效用比的高效用序列规则，通过左右扩展和右左扩展两种方法，并采用多种剪枝策略和优化技术提高效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于规则增长的高效用序列规则挖掘方法中，规则与其生成过程之间的关联不明确，不清楚新项目的添加如何影响规则的效用或置信度，因此需要研究具有递增效用比的规则挖掘。

Method: 提出SRIU算法，采用两种扩展方法（左右扩展和右左扩展），引入项目对估计效用剪枝策略（IPEUP）减少搜索空间，为两种扩展方法设计上界和剪枝策略，并采用Bitmap减少内存消耗和紧凑效用表等优化技术。

Result: 在真实世界和合成数据集上的广泛实验结果表明，SRIU方法有效且能提高挖掘结果的相关性。使用置信度和确信度等指标评估生成的序列规则质量，进一步证明SRIU能改善结果相关性。

Conclusion: SRIU算法成功解决了挖掘具有递增效用比的高效用序列规则问题，通过创新的扩展方法和优化策略，在效率和结果质量方面都表现出色，为决策提供了更可靠的序列规则信息。

Abstract: Utility-driven mining is an essential task in data science, as it can provide deeper insight into the real world. High-utility sequential rule mining (HUSRM) aims at discovering sequential rules with high utility and high confidence. It can certainly provide reliable information for decision-making because it uses confidence as an evaluation metric, as well as some algorithms like HUSRM and US-Rule. However, in current rule-growth mining methods, the linkage between HUSRs and their generation remains ambiguous. Specifically, it is unclear whether the addition of new items affects the utility or confidence of the former rule, leading to an increase or decrease in their values. Therefore, in this paper, we formulate the problem of mining HUSRs with an increasing utility ratio. To address this, we introduce a novel algorithm called SRIU for discovering all HUSRs with an increasing utility ratio using two distinct expansion methods, including left-right expansion and right-left expansion. SRIU also utilizes the item pair estimated utility pruning strategy (IPEUP) to reduce the search space. Moreover, for the two expansion methods, two sets of upper bounds and corresponding pruning strategies are introduced. To enhance the efficiency of SRIU, several optimizations are incorporated. These include utilizing the Bitmap to reduce memory consumption and designing a compact utility table for the mining procedure. Finally, extensive experimental results from both real-world and synthetic datasets demonstrate the effectiveness of the proposed method. Moreover, to better assess the quality of the generated sequential rules, metrics such as confidence and conviction are employed, which further demonstrate that SRIU can improve the relevance of mining results.

</details>


### [23] [High-utility Sequential Rule Mining Utilizing Segmentation Guided by Confidence](https://arxiv.org/abs/2601.22179)
*Chunkai Zhang,Jiarui Deng,Maohua Lyu,Wensheng Gan,Philip S. Yu*

Main category: cs.DB

TL;DR: 提出RSC算法，通过置信度引导的分割减少高效用序列规则挖掘中的冗余效用计算


<details>
  <summary>Details</summary>
Motivation: 现有高效用序列规则挖掘算法存在冗余效用计算问题，不同规则可能包含相同项序列，当这些项能形成多个不同规则时需要重复计算效用

Method: 采用置信度引导的分割方法，预计算分割规则的置信度，利用效用链接表加速候选序列生成，引入更严格的效用上界（序列的减少剩余效用）处理重复项

Result: 在多个数据集上评估RSC方法，结果显示相比现有最优方法有改进

Conclusion: RSC算法通过减少冗余效用计算，提高了高效用序列规则挖掘的效率

Abstract: Within the domain of data mining, one critical objective is the discovery of sequential rules with high utility. The goal is to discover sequential rules that exhibit both high utility and strong confidence, which are valuable in real-world applications. However, existing high-utility sequential rule mining algorithms suffer from redundant utility computations, as different rules may consist of the same sequence of items. When these items can form multiple distinct rules, additional utility calculations are required. To address this issue, this study proposes a sequential rule mining algorithm that utilizes segmentation guided by confidence (RSC), which employs confidence-guided segmentation to reduce redundant utility computation. It adopts a method that precomputes the confidence of segmented rules by leveraging the support of candidate subsequences in advance. Once the segmentation point is determined, all rules with different antecedents and consequents are generated simultaneously. RSC uses a utility-linked table to accelerate candidate sequence generation and introduces a stricter utility upper bound, called the reduced remaining utility of a sequence, to address sequences with duplicate items. Finally, the proposed RSC method was evaluated on multiple datasets, and the results demonstrate improvements over state-of-the-art approaches.

</details>


### [24] [COL-Trees: Efficient Hierarchical Object Search in Road Networks](https://arxiv.org/abs/2601.22183)
*Tenindra Abeywickrama,Muhammad Aamir Cheema,Sabine Storandt*

Main category: cs.DB

TL;DR: 提出COL-Tree数据结构，使用地标启发式实现高效层次图遍历，显著提升聚合k近邻、k最远邻等查询性能，相比现有方法提升达4个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有位置服务技术主要关注单个代理的最近邻查询，无法有效处理聚合k近邻、k最远邻等复杂查询。现有层次方法依赖欧几里得启发式，在道路网络等图结构中效果有限。

Method: 提出COL-Tree（紧凑对象-地标树）数据结构，使用更准确的地标启发式实现高效层次图遍历。基于COL-Tree设计查询算法，支持聚合k近邻、k最远邻等多种查询。

Result: 在真实世界和合成数据集上的实验表明，该方法显著优于现有方法，性能提升达4个数量级，且预处理开销在理论和实践中都很小。

Conclusion: COL-Tree通过地标启发式实现了高效的层次图遍历，能够有效解决聚合k近邻、k最远邻等复杂查询问题，为位置服务提供了更强大的工具。

Abstract: Location-based services rely heavily on efficient methods that search for relevant points-of-interest (POIs) near a given location. A k Nearest Neighbor (kNN) query is one such example that finds the k closest POIs from an agent's location. While most existing techniques focus on retrieving nearby POIs for a single agent, these search heuristics do not translate to many other applications. For example, Aggregate k Nearest Neighbor (AkNN) queries require POIs that are close to multiple agents. k Farthest Neighbor (kFN) queries require POIs that are the antithesis of nearest. Such problems naturally benefit from a hierarchical approach, but existing methods rely on Euclidean-based heuristics, which have diminished effectiveness in graphs such as road networks. We propose a novel data structure, COL-Tree (Compacted Object-Landmark Tree), to address this gap by enabling efficient hierarchical graph traversal using a more accurate landmark-based heuristic. We then present query algorithms that utilize COL-Trees to efficiently answer AkNN, kFN, and other queries. In our experiments on real-world and synthetic datasets, we demonstrate that our techniques significantly outperform existing approaches, achieving up to 4 orders of magnitude improvement. Moreover, this comes at a small pre-processing overhead in both theory and practice.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [25] [Compact Hypercube Embeddings for Fast Text-based Wildlife Observation Retrieval](https://arxiv.org/abs/2601.22783)
*Ilyass Moummad,Marius Miron,David Robinson,Kawtar Zaher,Hervé Goëau,Olivier Pietquin,Pierre Bonnet,Emmanuel Chemla,Matthieu Geist,Alexis Joly*

Main category: cs.IR

TL;DR: 提出紧凑超立方体嵌入方法，用于基于文本的野生动物观测快速检索，通过二进制表示实现大规模图像和音频数据库的高效搜索。


<details>
  <summary>Details</summary>
Motivation: 大规模生物多样性监测平台依赖多模态野生动物观测，但现有基础模型的高维相似性搜索计算成本高，难以从海量档案中检索相关观测。

Method: 基于跨视图代码对齐哈希框架，将轻量级哈希扩展到多模态设置，在共享汉明空间中对齐自然语言描述与视觉或听觉观测。利用预训练的野生动物基础模型（BioCLIP和BioLingual），通过参数高效微调进行哈希适配。

Result: 在iNaturalist2024（文本到图像检索）和iNatSounds2024（文本到音频检索）等大规模基准测试中，离散超立方体嵌入相比连续嵌入实现了竞争性甚至更优的性能，同时大幅降低内存和搜索成本。哈希目标持续改进底层编码器表示，增强了检索和零样本泛化能力。

Conclusion: 二进制、基于语言的检索方法为生物多样性监测系统提供了可扩展且高效的大规模野生动物档案搜索方案。

Abstract: Large-scale biodiversity monitoring platforms increasingly rely on multimodal wildlife observations. While recent foundation models enable rich semantic representations across vision, audio, and language, retrieving relevant observations from massive archives remains challenging due to the computational cost of high-dimensional similarity search. In this work, we introduce compact hypercube embeddings for fast text-based wildlife observation retrieval, a framework that enables efficient text-based search over large-scale wildlife image and audio databases using compact binary representations. Building on the cross-view code alignment hashing framework, we extend lightweight hashing beyond a single-modality setup to align natural language descriptions with visual or acoustic observations in a shared Hamming space. Our approach leverages pretrained wildlife foundation models, including BioCLIP and BioLingual, and adapts them efficiently for hashing using parameter-efficient fine-tuning. We evaluate our method on large-scale benchmarks, including iNaturalist2024 for text-to-image retrieval and iNatSounds2024 for text-to-audio retrieval, as well as multiple soundscape datasets to assess robustness under domain shift. Results show that retrieval using discrete hypercube embeddings achieves competitive, and in several cases superior, performance compared to continuous embeddings, while drastically reducing memory and search cost. Moreover, we observe that the hashing objective consistently improves the underlying encoder representations, leading to stronger retrieval and zero-shot generalization. These results demonstrate that binary, language-based retrieval enables scalable and efficient search over large wildlife archives for biodiversity monitoring systems.

</details>


### [26] [FITMM: Adaptive Frequency-Aware Multimodal Recommendation via Information-Theoretic Representation Learning](https://arxiv.org/abs/2601.22498)
*Wei Yang,Rui Zhong,Yiqun Chen,Shixuan Li,Heng Ping,Chi Lu,Peng Jiang*

Main category: cs.IR

TL;DR: FITMM是一个基于频率感知信息理论的跨模态推荐框架，通过频域分解和多模态融合提升推荐性能


<details>
  <summary>Details</summary>
Motivation: 传统多模态推荐在空间域融合模态，会模糊信号的频率结构，加剧模态错位和冗余。需要一种更有效的方法来利用频域信息

Method: 提出FITMM框架：1)构建图增强的物品表示；2)模态级谱分解获得正交频带；3)构建轻量级带内多模态组件；4)使用残差任务自适应门聚合频带；5)引入频域信息瓶颈正则化控制冗余；6)添加跨模态谱一致性损失对齐模态

Result: 在三个真实世界数据集上的广泛实验表明，FITMM始终显著优于先进基线方法

Conclusion: 通过频域视角和信息理论框架，FITMM为多模态推荐提供了一种更有效的分离-融合范式，能够更好地建模用户偏好

Abstract: Multimodal recommendation aims to enhance user preference modeling by leveraging rich item content such as images and text. Yet dominant systems fuse modalities in the spatial domain, obscuring the frequency structure of signals and amplifying misalignment and redundancy. We adopt a spectral information-theoretic view and show that, under an orthogonal transform that approximately block-diagonalizes bandwise covariances, the Gaussian Information Bottleneck objective decouples across frequency bands, providing a principled basis for separate-then-fuse paradigm. Building on this foundation, we propose FITMM, a Frequency-aware Information-Theoretic framework for multimodal recommendation. FITMM constructs graph-enhanced item representations, performs modality-wise spectral decomposition to obtain orthogonal bands, and forms lightweight within-band multimodal components. A residual, task-adaptive gate aggregates bands into the final representation. To control redundancy and improve generalization, we regularize training with a frequency-domain IB term that allocates capacity across bands (Wiener-like shrinkage with shut-off of weak bands). We further introduce a cross-modal spectral consistency loss that aligns modalities within each band. The model is jointly optimized with the standard recommendation loss. Extensive experiments on three real-world datasets demonstrate that FITMM consistently and significantly outperforms advanced baselines.

</details>


### [27] [SCaLRec: Semantic Calibration for LLM-enabled Cloud-Device Sequential Recommendation](https://arxiv.org/abs/2601.22543)
*Ruiqi Zheng,Jinli Cao,Jiao Yin,Hongzhi Yin*

Main category: cs.IR

TL;DR: SCaLRec提出了一种云-设备协同推荐系统，通过设备端语义校准模块解决缓存语义陈旧问题，无需每次请求都调用云端LLM，提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有云-设备推荐系统面临语义陈旧问题：由于无法为每个请求重新生成云端LLM语义嵌入，设备端重用缓存的语义表示会逐渐与用户最新交互脱节，导致推荐质量下降。

Method: SCaLRec包含两个核心组件：1) 评估缓存语义在用户最新交互下的可靠性；2) 设备端语义校准模块，利用最新交互证据调整缓存的语义嵌入，无需每次请求调用云端LLM。

Result: 在真实世界数据集上的实验表明，SCaLRec在云端语义陈旧的情况下，相比强基线方法能持续提升推荐性能。

Conclusion: SCaLRec有效解决了云-设备协同推荐中的语义陈旧问题，通过设备端校准机制在保持隐私和响应性的同时提升推荐质量，为实际大规模部署提供了可行方案。

Abstract: Cloud-device collaborative recommendation partitions computation across the cloud and user devices: the cloud provides semantic user modeling, while the device leverages recent interactions and cloud semantic signals for privacy-preserving, responsive reranking. With large language models (LLMs) on the cloud, semantic user representations can improve sequential recommendation by capturing high-level intent. However, regenerating such representations via cloud LLM inference for every request is often infeasible at real-world scale. As a result, on-device reranking commonly reuses a cached cloud semantic user embedding across requests. We empirically identify a cloud semantic staleness effect: reused embeddings become less aligned with the user's latest interactions, leading to measurable ranking degradation.
  Most existing LLM-enabled cloud-device recommenders are typically designed around on-demand cloud semantics, either by assuming low-latency cloud LLM access or by regenerating semantic embeddings per request. When per-request regeneration is infeasible and cached semantics must be reused, two technical challenges arise: (1) deciding when cached cloud semantics remain useful for on-device reranking, and (2) maintaining ranking quality when the cloud LLM cannot be invoked and only cached semantics are available. To address this gap, we introduce the Semantic Calibration for LLM-enabled Cloud-Device Recommendation (SCaLRec). First, it estimates the reliability of cached semantics under the user's latest interactions. Second, an on-device semantic calibration module is proposed to adjusts the cached semantic embedding on-device using up-to-date interaction evidence, without per-request cloud LLM involvement. Experiments on real-world datasets show that SCaLRec consistently improves recommendation performance over strong baselines under cloud semantic staleness.

</details>


### [28] [PersonaAct: Simulating Short-Video Users with Personalized Agents for Counterfactual Filter Bubble Auditing](https://arxiv.org/abs/2601.22547)
*Shilong Zhao,Qinggang Yang,Zhiyi Yin,Xiaoshi Wang,Zhenxing Chen,Du Su,Xueqi Cheng*

Main category: cs.IR

TL;DR: PersonaAct：一个基于多模态代理的短视频用户模拟框架，用于审计推荐系统中的过滤气泡现象，通过真实行为轨迹训练，在内容多样性和逃离潜力方面评估过滤气泡的广度和深度。


<details>
  <summary>Details</summary>
Motivation: 短视频平台依赖个性化推荐，引发了过滤气泡导致内容暴露狭窄的担忧。现有审计方法面临挑战：真实用户研究成本高且涉及隐私，现有模拟器依赖文本信号且个性化弱，无法复现真实行为。

Method: 提出PersonaAct框架，通过自动访谈结合行为分析和结构化提问合成可解释的用户画像，然后使用监督微调和强化学习在多模态观察上训练代理。部署训练后的代理进行过滤气泡审计，通过内容多样性评估气泡广度，通过逃离潜力评估气泡深度。

Result: 评估显示相比通用LLM基线有显著保真度提升，能够复现真实行为。结果显示交互过程中内容显著变窄，但发现Bilibili展现出最强的逃离潜力。发布了首个开源多模态短视频数据集和代码。

Conclusion: PersonaAct能够有效模拟短视频用户行为，为推荐系统的过滤气泡审计提供可扩展、隐私友好的解决方案，揭示了不同平台在内容多样性和用户逃离潜力方面的差异。

Abstract: Short-video platforms rely on personalized recommendation, raising concerns about filter bubbles that narrow content exposure. Auditing such phenomena at scale is challenging because real user studies are costly and privacy-sensitive, and existing simulators fail to reproduce realistic behaviors due to their reliance on textual signals and weak personalization. We propose PersonaAct, a framework for simulating short-video users with persona-conditioned multimodal agents trained on real behavioral traces for auditing filter bubbles in breadth and depth. PersonaAct synthesizes interpretable personas through automated interviews combining behavioral analysis with structured questioning, then trains agents on multimodal observations using supervised fine-tuning and reinforcement learning. We deploy trained agents for filter bubble auditing and evaluate bubble breadth via content diversity and bubble depth via escape potential. The evaluation demonstrates substantial improvements in fidelity over generic LLM baselines, enabling realistic behavior reproduction. Results reveal significant content narrowing over interaction. However, we find that Bilibili demonstrates the strongest escape potential. We release the first open multimodal short-video dataset and code to support reproducible auditing of recommender systems.

</details>


### [29] [Farewell to Item IDs: Unlocking the Scaling Potential of Large Ranking Models via Semantic Tokens](https://arxiv.org/abs/2601.22694)
*Zhen Zhao,Tong Zhang,Jie Xu,Qingliang Cai,Qile Zhang,Leyuan Yang,Daorui Xiao,Xiaojia Chang*

Main category: cs.IR

TL;DR: TRM框架用语义token替代传统item ID，解决了大规模排序系统中物品快速变化导致的嵌入训练和维护问题，在减少存储的同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统大规模排序系统依赖item ID，将每个物品视为独立的分类符号并映射到学习嵌入。随着物品快速出现和消失，这些嵌入变得难以训练和维护，这种不稳定性阻碍了神经网络参数的有效学习，限制了排序模型的可扩展性。

Method: 提出TRM框架，改进token生成和应用流程，使用语义token替代传统的item ID。语义token具有更好的扩展潜力，能够更好地处理物品的动态变化。

Result: TRM实现了33%的稀疏存储减少和0.85%的AUC提升。在模型容量扩展时，TRM能够持续优于最先进模型。在大规模个性化搜索引擎的A/B测试中，用户活跃天数提升0.26%，查询变化率提升0.75%。

Conclusion: 语义token比item ID具有更大的扩展潜力，TRM框架通过改进token生成和应用流程，有效解决了大规模排序系统中物品动态变化带来的挑战，实现了存储减少和性能提升，并已成功部署到实际系统中。

Abstract: Recent studies on scaling up ranking models have achieved substantial improvement for recommendation systems and search engines. However, most large-scale ranking systems rely on item IDs, where each item is treated as an independent categorical symbol and mapped to a learned embedding. As items rapidly appear and disappear, these embeddings become difficult to train and maintain. This instability impedes effective learning of neural network parameters and limits the scalability of ranking models. In this paper, we show that semantic tokens possess greater scaling potential compared to item IDs. Our proposed framework TRM improves the token generation and application pipeline, leading to 33% reduction in sparse storage while achieving 0.85% AUC increase. Extensive experiments further show that TRM could consistently outperform state-of-the-art models when model capacity scales. Finally, TRM has been successfully deployed on large-scale personalized search engines, yielding 0.26% and 0.75% improvement on user active days and change query ratio respectively through A/B test.

</details>


### [30] [BEAR: Towards Beam-Search-Aware Optimization for Recommendation with Large Language Models](https://arxiv.org/abs/2601.22925)
*Weiqin Yang,Bohao Wang,Zhenxiang Xu,Jiawei Chen,Shengjia Zhang,Jingbang Chen,Canghong Jin,Can Wang*

Main category: cs.IR

TL;DR: BEAR提出一种针对推荐系统中LLM微调的新正则化方法，解决训练与推理时beam search不一致的问题，确保正样本在beam search中不被过早剪枝。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的推荐方法使用监督微调(SFT)但存在训练-推理不一致：SFT优化正样本的整体概率，但beam search的贪婪剪枝机制可能导致即使整体概率高的正样本因前缀概率不足而被过早丢弃。

Method: 提出BEAR（Beam-SEarch-Aware Regularization）微调目标，在训练时显式考虑beam search行为。不直接模拟beam search（计算成本高），而是强制一个松弛的必要条件：正样本的每个token在每个解码步骤中必须排在前B个候选token内。

Result: 在四个真实世界数据集上的广泛实验表明，BEAR显著优于强基线方法，且相比标准SFT计算开销可忽略不计。

Conclusion: BEAR有效解决了LLM推荐系统中训练与beam search推理的不一致问题，通过beam search感知的正则化确保正样本在推理时被正确检索，显著提升推荐性能。

Abstract: Recent years have witnessed a rapid surge in research leveraging Large Language Models (LLMs) for recommendation. These methods typically employ supervised fine-tuning (SFT) to adapt LLMs to recommendation scenarios, and utilize beam search during inference to efficiently retrieve $B$ top-ranked recommended items. However, we identify a critical training-inference inconsistency: while SFT optimizes the overall probability of positive items, it does not guarantee that such items will be retrieved by beam search even if they possess high overall probabilities. Due to the greedy pruning mechanism, beam search can prematurely discard a positive item once its prefix probability is insufficient.
  To address this inconsistency, we propose BEAR (Beam-SEarch-Aware Regularization), a novel fine-tuning objective that explicitly accounts for beam search behavior during training. Rather than directly simulating beam search for each instance during training, which is computationally prohibitive, BEAR enforces a relaxed necessary condition: each token in a positive item must rank within the top-$B$ candidate tokens at each decoding step. This objective effectively mitigates the risk of incorrect pruning while incurring negligible computational overhead compared to standard SFT. Extensive experiments across four real-world datasets demonstrate that BEAR significantly outperforms strong baselines. Code will be released upon acceptance.

</details>


### [31] [OrLog: Resolving Complex Queries with LLMs and Probabilistic Reasoning](https://arxiv.org/abs/2601.23085)
*Mohanna Hoveyda,Jelle Piepenbrock,Arjen P de Vries,Maarten de Rijke,Faegheh Hasibi*

Main category: cs.IR

TL;DR: OrLog是一个神经符号检索框架，将谓词级可能性估计与逻辑推理解耦，使用LLM评估原子谓词可能性，再通过概率推理引擎计算查询满足的后验概率，显著提升约束感知检索性能。


<details>
  <summary>Details</summary>
Motivation: 当前检索系统要么忽略查询中的逻辑约束，要么在生成推理过程中近似处理，导致不一致和不可靠。现有神经符号方法局限于形式逻辑或数学问题，假设查询明确且证据完整，这在信息检索中很少见。需要一种能处理复杂约束的检索方法。

Method: OrLog框架将谓词级可能性估计与逻辑推理解耦：1）LLM在单次无解码前向传递中为原子谓词提供可能性分数；2）概率推理引擎基于这些分数推导查询满足的后验概率。这种方法避免了生成式推理的不一致性。

Result: 在多个骨干LLM、不同外部知识访问级别和多种逻辑约束下的评估显示：1）提供实体描述时，OrLog显著提升top-rank精度，尤其在析取查询上增益更大；2）效率更高，每个查询-实体对平均token减少约90%；3）优于基础检索器和LLM推理方法。

Conclusion: 无生成的谓词可能性估计结合概率推理能够实现约束感知检索，优于单一推理方法且使用更少token。OrLog为处理复杂信息需求中的逻辑约束提供了有效解决方案。

Abstract: Resolving complex information needs that come with multiple constraints should consider enforcing the logical operators encoded in the query (i.e., conjunction, disjunction, negation) on the candidate answer set. Current retrieval systems either ignore these constraints in neural embeddings or approximate them in a generative reasoning process that can be inconsistent and unreliable. Although well-suited to structured reasoning, existing neuro-symbolic approaches remain confined to formal logic or mathematics problems as they often assume unambiguous queries and access to complete evidence, conditions rarely met in information retrieval. To bridge this gap, we introduce OrLog, a neuro-symbolic retrieval framework that decouples predicate-level plausibility estimation from logical reasoning: a large language model (LLM) provides plausibility scores for atomic predicates in one decoding-free forward pass, from which a probabilistic reasoning engine derives the posterior probability of query satisfaction. We evaluate OrLog across multiple backbone LLMs, varying levels of access to external knowledge, and a range of logical constraints, and compare it against base retrievers and LLM-as-reasoner methods. Provided with entity descriptions, OrLog can significantly boost top-rank precision compared to LLM reasoning with larger gains on disjunctive queries. OrLog is also more efficient, cutting mean tokens by $\sim$90\% per query-entity pair. These results demonstrate that generation-free predicate plausibility estimation combined with probabilistic reasoning enables constraint-aware retrieval that outperforms monolithic reasoning while using far fewer tokens.

</details>
