{"id": "2510.21711", "categories": ["cs.IR", "H.3.3"], "pdf": "https://arxiv.org/pdf/2510.21711", "abs": "https://arxiv.org/abs/2510.21711", "authors": ["Rauf Aliev"], "title": "Improving E-commerce Search with Category-Aligned Retrieval", "comment": null, "summary": "Traditional e-commerce search systems often struggle with the semantic gap\nbetween user queries and product catalogs. In this paper, we propose a\nCategory-Aligned Retrieval System (CARS) that improves search relevance by\nfirst predicting the product category from a user's query and then boosting\nproducts within that category. We introduce a novel method for creating\n\"Trainable Category Prototypes\" from query embeddings. We evaluate this method\nwith two models: a lightweight all-MiniLM-L6-v2 and OpenAI's\ntext-embedding-ada-002. Our offline evaluation shows this method is highly\neffective, with the OpenAI model increasing Top-3 category prediction accuracy\nfrom a zero-shot baseline of 43.8% to 83.2% after training. The end-to-end\nsimulation, however, highlights the limitations of blindly applying category\nboosts in a complex retrieval pipeline: while accuracy is high, naive\nintegration can negatively affect search relevance metrics such as nDCG@10. We\nargue that this is partly due to dataset-specific ambiguities (e.g., polysemous\nqueries in the Amazon ESCI corpus) and partly due to the sensitivity of\nretrieval systems to over-constraining filters. Crucially, these results do not\ndiminish the value of the approach; rather, they emphasize the need for\nconfidence-aware and adaptive integration strategies."}
{"id": "2510.21712", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21712", "abs": "https://arxiv.org/abs/2510.21712", "authors": ["Hao Sun", "Zile Qiao", "Bo Wang", "Guoxin Chen", "Yingyan Hou", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Yan Zhang"], "title": "DecoupleSearch: Decouple Planning and Search via Hierarchical Reward Modeling", "comment": "EMNLP 2025 Main Conference", "summary": "Retrieval-Augmented Generation (RAG) systems have emerged as a pivotal\nmethodology for enhancing Large Language Models (LLMs) through the dynamic\nintegration of external knowledge. To further improve RAG's flexibility,\nAgentic RAG introduces autonomous agents into the workflow. However, Agentic\nRAG faces several challenges: (1) the success of each step depends on both\nhigh-quality planning and accurate search, (2) the lack of supervision for\nintermediate reasoning steps, and (3) the exponentially large candidate space\nfor planning and searching. To address these challenges, we propose\nDecoupleSearch, a novel framework that decouples planning and search processes\nusing dual value models, enabling independent optimization of plan reasoning\nand search grounding. Our approach constructs a reasoning tree, where each node\nrepresents planning and search steps. We leverage Monte Carlo Tree Search to\nassess the quality of each step. During inference, Hierarchical Beam Search\niteratively refines planning and search candidates with dual value models.\nExtensive experiments across policy models of varying parameter sizes,\ndemonstrate the effectiveness of our method."}
{"id": "2510.21713", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21713", "abs": "https://arxiv.org/abs/2510.21713", "authors": ["Yin Sun", "Yiwen Liu", "Junjie Song", "Chenyu Zhang", "Xinyuan Zhang", "Lingjie Liu", "Siqi Chen", "Yuji Cao"], "title": "asLLR: LLM based Leads Ranking in Auto Sales", "comment": null, "summary": "In the area of commercial auto sales system, high-quality lead score\nsequencing determines the priority of a sale's work and is essential for\noptimizing the efficiency of the sales system. Since CRM (Customer Relationship\nManagement) system contains plenty of textual interaction features between\nsales and customers, traditional techniques such as Click Through Rate (CTR)\nprediction struggle with processing the complex information inherent in natural\nlanguage features, which limits their effectiveness in sales lead ranking.\nBridging this gap is critical for enhancing business intelligence and\ndecision-making. Recently, the emergence of large language models (LLMs) has\nopened new avenues for improving recommendation systems, this study introduces\nasLLR (LLM-based Leads Ranking in Auto Sales), which integrates CTR loss and\nQuestion Answering (QA) loss within a decoder-only large language model\narchitecture. This integration enables the simultaneous modeling of both\ntabular and natural language features. To verify the efficacy of asLLR, we\nconstructed an innovative dataset derived from the customer lead pool of a\nprominent new energy vehicle brand, with 300,000 training samples and 40,000\ntesting samples. Our experimental results demonstrate that asLLR effectively\nmodels intricate patterns in commercial datasets, achieving the AUC of 0.8127,\nsurpassing traditional CTR estimation methods by 0.0231. Moreover, asLLR\nenhances CTR models when used for extracting text features by 0.0058. In\nreal-world sales scenarios, after rigorous online A/B testing, asLLR increased\nthe sales volume by about 9.5% compared to the traditional method, providing a\nvaluable tool for business intelligence and operational decision-making."}
{"id": "2510.22314", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.22314", "abs": "https://arxiv.org/abs/2510.22314", "authors": ["Christian Imenkamp", "Martin Kabierski", "Hendrik Reiter", "Matthias Weidlich", "Wilhelm Hasselbring", "Agnes Koschmider"], "title": "Determining Window Sizes using Species Estimation for Accurate Process Mining over Streams", "comment": null, "summary": "Streaming process mining deals with the real-time analysis of event streams.\nA common approach for it is to adopt windowing mechanisms that select event\ndata from a stream for subsequent analysis. However, the size of these windows\ndenotes a crucial parameter, as it influences the representativeness of the\nwindow content and, by extension, of the analysis results. Given that process\ndynamics are subject to changes and potential concept drift, a static, fixed\nwindow size leads to inaccurate representations that introduce bias in the\nanalysis. In this work, we present a novel approach for streaming process\nmining that addresses these limitations by adjusting window sizes.\nSpecifically, we dynamically determine suitable window sizes based on\nestimators for the representativeness of samples as developed for species\nestimation in biodiversity research. Evaluation results on real-world data sets\nshow improvements over existing approaches that adopt static window sizes in\nterms of accuracy and robustness to concept drifts."}
{"id": "2510.21714", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.21714", "abs": "https://arxiv.org/abs/2510.21714", "authors": ["Xian Hu", "Ming Yue", "Zhixiang Feng", "Junwei Pan", "Junjie Zhai", "Ximei Wang", "Xinrui Miao", "Qian Li", "Xun Liu", "Shangyu Zhang", "Letian Wang", "Hua Lu", "Zijian Zeng", "Chen Cai", "Wei Wang", "Fei Xiong", "Pengfei Xiong", "Jintao Zhang", "Zhiyuan Wu", "Chunhui Zhang", "Anan Liu", "Jiulong You", "Chao Deng", "Yuekui Yang", "Shudong Huang", "Dapeng Liu", "Haijie Gu"], "title": "Practice on Long Behavior Sequence Modeling in Tencent Advertising", "comment": null, "summary": "Long-sequence modeling has become an indispensable frontier in recommendation\nsystems for capturing users' long-term preferences. However, user behaviors\nwithin advertising domains are inherently sparse, posing a significant barrier\nto constructing long behavioral sequences using data from a single advertising\ndomain alone. This motivates us to collect users' behaviors not only across\ndiverse advertising scenarios, but also beyond the boundaries of the\nadvertising domain into content domains-thereby constructing unified commercial\nbehavior trajectories. This cross-domain or cross-scenario integration gives\nrise to the following challenges: (1) feature taxonomy gaps between distinct\nscenarios and domains, (2) inter-field interference arising from irrelevant\nfeature field pairs, and (3) target-wise interference in temporal and semantic\npatterns when optimizing for different advertising targets. To address these\nchallenges, we propose several practical approaches within the two-stage\nframework for long-sequence modeling. In the first (search) stage, we design a\nhierarchical hard search method for handling complex feature taxonomy\nhierarchies, alongside a decoupled embedding-based soft search to alleviate\nconflicts between attention mechanisms and feature representation. In the\nsecond (sequence modeling) stage, we introduce: (a) Decoupled Side Information\nTemporal Interest Networks (TIN) to mitigate inter-field conflicts; (b)\nTarget-Decoupled Positional Encoding and Target-Decoupled SASRec to address\ntarget-wise interference; and (c) Stacked TIN to model high-order behavioral\ncorrelations. Deployed in production on Tencent's large-scale advertising\nplatforms, our innovations delivered significant performance gains: an overall\n4.22% GMV lift in WeChat Channels and an overall 1.96% GMV increase in WeChat\nMoments."}
{"id": "2510.22316", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.22316", "abs": "https://arxiv.org/abs/2510.22316", "authors": ["Zhiyuan Hua", "Qiji Mo", "Zebin Yao", "Lixiao Cui", "Xiaoguang Liu", "Gang Wang", "Zijing Wei", "Xinyu Liu", "Tianxiao Tang", "Shaozhi Liu", "Lin Qu"], "title": "Dynamically Detect and Fix Hardness for Efficient Approximate Nearest Neighbor Search", "comment": "Accepted by SIGMOD2026", "summary": "Approximate Nearest Neighbor Search (ANNS) has become a fundamental component\nin many real-world applications. Among various ANNS algorithms, graph-based\nmethods are state-of-the-art. However, ANNS often suffers from a significant\ndrop in accuracy for certain queries, especially in Out-of-Distribution (OOD)\nscenarios. To address this issue, a recent approach named RoarGraph constructs\na bipartite graph between the base data and historical queries to bridge the\ngap between two different distributions. However, it suffers from some\nlimitations: (1) Building a bipartite graph between two distributions lacks\ntheoretical support, resulting in the query distribution not being effectively\nutilized by the graph index. (2) Requires a sufficient number of historical\nqueries before graph construction and suffers from high construction times. (3)\nWhen the query workload changes, it requires reconstruction to maintain high\nsearch accuracy.\n  In this paper, we first propose Escape Hardness, a metric to evaluate the\nquality of the graph structure around the query. Then we divide the graph\nsearch into two stages and dynamically identify and fix defective graph regions\nin each stage based on Escape Hardness. (1) From the entry point to the\nvicinity of the query. We propose Reachability Fixing (RFix), which enhances\nthe navigability of some key nodes. (2) Searching within the vicinity of the\nquery. We propose Neighboring Graph Defects Fixing (NGFix) to improve graph\nconnectivity in regions where queries are densely distributed. The results of\nextensive experiments show that our method outperforms other state-of-the-art\nmethods on real-world datasets, achieving up to 2.25x faster search speed for\nOOD queries at 99% recall compared with RoarGraph and 6.88x faster speed\ncompared with HNSW. It also accelerates index construction by 2.35-9.02x\ncompared to RoarGraph."}
{"id": "2510.22145", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.22145", "abs": "https://arxiv.org/abs/2510.22145", "authors": ["Minquan Cheng", "Yifei Huang", "Youlong Wu", "Jinyan Wang"], "title": "Fundamental Limits of Coded Caching with Fixed Subpacketization", "comment": "19 pages", "summary": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off."}
{"id": "2510.22060", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.22060", "abs": "https://arxiv.org/abs/2510.22060", "authors": ["Ahan Mishra"], "title": "An Optimal Density Bound for Discretized Point Patrolling", "comment": "SODA 2026", "summary": "The pinwheel problem is a real-time scheduling problem that asks, given $n$\ntasks with periods $a_i \\in \\mathbb{N}$, whether it is possible to infinitely\nschedule the tasks, one per time unit, such that every task $i$ is scheduled in\nevery interval of $a_i$ units. We study a corresponding version of this packing\nproblem in the covering setting, stylized as the discretized point patrolling\nproblem in the literature. Specifically, given $n$ tasks with periods $a_i$,\nthe problem asks whether it is possible to assign each day to a task such that\nevery task $i$ is scheduled at \\textit{most} once every $a_i$ days. The density\nof an instance in either case is defined as the sum of the inverses of task\nperiods. Recently, the long-standing $5/6$ density bound conjecture in the\npacking setting was resolved affirmatively. The resolution means any instance\nwith density at least $5/6$ is schedulable. A corresponding conjecture was made\nin the covering setting and renewed multiple times in more recent work. We\nresolve this conjecture affirmatively by proving that every discretized point\npatrolling instance with density at least $\\sum_{i = 0}^{\\infty} 1/(2^i + 1)\n\\approx 1.264$ is schedulable. This significantly improves upon the current\nbest-known density bound of 1.546 and is, in fact, optimal. We also study the\nbamboo garden trimming problem, an optimization variant of the pinwheel\nproblem. Specifically, given $n$ growth rates with values $h_i \\in \\mathbb{N}$,\nthe objective is to minimize the maximum height of a bamboo garden with the\ncorresponding growth rates, where we are allowed to trim one bamboo tree to\nheight zero per time step. We achieve an efficient $9/7$-approximation\nalgorithm for this problem, improving on the current best known approximation\nfactor of $4/3$."}
{"id": "2510.21904", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.21904", "abs": "https://arxiv.org/abs/2510.21904", "authors": ["Christoph Schlegel", "Xinyuan Sun"], "title": "Conditional Recall", "comment": null, "summary": "In the neon-lit nights of 2026, Johnson \\& Johnson unveiled X. A pill, not\nlarger than a snowflake, that promised a tempest of change. This miraculous\ndrug didn't just allow people to cherry-pick memories to erase from their\nminds, it could also leave a reminder of this erasure in the minds of those who\ningested it.\n  Amidst the iconic red-bricked walls of Harvard Law, you, with books in one\nhand and dreams in the other, are on a mission. You are not just another\nstudent; you carry the hope of revolutionizing the archaic chambers of the\nlegal world. Each night, as you pore over the tomes of law, you wonder what\ngreatness society can achieve.\n  On a cold evening, your phone buzzes. It's Dex, your old college friend\nturned underground dealer. His message is simple: ``Got X. Special price for\nyou.'' The temptation swirls around you. Would you trade the lessons of the\npast for a clearer, yet incomplete future? The decision rests in your hands.\n  We explore the game theoretic implications of a technology (such as TEEs)\nthat allows agents to commit to forget information and discuss several\napplications."}
{"id": "2510.23056", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2510.23056", "abs": "https://arxiv.org/abs/2510.23056", "authors": ["Panneer Selvam Santhalingam", "Swann Thantsin", "Ahmad Kamari", "Parth Pathak", "Kenneth De Haan"], "title": "Enabling American Sign Language Communication Under Low Data Rates", "comment": null, "summary": "In recent years, video conferencing applications have become increasingly\nprevalent, relying heavily on high-speed internet connectivity. When such\nconnectivity is lacking, users often default to audio-only communication, a\nmode that significantly disadvantages American Sign Language (ASL) users, whose\ncommunication relies on hand gestures, body movement, and facial expressions.\nIn this work, we introduce VC4ASL, a system designed to enable ASL\ncommunication over the audio channel of existing video conferencing\napplications, even in the absence of reliable video. VC4ASL integrates\nseamlessly with current platforms without requiring any modifications. Our\napproach establishes a communication channel through audio by encoding and\ntransmitting human pose information, which is then rendered to reconstruct\nsigned content. We propose novel receive-side error detection and correction\nmechanisms that exploit the inherent structural constraints of human pose data.\nTo evaluate the system, we simulate network-degraded environments, generate\npose-based ASL video sequences, and conduct user studies to assess\ncomprehension among ASL users. Experimental results demonstrate that VC4ASL\neffectively facilitates intelligible ASL communication over audio in\nlow-bandwidth scenarios where video transmission is impaired."}
{"id": "2510.21724", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21724", "abs": "https://arxiv.org/abs/2510.21724", "authors": ["Apoorva Chavali", "Reeve Menezes"], "title": "Words to Waves: Emotion-Adaptive Music Recommendation System", "comment": null, "summary": "Current recommendation systems often tend to overlook emotional context and\nrely on historical listening patterns or static mood tags. This paper\nintroduces a novel music recommendation framework employing a variant of Wide\nand Deep Learning architecture that takes in real-time emotional states\ninferred directly from natural language as inputs and recommends songs that\nclosely portray the mood. The system captures emotional contexts from\nuser-provided textual descriptions by using transformer-based embeddings, which\nwere finetuned to predict the emotional dimensions of valence-arousal. The deep\ncomponent of the architecture utilizes these embeddings to generalize unseen\nemotional patterns, while the wide component effectively memorizes user-emotion\nand emotion-genre associations through cross-product features. Experimental\nresults show that personalized music selections positively influence the user's\nemotions and lead to a significant improvement in emotional relevance."}
{"id": "2510.23587", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23587", "abs": "https://arxiv.org/abs/2510.23587", "authors": ["Yizhang Zhu", "Liangwei Wang", "Chenyu Yang", "Xiaotian Lin", "Boyan Li", "Wei Zhou", "Xinyu Liu", "Zhangyang Peng", "Tianqi Luo", "Yu Li", "Chengliang Chai", "Chong Chen", "Shimin Di", "Ju Fan", "Ji Sun", "Nan Tang", "Fugee Tsung", "Jiannan Wang", "Chenglin Wu", "Yanwei Xu", "Shaolei Zhang", "Yong Zhang", "Xuanhe Zhou", "Guoliang Li", "Yuyu Luo"], "title": "A Survey of Data Agents: Emerging Paradigm or Overstated Hype?", "comment": "Please refer to our paper list and companion materials at:\n  https://github.com/HKUSTDial/awesome-data-agents", "summary": "The rapid advancement of large language models (LLMs) has spurred the\nemergence of data agents--autonomous systems designed to orchestrate Data + AI\necosystems for tackling complex data-related tasks. However, the term \"data\nagent\" currently suffers from terminological ambiguity and inconsistent\nadoption, conflating simple query responders with sophisticated autonomous\narchitectures. This terminological ambiguity fosters mismatched user\nexpectations, accountability challenges, and barriers to industry growth.\nInspired by the SAE J3016 standard for driving automation, this survey\nintroduces the first systematic hierarchical taxonomy for data agents,\ncomprising six levels that delineate and trace progressive shifts in autonomy,\nfrom manual operations (L0) to a vision of generative, fully autonomous data\nagents (L5), thereby clarifying capability boundaries and responsibility\nallocation. Through this lens, we offer a structured review of existing\nresearch arranged by increasing autonomy, encompassing specialized data agents\nfor data management, preparation, and analysis, alongside emerging efforts\ntoward versatile, comprehensive systems with enhanced autonomy. We further\nanalyze critical evolutionary leaps and technical gaps for advancing data\nagents, especially the ongoing L2-to-L3 transition, where data agents evolve\nfrom procedural execution to autonomous orchestration. Finally, we conclude\nwith a forward-looking roadmap, envisioning the advent of proactive, generative\ndata agents."}
{"id": "2510.22230", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.22230", "abs": "https://arxiv.org/abs/2510.22230", "authors": ["Ziqi Diao", "Xingyu Zhou", "Le Liang", "Shi Jin"], "title": "Robust MIMO Channel Estimation Using Energy-Based Generative Diffusion Models", "comment": "5 pages, 4 figures, 1 table. This work has been submitted to the IEEE\n  for possible publication", "summary": "Channel estimation for massive multiple-input multiple-output (MIMO) systems\nis fundamentally constrained by excessive pilot overhead and high estimation\nlatency. To overcome these obstacles, recent studies have leveraged deep\ngenerative networks to capture the prior distribution of wireless channels. In\nthis paper, we propose a novel estimation framework that integrates an\nenergy-based generative diffusion model (DM) with the Metropolis-Hastings (MH)\nprinciple. By reparameterizing the diffusion process with an incorporated\nenergy function, the framework explicitly estimates the unnormalized log-prior,\nwhile MH corrections refine the sampling trajectory, mitigate deviations, and\nenhance robustness, ultimately enabling accurate posterior sampling for\nhigh-fidelity channel estimation. Numerical results reveal that the proposed\napproach significantly improves estimation accuracy compared with conventional\nparameterized DMs and other baseline methods, particularly in cases with\nlimited pilot overhead."}
{"id": "2510.22193", "categories": ["cs.DS", "cs.DM"], "pdf": "https://arxiv.org/pdf/2510.22193", "abs": "https://arxiv.org/abs/2510.22193", "authors": ["Kevin Pratt", "Yahel Uffenheimer", "Omri Weinstein"], "title": "(Approximate) Matrix Multiplication via Convolutions", "comment": null, "summary": "A longstanding open question in algorithm design is whether \"combinatorial\"\nmatrix multiplication algorithms -- avoiding Strassen-like divide-and-conquer\n-- can achieve truly subcubic runtime $n^{3-\\delta}$. We present an\n$O(n^{2.89})$-time exact algorithm, which only sums convolutions in\n$\\mathbb{Z}_m^k$ (multivariate polynomial multiplications) via FFT, building on\nthe work of Cohn, Kleinberg, Szegedy and Umans (CKSU'05). While the algorithm\navoids recursion, the asymptotic speedup arises only for impractically large\nmatrices.\n  Motivated by practical applications, we use this baseline to develop a new\nframework for fast approximate matrix multiplication (AMM), via low-degree\napproximations of the CKSU polynomials. We show that combining the\naforementioned algorithm with black-box linear sketching already breaks the\nlongstanding linear speed-accuracy tradeoff for AMM (Sarlos'06,\nClarkson-Woodruff'13 ,Pagh'11, Cohn-Lewis'00), achieving\n$\\frac{1}{r^{1.1}}\\|\\mathbf{A}\\|_F^2\\|\\mathbf{B}\\|_F^2$ error in\n$O(rn^2)$-time.\n  Our main result is a low-degree approximation scheme for the CKSU\npolynomials, based on a Fourier-concentration lemma, yielding substantially\nsmaller error in the distributional setting where $\\mathbf{A},\\mathbf{B}$ come\nfrom an i.i.d product-distribution; For random Gaussian matrices, this\npractical AMM algorithm attains smaller error than the best rank-$r$ SVD of the\noutput matrix $\\mathbf{A}\\mathbf{B}$, in time $O(rn^2)$. This is a substantial\nimprovement over iterative Krylov subspace methods for low-rank approximation.\nOur theoretical and empirical results suggest the possibility of replacing\nMatMuls with sums of convolutions in LLM training and inference."}
{"id": "2510.22232", "categories": ["cs.GT", "cs.AI", "econ.TH"], "pdf": "https://arxiv.org/pdf/2510.22232", "abs": "https://arxiv.org/abs/2510.22232", "authors": ["Daisuke Hirota"], "title": "Rational Adversaries and the Maintenance of Fragility: A Game-Theoretic Theory of Rational Stagnation", "comment": null, "summary": "Cooperative systems often remain in persistently suboptimal yet stable\nstates. This paper explains such \"rational stagnation\" as an equilibrium\nsustained by a rational adversary whose utility follows the principle of\npotential loss, $u_{D} = U_{ideal} - U_{actual}$. Starting from the Prisoner's\nDilemma, we show that the transformation $u_{i}' = a\\,u_{i} + b\\,u_{j}$ and the\nratio of mutual recognition $w = b/a$ generate a fragile cooperation band\n$[w_{\\min},\\,w_{\\max}]$ where both (C,C) and (D,D) are equilibria. Extending to\na dynamic model with stochastic cooperative payoffs $R_{t}$ and intervention\ncosts $(C_{c},\\,C_{m})$, a Bellman-style analysis yields three strategic\nregimes: immediate destruction, rational stagnation, and intervention\nabandonment. The appendix further generalizes the utility to a\nreference-dependent nonlinear form and proves its stability under reference\nshifts, ensuring robustness of the framework. Applications to social-media\nalgorithms and political trust illustrate how adversarial rationality can\ndeliberately preserve fragility."}
{"id": "2510.21726", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21726", "abs": "https://arxiv.org/abs/2510.21726", "authors": ["Weichen Wang", "Chengchun Shi"], "title": "From Authors to Reviewers: Leveraging Rankings to Improve Peer Review", "comment": null, "summary": "This paper is a discussion of the 2025 JASA discussion paper by Su et al.\n(2025). We would like to congratulate the authors on conducting a comprehensive\nand insightful empirical investigation of the 2023 ICML ranking data. The\nreview quality of machine learning (ML) conferences has become a big concern in\nrecent years, due to the rapidly growing number of submitted manuscripts. In\nthis discussion, we propose an approach alternative to Su et al. (2025) that\nleverages ranking information from reviewers rather than authors. We simulate\nreview data that closely mimics the 2023 ICML conference submissions. Our\nresults show that (i) incorporating ranking information from reviewers can\nsignificantly improve the evaluation of each paper's quality, often\noutperforming the use of ranking information from authors alone; and (ii)\ncombining ranking information from both reviewers and authors yields the most\naccurate evaluation of submitted papers in most scenarios."}
{"id": "2510.22259", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.22259", "abs": "https://arxiv.org/abs/2510.22259", "authors": ["Hao Chen", "Conghui Xie", "Cunsheng Ding"], "title": "Infinitely many families of distance-optimal binary linear codes with respect to the sphere packing bound", "comment": null, "summary": "R. W. Hamming published the Hamming codes and the sphere packing bound in\n1950. In the past 75 years, infinite families of distance-optimal linear codes\nover finite fields with minimum distance at most 8 with respect to the sphere\npacking bound have been reported in the literature. However, it is a\n75-year-old open problem in coding theory whether there is an infinite family\nof distance-optimal linear codes over finite fields with arbitrarily large\nminimum distance with respect to the sphere packing bound. This main objective\nof this paper is to settle this long-standing open problem in coding theory.\n  As by-products, several infinite families of distance-optimal binary codes\nwith small minimum distances are presented. Two infinite families of binary\nfive-weight codes are reported. Some open problems are also proposed."}
{"id": "2510.22401", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.22401", "abs": "https://arxiv.org/abs/2510.22401", "authors": ["Chengyuan Deng", "Jie Gao", "Kevin Lu", "Feng Luo", "Cheng Xin"], "title": "Johnson-Lindenstrauss Lemma Beyond Euclidean Geometry", "comment": "Accepted to Neurips 2025", "summary": "The Johnson-Lindenstrauss (JL) lemma is a cornerstone of dimensionality\nreduction in Euclidean space, but its applicability to non-Euclidean data has\nremained limited. This paper extends the JL lemma beyond Euclidean geometry to\nhandle general dissimilarity matrices that are prevalent in real-world\napplications. We present two complementary approaches: First, we show the JL\ntransform can be applied to vectors in pseudo-Euclidean space with signature\n$(p,q)$, providing theoretical guarantees that depend on the ratio of the $(p,\nq)$ norm and Euclidean norm of two vectors, measuring the deviation from\nEuclidean geometry. Second, we prove that any symmetric hollow dissimilarity\nmatrix can be represented as a matrix of generalized power distances, with an\nadditional parameter representing the uncertainty level within the data. In\nthis representation, applying the JL transform yields multiplicative\napproximation with a controlled additive error term proportional to the\ndeviation from Euclidean geometry. Our theoretical results provide fine-grained\nperformance analysis based on the degree to which the input data deviates from\nEuclidean geometry, making practical and meaningful reduction in dimensionality\naccessible to a wider class of data. We validate our approaches on both\nsynthetic and real-world datasets, demonstrating the effectiveness of extending\nthe JL lemma to non-Euclidean settings."}
{"id": "2510.22471", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22471", "abs": "https://arxiv.org/abs/2510.22471", "authors": ["Nivasini Ananthakrishnan", "Yuval Dagan", "Kunhe Yang"], "title": "Learning Local Stackelberg Equilibria from Repeated Interactions with a Learning Agent", "comment": null, "summary": "Motivated by the question of how a principal can maximize its utility in\nrepeated interactions with a learning agent, we study repeated games between an\nprincipal and an agent employing a mean-based learning algorithm. Prior work\nhas shown that computing or even approximating the global Stackelberg value in\nsimilar settings can require an exponential number of rounds in the size of the\nagent's action space, making it computationally intractable. In contrast, we\nshift focus to the computation of local Stackelberg equilibria and introduce an\nalgorithm that, within the smoothed analysis framework, constitutes a\nPolynomial Time Approximation Scheme (PTAS) for finding an epsilon-approximate\nlocal Stackelberg equilibrium. Notably, the algorithm's runtime is polynomial\nin the size of the agent's action space yet exponential in (1/epsilon) - a\ndependency we prove to be unavoidable."}
{"id": "2510.21727", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21727", "abs": "https://arxiv.org/abs/2510.21727", "authors": ["Yichi Zhang", "Jun Bai", "Zhixin Cai", "Shuhan Qin", "Zhuofan Chen", "Jinghua Guan", "Wenge Rong"], "title": "Your Dense Retriever is Secretly an Expeditious Reasoner", "comment": "16 pages, 11 figures", "summary": "Dense retrievers enhance retrieval by encoding queries and documents into\ncontinuous vectors, but they often struggle with reasoning-intensive queries.\nAlthough Large Language Models (LLMs) can reformulate queries to capture\ncomplex reasoning, applying them universally incurs significant computational\ncost. In this work, we propose Adaptive Query Reasoning (AdaQR), a hybrid query\nrewriting framework. Within this framework, a Reasoner Router dynamically\ndirects each query to either fast dense reasoning or deep LLM reasoning. The\ndense reasoning is achieved by the Dense Reasoner, which performs LLM-style\nreasoning directly in the embedding space, enabling a controllable trade-off\nbetween efficiency and accuracy. Experiments on large-scale retrieval\nbenchmarks BRIGHT show that AdaQR reduces reasoning cost by 28% while\npreserving-or even improving-retrieval performance by 7%."}
{"id": "2510.22288", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.22288", "abs": "https://arxiv.org/abs/2510.22288", "authors": ["Aimin Li", "Elif Uysal"], "title": "Optimal Sampling and Scheduling for Remote Fusion Estimation of Correlated Wiener Processes", "comment": "8 pages, 4 figures", "summary": "In distributed sensor networks, sensors often observe a dynamic process\nwithin overlapping regions. Due to random delays, these correlated observations\narrive at the fusion center asynchronously, raising a central question: How can\none fuse asynchronous yet correlated information for accurate remote fusion\nestimation? This paper addresses this challenge by studying the joint design of\nsampling, scheduling, and estimation policies for monitoring a correlated\nWiener process. Though this problem is coupled, we establish a separation\nprinciple and identify the joint optimal policy: the optimal fusion estimator\nis a weighted-sum fusion estimator conditioned on Age of Information (AoI), the\noptimal scheduler is a Maximum Age First (MAF) scheduler that prioritizes the\nmost stale source, and the optimal sampling can be designed given the optimal\nestimator and the MAF scheduler. To design the optimal sampling, we show that,\nunder the infinite-horizon average-cost criterion, optimizing AoI is equivalent\nto optimizing MSE under pull-based communications, despite the presence of\nstrong inter-sensor correlations. This structural equivalence allows us to\nidentify the MSE-optimal sampler as one that is AoI-optimal. This result\nunderscores an insight: information freshness can serve as a design surrogate\nfor optimal estimation in correlated sensing environments."}
{"id": "2510.22430", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.22430", "abs": "https://arxiv.org/abs/2510.22430", "authors": ["Marcin Briański", "Alexandra Lassota", "Kristýna Pekárková", "Michał Pilipczuk", "Janina Reuter"], "title": "On Integer Programs That Look Like Paths", "comment": null, "summary": "Solving integer programs of the form $\\min \\{\\mathbf{x} \\mid A\\mathbf{x} =\n\\mathbf{b}, \\mathbf{l} \\leq \\mathbf{x} \\leq \\mathbf{u}, \\mathbf{x} \\in\n\\mathbb{Z}^n \\}$ is, in general, $\\mathsf{NP}$-hard. Hence, great effort has\nbeen put into identifying subclasses of integer programs that are solvable in\npolynomial or $\\mathsf{FPT}$ time. A common scheme for many of these integer\nprograms is a star-like structure of the constraint matrix. The arguably\nsimplest form that is not a star is a path. We study integer programs where the\nconstraint matrix $A$ has such a path-like structure: every non-zero\ncoefficient appears in at most two consecutive constraints. We prove that even\nif all coefficients of $A$ are bounded by 8, deciding the feasibility of such\ninteger programs is $\\mathsf{NP}$-hard via a reduction from 3-SAT. Given the\nexistence of efficient algorithms for integer programs with star-like\nstructures and a closely related pattern where the sum of absolute values is\ncolumn-wise bounded by 2 (hence, there are at most two non-zero entries per\ncolumn of size at most 2), this hardness result is surprising."}
{"id": "2510.21728", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21728", "abs": "https://arxiv.org/abs/2510.21728", "authors": ["Mahsa Goodarzi", "M. Abdullah Canbaz"], "title": "Modeling Bias Evolution in Fashion Recommender Systems: A System Dynamics Approach", "comment": "Published in the proceedings of the 43rd International System\n  Dynamics Conference (ISDC 25):\n  https://proceedings.systemdynamics.org/2025/papers/P1254.pdf", "summary": "Bias in recommender systems not only distorts user experience but also\nperpetuates and amplifies existing societal stereotypes, particularly in\nsectors like fashion e-commerce. This study employs a dynamic modeling approach\nto scrutinize the mechanisms of bias activation and reinforcement within\nFashion Recommender Systems (FRS). By leveraging system dynamics modeling and\nexperimental simulations, we dissect the temporal evolution of bias and its\nmultifaceted impacts on system performance. Our analysis reveals that inductive\nbiases exert a more substantial influence on system outcomes than user biases,\nsuggesting critical areas for intervention. We demonstrate that while current\ndebiasing strategies, including data rebalancing and algorithmic\nregularization, are effective to an extent, they require further enhancement to\ncomprehensively mitigate biases. This research underscores the necessity for\nadvancing these strategies and extending system boundaries to incorporate\nbroader contextual factors such as user demographics and item diversity, aiming\nto foster inclusivity and fairness in FRS. The findings advocate for a\nproactive approach in recommender system design to counteract bias propagation\nand ensure equitable user experiences."}
{"id": "2510.22306", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.22306", "abs": "https://arxiv.org/abs/2510.22306", "authors": ["Qingjie Wu", "Miao Cui", "Guangchi Zhang", "Beixiong Zheng", "Xiaoli Chu", "Qingqing Wu"], "title": "Energy-Efficient UAV-Enabled MEC Systems: NOMA, FDMA, or TDMA Offloading?", "comment": null, "summary": "Unmanned aerial vehicle (UAV)-enabled mobile edge computing (MEC) systems can\nuse different multiple access schemes to coordinate multi-user task offloading.\nHowever, it is still unknown which scheme is the most energy-efficient,\nespecially when the offloading blocklength is finite. To answer this question,\nthis paper minimizes and compares the MEC-related energy consumption of\nnon-orthogonal multiple access (NOMA), frequency division multiple access\n(FDMA), and time division multiple access (TDMA)-based offloading schemes\nwithin UAV-enabled MEC systems, considering both infinite and finite\nblocklength scenarios. Through theoretically analysis of the minimum energy\nconsumption required by these three schemes, two novel findings are presented.\nFirst, TDMA consistently achieves lower energy consumption than FDMA in both\ninfinite and finite blocklength cases, due to the degrees of freedom afforded\nby sequential task offloading. Second, NOMA does not necessarily achieve lower\nenergy consumption than FDMA when the offloading blocklength is finite,\nespecially when the channel conditions and the offloaded task data sizes of two\nuser equipments (UEs) are relatively symmetric. Furthermore, an alternating\noptimization algorithm that jointly optimizes the portions of task offloaded,\nthe offloading times of all UEs, and the UAV location is proposed to solve the\nformulated energy consumption minimization problems. Simulation results verify\nthe correctness of our analytical findings and demonstrate that the proposed\nalgorithm effectively reduces MEC-related energy consumption compared to\nbenchmark schemes that do not optimize task offloading portions and/or\noffloading times."}
{"id": "2510.22490", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.22490", "abs": "https://arxiv.org/abs/2510.22490", "authors": ["Gramoz Goranci", "Shaofeng H. -C. Jiang", "Peter Kiss", "Qihao Kong", "Yi Qian", "Eva Szilagyi"], "title": "Tree Embedding in High Dimensions: Dynamic and Massively Parallel", "comment": null, "summary": "Tree embedding has been a fundamental method in algorithm design with wide\napplications. We focus on the efficiency of building tree embedding in various\ncomputational settings under high-dimensional Euclidean $\\mathbb{R}^d$. We\ndevise a new tree embedding construction framework that operates on an\narbitrary metric decomposition with bounded diameter, offering a tradeoff\nbetween distortion and the locality of its algorithmic steps. This framework\nworks for general metric spaces and may be of independent interest beyond the\nEuclidean setting. Using this framework, we obtain a dynamic algorithm that\nmaintains an $O_\\epsilon(\\log n)$-distortion tree embedding with update time\n$\\tilde O(n^\\epsilon + d)$ subject to point insertions/deletions, and a\nmassively parallel algorithm that achieves $O_\\epsilon(\\log n)$-distortion in\n$O(1)$ rounds and total space $\\tilde O(n^{1 + \\epsilon})$ (for constant\n$\\epsilon \\in (0, 1)$). These new tree embedding results allow for a wide range\nof applications. Notably, under a similar performance guarantee as in our tree\nembedding algorithms, i.e., $\\tilde O(n^\\epsilon + d)$ update time and $O(1)$\nrounds, we obtain $O_\\epsilon(\\log n)$-approximate dynamic and MPC algorithms\nfor $k$-median and earth-mover distance in $\\mathbb{R}^d$."}
{"id": "2510.21729", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21729", "abs": "https://arxiv.org/abs/2510.21729", "authors": ["Nathan Paull"], "title": "CustomIR: Unsupervised Fine-Tuning of Dense Embeddings for Known Document Corpora", "comment": null, "summary": "Dense embedding models have become critical for modern information retrieval,\nparticularly in RAG pipelines, but their performance often degrades when\napplied to specialized corpora outside their pre-training distribution. To\naddress thi we introduce \\textbf{CustomIR}, a framework for unsupervised\nadaptation of pre-trained language embedding models to domain-specific corpora\nusing synthetically generated query-document pairs. CustomIR leverages large\nlanguage models (LLMs) to create diverse queries grounded in a known target\ncorpus, paired with LLM-verified hard negatives, eliminating the need for\ncostly human annotation. Experiments on enterprise email and messaging datasets\nshow that CustomIR consistently improves retrieval effectiveness with small\nmodels gaining up to 2.3 points in Recall@10. This performance increase allows\nthese small models to rival the performance of much larger alternatives,\nallowing for cheaper RAG deployments. These results highlight that targeted\nsynthetic fine-tuning offers a scalable and cost-efficient strategy for\nincreasing domain-specific performance."}
{"id": "2510.22505", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.22505", "abs": "https://arxiv.org/abs/2510.22505", "authors": ["Alperen Duru", "Mohammad Mozaffari", "Ticao Zhang", "Mehrnaz Afshang"], "title": "Resource Allocation for XR with Edge Offloading: A Reinforcement Learning Approach", "comment": null, "summary": "Future immersive XR applications will require energy-efficient, high data\nrate, and low-latency wireless communications in uplink and downlink. One of\nthe key considerations for supporting such XR applications is intelligent and\nadaptive resource allocation with edge offloading. To address these demands,\nthis paper proposes a reinforcement learning-based resource allocation\nframework that dynamically allocates uplink and downlink slots while making\noffloading decisions based on the XR headset's capabilities and network\nconditions. The paper presents a numerical analysis of the tradeoff between\nframe loss rate (FLR) and energy efficiency, identifying decision regions for\npartial offloading to optimize performance. Results show that for the used set\nof system parameters, partial offloading can extend the coverage area by 55%\nand reduce energy consumption by up to 34%, compared to always or never\noffloading. The results demonstrate that the headset's local computing\ncapability plays a crucial role in offloading decisions. Higher computing\nabilities enable more efficient local processing, reduce the need for\noffloading, and enhance energy savings."}
{"id": "2510.22662", "categories": ["cs.DS", "cs.DM"], "pdf": "https://arxiv.org/pdf/2510.22662", "abs": "https://arxiv.org/abs/2510.22662", "authors": ["Bowie Liu", "Dennis Wong", "Chan-Tong Lam", "Sio-Kei Im"], "title": "Generating pivot Gray codes for spanning trees of complete graphs in constant amortized time", "comment": "Accepted by SODA 2026", "summary": "We present the first known pivot Gray code for spanning trees of complete\ngraphs, listing all spanning trees such that consecutive trees differ by\npivoting a single edge around a vertex. This pivot Gray code thus addresses an\nopen problem posed by Knuth in The Art of Computer Programming, Volume 4\n(Exercise 101, Section 7.2.1.6, [Knuth, 2011]), rated at a difficulty level of\n46 out of 50, and imposes stricter conditions than existing revolving-door or\nedge-exchange Gray codes for spanning trees of complete graphs. Our recursive\nalgorithm generates each spanning tree in constant amortized time using\n$O(n^2)$ space. In addition, we provide a novel proof of Cayley's formula,\n$n^{n-2}$, for the number of spanning trees in a complete graph, derived from\nour recursive approach. We extend the algorithm to generate edge-exchange Gray\ncodes for general graphs with $n$ vertices, achieving $O(n^2)$ time per tree\nusing $O(n^2)$ space. For specific graph classes, the algorithm can be\noptimized to generate edge-exchange Gray codes for spanning trees in constant\namortized time per tree for complete bipartite graphs, $O(n)$-amortized time\nper tree for fan graphs, and $O(n)$-amortized time per tree for wheel graphs,\nall using $O(n^2)$ space."}
{"id": "2510.21730", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.21730", "abs": "https://arxiv.org/abs/2510.21730", "authors": ["Hao Wang"], "title": "TriMat: Context-aware Recommendation by Tri-Matrix Factorization", "comment": null, "summary": "Search engine is the symbolic technology of Web 2.0, and many people used to\nbelieve recommender systems is the new frontier of Web 3.0. In the past 10\nyears, with the advent of TikTok and similar apps, recommender systems has\nmaterialized the vision of the machine learning pioneers. However, many\nresearch topics of the field remain unfixed until today. One such topic is CARS\n(Context-aware Recommender Systems) , which is largely a theoretical topic\nwithout much advance in real-world applications. In this paper, we utilize\ntri-matrix factorization technique to incorporate contextual information into\nour matrix factorization framework, and prove that our technique is effective\nin improving both the accuracy and fairness metrics in our experiments."}
{"id": "2510.22608", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.22608", "abs": "https://arxiv.org/abs/2510.22608", "authors": ["Harindu Jayarathne", "Dileepa Marasinghe", "Nandana Rajatheva", "Matti Latva-aho"], "title": "End-to-end Learning of Probabilistic and Geometric Constellation Shaping with Iterative Receivers", "comment": null, "summary": "An end-to-end learning method for constellation shaping with a\nshaping-encoder assisted transceiver architecture is presented. The shaping\nencoder, which produces shaping bits with a higher probability of zeros, is\nused to produce an efficient symbol probability distribution. Both the\nprobability distribution and the constellation geometry are jointly optimized,\nusing end-to-end learning. Optimized constellations are evaluated using two\niterative receiver architectures. Bit error rate (BER) performance gain is\nquantified against standard amplitude phase-shift keying (APSK) and quadrature\namplitude modulation (QAM) constellations. A maximum BER gain of 0.3 dB and\n0.15 dB are observed under two receivers for the learned constellations\ncompared to standard APSK or QAM. The basic approach is extended to incorporate\nthe full iterative detection and decoding loop, using the deep unfolding\ntechnique. A bit error rate gain of 0.1 dB is observed for the iterative scheme\nwith learned constellations under block fading channel conditions, when\ncompared to standard APSK."}
{"id": "2510.22721", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.22721", "abs": "https://arxiv.org/abs/2510.22721", "authors": ["Jason Li", "Connor Mowry", "Satish Rao"], "title": "Faster Negative-Weight Shortest Paths and Directed Low-Diameter Decompositions", "comment": "Merge of unpublished preprints arXiv:2411.19449 and arXiv:2505.10244.\n  Accepted at SODA 2026", "summary": "We present a faster algorithm for low-diameter decompositions on directed\ngraphs, matching the $O(\\log n\\log\\log n)$ loss factor from Bringmann, Fischer,\nHaeupler, and Latypov (ICALP 2025) and improving the running time to\n$O((m+n\\log\\log n)\\log n\\log\\log n)$ in expectation. We then apply our faster\nlow-diameter decomposition to obtain an algorithm for negative-weight single\nsource shortest paths on integer-weighted graphs in $O((m+n\\log\\log\nn)\\log(nW)\\log n\\log\\log n)$ time, a nearly log-factor improvement over the\nalgorithm of Bringmann, Cassis, and Fischer (FOCS 2023)."}
{"id": "2510.21733", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.21733", "abs": "https://arxiv.org/abs/2510.21733", "authors": ["Jia-Huei Ju", "Eugene Yang", "Trevor Adriaanse", "Andrew Yates"], "title": "Augmenting Researchy Questions with Sub-question Judgments", "comment": "3 pages", "summary": "The Researchy Questions dataset provides about 100k question queries with\ncomplex information needs that require retrieving information about several\naspects of a topic. Each query in ResearchyQuestions is associated with\nsub-questions that were produced by prompting GPT-4. While ResearchyQuestions\ncontains labels indicating what documents were clicked after issuing the query,\nthere are no associations in the dataset between sub-questions and relevant\ndocuments. In this work, we augment the Researchy Questions dataset with\nLLM-judged labels for each sub-question using a Llama3.3 70B model. We intend\nthese sub-question labels to serve as a resource for training retrieval models\nthat better support complex information needs."}
{"id": "2510.22671", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.22671", "abs": "https://arxiv.org/abs/2510.22671", "authors": ["Zhou Li", "Siyan Qin", "Xiang Zhang", "Jihao Fan", "Haiqiang Chen", "Giuseppe Caire"], "title": "Graph-Theoretic Characterization of Noise Capacity of Conditional Disclosure of Secrets", "comment": "34 pages, 6 figures", "summary": "In the problem of conditional disclosure of secrets (CDS), two parties, Alice\nand Bob, each has an input and shares a common secret. Their goal is to reveal\nthe secret to a third party, Carol, as efficiently as possible, only if the\ninputs of Alice and Bob satisfy a certain functional relation $f $. To prevent\nleakage of the secret to Carol when the input combination is unqualified, both\nAlice and Bob introduce noise. This work aims to determine the noise capacity,\ndefined as the maximum number of secret bits that can be securely revealed to\nCarol, normalized by the total number of independent noise bits held jointly by\nAlice and Bob. Our contributions are twofold. First, we establish the necessary\nand sufficient conditions under which the CDS noise capacity attains its\nmaximum value of $1$. Second, in addition to the above best-case scenarios, we\nderive an upper bound on the linear noise capacity for any CDS instance. In\nparticular, this upper bound is equal to $(\\rho-1)(d-1)/(\\rho d-1)$, where\n$\\rho$ is the covering parameter of the graph representation of $f$, and $d$ is\nthe number of unqualified edges in residing unqualified path."}
{"id": "2510.22816", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.22816", "abs": "https://arxiv.org/abs/2510.22816", "authors": ["Honghao Lin", "Zhao Song", "David P. Woodruff", "Shenghao Xie", "Samson Zhou"], "title": "$L_p$ Sampling in Distributed Data Streams with Applications to Adversarial Robustness", "comment": "SODA 2026", "summary": "In the distributed monitoring model, a data stream over a universe of size\n$n$ is distributed over $k$ servers, who must continuously provide certain\nstatistics of the overall dataset, while minimizing communication with a\ncentral coordinator. In such settings, the ability to efficiently collect a\nrandom sample from the global stream is a powerful primitive, enabling a wide\narray of downstream tasks such as estimating frequency moments, detecting heavy\nhitters, or performing sparse recovery. Of particular interest is the task of\nproducing a perfect $L_p$ sample, which given a frequency vector $f \\in\n\\mathbb{R}^n$, outputs an index $i$ with probability\n$\\frac{f_i^p}{\\|f\\|_p^p}+\\frac{1}{\\mathrm{poly}(n)}$. In this paper, we resolve\nthe problem of perfect $L_p$ sampling for all $p\\ge 1$ in the distributed\nmonitoring model. Specifically, our algorithm runs in $k^{p-1} \\cdot\n\\mathrm{polylog}(n)$ bits of communication, which is optimal up to\npolylogarithmic factors.\n  Utilizing our perfect $L_p$ sampler, we achieve adversarially-robust\ndistributed monitoring protocols for the $F_p$ moment estimation problem, where\nthe goal is to provide a $(1+\\varepsilon)$-approximation to\n$f_1^p+\\ldots+f_n^p$. Our algorithm uses\n$\\frac{k^{p-1}}{\\varepsilon^2}\\cdot\\mathrm{polylog}(n)$ bits of communication\nfor all $p\\ge 2$ and achieves optimal bounds up to polylogarithmic factors,\nmatching lower bounds by Woodruff and Zhang (STOC 2012) in the non-robust\nsetting. Finally, we apply our framework to achieve near-optimal adversarially\nrobust distributed protocols for central problems such as counting, frequency\nestimation, heavy-hitters, and distinct element estimation."}
{"id": "2510.21737", "categories": ["cs.IR", "68T30, 68T50", "I.2.7; I.2.4; H.3.3"], "pdf": "https://arxiv.org/pdf/2510.21737", "abs": "https://arxiv.org/abs/2510.21737", "authors": ["Liangliang Zhang", "Nandana Mihindukulasooriya", "Niharika S. D'Souza", "Sola Shirai", "Sarthak Dash", "Yao Ma", "Horst Samulowitz"], "title": "From Factoid Questions to Data Product Requests: Benchmarking Data Product Discovery over Tables and Text", "comment": "9 pages, 1 figure, 2 tables", "summary": "Data products are reusable, self-contained assets designed for specific\nbusiness use cases. Automating their discovery and generation is of great\nindustry interest, as it enables discovery in large data lakes and supports\nanalytical Data Product Requests (DPRs). Currently, there is no benchmark\nestablished specifically for data product discovery. Existing datasets focus on\nanswering single factoid questions over individual tables rather than\ncollecting multiple data assets for broader, coherent products. To address this\ngap, we introduce DPBench, the first user-request-driven data product benchmark\nover hybrid table-text corpora. Our framework systematically repurposes\nexisting table-text QA datasets by clustering related tables and passages into\ncoherent data products, generating professional-level analytical requests that\nspan both data sources, and validating benchmark quality through multi-LLM\nevaluation. DPBench preserves full provenance while producing actionable,\nanalyst-like data product requests. Baseline experiments with hybrid retrieval\nmethods establish the feasibility of DPR evaluation, reveal current\nlimitations, and point to new opportunities for automatic data product\ndiscovery research.\n  Code and datasets are available at:\nhttps://anonymous.4open.science/r/data-product-benchmark-BBA7/"}
{"id": "2510.22718", "categories": ["cs.IT", "cs.CV", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.22718", "abs": "https://arxiv.org/abs/2510.22718", "authors": ["Yujie Wan", "Chenxuan Liu", "Shuai Wang", "Tong Zhang", "James Jianqiao Yu", "Kejiang Ye", "Dusit Niyato", "Chengzhong Xu"], "title": "Edge Collaborative Gaussian Splatting with Integrated Rendering and Communication", "comment": "5 pages and 7 figures, submitted for possible publication", "summary": "Gaussian splatting (GS) struggles with degraded rendering quality on low-cost\ndevices. To address this issue, we present edge collaborative GS (ECO-GS),\nwhere each user can switch between a local small GS model to guarantee\ntimeliness and a remote large GS model to guarantee fidelity. However, deciding\nhow to engage the large GS model is nontrivial, due to the interdependency\nbetween rendering requirements and resource conditions. To this end, we propose\nintegrated rendering and communication (IRAC), which jointly optimizes\ncollaboration status (i.e., deciding whether to engage large GS) and edge power\nallocation (i.e., enabling remote rendering) under communication constraints\nacross different users by minimizing a newly-derived GS switching function.\nDespite the nonconvexity of the problem, we propose an efficient penalty\nmajorization minimization (PMM) algorithm to obtain the critical point\nsolution. Furthermore, we develop an imitation learning optimization (ILO)\nalgorithm, which reduces the computational time by over 100x compared to PMM.\nExperiments demonstrate the superiority of PMM and the real-time execution\ncapability of ILO."}
{"id": "2510.22837", "categories": ["cs.DS", "F.2.2"], "pdf": "https://arxiv.org/pdf/2510.22837", "abs": "https://arxiv.org/abs/2510.22837", "authors": ["Bob Dong"], "title": "Hierarchical Exponential Search Via K-Spines", "comment": null, "summary": "We introduce the concept of a k-spine of a tree. A k-spine is essentially a\npath in the tree whose removal leaves only \"less-bushy\" components of a smaller\npathwidth. Using a k-spine as a central guide, we introduce an O(klog dist)\nexponential search algorithm on a tree by searching mainly along the spine to\nnarrow down the target's vicinity and then recursively handling the smaller\ncomponents."}
{"id": "2510.21805", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21805", "abs": "https://arxiv.org/abs/2510.21805", "authors": ["Zhao Liu", "Yichen Zhu", "Yiqing Yang", "Guoping Tang", "Rui Huang", "Qiang Luo", "Xiao Lv", "Ruiming Tang", "Kun Gai", "Guorui Zhou"], "title": "DiffGRM: Diffusion-based Generative Recommendation Model", "comment": "13 pages, 5 figures", "summary": "Generative recommendation (GR) is an emerging paradigm that represents each\nitem via a tokenizer as an n-digit semantic ID (SID) and predicts the next item\nby autoregressively generating its SID conditioned on the user's history.\nHowever, two structural properties of SIDs make ARMs ill-suited. First,\nintra-item consistency: the n digits jointly specify one item, yet the\nleft-to-right causality trains each digit only under its prefix and blocks\nbidirectional cross-digit evidence, collapsing supervision to a single causal\npath. Second, inter-digit heterogeneity: digits differ in semantic granularity\nand predictability, while the uniform next-token objective assigns equal weight\nto all digits, overtraining easy digits and undertraining hard digits. To\naddress these two issues, we propose DiffGRM, a diffusion-based GR model that\nreplaces the autoregressive decoder with a masked discrete diffusion model\n(MDM), thereby enabling bidirectional context and any-order parallel generation\nof SID digits for recommendation. Specifically, we tailor DiffGRM in three\naspects: (1) tokenization with Parallel Semantic Encoding (PSE) to decouple\ndigits and balance per-digit information; (2) training with On-policy Coherent\nNoising (OCN) that prioritizes uncertain digits via coherent masking to\nconcentrate supervision on high-value signals; and (3) inference with\nConfidence-guided Parallel Denoising (CPD) that fills higher-confidence digits\nfirst and generates diverse Top-K candidates. Experiments show consistent gains\nover strong generative and discriminative recommendation baselines on multiple\ndatasets, improving NDCG@10 by 6.9%-15.5%. Code is available at\nhttps://github.com/liuzhao09/DiffGRM."}
{"id": "2510.22896", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.22896", "abs": "https://arxiv.org/abs/2510.22896", "authors": ["Yadong Jiao", "Xiaoyan Cheng", "Yuansheng Tang", "Ming Xu"], "title": "On the Arikan Transformations of Binary-Input Discrete Memoryless Channels", "comment": "arXiv admin note: substantial text overlap with arXiv:2506.04163", "summary": "The polar codes introduced by Arikan in 2009 achieve the capacity of\nbinary-input discrete memoryless channels (BIDMCs) with low complexity encoding\nand decoding. Identifying the unreliable synthetic channels, generated by\nArikan transformation during the construction of these polar codes, is crucial.\nCurrently, because of the large size of the output alphabets of synthetic\nchannels, there is no efficient and practical approach to evaluate their\nreliability in general. To tackle this problem, by converting the generation of\nsynthetic channels in polar code construction into algebraic operations, in\nthis paper we develop a method to characterize the synthetic channels as random\nswitching channels of binary symmetric channels when the underlying channels\nare symmetric. Moreover, a lower bound for the average number of elements that\npossess the same likelihood ratio within the output alphabet of any synthetic\nchannel generated in polar codes is also derived."}
{"id": "2510.22845", "categories": ["cs.DS", "68R05", "F.2.2"], "pdf": "https://arxiv.org/pdf/2510.22845", "abs": "https://arxiv.org/abs/2510.22845", "authors": ["Harish Chandramouleeswaran", "Ilan Newman", "Tomer Pelleg", "Nithin Varma"], "title": "Testing forbidden order-pattern properties on hypergrids", "comment": "51 pages. 7 figures. To appear at SODA 2026. This is the full version", "summary": "We study testing $\\pi$-freeness of functions $f:[n]^d\\to\\mathbb{R}$, where\n$f$ is $\\pi$-free if there there are no $k$ indices $x_1\\prec\\cdots\\prec x_k\\in\n[n]^d$ such that $f(x_i)<f(x_j)$ and $\\pi(i) < \\pi(j)$ for all $i,j \\in [k]$,\nwhere $\\prec$ is the natural partial order over $[n]^d$. Given\n$\\epsilon\\in(0,1)$, $\\epsilon$-testing $\\pi$-freeness asks to distinguish\n$\\pi$-free functions from those which are $\\epsilon$-far -- meaning at least\n$\\epsilon n^d$ function values must be modified to make it $\\pi$-free. While\n$k=2$ coincides with monotonicity testing, far less is known for $k>2$.\n  We initiate a systematic study of pattern freeness on higher-dimensional\ngrids. For $d=2$ and all permutations of size $k=3$, we design an adaptive\none-sided tester with query complexity $O(n^{4/5+o(1)})$. We also prove general\nlower bounds for $k=3$: every nonadaptive tester requires $\\Omega(n)$ queries,\nand every adaptive tester requires $\\Omega(\\sqrt{n})$ queries, yielding the\nfirst super-logarithmic lower bounds for $\\pi$-freeness. For the monotone\npatterns $\\pi=(1,2,3)$ and $(3,2,1)$, we present a nonadaptive tester with\npolylogarithmic query complexity, giving an exponential separation between\nmonotone and nonmonotone patterns (unlike the one-dimensional case).\n  A key ingredient in our $\\pi$-freeness testers is new erasure-resilient\n($\\delta$-ER) $\\epsilon$-testers for monotonicity over $[n]^d$ with query\ncomplexity $O(\\log^{O(d)}n/(\\epsilon(1-\\delta)))$, where $0<\\delta<1$ is an\nupper bound on the fraction of erasures. Prior ER testers worked only for\n$\\delta=O(\\epsilon/d)$. Our nonadaptive monotonicity tester is nearly optimal\nvia a matching lower bound due to Pallavoor, Raskhodnikova, and Waingarten\n(Random Struct. Algorithms, 2022). Finally, we show that current techniques\ncannot yield sublinear-query testers for patterns of length $4$ even on\ntwo-dimensional hypergrids."}
{"id": "2510.21812", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21812", "abs": "https://arxiv.org/abs/2510.21812", "authors": ["Chanyoung Chung", "Kyeongryul Lee", "Sunbin Park", "Joyce Jiyoung Whang"], "title": "Unifying Inductive, Cross-Domain, and Multimodal Learning for Robust and Generalizable Recommendation", "comment": "7 pages, 3 figures, and 4 tables. International Workshop on\n  Multimodal Generative Search and Recommendation (MMGenSR) at The 34th ACM\n  International Conference on Information and Knowledge Management (CIKM 2025)", "summary": "Recommender systems have long been built upon the modeling of interactions\nbetween users and items, while recent studies have sought to broaden this\nparadigm by generalizing to new users and items, incorporating diverse\ninformation sources, and transferring knowledge across domains. Nevertheless,\nthese efforts have largely focused on individual aspects, hindering their\nability to tackle the complex recommendation scenarios that arise in daily\nconsumptions across diverse domains. In this paper, we present MICRec, a\nunified framework that fuses inductive modeling, multimodal guidance, and\ncross-domain transfer to capture user contexts and latent preferences in\nheterogeneous and incomplete real-world data. Moving beyond the inductive\nbackbone of INMO, our model refines expressive representations through\nmodality-based aggregation and alleviates data sparsity by leveraging\noverlapping users as anchors across domains, thereby enabling robust and\ngeneralizable recommendation. Experiments show that MICRec outperforms 12\nbaselines, with notable gains in domains with limited training data."}
{"id": "2510.23230", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.23230", "abs": "https://arxiv.org/abs/2510.23230", "authors": ["Raúl Gutiérrez", "Víctor Rampérez", "Horacio Paggi", "Juan A. Lara", "Javier Soriano"], "title": "On the use of information fusion techniques to improve information quality: Taxonomy, opportunities and challenges", "comment": null, "summary": "The information fusion field has recently been attracting a lot of interest\nwithin the scientific community, as it provides, through the combination of\ndifferent sources of heterogeneous information, a fuller and/or more precise\nunderstanding of the real world than can be gained considering the above\nsources separately. One of the fundamental aims of computer systems, and\nespecially decision support systems, is to assure that the quality of the\ninformation they process is high. There are many different approaches for this\npurpose, including information fusion. Information fusion is currently one of\nthe most promising methods. It is particularly useful under circumstances where\nquality might be compromised, for example, either intrinsically due to\nimperfect information (vagueness, uncertainty) or because of limited resources\n(energy, time). In response to this goal, a wide range of research has been\nundertaken over recent years. To date, the literature reviews in this field\nhave focused on problem-specific issues and have been circumscribed to certain\nsystem types. Therefore, there is no holistic and systematic knowledge of the\nstate of the art to help establish the steps to be taken in the future. In\nparticular, aspects like what impact different information fusion methods have\non information quality, how information quality is characterised, measured and\nevaluated in different application domains depending on the problem data type\nor whether fusion is designed as a flexible process capable of adapting to\nchanging system circumstances and their intrinsically limited resources have\nnot been addressed. This paper aims precisely to review the literature on\nresearch into the use of information fusion techniques specifically to improve\ninformation quality, analysing the above issues in order to identify a series\nof challenges and research directions, which are presented in this paper."}
{"id": "2510.22882", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.22882", "abs": "https://arxiv.org/abs/2510.22882", "authors": ["Amit Joshi"], "title": "Multi-Way Co-Ranking: Index-Space Partitioning of Sorted Sequences Without Merge", "comment": "4 pages", "summary": "We present a merge-free algorithm for multi-way co-ranking, the problem of\ncomputing cut indices $i_1,\\dots,i_m$ that partition each of the $m$ sorted\nsequences such that all prefix segments together contain exactly $K$ elements.\nOur method extends two-list co-ranking to arbitrary $m$, maintaining\nper-sequence bounds that converge to a consistent global frontier without\nperforming any multi-way merge or value-space search. Rather, we apply binary\nsearch to \\emph{index-space}. The algorithm runs in $O(\\log(\\sum_t n_t)\\,\\log\nm)$ time and $O(m)$ space, independent of $K$. We prove correctness via an\nexchange argument and discuss applications to distributed fractional knapsack,\nparallel merge partitioning, and multi-stream joins.\n  Keywords: Co-ranking \\sep partitioning \\sep Merge-free algorithms \\sep\nIndex-space optimization \\sep Selection and merging \\sep Data structures"}
{"id": "2510.21831", "categories": ["cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.21831", "abs": "https://arxiv.org/abs/2510.21831", "authors": ["Alok Dutta", "Nilanjana Roy", "Rhythm Sen", "Sougata Dutta", "Prabhat Das"], "title": "Development of an Automated Web Application for Efficient Web Scraping: Design and Implementation", "comment": null, "summary": "This paper presents the design and implementation of a user-friendly,\nautomated web application that simplifies and optimizes the web scraping\nprocess for non-technical users. The application breaks down the complex task\nof web scraping into three main stages: fetching, extraction, and execution. In\nthe fetching stage, the application accesses target websites using the HTTP\nprotocol, leveraging the requests library to retrieve HTML content. The\nextraction stage utilizes powerful parsing libraries like BeautifulSoup and\nregular expressions to extract relevant data from the HTML. Finally, the\nexecution stage structures the data into accessible formats, such as CSV,\nensuring the scraped content is organized for easy use. To provide personalized\nand secure experiences, the application includes user registration and login\nfunctionalities, supported by MongoDB, which stores user data and scraping\nhistory. Deployed using the Flask framework, the tool offers a scalable, robust\nenvironment for web scraping. Users can easily input website URLs, define data\nextraction parameters, and download the data in a simplified format, without\nneeding technical expertise. This automated tool not only enhances the\nefficiency of web scraping but also democratizes access to data extraction by\nempowering users of all technical levels to gather and manage data tailored to\ntheir needs. The methodology detailed in this paper represents a significant\nadvancement in making web scraping tools accessible, efficient, and easy to use\nfor a broader audience."}
{"id": "2510.23315", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.23315", "abs": "https://arxiv.org/abs/2510.23315", "authors": ["Yushen Lin", "Zihan Chen", "Zhiguo Ding"], "title": "Pinching-antenna-enabled Federated Learning: Tail Latency, Participation, and Convergence Analysis", "comment": "13 pages, 8 figures", "summary": "Federated learning (FL) in wireless networks is limited by straggler delays\nfrom unpredictable channel conditions. In this paper, we investigate the\npinching-antenna system (PASS), which dynamically 'pinches' the radiator along\na dielectric waveguide to shorten the worst links. In synchronous FL (SFL), we\nprove that PASS shortens the worst-link distance, and it increases the on-time\ncompletion probability in asynchronous FL (AFL). Accordingly, SFL exhibits\nstochastic dominance on round time, while AFL yields explicit latency and\nparticipation gains. We then pair physical-layer (PHY)-aware sampling with\nerror-feedback compression and prove that pinching raises the minimum inclusion\nprobability, thus shrinking both the sampling variability and\ncompression-induced floors in a Lyapunov analysis. Simulations demonstrate\nconsistent wall clock speedups and markedly shorter latency tails. By\naddressing stragglers at their PHY root, PASS complements higher-layer\nscheduling and accelerates wireless FL in both SFL and AFL."}
{"id": "2510.21962", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.21962", "abs": "https://arxiv.org/abs/2510.21962", "authors": ["Yunsen Lei", "Kexin Bai", "Quan Li", "H. Howie Huang"], "title": "Temporal Graph Theoretic Analysis of Geopolitical Dynamics in the U.S. Entity List", "comment": "13 pages, 9 figures. Under review", "summary": "Export controls have become one of America's most prominent tools of economic\nstatecraft. They aim to block rival countries' access to sensitive\ntechnologies, safeguard U.S. supply chains, protect national security, and\nshape geopolitical competition. Among various instruments, the U.S. Entity List\nhas emerged as the most salient, yet its dynamics remain underexplored. This\npaper introduces a novel temporal graph framework that transforms the Entity\nList documents from a static registry of foreign entities of concern into a\ndynamic representation of geopolitical strategy. We construct the first\nevent-based dataset of U.S. government foreign entity designations and model\nthem as a temporal bipartite graph. Building on this representation, we develop\na multi-level analytical approach that reveals shifting roles, enforcement\nstrategy, and broader sanction ecosystems. Applied to 25 years of data, the\nframework uncovers dynamic patterns of escalation, persistence, and\ncoordination that static views cannot capture. More broadly, our study\ndemonstrates how temporal graph analysis offers systematic computational\ninsights into the geopolitical dynamics of export controls."}
{"id": "2510.23316", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.23316", "abs": "https://arxiv.org/abs/2510.23316", "authors": ["Jie Li", "Xiaohu Tang"], "title": "Efficient Repair of (k+2, k) Degraded Read Friendly MDS Array Codes With Sub-packetization 2", "comment": "13 pages, submitted to the IEEE Transactions on Information Theory", "summary": "In this paper, we present two constructions of degraded read friendly (DRF)\nMDS array codes with two parity nodes and a sub-packetization level of 2 over\nsmall finite fields, applicable for any arbitrary code length. The first\nconstruction achieves the smallest repair bandwidth among all existing\nconstructions with the same parameters, and is asymptotically optimal with\nrespect to the lower bound on the average repair bandwidth characterized by\nZhang et al. The second construction supports two repair mechanisms, depending\non whether computation within the helper nodes is permitted or not during the\nnode repair process, thereby optimizing either the repair bandwidth or the\nrebuilding access."}
{"id": "2510.22023", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.22023", "abs": "https://arxiv.org/abs/2510.22023", "authors": ["Yifan Liu", "Qianfeng Wen", "Jiazhou Liang", "Mark Zhao", "Justin Cui", "Anton Korikov", "Armin Torogh", "Junyoung Kim", "Scott Sanner"], "title": "Multimodal Item Scoring for Natural Language Recommendation via Gaussian Process Regression with LLM Relevance Judgments", "comment": "16 pages,20 figures", "summary": "Natural Language Recommendation (NLRec) generates item suggestions based on\nthe relevance between user-issued NL requests and NL item description passages.\nExisting NLRec approaches often use Dense Retrieval (DR) to compute item\nrelevance scores from aggregation of inner products between user request\nembeddings and relevant passage embeddings. However, DR views the request as\nthe sole relevance label, thus leading to a unimodal scoring function centered\non the query embedding that is often a weak proxy for query relevance. To\nbetter capture the potential multimodal distribution of the relevance scoring\nfunction that may arise from complex NLRec data, we propose GPR-LLM that uses\nGaussian Process Regression (GPR) with LLM relevance judgments for a subset of\ncandidate passages. Experiments on four NLRec datasets and two LLM backbones\ndemonstrate that GPR-LLM with an RBF kernel, capable of modeling multimodal\nrelevance scoring functions, consistently outperforms simpler unimodal kernels\n(dot product, cosine similarity), as well as baseline methods including DR,\ncross-encoder, and pointwise LLM-based relevance scoring by up to 65%. Overall,\nGPR-LLM provides an efficient and effective approach to NLRec within a minimal\nLLM labeling budget."}
{"id": "2510.22049", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22049", "abs": "https://arxiv.org/abs/2510.22049", "authors": ["Zhimin Chen", "Chenyu Zhao", "Ka Chun Mo", "Yunjiang Jiang", "Jane H. Lee", "Shouwei Chen", "Khushhall Chandra Mahajan", "Ning Jiang", "Kai Ren", "Jinhui Li", "Wen-Yun Yang"], "title": "Massive Memorization with Hundreds of Trillions of Parameters for Sequential Transducer Generative Recommenders", "comment": null, "summary": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users."}
{"id": "2510.22055", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22055", "abs": "https://arxiv.org/abs/2510.22055", "authors": ["V Venktesh", "Deepali Prabhu", "Avishek Anand"], "title": "A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim Decomposition", "comment": "16 pages", "summary": "Fact-checking numerical claims is critical as the presence of numbers provide\nmirage of veracity despite being fake potentially causing catastrophic impacts\non society. The prior works in automatic fact verification do not primarily\nfocus on natural numerical claims. A typical human fact-checker first retrieves\nrelevant evidence addressing the different numerical aspects of the claim and\nthen reasons about them to predict the veracity of the claim. Hence, the search\nprocess of a human fact-checker is a crucial skill that forms the foundation of\nthe verification process. Emulating a real-world setting is essential to aid in\nthe development of automated methods that encompass such skills. However,\nexisting benchmarks employ heuristic claim decomposition approaches augmented\nwith weakly supervised web search to collect evidences for verifying claims.\nThis sometimes results in less relevant evidences and noisy sources with\ntemporal leakage rendering a less realistic retrieval setting for claim\nverification. Hence, we introduce QuanTemp++: a dataset consisting of natural\nnumerical claims, an open domain corpus, with the corresponding relevant\nevidence for each claim. The evidences are collected through a claim\ndecomposition process approximately emulating the approach of human\nfact-checker and veracity labels ensuring there is no temporal leakage. Given\nthis dataset, we also characterize the retrieval performance of key claim\ndecomposition paradigms. Finally, we observe their effect on the outcome of the\nverification pipeline and draw insights. The code for data pipeline along with\nlink to data can be found at https://github.com/VenkteshV/QuanTemp_Plus"}
{"id": "2510.22101", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22101", "abs": "https://arxiv.org/abs/2510.22101", "authors": ["Kayhan Behdin", "Qingquan Song", "Sriram Vasudevan", "Jian Sheng", "Xiaojing Ma", "Z Zhou", "Chuanrui Zhu", "Guoyao Li", "Chanh Nguyen", "Sayan Ghosh", "Hejian Sang", "Ata Fatahi Baarzi", "Sundara Raman Ramachandran", "Xiaoqing Wang", "Qing Lan", "Vinay Y S", "Qi Guo", "Caleb Johnson", "Zhipeng Wang", "Fedor Borisyuk"], "title": "Scaling Up Efficient Small Language Models Serving and Deployment for Semantic Job Search", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive quality when\napplied to predictive tasks such as relevance ranking and semantic search.\nHowever, deployment of such LLMs remains prohibitively expensive for industry\napplications with strict latency and throughput requirements. In this work, we\npresent lessons and efficiency insights from developing a purely text-based\ndecoder-only Small Language Model (SLM) for a semantic search application at\nLinkedIn. Particularly, we discuss model compression techniques such as pruning\nthat allow us to reduce the model size by up to $40\\%$ while maintaining the\naccuracy. Additionally, we present context compression techniques that allow us\nto reduce the input context length by up to $10$x with minimal loss of\naccuracy. Finally, we present practical lessons from optimizing the serving\ninfrastructure for deploying such a system on GPUs at scale, serving millions\nof requests per second. Taken together, this allows us to increase our system's\nthroughput by $10$x in a real-world deployment, while meeting our quality bar."}
{"id": "2510.22215", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.22215", "abs": "https://arxiv.org/abs/2510.22215", "authors": ["Juyeon Kim", "Geon Lee", "Dongwon Choi", "Taeuk Kim", "Kijung Shin"], "title": "Hybrid-Vector Retrieval for Visually Rich Documents: Combining Single-Vector Efficiency and Multi-Vector Accuracy", "comment": null, "summary": "Retrieval over visually rich documents is essential for tasks such as legal\ndiscovery, scientific search, and enterprise knowledge management. Existing\napproaches fall into two paradigms: single-vector retrieval, which is efficient\nbut coarse, and multi-vector retrieval, which is accurate but computationally\nexpensive. To address this trade-off, we propose HEAVEN, a two-stage\nhybrid-vector framework. In the first stage, HEAVEN efficiently retrieves\ncandidate pages using a single-vector method over Visually-Summarized Pages\n(VS-Pages), which assemble representative visual layouts from multiple pages.\nIn the second stage, it reranks candidates with a multi-vector method while\nfiltering query tokens by linguistic importance to reduce redundant\ncomputations. To evaluate retrieval systems under realistic conditions, we also\nintroduce ViMDOC, the first benchmark for visually rich, multi-document, and\nlong-document retrieval. Across four benchmarks, HEAVEN attains 99.87% of the\nRecall@1 performance of multi-vector models on average while reducing per-query\ncomputation by 99.82%, achieving efficiency and accuracy. Our code and datasets\nare available at: https://github.com/juyeonnn/HEAVEN"}
{"id": "2510.22242", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22242", "abs": "https://arxiv.org/abs/2510.22242", "authors": ["Yutao Wu", "Xiao Liu", "Yunhao Feng", "Jiale Ding", "Xingjun Ma"], "title": "PaperAsk: A Benchmark for Reliability Evaluation of LLMs in Paper Search and Reading", "comment": null, "summary": "Large Language Models (LLMs) increasingly serve as research assistants, yet\ntheir reliability in scholarly tasks remains under-evaluated. In this work, we\nintroduce PaperAsk, a benchmark that systematically evaluates LLMs across four\nkey research tasks: citation retrieval, content extraction, paper discovery,\nand claim verification. We evaluate GPT-4o, GPT-5, and Gemini-2.5-Flash under\nrealistic usage conditions-via web interfaces where search operations are\nopaque to the user. Through controlled experiments, we find consistent\nreliability failures: citation retrieval fails in 48-98% of multi-reference\nqueries, section-specific content extraction fails in 72-91% of cases, and\ntopical paper discovery yields F1 scores below 0.32, missing over 60% of\nrelevant literature. Further human analysis attributes these failures to the\nuncontrolled expansion of retrieved context and the tendency of LLMs to\nprioritize semantically relevant text over task instructions. Across basic\ntasks, the LLMs display distinct failure behaviors: ChatGPT often withholds\nresponses rather than risk errors, whereas Gemini produces fluent but\nfabricated answers. To address these issues, we develop lightweight reliability\nclassifiers trained on PaperAsk data to identify unreliable outputs. PaperAsk\nprovides a reproducible and diagnostic framework for advancing the reliability\nevaluation of LLM-based scholarly assistance systems."}
{"id": "2510.22670", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.22670", "abs": "https://arxiv.org/abs/2510.22670", "authors": ["Xuan Lu", "Haohang Huang", "Rui Meng", "Yaohui Jin", "Wenjun Zeng", "Xiaoyu Shen"], "title": "Tools are under-documented: Simple Document Expansion Boosts Tool Retrieval", "comment": null, "summary": "Large Language Models (LLMs) have recently demonstrated strong capabilities\nin tool use, yet progress in tool retrieval remains hindered by incomplete and\nheterogeneous tool documentation. To address this challenge, we introduce\nTool-DE, a new benchmark and framework that systematically enriches tool\ndocumentation with structured fields to enable more effective tool retrieval,\ntogether with two dedicated models, Tool-Embed and Tool-Rank. We design a\nscalable document expansion pipeline that leverages both open- and\nclosed-source LLMs to generate, validate, and refine enriched tool profiles at\nlow cost, producing large-scale corpora with 50k instances for embedding-based\nretrievers and 200k for rerankers. On top of this data, we develop two models\nspecifically tailored for tool retrieval: Tool-Embed, a dense retriever, and\nTool-Rank, an LLM-based reranker. Extensive experiments on ToolRet and Tool-DE\ndemonstrate that document expansion substantially improves retrieval\nperformance, with Tool-Embed and Tool-Rank achieving new state-of-the-art\nresults on both benchmarks. We further analyze the contribution of individual\nfields to retrieval effectiveness, as well as the broader impact of document\nexpansion on both training and evaluation. Overall, our findings highlight both\nthe promise and limitations of LLM-driven document expansion, positioning\nTool-DE, along with the proposed Tool-Embed and Tool-Rank, as a foundation for\nfuture research in tool retrieval."}
{"id": "2510.22681", "categories": ["cs.IR", "H.3.3; H.3.4; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.22681", "abs": "https://arxiv.org/abs/2510.22681", "authors": ["Rikiya Takehi", "Fernando Diaz", "Tetsuya Sakai"], "title": "Diversification as Risk Minimization", "comment": "Preprint, accepted at WSDM 2026 (Full Paper). 16 pages, 8 figures", "summary": "Users tend to remember failures of a search session more than its many\nsuccesses. This observation has led to work on search robustness, where systems\nare penalized if they perform very poorly on some queries. However, this\nprinciple of robustness has been overlooked within a single query. An ambiguous\nor underspecified query (e.g., ``jaguar'') can have several user intents, where\npopular intents often dominate the ranking, leaving users with minority intents\nunsatisfied. Although the diversification literature has long recognized this\nissue, existing metrics only model the average relevance across intents and\nprovide no robustness guarantees. More surprisingly, we show theoretically and\nempirically that many well-known diversification algorithms are no more robust\nthan a naive, non-diversified algorithm. To address this critical gap, we\npropose to frame diversification as a risk-minimization problem. We introduce\nVRisk, which measures the expected risk faced by the least-served fraction of\nintents in a query. Optimizing VRisk produces a robust ranking, reducing the\nlikelihood of poor user experiences. We then propose VRisker, a fast greedy\nre-ranker with provable approximation guarantees. Finally, experiments on NTCIR\nINTENT-2, TREC Web 2012, and MovieLens show the vulnerability of existing\nmethods. VRisker reduces worst-case intent failures by up to 33% with a minimal\n2% drop in average performance."}
{"id": "2510.22739", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22739", "abs": "https://arxiv.org/abs/2510.22739", "authors": ["Yiwen Tang", "Qiuyu Zhao", "Zenghui Sun", "Jinsong Lan", "Xiaoyong Zhu", "Bo Zheng", "Kaifu Zhang"], "title": "REVISION:Reflective Intent Mining and Online Reasoning Auxiliary for E-commerce Visual Search System Optimization", "comment": null, "summary": "In Taobao e-commerce visual search, user behavior analysis reveals a large\nproportion of no-click requests, suggesting diverse and implicit user intents.\nThese intents are expressed in various forms and are difficult to mine and\ndiscover, thereby leading to the limited adaptability and lag in platform\nstrategies. This greatly restricts users' ability to express diverse intents\nand hinders the scalability of the visual search system. This mismatch between\nuser implicit intent expression and system response defines the User-SearchSys\nIntent Discrepancy. To alleviate the issue, we propose a novel framework\nREVISION. This framework integrates offline reasoning mining with online\ndecision-making and execution, enabling adaptive strategies to solve implicit\nuser demands. In the offline stage, we construct a periodic pipeline to mine\ndiscrepancies from historical no-click requests. Leveraging large models, we\nanalyze implicit intent factors and infer optimal suggestions by jointly\nreasoning over query and product metadata. These inferred suggestions serve as\nactionable insights for refining platform strategies. In the online stage,\nREVISION-R1-3B, trained on the curated offline data, performs holistic analysis\nover query images and associated historical products to generate optimization\nplans and adaptively schedule strategies across the search pipeline. Our\nframework offers a streamlined paradigm for integrating large models with\ntraditional search systems, enabling end-to-end intelligent optimization across\ninformation aggregation and user interaction. Experimental results demonstrate\nthat our approach improves the efficiency of implicit intent mining from\nlarge-scale search logs and significantly reduces the no-click rate."}
{"id": "2510.22865", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.22865", "abs": "https://arxiv.org/abs/2510.22865", "authors": ["James Meese", "Kyle Herbertson"], "title": "Civic Ground Truth in News Recommenders: A Method for Public Value Scoring", "comment": "Presented at NORMalize 2025: The Third Workshop on the Normative\n  Design and Evaluation of Recommender Systems, co-located with the ACM\n  Conference on Recommender Systems 2025 (RecSys 2025), Prague", "summary": "Research in news recommendation systems (NRS) continues to explore the best\nways to integrate normative goals such as editorial objectives and public\nservice values into existing systems. Prior efforts have incorporated expert\ninput or audience feedback to quantify these values, laying the groundwork for\nmore civic-minded recommender systems. This paper contributes to that\ntrajectory, introducing a method for embedding civic values into NRS through\nlarge-scale, structured audience evaluations. The proposed civic ground truth\napproach aims to generate value-based labels through a nationally\nrepresentative survey that are generalisable across a wider news corpus, using\nautomated metadata enrichment."}
{"id": "2510.22888", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.22888", "abs": "https://arxiv.org/abs/2510.22888", "authors": ["Shihao Cai", "Chongming Gao", "Haoyan Liu", "Wentao Shi", "Jianshan Sun", "Ruiming Tang", "Fuli Feng"], "title": "MGFRec: Towards Reinforced Reasoning Recommendation with Multiple Groundings and Feedback", "comment": null, "summary": "The powerful reasoning and generative capabilities of large language models\n(LLMs) have inspired researchers to apply them to reasoning-based\nrecommendation tasks, which require in-depth reasoning about user interests and\nthe generation of recommended items. However, previous reasoning-based\nrecommendation methods have typically performed inference within the language\nspace alone, without incorporating the actual item space. This has led to\nover-interpreting user interests and deviating from real items. Towards this\nresearch gap, we propose performing multiple rounds of grounding during\ninference to help the LLM better understand the actual item space, which could\nensure that its reasoning remains aligned with real items. Furthermore, we\nintroduce a user agent that provides feedback during each grounding step,\nenabling the LLM to better recognize and adapt to user interests. Comprehensive\nexperiments conducted on three Amazon review datasets demonstrate the\neffectiveness of incorporating multiple groundings and feedback. These findings\nunderscore the critical importance of reasoning within the actual item space,\nrather than being confined to the language space, for recommendation tasks."}
{"id": "2510.23018", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.23018", "abs": "https://arxiv.org/abs/2510.23018", "authors": ["JaeEun Lim", "Soomin Kim", "Jaeyong Seo", "Iori Ono", "Qimu Ran", "Jae-woong Lee"], "title": "Improving Product Search Relevance with EAR-MP: A Solution for the CIKM 2025 AnalytiCup", "comment": null, "summary": "Multilingual e-commerce search is challenging due to linguistic diversity and\nthe noise inherent in user-generated queries. This paper documents the solution\nemployed by our team (EAR-MP) for the CIKM 2025 AnalytiCup, which addresses two\ncore tasks: Query-Category (QC) relevance and Query-Item (QI) relevance. Our\napproach first normalizes the multilingual dataset by translating all text into\nEnglish, then mitigates noise through extensive data cleaning and\nnormalization. For model training, we build on DeBERTa-v3-large and improve\nperformance with label smoothing, self-distillation, and dropout. In addition,\nwe introduce task-specific upgrades, including hierarchical token injection for\nQC and a hybrid scoring mechanism for QI. Under constrained compute, our method\nachieves competitive results, attaining an F1 score of 0.8796 on QC and 0.8744\non QI. These findings underscore the importance of systematic data\npreprocessing and tailored training strategies for building robust,\nresource-efficient multilingual relevance systems."}
{"id": "2510.23066", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.23066", "abs": "https://arxiv.org/abs/2510.23066", "authors": ["Yichao Jin", "Yushuo Wang", "Qishuai Zhong", "Kent Chiu Jin-Chun", "Kenneth Zhu Ke", "Donald MacDonald"], "title": "Multi-Stage Field Extraction of Financial Documents with OCR and Compact Vision-Language Models", "comment": null, "summary": "Financial documents are essential sources of information for regulators,\nauditors, and financial institutions, particularly for assessing the wealth and\ncompliance of Small and Medium-sized Businesses. However, SMB documents are\noften difficult to parse. They are rarely born digital and instead are\ndistributed as scanned images that are none machine readable. The scans\nthemselves are low in resolution, affected by skew or rotation, and often\ncontain noisy backgrounds. These documents also tend to be heterogeneous,\nmixing narratives, tables, figures, and multilingual content within the same\nreport. Such characteristics pose major challenges for automated information\nextraction, especially when relying on end to end large Vision Language Models,\nwhich are computationally expensive, sensitive to noise, and slow when applied\nto files with hundreds of pages.\n  We propose a multistage pipeline that leverages traditional image processing\nmodels and OCR extraction, together with compact VLMs for structured field\nextraction of large-scale financial documents. Our approach begins with image\npre-processing, including segmentation, orientation detection, and size\nnormalization. Multilingual OCR is then applied to recover page-level text.\nUpon analyzing the text information, pages are retrieved for coherent sections.\nFinally, compact VLMs are operated within these narrowed-down scopes to extract\nstructured financial indicators.\n  Our approach is evaluated using an internal corpus of multi-lingual, scanned\nfinancial documents. The results demonstrate that compact VLMs, together with a\nmultistage pipeline, achieves 8.8 times higher field level accuracy relative to\ndirectly feeding the whole document into large VLMs, only at 0.7 percent of the\nGPU cost and 92.6 percent less end-to-end service latency."}
{"id": "2510.23077", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23077", "abs": "https://arxiv.org/abs/2510.23077", "authors": ["Xiaoyu Kong", "Junguang Jiang", "Bin Liu", "Ziru Xu", "Han Zhu", "Jian Xu", "Bo Zheng", "Jiancan Wu", "Xiang Wang"], "title": "Think before Recommendation: Autonomous Reasoning-enhanced Recommender", "comment": "NeurIPS 2025 poster", "summary": "The core task of recommender systems is to learn user preferences from\nhistorical user-item interactions. With the rapid development of large language\nmodels (LLMs), recent research has explored leveraging the reasoning\ncapabilities of LLMs to enhance rating prediction tasks. However, existing\ndistillation-based methods suffer from limitations such as the teacher model's\ninsufficient recommendation capability, costly and static supervision, and\nsuperficial transfer of reasoning ability. To address these issues, this paper\nproposes RecZero, a reinforcement learning (RL)-based recommendation paradigm\nthat abandons the traditional multi-model and multi-stage distillation\napproach. Instead, RecZero trains a single LLM through pure RL to autonomously\ndevelop reasoning capabilities for rating prediction. RecZero consists of two\nkey components: (1) \"Think-before-Recommendation\" prompt construction, which\nemploys a structured reasoning template to guide the model in step-wise\nanalysis of user interests, item features, and user-item compatibility; and (2)\nrule-based reward modeling, which adopts group relative policy optimization\n(GRPO) to compute rewards for reasoning trajectories and optimize the LLM.\nAdditionally, the paper explores a hybrid paradigm, RecOne, which combines\nsupervised fine-tuning with RL, initializing the model with cold-start\nreasoning samples and further optimizing it with RL. Experimental results\ndemonstrate that RecZero and RecOne significantly outperform existing baseline\nmethods on multiple benchmark datasets, validating the superiority of the RL\nparadigm in achieving autonomous reasoning-enhanced recommender systems."}
