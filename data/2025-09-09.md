<div id=toc></div>

# Table of Contents

- [cs.GT](#cs.GT) [Total: 3]
- [cs.IT](#cs.IT) [Total: 8]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.LG](#cs.LG) [Total: 107]
- [cs.AI](#cs.AI) [Total: 48]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 10]


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [1] [Bi-Level Game-Theoretic Planning of Cyber Deception for Cognitive Arbitrage](https://arxiv.org/abs/2509.05498)
*Ya-Ting Yang,Quanyan Zhu*

Main category: cs.GT

TL;DR: 这篇论文探讨了利用APT攻击者的认知弱点的防御策略，通过游戏理论模型设计决策欺证机制，实现了至少40%的效果提升。


<details>
  <summary>Details</summary>
Motivation: 利用攻击者的认知弱点（认知能力差异和认知偏见）进行认知套利，获得战略优势并反制APT攻击。

Method: 提出双层次网络战争游戏模型，结合游戏理论分析，设计战略层次的防御欺证机制和操作层次的TTP执行。

Result: 防御方初始优势虽然随时间减弱，但策略性时机和部署的欺证技术能够在规划阶段将攻击者的负面价值转为正面，执行阶段总奖励提升至少40%。

Conclusion: 防御方可以放大小小的初始优势，持续保持对攻击者的战略优势，实现长期保护关键资产的目标。

Abstract: Cognitive vulnerabilities shape human decision-making and arise primarily
from two sources: (1) cognitive capabilities, which include disparities in
knowledge, education, expertise, or access to information, and (2) cognitive
biases, such as rational inattention, confirmation bias, and base rate neglect,
which influence how individuals perceive and process information. Exploiting
these vulnerabilities allows an entity with superior cognitive awareness to
gain a strategic advantage, a concept referred to as cognitive arbitrage. This
paper investigates how to exploit the cognitive vulnerabilities of Advanced
Persistent Threat (APT) attackers and proposes cognition-aware defenses that
leverage windows of superiority to counteract attacks. Specifically, the
proposed bi-level cyber warfare game focuses on "strategic-level" design for
defensive deception mechanisms, which then facilitates "operational-level"
actions and tactical-level execution of Tactics, Techniques, and Procedures
(TTPs). Game-theoretic reasoning and analysis play a significant role in the
cross-echelon quantitative modeling and design of cognitive arbitrage
strategies. Our numerical results demonstrate that although the defender's
initial advantage diminishes over time, strategically timed and deployed
deception techniques can turn a negative value for the attacker into a positive
one during the planning phase, and achieve at least a 40% improvement in total
rewards during execution. This demonstrates that the defender can amplify even
small initial advantages, sustain a strategic edge over the attacker, and
secure long-term objectives, such as protecting critical assets throughout the
attacker's lifecycle.

</details>


### [2] [Knapsack Contracts and the Importance of Return-on-Investment](https://arxiv.org/abs/2509.05956)
*Zohar Barak,Asnat Berlin,Ilan Reuven Cohen,Alon Eden,Omri Porat,Inbal Talgam-Cohen*

Main category: cs.GT

TL;DR: 本文提出了Knapsack Contracts问题，将经典的随机背包问题与合约设计相结合，研究在代理策略性努力控制随机处理时间情况下的计算复杂性和近似算法。


<details>
  <summary>Details</summary>
Motivation: 结合随机优化和合约设计中的固有随机性，研究当优化问题中的随机性由策略性努力控制时产生的计算挑战。

Method: 将Knapsack Contracts问题视为带成本和多重选择的随机背包问题，引入投资回报率倒数(IOR)作为关键参数，开发了不依赖自适应性的O(α)近似算法。

Result: 证明了IOR参数精确刻画了随机背包问题近似保证到其策略对应问题的扩展程度，建立了匹配的Ω(α)下界，包括自适应间隙和在不完全分布知识下的性能限制。

Conclusion: IOR是理解Knapsack Contracts问题复杂性和近似性的基本参数，限制IOR对于实现非平凡近似保证既是必要的也是充分的。

Abstract: We formulate the Knapsack Contracts problem -- a strategic version of the
classic Stochastic Knapsack problem, which builds upon the inherent randomness
shared by stochastic optimization and contract design. In this problem, the
principal incentivizes agents to perform jobs with stochastic processing times,
the realization of which depends on the agents' efforts.
  Algorithmically, we show that Knapsack Contracts can be viewed as Stochastic
Knapsack with costs and multi-choice, features that introduce significant new
challenges. We identify a crucial and economically meaningful parameter -- the
Return on Investment (ROI) value. We show that the Inverse of ROI (or IOR for
short) precisely characterizes the extent to which the approximation guarantees
for Stochastic Knapsack extend to its strategic counterpart.
  For IOR of $\alpha$, we develop an algorithm that finds an
$O(\alpha)$-approximation policy that does not rely on adaptivity. We establish
matching $\Omega(\alpha)$ lower bounds, both on the adaptivity gap, and on what
can be achieved without full distributional knowledge of the processing times.
Taken together, our results show that IOR is fundamental to understanding the
complexity and approximability of Knapsack Contracts, and bounding it is both
necessary and sufficient for achieving non-trivial approximation guarantees.
Our results highlight the computational challenges arising when stochasticity
in optimization problems is controlled by strategic effort.

</details>


### [3] [The Keychain Problem: On Minimizing the Opportunity Cost of Uncertainty](https://arxiv.org/abs/2509.06187)
*Ramiro N. Deo-Campo Vuong,Robert Kleinberg,Aditya Prasad,Eric Xiao,Haifeng Xu*

Main category: cs.GT

TL;DR: 钥匙问题是一类序列决策问题，模拟销匠选择钥匙中的钥匙开锁的过程，目标是最大化预期开锁次数。研究了固定序列、概率场景和自选序列三种情况，提出了精确算法和近似算法，并展示了该技术在其他问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究钥匙问题的动机是为了解决实际中常见的序列选择问题，比如在每个阶段只有部分选择可用的情况下如何最大化预期收益。通过销匠选择钥匙开锁的模型，探讨不同序列策略对性能的影响。

Method: 采用贝叶斯预期模型，考虑三种不同的序列测试方式：固定序列、概率场景分布和自选序列。对于最简单的固定序列情况提出了精确算法，对其他情况则提出了近似算法和进行了难度分析。还展示了从组合招标到序列决策问题策略设计的新题连接。

Result: 在概率场景设置中，研究者提出了基于组合招标与序列决策策略设计新联系的近似算法。该方法还成功应用于在线二分匹配问题及其扩展问题，产生了哲学家不等式，证明了该技术的普遍适用性。

Conclusion: 钥匙问题提供了一个有趣的序列决策模型，在不同的序列策略下都有相应的算法解决方案。研究还发现了组合招标与序列决策问题之间的新联系，这种联系不仅在钥匙问题中有效，还能扩展到其他相关问题中，为序列决策策略设计提供了新的研究方向。

Abstract: In this paper, we introduce a family of sequential decision-making problems,
collectively called the Keychain Problem, that involve exploring a set of
actions to maximize expected payoff when only a subset of actions are available
in each stage. In an instance of the Keychain Problem, a locksmith faces a
sequence of choices, each of which involves selecting one key from a specified
subset (a keychain) to attempt to open a lock. Given a Bayesian prior on the
effectiveness of keys, the locksmith's goal is to maximize the expected number
of rounds in which the lock is opened -- or equivalently, minimize the
opportunity cost which is the expected number of rounds in which the chain has
a correct key but our selected key is incorrect. We investigate Keychain
Problems under three assumptions on the order in which keychains are tested by
the locksmith: a fixed, known order; a random order sampled from a known
distribution on a set of ``scenarios''; or an order selected by the locksmith
themself. We present an exact algorithm for the simplest of these settings, and
we present approximation algorithms and hardness results for the others. In the
Probabilistic Scenarios setting, our approximation algorithm is based on a
novel connection between combinatorial auctions and policy design for
sequential decision-making problems. To illustrate the generality of this
technique, we apply the same ideas to obtain Philosopher Inequalities for
Online Bipartite Matching and some of its extensions.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [4] [Multiport Network Modeling and Optimization for Reconfigurable Pinching-Antenna Systems](https://arxiv.org/abs/2509.05612)
*Zhaolin Wang,Jiaqi Xu,Chongjun Ouyang,Xidong Mu,Yuanwei Liu*

Main category: cs.IT

TL;DR: 重构针扣天线系统(PASS)通过多端口网络理论建模，实现改进的幅相可控辐射特性，为天线设计提供新方案


<details>
  <summary>Details</summary>
Motivation: 传统针扣天线(PA)在幅相控制能力上有限，需要一种新的重构系统来实现更完整的辐射特性控制

Method: 使用多端口网络理论建立物理一致模型，提出方向耦合器(DC)基础的PA模型，并研究了球面优化算法

Result: 单用户场景下：(i)优化PA位置时，DC基PA接近理想性能(ii)固定PA位置时，DC基PA存在可观性能损失

Conclusion: PASS系统为天线设计提供了新的重构可能性，在不同配置情况下展现出不同的性能特征，为实际应用提供了重要参考

Abstract: A reconfigurable pinching-antenna system (PASS) is presented, endowing
pinching antennas (PAs) with both amplitude- and phase-controllable radiation
beyond conventional implementations. To characterize this feature, a general
and physically consistent model is established for PASS via multiport network
theory. Within this model, the fundamental constraint of ideal
reconfigurability of PAs is identified, allowing the full control of signal
amplitudes and phases. A practical directional-coupler (DC)-based PA model is
then proposed, enabling both amplitude-only control and amplitude-constrained
phase control. Beamforming optimization is investigated for both ideal and
practical cases: an optimal solution is obtained for ideal PAs, whereas a
high-quality iterative algorithm is developed for DC-based PAs. Numerical
results suggest that in single-user scenarios: (i) with optimized PA positions,
performance gains arise primarily from amplitude reconfigurability and DC-based
PAs approach ideal performance, and (ii) with fixed PA positions, both
amplitude and phase reconfigurability are critical and DC-based PAs incur
non-negligible loss.

</details>


### [5] [Study of Iterative Detection, Decoding and Channel Estimation for RIS-Aided MIMO Networks](https://arxiv.org/abs/2509.05875)
*Roberto C. G. Porto,Rodrigo C. de Lamare*

Main category: cs.IT

TL;DR: 通过迭代检测、解码和通道估计方案，结合LDPC编码和RIS技术，在减少导频符号的同时提高了通道估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决多天线系统中通道估计的挑战，特别是在使用多个反射智能表面(RIS)时，如何在减少导频符号的情况下提高估计准确性。

Method: 开发了一种新题的通道估计技术，利用LDPC编码和迭代处理，通过编码导频来改善迭代过程，不仅使用导频比特还利用编码数据包中的奇偶检验比特来精炼通道估计。

Result: 在次-6GHz频段的模拟中，考虑了LOS和NLOS条件，数值结果显示该方案在性能上远超竞争方案。

Conclusion: 该研究提出的迭代检测、解码和通道估计方案，结合LDPC编码和RIS技术，为多天线系统提供了一种高效的通道估计解决方案，在减少导频开销的同时显著提升了估计性能。

Abstract: This work proposes an iterative detection, decoding and channel estimation
scheme for multiple-antenna systems assisted by multiple reflective intelligent
surfaces (RIS). A novel channel estimation technique that exploits low-density
parity-check (LDPC) codes and iterative processing is developed to enhance
estimation accuracy while reducing the number of required pilot symbols. The
key idea is to exploit encoded pilots to improve the iterative process,
enabling the use of not only pilot bits but also parity bits from the coded
packet to refine channel estimation. Simulations analyze a sub-6 GHz scenario
where the channel propagation is not sparse and multiple RIS are deployed,
considering both LOS and NLOS conditions. Numerical results show significant
performance gains for the proposed scheme and estimator over competing
approaches.

</details>


### [6] [Beyond Diagonal IRS Aided OFDM: Rate Maximization under Frequency-Dependent Reflection](https://arxiv.org/abs/2509.06378)
*Ye Yuan,Shuowen Zhang*

Main category: cs.IT

TL;DR: 本文研究宽带OFDM系统中使用超对角智能反射面(BD-IRS)的技术，解决了不同子载波间反射矩阵耦合的挑战，提出了联合优化可调电容矩阵和功率分配的有效算法。


<details>
  <summary>Details</summary>
Motivation: 由于BD-IRS的反射矩阵依赖于电路参数和操作频率，导致不同子载波间的反射矩阵存在耦合，这给BD-IRS设计带来了新的挑战，需要开发有效的优化方法。

Method: 首先建模BD-IRS反射矩阵与可调电容矩阵的关系，然后提出交替优化和逐次凸逼近算法来联合优化可调电容矩阵和OFDM子载波功率分配。

Result: 数值结果表明，所提出的设计方案在实现速率方面优于基准设计，证明了算法的有效性。

Conclusion: 该研究成功解决了宽带OFDM系统中BD-IRS的耦合问题，提出的优化算法能够找到高质量可行解，为实际应用提供了有效解决方案。

Abstract: This paper studies a broadband orthogonal frequency division multiplexing
(OFDM) system aided by a beyond diagonal intelligent reflecting surface
(BD-IRS), where inter-connections exist among different elements such that the
reflection matrix can exhibit a beyond diagonal structure. Under practical
circuit structures, the reflection matrix of the BD-IRS is generally dependent
on the circuit parameters (e.g., capacitance matrix for all tunable capacitors)
as well as the operating frequency, which leads to couplings among the BD-IRS
reflection matrices over different sub-carriers and consequently new challenges
in the BD-IRS design. Motivated by this, we first model the relationship
between the BD-IRS reflection matrices over different sub-carriers and the
tunable capacitance matrix, and then formulate the joint optimization problem
of the tunable capacitance matrix and power allocation over OFDM sub-carriers
to maximize the achievable rate of the OFDM system. Despite the non-convexity
of the problem, we propose an effective algorithm for finding a high-quality
feasible solution via leveraging alternating optimization and successive convex
approximation. Numerical results show the superiority of our proposed design
over benchmark designs.

</details>


### [7] [Trace Repair Never Loses to Classical Repair: Exact and Explicit Helper Nodes Selection](https://arxiv.org/abs/2509.06492)
*Wilton Kim,Stanislav Kruglik,Han Mao Kiah*

Main category: cs.IT

TL;DR: 本文研究了Reed-Solomon码在有限域扩展上的追踪修复问题，通过循环陪集方法精确计算了关键子空间维度，并给出了达到最优修复带宽的显式辅助节点选择方案。


<details>
  <summary>Details</summary>
Motivation: 基于Guruswami-Wootters的追踪框架和Liu-Wan-Xing的最新工作，需要进一步优化Reed-Solomon码的修复带宽，特别是确定关键子空间W_k的确切维度和构造显式修复方案。

Method: 使用循环陪集理论精确计算子空间W_k的维度，并构造显式的辅助节点集合来实现最优修复带宽(n-d-1)log|B| bits，其中d=dim(W_k)。

Result: 确定了W_k维度的精确表达式，证明了(n-d-1)≤kt，表明追踪修复方法在带宽上不会劣于经典修复方法。

Conclusion: 该工作为Reed-Solomon码提供了理论上的最优修复带宽界限和实用的显式修复方案，证明了追踪修复方法的优越性。

Abstract: We study the repair of Reed--Solomon codes over $\mathbb{F}=\mathbb{B}^t$
using traces over $\mathbb{B}$. Building on the trace framework of
Guruswami--Wootters (2017), recent work of Liu--Wan--Xing (2024) reduced repair
bandwidth by studying a related subspace $\mathcal{W}_k$. In this work, we
determine the dimension of $\mathcal{W}_k$ exactly using cyclotomic cosets and
provide an explicit set of helper nodes that attains bandwidth $(n-d-1)\log
|\mathbb{B}|$ bits with $d=\text{dim}(\mathcal{W}_k)$. Moreover, we show that
$(n-d-1)\le kt$, and so, trace repair never loses to the classical repair.

</details>


### [8] [On catastrophicity of convolutional codes and their encoders over $\Z_{p^r}$](https://arxiv.org/abs/2509.06670)
*Mohammed El Oued*

Main category: cs.IT

TL;DR: 本文证明了卷积码在Z_{p^r}上存在最小p-编码器，确认了相关猜想，并提出了新的多项式不变量来刻画自由码的非灾难性条件。


<details>
  <summary>Details</summary>
Motivation: 解决关于Z_{p^r}上卷积码存在最小p-编码器的猜想，该猜想意味着当输入序列系数限制在{0,...,p-1}时，所有此类卷积码都是非灾难性的。

Method: 引入新的多项式不变量来刻画自由码，建立自由码在Z_{p^r}上非灾难性的充要条件，并构造性地给出获得最小p-编码器的方法。

Result: 确认了猜想成立，为任意Z_{p^r}上的卷积码提供了构造最小p-编码器的方法。

Conclusion: 成功解决了关于最小p-编码器存在的猜想，为Z_{p^r}上卷积码的理论研究提供了重要工具和结论。

Abstract: This paper investigates the existence of minimal $p$-encoders for
convolutional codes over $\mathbb{Z}_{p^r}$, where $p$ is a prime. This
addresses a conjecture from \cite{k}, which posits that every such code admits
a minimal $p$-encoder, implying that all convolutional codes over
$\mathbb{Z}_{p^r}$ are noncatastrophic when input sequences are restricted to
coefficients in $\{0, \dots, p-1\}$. Our contributions include the introduction
of a new polynomial invariant that characterizes free codes, which enables us
to establish a necessary and sufficient condition for a free code over
$\mathbb{Z}_{p^r}$ to be noncatastrophic in the usual sense (where input
coefficients are from $\mathbb{Z}_{p^r}$). Based on these findings, we affirm
the conjecture by providing a constructive method for obtaining a minimal
$p$-encoder for any convolutional code over $\mathbb{Z}_{p^r}$.

</details>


### [9] [Codes Correcting Transpositions of Consecutive Symbols](https://arxiv.org/abs/2509.06692)
*Mladen Kovačević,Keshav Goyal,Han Mao Kiah*

Main category: cs.IT

TL;DR: 本文研究q元字符串中连续符号转置错误的纠错问题，提出了多种纠错码构造方法并分析了其性能界限


<details>
  <summary>Details</summary>
Motivation: 研究q元字符串中连续符号转置（交换）错误的纠错问题，这类错误在数据传输和存储中常见，需要设计高效的纠错码

Method: 提出了一族纠正任意位置单个转置错误的码构造方法，给出了二进制字母表上的改进构造，推导了纠正常数个转置错误的码容量界限，分析了纠正线性数量转置错误的渐近率下界，并提出了纠正所有可能转置模式的码构造

Result: 证明了所提码构造具有渐近最优冗余度，获得了常数个转置纠错码的容量界限，推导了纠正线性数量转置错误的最优码渐近率下界，给出了q元转置信道的零错误容量下界

Conclusion: 本文系统研究了转置错误的纠错问题，提出了多种有效的码构造方法并建立了相应的理论界限，为转置信道编码提供了理论基础和实用方案

Abstract: The problem of correcting transpositions (or swaps) of consecutive symbols in
$ q $-ary strings is studied. A family of codes correcting a transposition at
an arbitrary location is described and proved to have asymptotically optimal
redundancy. Additionally, an improved construction is given over a binary
alphabet. Bounds on the cardinality of codes correcting $ t = \textrm{const} $
transpositions are obtained. A lower bound on the achievable asymptotic rate of
optimal codes correcting $ t = \tau n $ transpositions is derived. Finally, a
construction of codes correcting all possible patterns of transpositions is
presented, and the corresponding lower bound on the zero-error capacity of the
$ q $-ary transposition channel is stated.

</details>


### [10] [Rate-Optimal Streaming Codes over Three-Node Relay Networks with Burst Erasures](https://arxiv.org/abs/2509.06912)
*Zhipeng Li,Wenjie Ma,Zhifang Zhang*

Main category: cs.IT

TL;DR: 本文研究三节点中继网络中的流式码，在突发包擦除和延迟约束T下，构建了速率最优的流式码，扩展了现有最优码的适用范围。


<details>
  <summary>Details</summary>
Motivation: 研究三节点中继网络中在突发包擦除和延迟约束下的流式码设计，解决Singhvi等人构造中需要max{b1,b2}整除(T-b1-b2)的限制条件。

Method: 提出新的流式码构造方法，在条件T ≥ b1+b2 + b1b2/|b1-b2|下实现最优码率，突破了原有构造的限制条件。

Result: 成功构造出在三节点中继网络中速率最优的流式码，扩展了最优码的适用范围，丰富了速率最优流式码的家族。

Conclusion: 本文提出的构造方法在更宽松的条件下实现了三节点中继网络中流式码的最优速率，为相关网络编码设计提供了新的解决方案。

Abstract: This paper investigates streaming codes over three-node relay networks under
burst packet erasures with a delay constraint $T$. In any sliding window of
$T+1$ consecutive packets, the source-to-relay and relay-to-destination
channels may introduce burst erasures of lengths at most $b_1$ and $b_2$,
respectively. Singhvi et al. proposed a construction achieving the optimal code
rate when $\max\{b_1,b_2\}\mid (T-b_1-b_2)$. We construct streaming codes with
the optimal rate under the condition
  $T\geq b_1+b_2+\frac{b_1b_2}{|b_1-b_2|}$, thereby enriching the family of
rate-optimal streaming codes for three-node relay networks.

</details>


### [11] [Row-Column Twisted Reed-Solomon codes](https://arxiv.org/abs/2509.06919)
*Anuj Kumar Bhagat,Harshdeep Singh,Ritumoni Sarma*

Main category: cs.IT

TL;DR: 提出一种新的行列扭曲Reed-Solomon码(RCTRS)，证明其MDS性质和与其他码的不等价性


<details>
  <summary>Details</summary>
Motivation: 受到扭曲Reed-Solomon码和列扭曲Reed-Solomon码的启发，构造一种新的非RS类型的MDS码家族

Method: 通过明确提供MDS码的条件和存在性，利用Schur平方维度来证明与Reed-Solomon码的不等价性

Result: 成功构造了一系列新的非RS MDS码，并证明其与现有列扭曲Reed-Solomon码也不等价

Conclusion: RCTRS码作为一种新的非RS MDS码构造，拓展了MDS码的家族，具有新题性和应用价值

Abstract: In this article, we present a new class of codes known as row-column twisted
Reed-Solomon codes (abbreviated as RCTRS), motivated by the works of
\cite{beelen2017twisted} and \cite{liu2025column}. We explicitly provide
conditions for such codes to be MDS and also ensure their existence. By
determining the dimensions of their Schur squares, we prove that these MDS
codes are not equivalent to Reed-Solomon codes, thus presenting a new family of
non-RS MDS codes. Additionally, we prove that these MDS codes are also not
equivalent to column twisted Reed-Solomon codes described in
\cite{liu2025column}, showing the novelty of our construction.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [12] [A Unified Framework for Cultural Heritage Data Historicity and Migration: The ARGUS Approach](https://arxiv.org/abs/2509.06044)
*Lingxiao Kong,Apostolos Sarris,Miltiadis Polidorou,Victor Klingenberg,Vasilis Sevetlidis,Vasilis Arampatzakis,George Pavlidis,Cong Yang,Zeyd Boukhers*

Main category: cs.DB

TL;DR: 本文提出了一个用于文化遗产保护的综合性数据历史性和迁移框架，通过标准化、丰富化、集成和可视化等处理流程，将异构数据转换为符合FAIR原则的标准化格式，并在五个欧洲试点站点成功应用。


<details>
  <summary>Details</summary>
Motivation: 文化遗产保护面临多源、多尺度异构数据管理的挑战，需要有效的监测和保护方法。

Method: 采用系统化的数据处理流水线，包括标准化、数据丰富（使用插补技术）、数据库集成、可视化、数据摄取和发布策略，并利用LLM驱动的自然语言处理提升查询能力。

Result: 在五个欧洲试点站点应用表明，该框架提高了数据可访问性、增强了分析能力，并改善了保护决策效果。

Conclusion: 该框架成功解决了文化遗产数据处理的复杂性，展示了在不同文化遗产背景下的适应性和有效性。

Abstract: Cultural heritage preservation faces significant challenges in managing
diverse, multi-source, and multi-scale data for effective monitoring and
conservation. This paper documents a comprehensive data historicity and
migration framework implemented within the ARGUS project, which addresses the
complexities of processing heterogeneous cultural heritage data. We describe a
systematic data processing pipeline encompassing standardization, enrichment,
integration, visualization, ingestion, and publication strategies. The
framework transforms raw, disparate datasets into standardized formats
compliant with FAIR principles. It enhances sparse datasets through established
imputation techniques, ensures interoperability through database integration,
and improves querying capabilities through LLM-powered natural language
processing. This approach has been applied across five European pilot sites
with varying preservation challenges, demonstrating its adaptability to diverse
cultural heritage contexts. The implementation results show improved data
accessibility, enhanced analytical capabilities, and more effective
decision-making for conservation efforts.

</details>


### [13] [Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research](https://arxiv.org/abs/2509.06093)
*Yuze Liu,Zhaoyuan Zhang,Xiangsheng Zeng,Yihe Zhang,Leping Yu,Lejia Wang,Xi Yu*

Main category: cs.DB

TL;DR: 这篇论文提出了一种语言本地数据库框架，用于摘取化学材料研究论文中的轻结构化信息，支持高保真度的RAG和工具增强精灵，为LLM驱动的材料发现提供语言丰富的基础。


<details>
  <summary>Details</summary>
Motivation: 化学材料研究传统上依赖知识叙述而非表格，限制了传统数据库和机器学习的利用。需要一种能够摘取论文中语言描述的轻结构化信息的方法。

Method: 构建了一种语言本地数据库，摘取研究论文中的轻结构化信息，包括准备、形容、理论计算和机理推理等领域，并通过复合检索（语义、关键词和值过滤器）进行查询。

Result: 该系统能够将文献合成为准确、可验证且专业风格的指导，支持高保真度的RAG和工具增强精灵，能够交替进行检索与推理并提供可执行的SOP。

Conclusion: 该框架为LLM驱动的材料发现提供了语言丰富的基础，能够充分利用化学材料研究中的知识叙述信息，推动材料科学的进步。

Abstract: Chemical and materials research has traditionally relied heavily on knowledge
narrative, with progress often driven by language-based descriptions of
principles, mechanisms, and experimental experiences, rather than tables,
limiting what conventional databases and ML can exploit. We present a
language-native database for boron nitride nanosheet (BNNS) polymer thermally
conductive composites that captures lightly structured information from papers
across preparation, characterization, theory-computation, and mechanistic
reasoning, with evidence-linked snippets. Records are organized in a
heterogeneous database and queried via composite retrieval with semantics, key
words and value filters. The system can synthesizes literature into accurate,
verifiable, and expert style guidance. This substrate enables high fidelity
efficient Retrieval Augmented Generation (RAG) and tool augmented agents to
interleave retrieval with reasoning and deliver actionable SOP. The framework
supplies the language rich foundation required for LLM-driven materials
discovery.

</details>


### [14] [MCTuner: Spatial Decomposition-Enhanced Database Tuning via LLM-Guided Exploration](https://arxiv.org/abs/2509.06298)
*Zihan Yan,Rui Xi,Mengshu Hou*

Main category: cs.DB

TL;DR: MCTuner是一个自适应数据库参数调优框架，通过混合专家机制识别关键参数，并采用空间分解算法和贝叶斯优化来高效搜索最优配置，在多个基准测试中取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 数据库参数调优面临参数数量多、配置空间大的挑战，现有学习方法要么忽略领域知识，要么难以探索高维参数空间，导致调优成本高且性能不佳

Method: 提出MCTuner框架：1）使用混合专家机制（MoE）和专门的大语言模型识别性能关键参数；2）引入首个空间分解算法，递归将空间划分为层次子空间；3）在子空间上执行贝叶斯优化搜索近最优配置

Result: 在OLAP、OLTP和HTAP等不同基准测试中，MCTuner实现了最高19.2%的性能提升，每次迭代的配置发现速度比最先进方法快1.4倍

Conclusion: MCTuner通过有效减少无效配置空间的探索，显著提高了数据库参数调优的效率和效果，为解决高维参数调优问题提供了有效方案

Abstract: Database knob tuning is essential for optimizing the performance of modern
database management systems, which often expose hundreds of knobs with
continuous or categorical values. However, the large number of knobs and the
vast configuration space make it difficult to identify optimal settings
efficiently. Although learning-based tuning has shown promise, existing
approaches either ignore domain knowledge by relying solely on benchmark
feedback or struggle to explore the high-dimensional knob space, resulting in
high tuning costs and suboptimal performance. To address these challenges, we
propose MCTuner, an adaptive knob tuning framework that minimizes exploration
in ineffective regions of the configuration space. MCTuner employs a
Mixture-of-Experts (MoE) mechanism with specialized LLMs to identify
performance-critical knobs. In further, MCTuner introduces the first spatial
decomposition algorithm that recursively partitions the space into hierarchical
subspaces, on which Bayesian Optimization is performed to efficiently search
for near-optimal configurations. Evaluated on different benchmarks (OLAP, OLTP,
and HTAP), MCTuner achieves up to 19.2% performance gains and 1.4x faster
configuration discovery per iteration compared to state-of-the-art methods.

</details>


### [15] [Relational Algebras for Subset Selection and Optimisation](https://arxiv.org/abs/2509.06439)
*David Robert Pratten,Luke Mathieson,Fahimeh Ramezani*

Main category: cs.DB

TL;DR: 提出了第一个统一的关系代数基础，通过关系幂运算扩展关系代数，引入解集概念，能够表达子集选择和优化查询，并提供到标准关系代数的结构保持转换语义。


<details>
  <summary>Details</summary>
Motivation: 数据库领域缺乏统一的子集选择和优化查询的关系查询语言，现有研究采用不兼容的临时SQL扩展，限制了用户表达和查询优化器的推理能力。

Method: 扩展关系代数以支持域关系，引入解集概念作为关系集合上的高阶关系代数，提供从解集到标准关系代数的结构保持转换语义。

Result: 实现了与最强大现有方法相同的表达能力，同时提供了先前工作缺乏的理论清晰度和组合性质。

Conclusion: 该框架为子集选择和优化查询提供了统一的理论基础，通过多态SQL实现了数据管理、子集选择和优化查询在单一范式中的无缝表达。

Abstract: The database community lacks a unified relational query language for subset
selection and optimisation queries, limiting both user expression and query
optimiser reasoning about such problems. Decades of research (latterly under
the rubric of prescriptive analytics) have produced powerful evaluation
algorithms with incompatible, ad-hoc SQL extensions that specify and filter
through distinct mechanisms. We present the first unified algebraic foundation
for these queries, introducing relational exponentiation to complete the
fundamental algebraic operations alongside union (addition) and cross product
(multiplication). First, we extend relational algebra to complete domain
relations-relations defined by characteristic functions rather than explicit
extensions-achieving the expressiveness of NP-complete/hard problems, while
simultaneously providing query safety for finite inputs. Second, we introduce
solution sets, a higher-order relational algebra over sets of relations that
naturally expresses search spaces as functions f: Base to Decision, yielding
|Decision|^|Base| candidate relations. Third, we provide structure-preserving
translation semantics from solution sets to standard relational algebra,
enabling mechanical translation to existing evaluation algorithms. This
framework achieves the expressiveness of the most powerful prior approaches
while providing the theoretical clarity and compositional properties absent in
previous work. We demonstrate the capabilities these algebras open up through a
polymorphic SQL where standard clauses seamlessly express data management,
subset selection, and optimisation queries within a single paradigm.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [16] [Subsequence Covers of Words](https://arxiv.org/abs/2509.05827)
*Panagiotis Charalampopoulos,Solon P. Pissis,Jakub Radoszewski,Wojciech Rytter,Tomasz Waleń,Wiktor Zuba*

Main category: cs.DS

TL;DR: 本文介绍了子序列覆盖(s-covers)的新概念，提出了线性时间算法来测试候选词是否为s-cover，并给出了寻找最短s-cover的算法。对于常数大小字母表，算法也是线性时间。


<details>
  <summary>Details</summary>
Motivation: 研究子序列覆盖这一新型覆盖概念，相比传统覆盖计算上更困难，但比相关的洗牌幂更容易处理，旨在开发高效算法并分析其性质。

Method: 提出了线性时间算法来测试候选词C是否为词S的s-cover（适用于多项式有界整数字母表），以及寻找最短s-cover的算法。对于常数大小字母表，算法保持线性时间复杂度。

Result: 算法在多项式有界整数字母表下具有线性时间复杂度，对于常数大小字母表同样保持线性性能。同时给出了最长s-本原词长度的显式上下界，均为字母表大小的指数级。

Conclusion: s-covers在计算复杂度上介于传统覆盖和洗牌幂之间，提出的高效算法解决了相关问题，指数级的上下界表明s-本原词的长度随字母表大小快速增长。

Abstract: We introduce subsequence covers (s-covers, in short), a new type of covers of
a word. A word $C$ is an s-cover of a word $S$ if the occurrences of $C$ in $S$
as subsequences cover all the positions in $S$.
  The s-covers seem to be computationally much harder than standard covers of
words (cf. Apostolico et al., Inf. Process. Lett. 1991), but, on the other
hand, much easier than the related shuffle powers (Warmuth and Haussler, J.
Comput. Syst. Sci. 1984).
  We give a linear-time algorithm for testing if a candidate word $C$ is an
s-cover of a word $S$ over a polynomially-bounded integer alphabet. We also
give an algorithm for finding a shortest s-cover of a word $S$, which in the
case of a constant-sized alphabet, also runs in linear time.
  The words without proper s-cover are called s-primitive. We complement our
algorithmic results with explicit lower and an upper bound on the length of a
longest s-primitive word. Both bounds are exponential in the size of the
alphabet. The upper bound presented here improves the bound given in the
conference version of this paper [SPIRE 2022].

</details>


### [17] [Generalized Graph Packing Problems Parameterized by Treewidth](https://arxiv.org/abs/2509.06091)
*Barış Can Esmer,Dániel Marx*

Main category: cs.DS

TL;DR: 该论文研究了在有限树宽图上解决H-Packing和H-Partition问题及其推广形式的时间复杂度上下界，特别关注了当H是团和非团时的不同算法复杂度表现。


<details>
  <summary>Details</summary>
Motivation: 研究在有限树宽图上解决顶点不相交子图打包和划分问题的时间复杂度，特别是探索当允许顶点重复使用和H不是团时的算法复杂度边界。

Method: 通过树分解技术分析算法复杂度，使用强指数时间假说(SETH)和指数时间假说(ETH)来证明时间复杂度的下界，对H是团和非团的情况分别进行讨论。

Result: 对于团K_d，最优运行时间为(c+1)^{tw}·n^{O(1)}；对于非团H，如果其2-连通分量不全是团，则不存在2^{o(tw log tw)}·n^{O(1)}算法。

Conclusion: H-Packing和H-Partition问题的时间复杂度严重依赖于H的结构特性，团结构允许单指数时间算法，而非团结构可能需要超指数时间，这为图算法设计提供了重要的理论界限。

Abstract: $H$-Packing is the problem of finding a maximum number of vertex-disjoint
copies of $H$ in a given graph $G$. $H$-Partition is the special case of
finding a set of vertex-disjoint copies that cover each vertex of $G$ exactly
once. Our goal is to study these problems and some generalizations on
bounded-treewidth graphs. The case of $H$ being a triangle is well understood:
given a tree decomposition of $G$ having treewidth $tw$, the $K_3$-Packing
problem can be solved in time $2^{tw} \cdot n^{O(1)}$, while Lokshtanov et
al.~[{\it ACM Transactions on Algorithms} 2018] showed, under the Strong
Exponential-Time Hypothesis (SETH), that there is no $(2-\epsilon)^{tw}\cdot
n^{O(1)}$ algorithm for any $\epsilon>0$ even for $K_3$-Partition. Similar
results can be obtained for any other clique $K_d$ for $d\ge 3$. We provide
generalizations in two directions:
  - We consider a generalization of the problem where every vertex can be used
at most $c$ times for some $c\ge 1$. When $H$ is any clique $K_d$ with $d\ge
3$, then we give upper and lower bounds showing that the optimal running time
increases to $(c+1)^{tw}\cdot n^{O(1)}$. We consider two variants depending on
whether a copy of $H$ can be used multiple times in the packing.
  - If $H$ is not a clique, then the dependence of the running time on
treewidth may not be even single exponential. Specifically, we show that if $H$
is any fixed graph where not every 2-connected component is a clique, then
there is no $2^{o({tw}\log {tw})}\cdot n^{O(1)}$ algorithm for
\textsc{$H$-Partition}, assuming the Exponential-Time Hypothesis (ETH).

</details>


### [18] [Parameterized Algorithms for Computing Pareto Sets](https://arxiv.org/abs/2509.06124)
*Joshua Könen,Heiko Röglin,Tarek Stuck*

Main category: cs.DS

TL;DR: 该论文研究了如何将树分解上的动态规划技术应用于多目标优化问题的帕累托集计算，提出了针对多目标s-t割问题、最小生成树问题和TSP问题的算法，并在实际多边形聚合问题上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 研究树分解上的动态规划技术是否能够应用于计算多目标优化问题的帕累托集，特别是在处理具有树状结构的组合优化问题时提供高效算法。

Method: 使用树分解技术结合动态规划，设计了计算多目标s-t割问题帕累托集的算法，并扩展到多目标最小生成树和TSP问题。开发了专门的数据结构来高效存储和修改帕累托最优解集。

Result: 算法运行时间为O(f(w)·poly(n,pmax))，其中w为树宽，n为输入规模，pmax为子问题帕累托集大小的上界。实验表明能够处理树宽高达22的实例，并在实现过程中通过改进策略和启发式方法显著降低了运行时间和内存使用。

Conclusion: 树分解上的动态规划技术可以成功应用于多目标优化问题的帕累托集计算，为处理具有树状结构的复杂多目标优化问题提供了有效的解决方案。

Abstract: Dynamic programming over tree decompositions is a common technique in
parameterized algorithms. In this paper, we study whether this technique can
also be applied to compute Pareto sets of multiobjective optimization problems.
We first derive an algorithm to compute the Pareto set for the multicriteria
s-t cut problem and show how this result can be applied to a polygon
aggregation problem arising in cartography that has recently been introduced by
Rottmann et al. (GIScience 2021). We also show how to apply these techniques to
also compute the Pareto set of the multiobjective minimum spanning tree problem
and for the multiobjective TSP. The running time of our algorithms is
$O(f(w)\cdot\mathrm{poly}(n,p_{\text{max}}))$, where $f$ is some function in
the treewidth $w$, $n$ is the input size, and $p_{\text{max}}$ is an upper
bound on the size of the Pareto sets of the subproblems that occur in the
dynamic program. Finally, we present an experimental evaluation of computing
Pareto sets on real-world instances of polygon aggregation problems. For this
matter we devised a task-specific data structure that allows for efficient
storage and modification of large sets of Pareto-optimal solutions. Throughout
the implementation process, we incorporated several improved strategies and
heuristics that significantly reduced both runtime and memory usage, enabling
us to solve instances with treewidth of up to 22 within reasonable amount of
time. Moreover, we conducted a preprocessing study to compare different tree
decompositions in terms of their estimated overall runtime.

</details>


### [19] [Efficient Catalytic Graph Algorithms](https://arxiv.org/abs/2509.06209)
*James Cook,Edward Pyne*

Main category: cs.DS

TL;DR: 提出了两种高效的催化对数空间算法：随机化算法解决s→t连通性问题，时间复杂度O~(nm)；确定性算法模拟随机游走，时间复杂度O~(mT²/ε)。这些算法首次在CL中使用了随机化，并为催化算法提供了明确的时间复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 先前针对图连通性和随机游走模拟的催化算法虽然存在，但缺乏明确的时间复杂度界限，仅知道是多项式时间。本文旨在开发具有明确时间复杂度分析的高效催化对数空间算法。

Method: 1. 对于s→t连通性问题：使用随机化催化算法，每个顶点一个寄存器，沿边反复"推送"值；2. 对于随机游走模拟：使用确定性催化算法，每个顶点一个寄存器，每次访问时递增以确保重复访问选择不同的出边。

Result: 1. 随机化催化算法解决s→t连通性问题的时间复杂度为O~(nm)；2. 确定性催化算法模拟随机游走的时间复杂度为O~(mT²/ε)，能以ε加性误差估计T步随机游走到达特定顶点的概率。

Conclusion: 本文首次在催化对数空间(CL)中使用了随机化技术，为图连通性和随机游走模拟问题提供了具有明确时间复杂度界限的高效催化算法，填补了先前工作的空白。

Abstract: We give fast, simple, and implementable catalytic logspace algorithms for two
fundamental graph problems.
  First, a randomized catalytic algorithm for $s\to t$ connectivity running in
$\widetilde{O}(nm)$ time, and a deterministic catalytic algorithm for the same
running in $\widetilde{O}(n^3 m)$ time. The former algorithm is the first
algorithmic use of randomization in $\mathsf{CL}$. The algorithm uses one
register per vertex and repeatedly ``pushes'' values along the edges in the
graph.
  Second, a deterministic catalytic algorithm for simulating random walks which
in $\widetilde{O}( m T^2 / \varepsilon )$ time estimates the probability a
$T$-step random walk ends at a given vertex within $\varepsilon$ additive
error. The algorithm uses one register for each vertex and increments it at
each visit to ensure repeated visits follow different outgoing edges.
  Prior catalytic algorithms for both problems did not have explicit runtime
bounds beyond being polynomial in $n$.

</details>


### [20] [Zero-Freeness is All You Need: A Weitz-Type FPTAS for the Entire Lee-Yang Zero-Free Region](https://arxiv.org/abs/2509.06623)
*Shuai Shao,Ke Shi*

Main category: cs.DS

TL;DR: \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56\u5f3a\u7a7a\u95f4\u6df7\u5408\u6027\u7684Weitz\u578bFPTAS\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728Lee-Yang\u96f6\u70b9\u514d\u533a\u57df\u5185\u8fd1\u4f3c\u8ba1\u7b97\u78c1\u6027Ising\u6a21\u578b\u7684\u5206\u914d\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u4f9d\u8d56\u7cfb\u6570\u8bc1\u660e\u4e86\u968f\u673a\u805a\u96c6\u6a21\u578b\u7684\u65b0\u5f3a\u7a7a\u95f4\u6df7\u5408\u6027\u7ed3\u679c\u3002


<details>
  <summary>Details</summary>
Motivation: \u76ee\u524d\u7684Weitz\u7b97\u6cd5\u4f9d\u8d56\u5f3a\u7a7a\u95f4\u6df7\u5408\u6027(SSM)\u6765\u8fd1\u4f3c\u8ba1\u7b97Ising\u6a21\u578b\uff0c\u8fd9\u4e2a\u5047\u8bbe\u5728\u4e00\u822c\u56fe\u4e0a\u5e76\u4e0d\u603b\u662f\u6210\u7acb\u3002\u672c\u6587\u60f3\u8981\u8fbe\u5230\u4e24\u4e2a\u76ee\u6807\uff1a1)\u8bbe\u8ba1\u4e00\u79cd\u4e0d\u9700\u8981SSM\u7684FPTAS\u7b97\u6cd5\uff1b2)\u901a\u8fc7\u5c40\u90e8\u4f9d\u8d56\u7cfb\u6570(LDC)\u8bc1\u660e\u968f\u673a\u805a\u96c6\u6a21\u578b\u7684SSM\u6027\u8d28\u3002

Method: \u7b97\u6cd5\u91c7\u7528Weitz\u81ea\u907f\u884c\u8d70\u6811\u7ed3\u6784\uff0c\u5c06\u5206\u914d\u51fd\u6570\u8868\u793a\u4e3a\u8fdc\u955c\u4e58\u79ef\u6bd4\u503c\u5f62\u5f0f\u3002\u5173\u952e\u521b\u65b0\u5728\u4e8e\u8fd1\u4f3c\u8ba1\u7b97\u8fb9\u5220\u9664\u6bd4\u503c\uff0c\u800c\u975e\u9876\u70b9\u78b3\u7684\u8fb9\u9645\u6982\u7387\u3002\u901a\u8fc7\u5c0f\u4e8e\u5bf9\u6570\u6df1\u5ea6\u622a\u65ad\u6811\u6765\u4fdd\u8bc1\u6548\u7387\u3002\u8fd8\u5f15\u5165\u4e86\u5c40\u90e8\u4f9d\u8d56\u7cfb\u6570(LDC)\u6982\u5ff5\u548c\u65b0\u7684\u9664\u6cd5\u5173\u7cfb\u6765\u8bc1\u660eSSM\u3002

Result: \u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u4e0d\u9700\u8981SSM\u7684Weitz\u578bFPTAS\u7b97\u6cd5\uff0c\u80fd\u591f\u5728Lee-Yang\u96f6\u70b9\u514d\u533a\u57df\u5185\u6709\u6548\u8fd1\u4f3c\u78c1\u6027Ising\u6a21\u578b\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u901a\u8fc7LDC\u8bc1\u660e\u4e86\u968f\u673a\u805a\u96c6\u6a21\u578k\u5728\u4e00\u822c\u56fe\u4e0a\u7684\u5f3a\u7a7a\u95f4\u6df7\u5408\u6027\uff0c\u8fd9\u662f\u8be5\u6a21\u578b\u5728\u975e\u68af\u5b50\u7ed3\u6784\u4e0a\u7684\u9996\u4e2aSSM\u7ed3\u679c\u3002\u8fd8\u5c06\u7ed3\u679c\u6269\u5c55\u5230\u4e86\u5176\u4ed6\u6a21\u578b\u3002

Conclusion: \u8bba\u6587\u8868\u660e\uff0c\u96f6\u70b9\u514d\u6027\u5c31\u8db3\u4ee5\u5bfc\u51faWeitz\u578bFPTAS\u7b97\u6cd5\uff0c\u800c\u5f3a\u7a7a\u95f4\u6df7\u5408\u6027\u8fd8\u9700\u8981\u5c40\u90e8\u4f9d\u8d56\u7cfb\u6570\u8fd9\u79cd\u7ec4\u5408\u6027\u8d28\u7684\u652f\u6301\u3002\u8fd9\u4e3a\u7406\u89e3\u8fd1\u4f3c\u8ba1\u7b97\u548c\u6df7\u5408\u6027\u6027\u8d28\u4e4b\u95f4\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002

Abstract: We present a Weitz-type FPTAS for the ferromagnetic Ising model across the
entire Lee-Yang zero-free region, without relying on the strong spatial mixing
(SSM) property. Our algorithm is Weitz-type for two reasons. First, it
expresses the partition function as a telescoping product of ratios, with the
key being to approximate each ratio. Second, it uses Weitz's self-avoiding walk
tree, and truncates it at logarithmic depth to give a good and efficient
approximation. The key difference from the standard Weitz algorithm is that we
approximate a carefully designed edge-deletion ratio instead of the marginal
probability of a vertex's spin, ensuring our algorithm does not require SSM.
  Furthermore, by establishing local dependence of coefficients (LDC), we
indeed prove a novel form of SSM for these edge-deletion ratios, which, in
turn, implies the standard SSM for the random cluster model. This is the first
SSM result for the random cluster model on general graphs, beyond lattices. We
prove LDC using a new division relation, and remarkably, such relations hold
quite universally. As a result, we establish LDC for a variety of models.
Combined with existing zero-freeness results for these models, we derive new
SSM results for them. Our work suggests that both Weitz-type FPTASes and SSM
can be derived from zero-freeness, while zero-freeness alone suffices for
Weitz-type FPTASes, SSM additionally requires LDC, a combinatorial property
independent of zero-freeness.

</details>


### [21] [The Steiner Shortest Path Tree Problem](https://arxiv.org/abs/2509.06789)
*Omer Asher,Yefim Dinitz,Shlomi Dolev,Li-on Raviv,Baruch Schieber*

Main category: cs.DS

TL;DR: 该论文研究最短路径树中最小化非终端节点数量的问题，这是一个NP难问题，提出了基于有向Steiner树问题的近似算法


<details>
  <summary>Details</summary>
Motivation: 在需要从源点建立最短路径连接的应用中，减少中间节点数量有助于降低成本、复杂性和开销

Method: 将问题归约为均匀顶点加权的有向Steiner树问题(UVDST)，利用现有DST近似算法，并提出针对受限图类的多项式polylog近似算法

Result: 证明了SSPT问题是NP难的，通过归约到UVDST问题获得了准多项式polylog近似算法，并为受限图类提出了多项式polylog近似算法

Conclusion: 该研究为最短路径树最小化非终端节点问题提供了理论分析和有效的近似解决方案，特别适用于需要优化中间节点数量的网络应用场景

Abstract: We introduce and study a novel problem of computing a shortest path tree with
a minimum number of non-terminals. It can be viewed as an (unweighted) Steiner
Shortest Path Tree (SSPT) that spans a given set of terminal vertices by
shortest paths from a given source while minimizing the number of nonterminal
vertices included in the tree. This problem is motivated by applications where
shortest-path connections from a source are essential, and where reducing the
number of intermediate vertices helps limit cost, complexity, or overhead. We
show that the SSPT problem is NP-hard. To approximate it, we introduce and
study the shortest path subgraph of a graph. Using it, we show an
approximation-preserving reduction of SSPT to the uniform vertex-weighted
variant of the Directed Steiner Tree (DST) problem, termed UVDST. Consequently,
the algorithm of [Grandoni et al., 2023] approximating DST implies a
quasi-polynomial polylog-approximation algorithm for SSPT. We present a
polynomial polylog-approximation algorithm for UVDST, and thus for SSPT, for a
restricted class of graphs.

</details>


### [22] [Engineering Select Support for Hybrid Bitvectors](https://arxiv.org/abs/2509.06900)
*Eric Chiu,Dominik Kempa*

Main category: cs.DS

TL;DR: 为混合位向量添加select查询支持的方法，使其在保持优秀性能的同时支持完整的rank和select操作


<details>
  <summary>Details</summary>
Motivation: 当前混合位向量实现仅支持rank查询，缺乏select查询支持，这严重限制了其应用范围。需要扩展功能以支持完整的查询操作。

Method: 在混合位向量基础上添加select查询支持，通过分块和自适应编码策略（如游程编码、普通编码或少数编码）来优化性能

Result: 实验结果显示混合位向量在支持select查询后，性能表现优异，速度与最快的位向量相当，空间效率与最紧凑的位向量相当

Conclusion: 提出的方法成功为混合位向量添加了select查询支持，使其成为在重复和非重复输入上都表现优异的完整解决方案

Abstract: One of the central problems in the design of compressed data structures is
the efficient support for rank and select queries on bitvectors. These two
operations form the backbone of more complex data structures (such as wavelet
trees) used for the compact representation of texts, trees, graphs, or grids.
Their efficient implementation is one of the most frequently studied problems
in compressed data structures.
  One effective solution is the so-called hybrid bitvector implementation,
which partitions the input bitvector into blocks and adaptively selects an
encoding method, such as run-length, plain, or minority encoding, based on
local redundancy. Experiments have shown that hybrid bitvectors achieve
excellent all-around performance on repetitive and non-repetitive inputs.
  However, current implementations support only rank queries (i.e., counting
the number of ones up to a given position) and lack support for select queries.
This limitation significantly restricts their applicability. In this paper, we
propose a method to add support for select queries to hybrid bitvectors, and we
conduct an extensive set of experiments. Our results show that hybrid
bitvectors offer excellent performance, matching the speed of the fastest and
the space efficiency of the most compact existing bitvectors.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Benchmarking Robust Aggregation in Decentralized Gradient Marketplaces](https://arxiv.org/abs/2509.05833)
*Zeyu Song,Sainyam Galhotra,Shagufta Mehnaz*

Main category: cs.LG

TL;DR: 这篇论文提出了一个全面的检测框架，用于评估去中心化梯度市场中的粗糖聚合方法，考虑了经济效率、公平性和市场稳定性等关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习测试并没有考虑去中心化梯度市场中的关键经济和系统因素，特别是当买家依赖私有基准数据集进行评估时的成本效益、卖家公平性和市场稳定性问题。

Method: 研究包含：(1) 模拟环境，模拟具有变化买家基准和多样卖家分布的市场动态；(2) 评估方法，在标准FL指标基础上增加了市场联相关指标如经济效率、公平性和选择动态；(3) 对MartFL框架进行深入实证分析，包括集成和比较评估适配的FLTrust和SkyMask聚合策略。

Result: 该检测框架涵盖了多样数据集、本地攻击和针对市场选择过程的Sybil攻击，提供了关于模型性能、稳健性、成本、公平性和稳定性之间变换关系的可操作性见解。

Conclusion: 该检测框架为社区提供了重要工具和实证证据，以评估和设计更稳健、公平且经济可行的去中心化梯度市场。

Abstract: The rise of distributed and privacy-preserving machine learning has sparked
interest in decentralized gradient marketplaces, where participants trade
intermediate artifacts like gradients. However, existing Federated Learning
(FL) benchmarks overlook critical economic and systemic factors unique to such
marketplaces-cost-effectiveness, fairness to sellers, and market
stability-especially when a buyer relies on a private baseline dataset for
evaluation.
  We introduce a comprehensive benchmark framework to holistically evaluate
robust gradient aggregation methods within these buyer-baseline-reliant
marketplaces. Our contributions include: (1) a simulation environment modeling
marketplace dynamics with a variable buyer baseline and diverse seller
distributions; (2) an evaluation methodology augmenting standard FL metrics
with marketplace-centric dimensions such as Economic Efficiency, Fairness, and
Selection Dynamics; (3) an in-depth empirical analysis of the existing
Distributed Gradient Marketplace framework, MartFL, including the integration
and comparative evaluation of adapted FLTrust and SkyMask as alternative
aggregation strategies within it. This benchmark spans diverse datasets, local
attacks, and Sybil attacks targeting the marketplace selection process; and (4)
actionable insights into the trade-offs between model performance, robustness,
cost, fairness, and stability.
  This benchmark equips the community with essential tools and empirical
evidence to evaluate and design more robust, equitable, and economically viable
decentralized gradient marketplaces.

</details>


### [24] [Standard vs. Modular Sampling: Best Practices for Reliable LLM Unlearning](https://arxiv.org/abs/2509.05316)
*Praveen Bushipaka,Lucia Passaro,Tommaso Cucinotta*

Main category: cs.LG

TL;DR: 本文系统性评估LLM反学习中的常见实践，发现依赖单一邻居集和标准采样方法存在问题，提出了包含多样化邻居集和模块化实体级反学习策略的最佳实践。


<details>
  <summary>Details</summary>
Motivation: 现有LLM反学习研究中常见使用单一邻居集和标准采样方法，这些方法的效果和稳定性未经过系统评估，不能反映真实世界数据的复杂性和关系。

Method: 系统性评估了常见的反学习实践，包括采样方法和邻居集组成，并提出了模块化实体级反学习(MELU)策略作为替代方案。

Result: 研究发现依赖单一邻居集是次优的，标准采样方法效率低且效果差，而提出的MELU策略能够提供明确稳定的反学习效果。

Conclusion: 建议采用多样化邻居集来平衡反学习效果和模型效用，避免使用标准采样方法，并采用模块化实体级反学习策略来实现更有效的反学习。

Abstract: A conventional LLM Unlearning setting consists of two subsets -"forget" and
"retain", with the objectives of removing the undesired knowledge from the
forget set while preserving the remaining knowledge from the retain. In
privacy-focused unlearning research, a retain set is often further divided into
neighbor sets, containing either directly or indirectly connected to the forget
targets; and augmented by a general-knowledge set. A common practice in
existing benchmarks is to employ only a single neighbor set, with general
knowledge which fails to reflect the real-world data complexities and
relationships. LLM Unlearning typically involves 1:1 sampling or cyclic
iteration sampling. However, the efficacy and stability of these de facto
standards have not been critically examined. In this study, we systematically
evaluate these common practices. Our findings reveal that relying on a single
neighbor set is suboptimal and that a standard sampling approach can obscure
performance trade-offs. Based on this analysis, we propose and validate an
initial set of best practices: (1) Incorporation of diverse neighbor sets to
balance forget efficacy and model utility, (2) Standard 1:1 sampling methods
are inefficient and yield poor results, (3) Our proposed Modular Entity-Level
Unlearning (MELU) strategy as an alternative to cyclic sampling. We demonstrate
that this modular approach, combined with robust algorithms, provides a clear
and stable path towards effective unlearning.

</details>


### [25] [Feed Two Birds with One Scone: Exploiting Function-Space Regularization for Both OOD Robustness and ID Fine-Tuning Performance](https://arxiv.org/abs/2509.05328)
*Xiang Yuan,Jun Shu,Deyu meng,Zongben Xu*

Main category: cs.LG

TL;DR: 提出了一种新的正则化方法，通过在函数空间中约束微调模型与预训练模型的距离，并引入一致性正则化来提升OOD鲁棒性，在各种CLIP骨干网络上都能一致改善下游任务的ID性能和OOD鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的鲁棒微调方法通过保留预训练权重、特征或logits来保持OOD鲁棒性，但这些方法不能总是改善不同模型架构的OOD鲁棒性，因为OOD鲁棒性需要模型函数对下游任务输入信息产生稳定预测，而现有方法在函数空间优化中作为代理效果不佳。

Method: 提出在函数空间中用模拟OOD样本约束微调模型与预训练模型距离的正则化方法，并引入额外的一致性正则化来促进扰动样本的稳定预测。

Result: 大量实验表明，该方法在各种CLIP骨干网络上都能一致改善下游任务的ID微调性能和OOD鲁棒性，优于现有的基于正则化的鲁棒微调方法。

Conclusion: 通过在函数空间中进行约束并引入一致性正则化，能够有效保持预训练模型的OOD鲁棒性，同时在微调过程中提升下游任务的性能。

Abstract: Robust fine-tuning aims to achieve competitive in-distribution (ID)
performance while maintaining the out-of-distribution (OOD) robustness of a
pre-trained model when transferring it to a downstream task. To remedy this,
most robust fine-tuning methods aim to preserve the pretrained weights,
features, or logits. However, we find that these methods cannot always improve
OOD robustness for different model architectures. This is due to the OOD
robustness requiring the model function to produce stable prediction for input
information of downstream tasks, while existing methods might serve as a poor
proxy for the optimization in the function space. Based on this finding, we
propose a novel regularization that constrains the distance of fine-tuning and
pre-trained model in the function space with the simulated OOD samples, aiming
to preserve the OOD robustness of the pre-trained model. Besides, to further
enhance the OOD robustness capability of the fine-tuning model, we introduce an
additional consistency regularization to promote stable predictions of
perturbed samples. Extensive experiments demonstrate our approach could
consistently improve both downstream task ID fine-tuning performance and OOD
robustness across a variety of CLIP backbones, outperforming existing
regularization-based robust fine-tuning methods.

</details>


### [26] [Safeguarding Graph Neural Networks against Topology Inference Attacks](https://arxiv.org/abs/2509.05429)
*Jie Fu,Hong Yuan,Zhili Chen,Wendy Hui Wang*

Main category: cs.LG

TL;DR: 该论文研究了图神经网络中的拓扑隐私风险，提出了拓扑推理攻击(TIAs)并发现现有隐私保护机制不足，进而设计了新的防御框架PGR来保护拓扑隐私同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 图神经网络(GNNs)的广泛应用引发了严重的隐私问题，现有研究主要关注边级隐私，而拓扑隐私(图整体结构的机密性)这一关键威胁尚未得到充分探索。

Method: 提出了拓扑推理攻击(TIAs)来重建目标训练图结构，并设计了Private Graph Reconstruction (PGR)防御框架，采用双层优化方法通过元梯度迭代生成合成训练图，同时基于演化图更新GNN模型。

Result: 研究发现GNNs对这些攻击高度脆弱，现有边级差分隐私机制要么无法缓解风险，要么严重损害模型精度。PGR框架显著减少了拓扑泄露，同时对模型精度影响最小。

Conclusion: 拓扑隐私是GNNs中一个重要但被忽视的威胁，需要专门的保护机制。PGR框架为保护图结构隐私提供了有效解决方案，在隐私保护和模型效用之间取得了良好平衡。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful models for learning
from graph-structured data. However, their widespread adoption has raised
serious privacy concerns. While prior research has primarily focused on
edge-level privacy, a critical yet underexplored threat lies in topology
privacy - the confidentiality of the graph's overall structure. In this work,
we present a comprehensive study on topology privacy risks in GNNs, revealing
their vulnerability to graph-level inference attacks. To this end, we propose a
suite of Topology Inference Attacks (TIAs) that can reconstruct the structure
of a target training graph using only black-box access to a GNN model. Our
findings show that GNNs are highly susceptible to these attacks, and that
existing edge-level differential privacy mechanisms are insufficient as they
either fail to mitigate the risk or severely compromise model accuracy. To
address this challenge, we introduce Private Graph Reconstruction (PGR), a
novel defense framework designed to protect topology privacy while maintaining
model accuracy. PGR is formulated as a bi-level optimization problem, where a
synthetic training graph is iteratively generated using meta-gradients, and the
GNN model is concurrently updated based on the evolving graph. Extensive
experiments demonstrate that PGR significantly reduces topology leakage with
minimal impact on model accuracy. Our code is anonymously available at
https://github.com/JeffffffFu/PGR.

</details>


### [27] [MCIGLE: Multimodal Exemplar-Free Class-Incremental Graph Learning](https://arxiv.org/abs/2509.06219)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: MCIGLE是一个无需样本的类增量学习框架，专门处理多模态图结构数据，通过特征提取对齐和递归最小二乘法解决灾难性遗忘等问题


<details>
  <summary>Details</summary>
Motivation: 随着多模态图结构数据的普及，现有方法在灾难性遗忘、分布偏差、内存限制和泛化能力弱等方面面临挑战，需要新的解决方案

Method: 提出MCIGLE框架，通过提取和对齐多模态图特征，应用串联递归最小二乘法进行知识保留，采用多通道处理平衡准确性和内存保护

Result: 在公共数据集上的实验验证了该方法的有效性和泛化能力

Conclusion: MCIGLE框架成功解决了多模态图数据类增量学习中的关键问题，为相关领域提供了有效的解决方案

Abstract: Exemplar-free class-incremental learning enables models to learn new classes
over time without storing data from old ones. As multimodal graph-structured
data becomes increasingly prevalent, existing methods struggle with challenges
like catastrophic forgetting, distribution bias, memory limits, and weak
generalization. We propose MCIGLE, a novel framework that addresses these
issues by extracting and aligning multimodal graph features and applying
Concatenated Recursive Least Squares for effective knowledge retention. Through
multi-channel processing, MCIGLE balances accuracy and memory preservation.
Experiments on public datasets validate its effectiveness and generalizability.

</details>


### [28] [Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis](https://arxiv.org/abs/2509.05449)
*Disha Makhija,Manoj Ghuhan Arivazhagan,Vinayshekhar Bannihatti Kumar,Rashmi Gangadharaiah*

Main category: cs.LG

TL;DR: 本文提出了memTrace框架，通过分析LLM内部表示（隐藏状态和注意力模式）而非仅输出，实现了强大的成员推理攻击，平均AUC达0.85，表明即使输出看似安全，内部行为仍可能泄露训练数据隐私。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明成员推理攻击对大型语言模型效果有限，仅略优于随机猜测，但作者认为仅分析模型输出可能忽略了内部表示中的隐私泄露信号，需要探索更深入的检测方法。

Method: 提出memTrace框架，通过分析transformer隐藏状态和注意力模式的"神经痕迹"，包括层级表示动态、注意力分布特征和跨层转换模式，来检测传统基于损失的方法可能遗漏的记忆指纹。

Result: 在多个模型家族上实现了强大的成员检测性能，在流行的MIA基准测试中平均AUC得分达到0.85，显著优于传统方法。

Conclusion: 即使基于输出的信号看似受到保护，内部模型行为仍能揭示训练数据暴露的方面，强调需要进一步研究成员隐私问题，并开发更强大的隐私保护训练技术。

Abstract: Membership inference attacks (MIAs) reveal whether specific data was used to
train machine learning models, serving as important tools for privacy auditing
and compliance assessment. Recent studies have reported that MIAs perform only
marginally better than random guessing against large language models,
suggesting that modern pre-training approaches with massive datasets may be
free from privacy leakage risks. Our work offers a complementary perspective to
these findings by exploring how examining LLMs' internal representations,
rather than just their outputs, may provide additional insights into potential
membership inference signals. Our framework, \emph{memTrace}, follows what we
call \enquote{neural breadcrumbs} extracting informative signals from
transformer hidden states and attention patterns as they process candidate
sequences. By analyzing layer-wise representation dynamics, attention
distribution characteristics, and cross-layer transition patterns, we detect
potential memorization fingerprints that traditional loss-based approaches may
not capture. This approach yields strong membership detection across several
model families achieving average AUC scores of 0.85 on popular MIA benchmarks.
Our findings suggest that internal model behaviors can reveal aspects of
training data exposure even when output-based signals appear protected,
highlighting the need for further research into membership privacy and the
development of more robust privacy-preserving training techniques for large
language models.

</details>


### [29] [Calibrated Recommendations with Contextual Bandits](https://arxiv.org/abs/2509.05460)
*Diego Feijer,Himan Abdollahpouri,Sanket Gupta,Alexander Clare,Yuxiao Wen,Todd Wasson,Maria Dimakopoulou,Zahra Nazari,Kyle Kretschman,Mounia Lalmas*

Main category: cs.LG

TL;DR: Spotify提出基于上下文多臂老虎机的校准方法，根据用户上下文动态学习最优内容类型分布，解决了历史数据偏向音乐内容的问题，提升了用户参与度和非音乐内容的曝光。


<details>
  <summary>Details</summary>
Motivation: Spotify首页包含多种内容类型，但历史数据严重偏向音乐内容，难以实现平衡和个性化的内容推荐。用户对不同内容类型的偏好会随时间、星期和设备等上下文因素变化。

Method: 使用上下文多臂老虎机（contextual bandits）方法，动态学习每个用户在特定上下文下的最优内容类型分布，而不是依赖历史平均值。

Result: 离线和在线实验结果都显示，该方法提高了Spotify首页的推荐精度和用户参与度，特别是对播客等代表性不足的内容类型有显著改善。

Conclusion: 基于上下文多臂老虎机的校准方法能够有效适应用户兴趣在不同上下文中的变化，比传统校准方法更能提升用户参与度和内容多样性。

Abstract: Spotify's Home page features a variety of content types, including music,
podcasts, and audiobooks. However, historical data is heavily skewed toward
music, making it challenging to deliver a balanced and personalized content
mix. Moreover, users' preference towards different content types may vary
depending on the time of day, the day of week, or even the device they use. We
propose a calibration method that leverages contextual bandits to dynamically
learn each user's optimal content type distribution based on their context and
preferences. Unlike traditional calibration methods that rely on historical
averages, our approach boosts engagement by adapting to how users interests in
different content types varies across contexts. Both offline and online results
demonstrate improved precision and user engagement with the Spotify Home page,
in particular with under-represented content types such as podcasts.

</details>


### [30] [PLanTS: Periodicity-aware Latent-state Representation Learning for Multivariate Time Series](https://arxiv.org/abs/2509.05478)
*Jia Wang,Xiao Wang,Chi Zhang*

Main category: cs.LG

TL;DR: PLanTS是一个周期性感知的自监督学习框架，通过多粒度分块机制和对比损失来建模多元时间序列的周期性结构和潜在状态转换，在多个下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列具有高维度、标注数据有限和非平稳性等挑战，现有自监督学习方法忽略了内在周期性结构，无法捕捉潜在状态的动态演化。

Method: 提出周期性感知的多粒度分块机制和广义对比损失来保持实例级和状态级相似性；设计下一状态转换预测任务来编码未来状态演化的预测信息。

Result: 在多元分类、多标签分类、预测、轨迹跟踪和异常检测等任务中，PLanTS持续提升表示质量，且比基于DTW的方法具有更优的运行效率。

Conclusion: PLanTS通过显式建模周期性结构和状态转换，有效解决了多元时间序列表示学习的挑战，为相关领域提供了强大的自监督学习框架。

Abstract: Multivariate time series (MTS) are ubiquitous in domains such as healthcare,
climate science, and industrial monitoring, but their high dimensionality,
limited labeled data, and non-stationary nature pose significant challenges for
conventional machine learning methods. While recent self-supervised learning
(SSL) approaches mitigate label scarcity by data augmentations or time
point-based contrastive strategy, they neglect the intrinsic periodic structure
of MTS and fail to capture the dynamic evolution of latent states. We propose
PLanTS, a periodicity-aware self-supervised learning framework that explicitly
models irregular latent states and their transitions. We first designed a
period-aware multi-granularity patching mechanism and a generalized contrastive
loss to preserve both instance-level and state-level similarities across
multiple temporal resolutions. To further capture temporal dynamics, we design
a next-transition prediction pretext task that encourages representations to
encode predictive information about future state evolution. We evaluate PLanTS
across a wide range of downstream tasks-including multi-class and multi-label
classification, forecasting, trajectory tracking and anomaly detection. PLanTS
consistently improves the representation quality over existing SSL methods and
demonstrates superior runtime efficiency compared to DTW-based methods.

</details>


### [31] [X-SQL: Expert Schema Linking and Understanding of Text-to-SQL with Multi-LLMs](https://arxiv.org/abs/2509.05899)
*Dazhi Peng*

Main category: cs.LG

TL;DR: X-SQL是一个基于开源模型的领先Text-to-SQL框架，通过创新的数据库模式专家组件（X-Linking和X-Admin）显著提升模式信息利用，在Spider数据集上取得了最先进的执行准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管现有LLM-based Text-to-SQL框架表现强劲，但研究社区往往忽视了数据库模式信息对生成高质量SQL查询的重要性。研究发现模式信息在该任务中起着显著甚至主导作用。

Method: 提出包含两个组件的数据库模式专家：1）X-Linking：基于LLM监督微调的Schema Linking方法；2）X-Admin：创新的Schema Understanding组件，弥合抽象模式信息与用户自然语言问题之间的差距。还采用多LLM策略为系统不同组件使用不同模型。

Result: 在Spider-Dev数据集上达到84.9%的执行准确率，在Spider-Test数据集上达到82.5%的执行准确率，成为基于开源模型的领先Text-to-SQL框架。

Conclusion: 通过有效利用数据库模式信息和多LLM策略，X-SQL框架在Text-to-SQL任务中取得了卓越性能，证明了模式信息在该任务中的关键作用以及所提出方法的有效性。

Abstract: With Large Language Models' (LLMs) emergent abilities on code generation
tasks, Text-to-SQL has become one of the most popular downstream applications.
Despite the strong results of multiple recent LLM-based Text-to-SQL frameworks,
the research community often overlooks the importance of database schema
information for generating high-quality SQL queries. We find that such schema
information plays a significant or even dominant role in the Text-to-SQL task.
To tackle this challenge, we propose a novel database schema expert with two
components. We first introduce X-Linking, an LLM Supervised Finetuning
(SFT)-based method that achieves superior Schema Linking results compared to
existing open-source Text-to-SQL methods. In addition, we innovatively propose
an X-Admin component that focuses on Schema Understanding by bridging the gap
between abstract schema information and the user's natural language question.
Aside from better learning with schema information, we experiment with
Multi-LLMs for different components within the system to further boost its
performance. By incorporating these techniques into our end-to-end framework,
X-SQL, we have achieved Execution Accuracies of 84.9% on the Spider-Dev dataset
and 82.5% on the Spider-Test dataset. This outstanding performance establishes
X-SQL as the leading Text-to-SQL framework based on open-source models.

</details>


### [32] [STL-based Optimization of Biomolecular Neural Networks for Regression and Control](https://arxiv.org/abs/2509.05481)
*Eric Palanques-Tost,Hanna Krasowski,Murat Arcak,Ron Weiss,Calin Belta*

Main category: cs.LG

TL;DR: 利用信号时序逻辑(STL)规范来训练生物分子神经网络(BNNs)，解决了BNNs缺乏目标数据的训练挑战，实现了回归和控制任务


<details>
  <summary>Details</summary>
Motivation: 生物分子神经网络具有超越简单生物电路的通用函数逼近能力，但由于缺乏目标数据，训练BNNs仍然具有挑战性

Method: 基于STL的定量语义，开发梯度优化算法来训练BNN权重，使其能够执行生物系统中的回归和控制任务

Result: 数值实验表明，基于STL的学习方法能够高效解决研究的回归和控制任务，包括作为失调状态报告器和慢性疾病模型的反馈控制

Conclusion: STL规范为BNNs提供了有效的训练目标定义方法，使BNNs能够在生物系统中成功执行复杂任务

Abstract: Biomolecular Neural Networks (BNNs), artificial neural networks with
biologically synthesizable architectures, achieve universal function
approximation capabilities beyond simple biological circuits. However, training
BNNs remains challenging due to the lack of target data. To address this, we
propose leveraging Signal Temporal Logic (STL) specifications to define
training objectives for BNNs. We build on the quantitative semantics of STL,
enabling gradient-based optimization of the BNN weights, and introduce a
learning algorithm that enables BNNs to perform regression and control tasks in
biological systems. Specifically, we investigate two regression problems in
which we train BNNs to act as reporters of dysregulated states, and a feedback
control problem in which we train the BNN in closed-loop with a chronic disease
model, learning to reduce inflammation while avoiding adverse responses to
external infections. Our numerical experiments demonstrate that STL-based
learning can solve the investigated regression and control tasks efficiently.

</details>


### [33] [Prior Distribution and Model Confidence](https://arxiv.org/abs/2509.05485)
*Maksim Kazanskii,Artem Kasianov*

Main category: cs.LG

TL;DR: 通过分析训练数据在嵌入空间中的分布，提出了一种无需重新训练的方法来评估模型对未见数据的预测信心度，通过过滤低信心度预测显著提高了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 研究训练数据分布对图像分类模型性能的影响，并开发一种方法来在不需重新训练的情况下评估模型对未见数据的预测信心度。

Method: 分析训练集在嵌入空间中的表示，基于未见数据与训练分布在嵌入空间中的距离来过滤低信心度预测。使用多个嵌入模型来提取补充特征，实现更稳健的信心度估计。

Result: 在多个分类模型上均实现了一致的性能提升，显著提高了分类准确性。多嵌入模型的结合能够更好地检测和排除分布外样本，带来进一步的准确性提升。

Conclusion: 该方法不依赖于特定模型结构，具有良好的可扩展性，在计算机视觉和自然语言处理等预测可靠性关键的领域都有应用潜力。

Abstract: This paper investigates the impact of training data distribution on the
performance of image classification models. By analyzing the embeddings of the
training set, we propose a framework to understand the confidence of model
predictions on unseen data without the need for retraining. Our approach
filters out low-confidence predictions based on their distance from the
training distribution in the embedding space, significantly improving
classification accuracy. We demonstrate this on the example of several
classification models, showing consistent performance gains across
architectures. Furthermore, we show that using multiple embedding models to
represent the training data enables a more robust estimation of confidence, as
different embeddings capture complementary aspects of the data. Combining these
embeddings allows for better detection and exclusion of out-of-distribution
samples, resulting in further accuracy improvements. The proposed method is
model-agnostic and generalizable, with potential applications beyond computer
vision, including domains such as Natural Language Processing where prediction
reliability is critical.

</details>


### [34] [MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs](https://arxiv.org/abs/2509.05488)
*Hongjun Xu,Junxi Xia,Weisi Yang,Yueyuan Sui,Stephen Xia*

Main category: cs.LG

TL;DR: 首个在资源受限MCU上部署Mamba模型的方案MambaLite-Micro，通过权重导出、算子融合和内存优化，减少83%峰值内存，保持1.7x10-5数值误差，在KWS和HAR任务上实现100%准确率一致性。


<details>
  <summary>Details</summary>
Motivation: 解决Mamba模型在微控制器(MCU)上部署的挑战，包括有限内存、缺乏原生算子支持和嵌入式友好工具链的缺失。

Method: 提出MambaLite-Micro：1) 导出PyTorch模型权重为轻量格式；2) 手工实现C语言的Mamba层和支持算子，采用算子融合和内存布局优化；3) 消除大型中间张量。

Result: 减少83.0%峰值内存，平均数值误差仅1.7x10-5，在关键词检测(KWS)和人体活动识别(HAR)任务上实现100%准确率一致性，在ESP32S3和STM32H7上验证可移植性。

Conclusion: MambaLite-Micro成功将先进的序列模型如Mamba引入资源受限的嵌入式平台，为实际应用铺平道路。

Abstract: Deploying Mamba models on microcontrollers (MCUs) remains challenging due to
limited memory, the lack of native operator support, and the absence of
embedded-friendly toolchains. We present, to our knowledge, the first
deployment of a Mamba-based neural architecture on a resource-constrained MCU,
a fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline
maps a trained PyTorch Mamba model to on-device execution by (1) exporting
model weights into a lightweight format, and (2) implementing a handcrafted
Mamba layer and supporting operators in C with operator fusion and memory
layout optimization. MambaLite-Micro eliminates large intermediate tensors,
reducing 83.0% peak memory, while maintaining an average numerical error of
only 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on
keyword spotting(KWS) and human activity recognition (HAR) tasks,
MambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully
preserving classification accuracy. We further validated portability by
deploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating
consistent operation across heterogeneous embedded platforms and paving the way
for bringing advanced sequence models like Mamba to real-world
resource-constrained applications.

</details>


### [35] [On optimal solutions of classical and sliced Wasserstein GANs with non-Gaussian data](https://arxiv.org/abs/2509.06505)
*Yu-Jui Huang,Hsin-Hua Shen,Yu-Chih Huang,Wan-Yi Lin,Shih-Chun Lin*

Main category: cs.LG

TL;DR: 本文研究了Wasserstein GAN（WGAN）在非LQG设置下的最优参数选择问题，推导了一维情况下非线性激活函数和非高斯数据的最优闭式解，并通过切片Wasserstein框架扩展到高维情况。


<details>
  <summary>Details</summary>
Motivation: 现有WGAN参数选择方法主要局限于线性-二次-高斯（LQG）设置，缺乏对非线性激活函数和非高斯数据的理论最优参数研究，需要开发更通用的参数选择方法。

Method: 针对一维WGAN推导非线性激活函数和非高斯数据下的闭式最优参数；采用切片Wasserstein框架扩展到高维情况，用原始数据联合分布约束替代边际分布约束；证明线性生成器在切片WGAN中的渐近最优性。

Result: 推导出的闭式WGAN参数在高斯和拉普拉斯分布数据下均表现出良好的收敛性；相比r-PCA方法，所提切片WGAN解决方案能达到相同性能但计算资源需求更少。

Conclusion: 本文突破了LQG限制，为WGAN在更广泛设置下的参数选择提供了理论依据和实用方法，证明了线性生成器在非高斯数据下的有效性，为高维WGAN应用提供了高效解决方案。

Abstract: The generative adversarial network (GAN) aims to approximate an unknown
distribution via a parameterized neural network (NN). While GANs have been
widely applied in reinforcement and semisupervised learning as well as computer
vision tasks, selecting their parameters often needs an exhaustive search and
only a few selection methods can be proved to be theoretically optimal. One of
the most promising GAN variants is the Wasserstein GAN (WGAN). Prior work on
optimal parameters for WGAN is limited to the linear-quadratic-Gaussian (LQG)
setting, where the NN is linear and the data is Gaussian. In this paper, we
focus on the characterization of optimal WGAN parameters beyond the LQG
setting. We derive closed-form optimal parameters for one-dimensional WGANs
when the NN has non-linear activation functions and the data is non-Gaussian.
To extend this to high-dimensional WGANs, we adopt the sliced Wasserstein
framework and replace the constraint on marginal distributions of the randomly
projected data by a constraint on the joint distribution of the original
(unprojected) data. We show that the linear generator can be asymptotically
optimal for sliced WGAN with non-Gaussian data. Empirical studies show that our
closed-form WGAN parameters have good convergence behavior with data under both
Gaussian and Laplace distributions. Also, compared to the r principal component
analysis (r-PCA) solution, our proposed solution for sliced WGAN can achieve
the same performance while requiring less computational resources.

</details>


### [36] [Self-Aligned Reward: Towards Effective and Efficient Reasoners](https://arxiv.org/abs/2509.05489)
*Peixuan Han,Adit Krishnan,Gerald Friedland,Jiaxuan You,Chris Kong*

Main category: cs.LG

TL;DR: 提出了自对齐奖励(SAR)方法，通过相对困惑度差异来同时提升LLM推理的准确性和效率，在7个基准测试中实现4%准确率提升和30%推理成本降低。


<details>
  <summary>Details</summary>
Motivation: 现有可验证奖励信号过于粗糙（只有二元正确性反馈），导致推理过程冗长、计算成本高，且现有解决方案往往牺牲准确性。

Method: 定义自对齐奖励(SAR)为答案在查询条件下的困惑度与独立答案困惑度的相对差异，偏好简洁且查询特定的响应。

Result: 在4个模型和7个基准测试中，SAR与PPO、GRPO等RL算法结合使准确率提升4%，推理成本降低30%，实现了正确性与效率的帕累托最优权衡。

Conclusion: SAR作为可验证奖励的细粒度补充，能够在不丢失关键推理的情况下抑制不必要的阐述，为更高效有效的LLM训练铺平道路。

Abstract: Reinforcement learning with verifiable rewards has significantly advanced
reasoning in large language models (LLMs), but such signals remain coarse,
offering only binary correctness feedback. This limitation often results in
inefficiencies, including overly verbose reasoning and high computational cost,
while existing solutions often compromise accuracy. To address this, we
introduce self-aligned reward (SAR), a self-guided signal that complements
verifiable rewards to encourage both reasoning accuracy and efficiency. SAR is
defined as the relative perplexity difference between an answer conditioned on
the query and the standalone answer, thereby favoring responses that are
concise and query-specific. Quantitative analysis reveals that SAR reliably
distinguishes answer quality: concise, correct answers score higher than
redundant ones, and partially correct answers score higher than entirely
incorrect ones. Evaluation on 4 models across 7 benchmarks shows that
integrating SAR with prevalent RL algorithms like PPO and GRPO improves
accuracy by 4%, while reducing inference cost by 30%. Further analysis
demonstrates that SAR achieves a Pareto-optimal trade-off between correctness
and efficiency compared to reward signals based on length or self-confidence.
We also show that SAR shortens responses while preserving advanced reasoning
behaviors, demonstrating its ability to suppress unnecessary elaboration
without losing critical reasoning. These results highlight the promise of
self-aligned reward as a fine-grained complement to verifiable rewards, paving
the way for more efficient and effective LLM training.

</details>


### [37] [DreamPRM-1.5: Unlocking the Potential of Each Instance for Multimodal Process Reward Model Training](https://arxiv.org/abs/2509.05542)
*Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: DreamPRM-1.5是一个基于实例重加权的多模态过程奖励模型训练框架，通过双层优化自适应调整训练样本重要性，在MMMU基准测试中达到84.6%准确率，超越GPT-5。


<details>
  <summary>Details</summary>
Motivation: 多模态过程奖励模型(PRMs)训练面临分布偏移和噪声数据的挑战，需要开发能够自适应处理这些问题的新方法。

Method: 提出实例重加权框架，采用双层优化自适应调整每个训练样本的重要性权重。设计两种互补策略：Instance Table（适用于较小数据集）和Instance Net（可扩展到较大数据集），并集成到测试时扩展中。

Result: 在MMMU基准测试中达到84.6%的准确率，性能超过GPT-5模型。

Conclusion: DreamPRM-1.5框架有效解决了多模态PRM训练中的分布偏移和噪声问题，通过实例重加权策略显著提升了模型性能。

Abstract: Training multimodal process reward models (PRMs) is challenged by
distribution shifts and noisy data. We introduce DreamPRM-1.5, an
instance-reweighted framework that adaptively adjusts the importance of each
training example via bi-level optimization. We design two complementary
strategies: Instance Table, effective for smaller datasets, and Instance Net,
scalable to larger ones. Integrated into test-time scaling, DreamPRM-1.5
achieves 84.6 accuracy on the MMMU benchmark, surpassing GPT-5.

</details>


### [38] [Reinforcement Learning with Anticipation: A Hierarchical Approach for Long-Horizon Tasks](https://arxiv.org/abs/2509.05545)
*Yang Yu*

Main category: cs.LG

TL;DR: RLA框架通过分层学习和前瞻模型解决长时程目标导向任务，结合价值几何一致性原则确保收敛性


<details>
  <summary>Details</summary>
Motivation: 解决分层强化学习中层次结构自动发现和多级策略联合训练的不稳定性问题，缺乏理论保证

Method: 学习两个协同模型：低级目标条件策略（到达子目标）和高级前瞻模型（规划最优路径上的中间子目标），通过价值几何一致性原则训练前瞻模型

Result: RLA在各种条件下能够逼近全局最优策略，为长时程目标导向任务提供了理论上有保证的分层规划和执行方法

Conclusion: RLA提供了一个原则性且可扩展的框架，通过前瞻模型和价值几何一致性正则化，有效解决了分层强化学习的稳定性和收敛性问题

Abstract: Solving long-horizon goal-conditioned tasks remains a significant challenge
in reinforcement learning (RL). Hierarchical reinforcement learning (HRL)
addresses this by decomposing tasks into more manageable sub-tasks, but the
automatic discovery of the hierarchy and the joint training of multi-level
policies often suffer from instability and can lack theoretical guarantees. In
this paper, we introduce Reinforcement Learning with Anticipation (RLA), a
principled and potentially scalable framework designed to address these
limitations. The RLA agent learns two synergistic models: a low-level,
goal-conditioned policy that learns to reach specified subgoals, and a
high-level anticipation model that functions as a planner, proposing
intermediate subgoals on the optimal path to a final goal. The key feature of
RLA is the training of the anticipation model, which is guided by a principle
of value geometric consistency, regularized to prevent degenerate solutions. We
present proofs that RLA approaches the globally optimal policy under various
conditions, establishing a principled and convergent method for hierarchical
planning and execution in long-horizon goal-conditioned tasks.

</details>


### [39] [Learning to Construct Knowledge through Sparse Reference Selection with Reinforcement Learning](https://arxiv.org/abs/2509.05874)
*Shao-An Yin*

Main category: cs.LG

TL;DR: 基于深度强化学习的稀疏参考文献选择框架，模拟人类知识构建过程，在有限时间和成本下优先选择阅读哪些论文


<details>
  <summary>Details</summary>
Motivation: 科学文献快速增长使得获取新知识变得困难，特别是在专业领域，推理复杂、全文访问受限、目标参考文献在大量候选中稀疏分布

Method: 采用深度强化学习框架进行稀疏参考文献选择，模拟人类知识构建过程，在药物-基因关系发现任务中仅使用标题和摘要信息

Result: 评估表明人类和机器都能从部分信息中有效构建知识

Conclusion: 该框架为在信息受限环境下高效获取科学知识提供了有效解决方案

Abstract: The rapid expansion of scientific literature makes it increasingly difficult
to acquire new knowledge, particularly in specialized domains where reasoning
is complex, full-text access is restricted, and target references are sparse
among a large set of candidates. We present a Deep Reinforcement Learning
framework for sparse reference selection that emulates human knowledge
construction, prioritizing which papers to read under limited time and cost.
Evaluated on drug--gene relation discovery with access restricted to titles and
abstracts, our approach demonstrates that both humans and machines can
construct knowledge effectively from partial information.

</details>


### [40] [ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization](https://arxiv.org/abs/2509.05584)
*Sadegh Jafari,Aishwarya Sarkar,Mohiuddin Bilwal,Ali Jannesari*

Main category: cs.LG

TL;DR: ProfilingAgent是一个基于LLM的多智能体系统，通过分析静态指标和动态性能信号，为不同神经网络架构自动定制剪枝和量化策略，实现高效的模型压缩。


<details>
  <summary>Details</summary>
Motivation: 基础模型面临计算和内存瓶颈，传统压缩技术使用统一启发式方法，忽略了架构和运行时异质性，需要更智能的自动化压缩方案。

Method: 使用大型语言模型构建模块化多智能体系统，结合静态指标（MACs、参数量）和动态信号（延迟、内存）进行分析，实现结构化剪枝和训练后动态量化。

Result: 在多个数据集和模型上验证：剪枝保持竞争力或提升准确率（ImageNet-1K下降约1%，ViT-B/16在小数据集上提升2%）；量化实现最高74%内存节省，准确率损失<0.5%，推理速度提升1.74倍。

Conclusion: 智能体系统是面向性能分析的模型优化的可扩展解决方案，LLM推理质量对迭代剪枝至关重要。

Abstract: Foundation models face growing compute and memory bottlenecks, hindering
deployment on resource-limited platforms. While compression techniques such as
pruning and quantization are widely used, most rely on uniform heuristics that
ignore architectural and runtime heterogeneity. Profiling tools expose
per-layer latency, memory, and compute cost, yet are rarely integrated into
automated pipelines. We propose ProfilingAgent, a profiling-guided, agentic
approach that uses large language models (LLMs) to automate compression via
structured pruning and post-training dynamic quantization. Our modular
multi-agent system reasons over static metrics (MACs, parameter counts) and
dynamic signals (latency, memory) to design architecture-specific strategies.
Unlike heuristic baselines, ProfilingAgent tailors layer-wise decisions to
bottlenecks. Experiments on ImageNet-1K, CIFAR-10, and CIFAR-100 with
ResNet-101, ViT-B/16, Swin-B, and DeiT-B/16 show pruning maintains competitive
or improved accuracy (about 1% drop on ImageNet-1K, +2% gains for ViT-B/16 on
smaller datasets), while quantization achieves up to 74% memory savings with
<0.5% accuracy loss. Our quantization also yields consistent inference speedups
of up to 1.74 times faster. Comparative studies with GPT-4o and GPT-4-Turbo
highlight the importance of LLM reasoning quality for iterative pruning. These
results establish agentic systems as scalable solutions for profiling-guided
model optimization.

</details>


### [41] [Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing](https://arxiv.org/abs/2509.06552)
*Zheqi Lv,Wenqiao Zhang,Kairui Fu,Qi Tian,Shengyu Zhang,Jiajie Su,Jingyuan Chen,Kun Kuang,Fei Wu*

Main category: cs.LG

TL;DR: Persona是一个无需反向传播的原型化参数编辑框架，通过在云端生成参数编辑矩阵来动态适应设备上的实时数据分布变化，提升轻量级模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决设备上实时数据分布变化对轻量级模型泛化能力的挑战，避免传统数据密集型和计算昂贵的微调方法。

Method: 使用基于原型的参数编辑框架，云端神经适配器生成参数编辑矩阵，动态聚类原型模型，结合跨层知识转移实现多层参数一致性调整。

Result: 在多个视觉和推荐任务数据集上的广泛实验证实了Persona的有效性和通用性。

Conclusion: Persona提供了一种无需部署后重新训练的高效个性化方法，能够有效应对设备上的实时数据分布偏移问题。

Abstract: The on-device real-time data distribution shift on devices challenges the
generalization of lightweight on-device models. This critical issue is often
overlooked in current research, which predominantly relies on data-intensive
and computationally expensive fine-tuning approaches. To tackle this, we
introduce Persona, a novel personalized method using a prototype-based,
backpropagation-free parameter editing framework to enhance model
generalization without post-deployment retraining. Persona employs a neural
adapter in the cloud to generate a parameter editing matrix based on real-time
device data. This matrix adeptly adapts on-device models to the prevailing data
distributions, efficiently clustering them into prototype models. The
prototypes are dynamically refined via the parameter editing matrix,
facilitating efficient evolution. Furthermore, the integration of cross-layer
knowledge transfer ensures consistent and context-aware multi-layer parameter
changes and prototype assignment. Extensive experiments on vision task and
recommendation task on multiple datasets confirm Persona's effectiveness and
generality.

</details>


### [42] [Causal Debiasing Medical Multimodal Representation Learning with Missing Modalities](https://arxiv.org/abs/2509.05615)
*Xiaoguang Zhu,Lianlong Sun,Yang Liu,Pengyi Jiang,Uma Srivatsa,Nipavan Chiamvimonvat,Vladimir Filkov*

Main category: cs.LG

TL;DR: 提出一个因果推理框架来解决医学多模态学习中的缺失模态偏差问题，通过结构因果分析和去混淆模块提升模型泛化能力


<details>
  <summary>Details</summary>
Motivation: 现实医学数据集常因成本、协议或患者限制而存在模态缺失，现有方法忽略了数据采集过程引入的偏差，阻碍模型泛化

Method: 结构因果分析数据生成过程，包含缺失性去混淆模块（基于后门调整近似因果干预）和双分支神经网络（显式解耦因果特征与伪相关）

Result: 在真实世界公共数据集和院内数据集上验证了方法的有效性和因果洞察力

Conclusion: 提出的统一框架能够有效处理医学多模态学习中的缺失模态偏差问题，提供因果解释并提升预测性能

Abstract: Medical multimodal representation learning aims to integrate heterogeneous
clinical data into unified patient representations to support predictive
modeling, which remains an essential yet challenging task in the medical data
mining community. However, real-world medical datasets often suffer from
missing modalities due to cost, protocol, or patient-specific constraints.
Existing methods primarily address this issue by learning from the available
observations in either the raw data space or feature space, but typically
neglect the underlying bias introduced by the data acquisition process itself.
In this work, we identify two types of biases that hinder model generalization:
missingness bias, which results from non-random patterns in modality
availability, and distribution bias, which arises from latent confounders that
influence both observed features and outcomes. To address these challenges, we
perform a structural causal analysis of the data-generating process and propose
a unified framework that is compatible with existing direct prediction-based
multimodal learning methods. Our method consists of two key components: (1) a
missingness deconfounding module that approximates causal intervention based on
backdoor adjustment and (2) a dual-branch neural network that explicitly
disentangles causal features from spurious correlations. We evaluated our
method in real-world public and in-hospital datasets, demonstrating its
effectiveness and causal insights.

</details>


### [43] [OptiProxy-NAS: Optimization Proxy based End-to-End Neural Architecture Search](https://arxiv.org/abs/2509.05656)
*Bo Lyu,Yu Cui,Tuo Shi,Ke Li*

Main category: cs.LG

TL;DR: OptiProxy-NAS是一个端到端的神经架构搜索优化框架，通过代理表示将离散的NAS空间转化为连续、可微且平滑的空间，从而可以使用梯度优化方法进行架构搜索。


<details>
  <summary>Details</summary>
Motivation: 神经架构搜索(NAS)是一个计算昂贵的离散优化问题，现有方法如基于预测器的方法和可微分架构搜索存在局限性，需要一种更高效的优化代理方法。

Method: 使用代理表示重新表述NAS空间，使其变得连续、可微且平滑，然后应用任何可微分优化方法进行梯度搜索。

Result: 在12个NAS任务、4个搜索空间和三个不同领域（计算机视觉、自然语言处理、资源受限NAS）的全面实验中展示了优越的搜索结果和效率，在低保真度场景下也验证了灵活性。

Conclusion: OptiProxy-NAS提供了一个有效的端到端优化框架，通过空间转换使得NAS问题可以使用标准梯度优化方法解决，在多个领域都表现出优异的性能和效率。

Abstract: Neural architecture search (NAS) is a hard computationally expensive
optimization problem with a discrete, vast, and spiky search space. One of the
key research efforts dedicated to this space focuses on accelerating NAS via
certain proxy evaluations of neural architectures. Different from the prevalent
predictor-based methods using surrogate models and differentiable architecture
search via supernetworks, we propose an optimization proxy to streamline the
NAS as an end-to-end optimization framework, named OptiProxy-NAS. In
particular, using a proxy representation, the NAS space is reformulated to be
continuous, differentiable, and smooth. Thereby, any differentiable
optimization method can be applied to the gradient-based search of the relaxed
architecture parameters. Our comprehensive experiments on $12$ NAS tasks of $4$
search spaces across three different domains including computer vision, natural
language processing, and resource-constrained NAS fully demonstrate the
superior search results and efficiency. Further experiments on low-fidelity
scenarios verify the flexibility.

</details>


### [44] [DQS: A Low-Budget Query Strategy for Enhancing Unsupervised Data-driven Anomaly Detection Approaches](https://arxiv.org/abs/2509.05663)
*Lucas Correia,Jan-Christoph Goos,Thomas Bäck,Anna V. Kononova*

Main category: cs.LG

TL;DR: 这篇论文提出了一种结合主动学习的时间序列异常检测方法，通过新颖的查询策略DQS选择标签数据来改善阈值设置，提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常检测方法存在阈值设置困难或需要标签数据调整的问题，而真实场景中标签数据缺乏，需要一种更实用的方案。

Method: 集成主动学习到现有无监督异常检测方法中，提出基于动态时间扩展的差异性查询策略(DQS)，通过选择性查询多元时间序列标签来精炼阈值选择过程。

Result: DQS在小预算场景下表现最佳，其他策略在面对标签错误时更稳健。所有主动学习策略都超过了无监督阈值的性能。

Conclusion: 在可行的情况下，建议采用基于主动学习的阈值设置方法，查询策略的选择取决于专家知识和可用标签数量。

Abstract: Truly unsupervised approaches for time series anomaly detection are rare in
the literature. Those that exist suffer from a poorly set threshold, which
hampers detection performance, while others, despite claiming to be
unsupervised, need to be calibrated using a labelled data subset, which is
often not available in the real world. This work integrates active learning
with an existing unsupervised anomaly detection method by selectively querying
the labels of multivariate time series, which are then used to refine the
threshold selection process. To achieve this, we introduce a novel query
strategy called the dissimilarity-based query strategy (DQS). DQS aims to
maximise the diversity of queried samples by evaluating the similarity between
anomaly scores using dynamic time warping. We assess the detection performance
of DQS in comparison to other query strategies and explore the impact of
mislabelling, a topic that is underexplored in the literature. Our findings
indicate that DQS performs best in small-budget scenarios, though the others
appear to be more robust when faced with mislabelling. Therefore, in the real
world, the choice of query strategy depends on the expertise of the oracle and
the number of samples they are willing to label. Regardless, all query
strategies outperform the unsupervised threshold even in the presence of
mislabelling. Thus, whenever it is feasible to query an oracle, employing an
active learning-based threshold is recommended.

</details>


### [45] [GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR](https://arxiv.org/abs/2509.05671)
*Labani Halder,Tanmay Sen,Sarbani Palit*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于图神经网络的多模态联邦学习框架GraMFedDHAR，用于解决多模态传感器数据在人体活动识别中的噪声、标签数据稀缩和隐私问题，在差分隐私条件下显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态传感器数据在人体活动识别中遇到的噪声、标签数据稀缩、隐私问题，以及传统联邦学习在处理异构多模态数据和差分隐私要求时的挑战。

Method: 提出GraMFedDHAR框架，将压力地毯、深度摄像头、加速度计等多种传感器数据建模为模态特征图，通过殊差图卷积神经网络处理，采用注意力机制融合而非简单拼接，并在联邦学习联合过程中使用差分隐私保护数据。

Result: 在非差分隐私设置下比基准模型准确率提高2%，在差分隐私条件下性能优势更为显著，比基准模型提高7-13%，显示图神经网络对差分隐私噪声具有更好的耐受性。

Conclusion: 图基础的多模态学习模型在联邦学习环境下显示出优秀的性能和耐噪性，特别是在差分隐私条件下，为解决多模态传感器数据的隐私保护问题提供了有效方案。

Abstract: Human Activity Recognition (HAR) using multimodal sensor data remains
challenging due to noisy or incomplete measurements, scarcity of labeled
examples, and privacy concerns. Traditional centralized deep learning
approaches are often constrained by infrastructure availability, network
latency, and data sharing restrictions. While federated learning (FL) addresses
privacy by training models locally and sharing only model parameters, it still
has to tackle issues arising from the use of heterogeneous multimodal data and
differential privacy requirements. In this article, a Graph-based Multimodal
Federated Learning framework, GraMFedDHAR, is proposed for HAR tasks. Diverse
sensor streams such as a pressure mat, depth camera, and multiple
accelerometers are modeled as modality-specific graphs, processed through
residual Graph Convolutional Neural Networks (GCNs), and fused via
attention-based weighting rather than simple concatenation. The fused
embeddings enable robust activity classification, while differential privacy
safeguards data during federated aggregation. Experimental results show that
the proposed MultiModalGCN model outperforms the baseline MultiModalFFN, with
up to 2 percent higher accuracy in non-DP settings in both centralized and
federated paradigms. More importantly, significant improvements are observed
under differential privacy constraints: MultiModalGCN consistently surpasses
MultiModalFFN, with performance gaps ranging from 7 to 13 percent depending on
the privacy budget and setting. These results highlight the robustness of
graph-based modeling in multimodal learning, where GNNs prove more resilient to
the performance degradation introduced by DP noise.

</details>


### [46] [Distributed Deep Learning using Stochastic Gradient Staleness](https://arxiv.org/abs/2509.05679)
*Viet Hoang Pham,Hyo-Sung Ahn*

Main category: cs.LG

TL;DR: 这篇论文提出了一种数据并行与全解耦并行反向传播算法相结合的分布式训练方法，用于加速深度神经网络的训练效率。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练需要大量时间，特别是当网络深度增加和数据集扩大时，需要找到更高效的训练方法。

Method: 结合数据并行和全解耦并行反向传播算法，利用多个计算单元并行运行，提高每迭代处理的训练数据量，并减少反向传播算法中的锁定问题。

Result: 计算证明该方法在某些条件下能够收敛到关键点，通过在CIFAR-10数据集上的分类任务进行实验验证，证明了其有效性。

Conclusion: 该分布式训练方法能够显著提高深度神经网络的训练效率，解决了训练时间过长的问题。

Abstract: Despite the notable success of deep neural networks (DNNs) in solving complex
tasks, the training process still remains considerable challenges. A primary
obstacle is the substantial time required for training, particularly as high
performing DNNs tend to become increasingly deep (characterized by a larger
number of hidden layers) and require extensive training datasets. To address
these challenges, this paper introduces a distributed training method that
integrates two prominent strategies for accelerating deep learning: data
parallelism and fully decoupled parallel backpropagation algorithm. By
utilizing multiple computational units operating in parallel, the proposed
approach enhances the amount of training data processed in each iteration while
mitigating locking issues commonly associated with the backpropagation
algorithm. These features collectively contribute to significant improvements
in training efficiency. The proposed distributed training method is rigorously
proven to converge to critical points under certain conditions. Its
effectiveness is further demonstrated through empirical evaluations, wherein an
DNN is trained to perform classification tasks on the CIFAR-10 dataset.

</details>


### [47] [Morphological Perceptron with Competitive Layer: Training Using Convex-Concave Procedure](https://arxiv.org/abs/2509.05697)
*Iara Cunha,Marcos Eduardo Valle*

Main category: cs.LG

TL;DR: 提出使用凸凹过程(CCP)训练具有竞争层的形态感知器(MPCL)网络，通过将训练问题表述为凸差(DC)函数并迭代求解线性规划子问题，解决了形态算子不可微分导致的梯度优化不适用问题。


<details>
  <summary>Details</summary>
Motivation: 形态感知器中的形态算子具有不可微分性，使得基于梯度的优化方法不适用于训练这类网络，需要寻找不依赖梯度信息的替代训练策略。

Method: 将MPCL网络的训练问题表述为凸差(DC)函数优化问题，采用凸凹过程(CCP)进行迭代求解，每次迭代产生一个线性规划子问题。

Result: 计算实验表明，所提出的CCP训练方法在解决MPCL网络分类任务方面具有有效性。

Conclusion: 凸凹过程为训练不可微分的形态感知器网络提供了一种有效的替代方案，能够成功解决多类分类任务。

Abstract: A morphological perceptron is a multilayer feedforward neural network in
which neurons perform elementary operations from mathematical morphology. For
multiclass classification tasks, a morphological perceptron with a competitive
layer (MPCL) is obtained by integrating a winner-take-all output layer into the
standard morphological architecture. The non-differentiability of morphological
operators renders gradient-based optimization methods unsuitable for training
such networks. Consequently, alternative strategies that do not depend on
gradient information are commonly adopted. This paper proposes the use of the
convex-concave procedure (CCP) for training MPCL networks. The training problem
is formulated as a difference of convex (DC) functions and solved iteratively
using CCP, resulting in a sequence of linear programming subproblems.
Computational experiments demonstrate the effectiveness of the proposed
training method in addressing classification tasks with MPCL networks.

</details>


### [48] [Simulation Priors for Data-Efficient Deep Learning](https://arxiv.org/abs/2509.05732)
*Lenart Treven,Bhavya Sukhija,Jonas Rothfuss,Stelian Coros,Florian Dörfler,Andreas Krause*

Main category: cs.LG

TL;DR: SimPEL是一种将第一性原理模型与贝叶斯深度学习相结合的方法，使用低保真模拟器作为先验，在数据稀缺时利用模拟器知识，在数据充足时发挥深度学习的灵活性，同时量化认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统在现实世界中高效学习的问题。第一性原理模型因简化假设而无法捕捉现实复杂性，而深度学习方法需要大量代表性数据。需要一种能结合两者优势的方法。

Method: 提出SimPEL方法，将低保真模拟器作为贝叶斯深度学习的先验，实现第一性原理模型与数据驱动学习的有效结合，同时仔细量化认知不确定性。

Result: 在生物、农业和机器人等多个领域评估显示，SimPEL在学习复杂动力学方面表现优异。在高速RC汽车任务中，用比最先进基线少得多的数据学会了高度动态的漂移停车动作。

Conclusion: SimPEL在复杂现实环境中具有数据高效学习和控制的潜力，能够弥合基于模型的强化学习中的模拟到现实差距。

Abstract: How do we enable AI systems to efficiently learn in the real-world?
First-principles models are widely used to simulate natural systems, but often
fail to capture real-world complexity due to simplifying assumptions. In
contrast, deep learning approaches can estimate complex dynamics with minimal
assumptions but require large, representative datasets. We propose SimPEL, a
method that efficiently combines first-principles models with data-driven
learning by using low-fidelity simulators as priors in Bayesian deep learning.
This enables SimPEL to benefit from simulator knowledge in low-data regimes and
leverage deep learning's flexibility when more data is available, all the while
carefully quantifying epistemic uncertainty. We evaluate SimPEL on diverse
systems, including biological, agricultural, and robotic domains, showing
superior performance in learning complex dynamics. For decision-making, we
demonstrate that SimPEL bridges the sim-to-real gap in model-based
reinforcement learning. On a high-speed RC car task, SimPEL learns a highly
dynamic parking maneuver involving drifting with substantially less data than
state-of-the-art baselines. These results highlight the potential of SimPEL for
data-efficient learning and control in complex real-world environments.

</details>


### [49] [Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies](https://arxiv.org/abs/2509.05735)
*Jiaqi Chen,Ji Shi,Cansu Sancaktar,Jonas Frey,Georg Martius*

Main category: cs.LG

TL;DR: 模型基于强化学习中，在线数据收集比离线数据更有效，离线训练存在分布外状态问题，通过添加在线交互或探索数据可缓解这个问题。


<details>
  <summary>Details</summary>
Motivation: 研究在线与离线数据收集对模型基于强化学习中世界模型学习效果的影响，展现离线训练的性能次劳问题。

Method: 在31个不同环境中进行实验对比，分析在线与离线训练的性能差异，识别分布外状态问题，并测试通过添加在线交互或探索数据来缓解这个问题。

Result: 在线训练表现更优，离线训练存在分布外状态问题，但通过添加在线交互或探索数据可以恢复性能。

Conclusion: 建议在收集大规模数据集时添加探索数据，而不仅仅依赖专家数据，以提升离线训练的效果。

Abstract: Data collection is crucial for learning robust world models in model-based
reinforcement learning. The most prevalent strategies are to actively collect
trajectories by interacting with the environment during online training or
training on offline datasets. At first glance, the nature of learning
task-agnostic environment dynamics makes world models a good candidate for
effective offline training. However, the effects of online vs. offline data on
world models and thus on the resulting task performance have not been
thoroughly studied in the literature. In this work, we investigate both
paradigms in model-based settings, conducting experiments on 31 different
environments. First, we showcase that online agents outperform their offline
counterparts. We identify a key challenge behind performance degradation of
offline agents: encountering Out-Of-Distribution states at test time. This
issue arises because, without the self-correction mechanism in online agents,
offline datasets with limited state space coverage induce a mismatch between
the agent's imagination and real rollouts, compromising policy training. We
demonstrate that this issue can be mitigated by allowing for additional online
interactions in a fixed or adaptive schedule, restoring the performance of
online training with limited interaction data. We also showcase that
incorporating exploration data helps mitigate the performance degradation of
offline agents. Based on our insights, we recommend adding exploration data
when collecting large datasets, as current efforts predominantly focus on
expert data alone.

</details>


### [50] [Ensemble of Precision-Recall Curve (PRC) Classification Trees with Autoencoders](https://arxiv.org/abs/2509.05766)
*Jiaju Miao,Wei Zhu*

Main category: cs.LG

TL;DR: 提出了一种结合PRC随机森林和自编码器的混合框架，用于解决异常检测中的类别不平衡和高维问题，在多个基准数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 异常检测在网络安全、入侵检测和欺诈预防等关键应用中至关重要，但面临极端类别不平衡和维度灾难两大挑战。

Method: 将之前提出的PRC随机森林(PRC-RF)与自编码器相结合，自编码器学习紧凑的潜在表示来处理高维问题，PRC-RF处理类别不平衡。

Result: 在多个基准数据集上的广泛实验表明，Autoencoder-PRC-RF模型相比现有方法具有更高的准确性、可扩展性和可解释性。

Conclusion: 该混合框架在高风险异常检测任务中展现出巨大潜力，能够同时有效应对类别不平衡和高维挑战。

Abstract: Anomaly detection underpins critical applications from network security and
intrusion detection to fraud prevention, where recognizing aberrant patterns
rapidly is indispensable. Progress in this area is routinely impeded by two
obstacles: extreme class imbalance and the curse of dimensionality. To combat
the former, we previously introduced Precision-Recall Curve (PRC)
classification trees and their ensemble extension, the PRC Random Forest
(PRC-RF). Building on that foundation, we now propose a hybrid framework that
integrates PRC-RF with autoencoders, unsupervised machine learning methods that
learn compact latent representations, to confront both challenges
simultaneously. Extensive experiments across diverse benchmark datasets
demonstrate that the resulting Autoencoder-PRC-RF model achieves superior
accuracy, scalability, and interpretability relative to prior methods,
affirming its potential for high-stakes anomaly-detection tasks.

</details>


### [51] [Real-E: A Foundation Benchmark for Advancing Robust and Generalizable Electricity Forecasting](https://arxiv.org/abs/2509.05768)
*Chen Shao,Yue Wang,Zhenyi Zhu,Zhanbo Huang,Sebastian Pütz,Benjamin Schäfer,Tobais Käfer,Michael Färber*

Main category: cs.LG

TL;DR: 提出了Real-E数据集，覆盖30+欧洲国家74个发电站10年数据，包含丰富元数据。基准测试显示现有方法在该数据集上表现不佳，提出了新指标量化相关性结构变化。


<details>
  <summary>Details</summary>
Motivation: 现有能源预测基准在时空范围和多能源特征方面有限，影响其在真实世界部署的可靠性和适用性。

Method: 构建Real-E数据集，进行广泛数据分析，基准测试20+种模型，引入新指标量化相关性结构变化。

Result: 现有方法在Real-E数据集上表现不佳，该数据集展现出更复杂和非平稳的相关性动态。

Conclusion: 研究结果揭示了当前方法的关键局限性，为构建更鲁棒的预测模型提供了强有力的实证基础。

Abstract: Energy forecasting is vital for grid reliability and operational efficiency.
Although recent advances in time series forecasting have led to progress,
existing benchmarks remain limited in spatial and temporal scope and lack
multi-energy features. This raises concerns about their reliability and
applicability in real-world deployment. To address this, we present the Real-E
dataset, covering over 74 power stations across 30+ European countries over a
10-year span with rich metadata. Using Real- E, we conduct an extensive data
analysis and benchmark over 20 baselines across various model types. We
introduce a new metric to quantify shifts in correlation structures and show
that existing methods struggle on our dataset, which exhibits more complex and
non-stationary correlation dynamics. Our findings highlight key limitations of
current methods and offer a strong empirical basis for building more robust
forecasting models

</details>


### [52] [DCV-ROOD Evaluation Framework: Dual Cross-Validation for Robust Out-of-Distribution Detection](https://arxiv.org/abs/2509.05778)
*Arantxa Urrea-Castaño,Nicolás Segura-Kunsagi,Juan Luis Suárez-Díaz,Rosana Montes,Francisco Herrera*

Main category: cs.LG

TL;DR: 本文提出了一种双重交叉验证框架DCV-ROOD，用于更稳健地评估分布外检测模型的性能，通过分别处理分布内和分布外数据的特性，并考虑类别层次结构来获得公平的数据分割。


<details>
  <summary>Details</summary>
Motivation: 分布外(OOD)检测对于提高AI系统的稳健性至关重要，但现有的评估方法并不能有效地处理ID和OOD数据的不同特性。需要一种更稳健的评估框架来确保OOD检测方法的可靠性。

Method: 提出双重交叉验证框架(DCV-ROOD)，对ID数据采用标准分割方式，对OOD数据按类别分组进行分割。在类别层次数据中，考虑整个类别层次结构来获得公平的ID-OOD分割。

Result: 对多个独创新的OOD检测方法进行测试，包括使用和不使用离群暴露的方法。结果显示该方法能够快速收敛到真实性能。

Conclusion: DCV-ROOD框架为OOD检测方法提供了一种可靠的评估方法，通过适应性地处理ID和OOD数据的特性，并考虑类别层次结构，从而提高了评估结果的稳健性和可靠性。

Abstract: Out-of-distribution (OOD) detection plays a key role in enhancing the
robustness of artificial intelligence systems by identifying inputs that differ
significantly from the training distribution, thereby preventing unreliable
predictions and enabling appropriate fallback mechanisms. Developing reliable
OOD detection methods is a significant challenge, and rigorous evaluation of
these techniques is essential for ensuring their effectiveness, as it allows
researchers to assess their performance under diverse conditions and to
identify potential limitations or failure modes. Cross-validation (CV) has
proven to be a highly effective tool for providing a reasonable estimate of the
performance of a learning algorithm. Although OOD scenarios exhibit particular
characteristics, an appropriate adaptation of CV can lead to a suitable
evaluation framework for this setting. This work proposes a dual CV framework
for robust evaluation of OOD detection models, aimed at improving the
reliability of their assessment. The proposed evaluation framework aims to
effectively integrate in-distribution (ID) and OOD data while accounting for
their differing characteristics. To achieve this, ID data are partitioned using
a conventional approach, whereas OOD data are divided by grouping samples based
on their classes. Furthermore, we analyze the context of data with class
hierarchy to propose a data splitting that considers the entire class hierarchy
to obtain fair ID-OOD partitions to apply the proposed evaluation framework.
This framework is called Dual Cross-Validation for Robust Out-of-Distribution
Detection (DCV-ROOD). To test the validity of the evaluation framework, we
selected a set of state-of-the-art OOD detection methods, both with and without
outlier exposure. The results show that the method achieves very fast
convergence to the true performance.

</details>


### [53] [Select, then Balance: A Plug-and-Play Framework for Exogenous-Aware Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.05779)
*Wei Chen,Yuqian Wu,Yuanshao Zhu,Xixuan Hao,Shiyu Wang,Yuxuan Liang*

Main category: cs.LG

TL;DR: 提出ExoST框架，通过"选择-平衡"范式建模外生变量，提升时空预测精度


<details>
  <summary>Details</summary>
Motivation: 现有方法仅使用有限观测变量，而现实中外生变量可作为额外输入特征提升预测准确性，但面临两个挑战：不同外生变量对目标系统的影响不一致，以及历史变量与未来变量之间的影响不平衡

Method: 构建潜在空间门控专家模块，将融合的外生信息投影到潜在空间，通过专门子专家动态选择和重组显著信号；设计孪生网络架构，将过去和未来外生变量的重组表示输入双分支时空主干网络，通过上下文感知加权机制实现动态平衡

Result: 在真实世界数据集上的大量实验证明了所提框架的有效性、通用性、鲁棒性和效率

Conclusion: ExoST框架成功解决了外生变量建模中的选择和平衡问题，为时空预测提供了有效的解决方案

Abstract: Spatio-temporal forecasting aims to predict the future state of dynamic
systems and plays an important role in multiple fields. However, existing
solutions only focus on modeling using a limited number of observed target
variables. In real-world scenarios, exogenous variables can be integrated into
the model as additional input features and associated with the target signal to
promote forecast accuracy. Although promising, this still encounters two
challenges: the inconsistent effects of different exogenous variables to the
target system, and the imbalance effects between historical variables and
future variables. To address these challenges, this paper introduces \model, a
novel framework for modeling \underline{exo}genous variables in
\underline{s}patio-\underline{t}emporal forecasting, which follows a ``select,
then balance'' paradigm. Specifically, we first construct a latent space gated
expert module, where fused exogenous information is projected into a latent
space to dynamically select and recompose salient signals via specialized
sub-experts. Furthermore, we design a siamese network architecture in which
recomposed representations of past and future exogenous variables are fed into
dual-branch spatio-temporal backbones to capture dynamic patterns. The outputs
are integrated through a context-aware weighting mechanism to achieve dynamic
balance during the modeling process. Extensive experiments on real-world
datasets demonstrate the effectiveness, generality, robustness, and efficiency
of our proposed framework.

</details>


### [54] [time2time: Causal Intervention in Hidden States to Simulate Rare Events in Time Series Foundation Models](https://arxiv.org/abs/2509.05801)
*Debdeep Sanyal,Aaryan Nagpal,Dhruv Kumar,Murari Mandal,Saurabh Deshpande*

Main category: cs.LG

TL;DR: 通过激活移植技术，研究发现基础模型内部存在可操控的语义表征空间，能够模拟市场异常事件并进行压力测试


<details>
  <summary>Details</summary>
Motivation: 探索变换器基础模型是否真正理解市场治理等语义概念，以及能否利用其内部表征模拟稀罕但高风险的市场崩盘事件

Method: 提出激活移植技术，通过将某事件（如历史崩盘）的统计矩征强制注入到另一事件（如平静期）的隐藏状态中，实现因果干预

Result: 注入崩盘语义会导致下跌预测，注入平静语义则可压制崩盘预测；模型编码了事件严重程度的等级概念，隐含向量模长与系统性冲击强度直接相关

Conclusion: 大型时间序列变换器具有可操控、语义基础的表征能力，存在经营模型预测的隐含概念空间，为战略性压力测试提供了语义"假如"分析能力

Abstract: While transformer-based foundation models excel at forecasting routine
patterns, two questions remain: do they internalize semantic concepts such as
market regimes, or merely fit curves? And can their internal representations be
leveraged to simulate rare, high-stakes events such as market crashes? To
investigate this, we introduce activation transplantation, a causal
intervention that manipulates hidden states by imposing the statistical moments
of one event (e.g., a historical crash) onto another (e.g., a calm period)
during the forward pass. This procedure deterministically steers forecasts:
injecting crash semantics induces downturn predictions, while injecting calm
semantics suppresses crashes and restores stability. Beyond binary control, we
find that models encode a graded notion of event severity, with the latent
vector norm directly correlating with the magnitude of systemic shocks.
Validated across two architecturally distinct TSFMs, Toto (decoder only) and
Chronos (encoder-decoder), our results demonstrate that steerable, semantically
grounded representations are a robust property of large time series
transformers. Our findings provide evidence for a latent concept space that
governs model predictions, shifting interpretability from post-hoc attribution
to direct causal intervention, and enabling semantic "what-if" analysis for
strategic stress-testing.

</details>


### [55] [Simple Optimizers for Convex Aligned Multi-Objective Optimization](https://arxiv.org/abs/2509.05811)
*Ben Kretzu,Karen Ullrich,Yonathan Efroni*

Main category: cs.LG

TL;DR: 本文放宽了AMOO框架的强凸性假设，在更符合深度学习实践的凸性、平滑性和Lipschitz连续性条件下，提出了可扩展的梯度下降算法并建立了收敛保证。


<details>
  <summary>Details</summary>
Motivation: 现有AMOO分析依赖强凸性假设，这与深度学习实践不符。需要研究在更一般的凸性条件下AMOO的收敛性。

Method: 开发新的分析工具和度量指标，提出可扩展的凸AMOO梯度下降算法。

Result: 建立了算法在标准平滑性或Lipschitz连续性条件下的收敛保证，证明了朴素等权重方法的次优性。

Conclusion: 成功将AMOO框架扩展到更一般的凸性条件，为深度学习实践提供了理论支持和新算法。

Abstract: It is widely recognized in modern machine learning practice that access to a
diverse set of tasks can enhance performance across those tasks. This
observation suggests that, unlike in general multi-objective optimization, the
objectives in many real-world settings may not be inherently conflicting. To
address this, prior work introduced the Aligned Multi-Objective Optimization
(AMOO) framework and proposed gradient-based algorithms with provable
convergence guarantees. However, existing analysis relies on strong
assumptions, particularly strong convexity, which implies the existence of a
unique optimal solution. In this work, we relax this assumption and study
gradient-descent algorithms for convex AMOO under standard smoothness or
Lipschitz continuity conditions-assumptions more consistent with those used in
deep learning practice. This generalization requires new analytical tools and
metrics to characterize convergence in the convex AMOO setting. We develop such
tools, propose scalable algorithms for convex AMOO, and establish their
convergence guarantees. Additionally, we prove a novel lower bound that
demonstrates the suboptimality of naive equal-weight approaches compared to our
methods.

</details>


### [56] [Performance of Conformal Prediction in Capturing Aleatoric Uncertainty](https://arxiv.org/abs/2509.05826)
*Misgina Tsighe Hagos,Claes Lundström*

Main category: cs.LG

TL;DR: 本文研究发现共形预测方法在捕捉数据固有歧义（偶然不确定性）方面效果有限，预测集大小与人类标注者标注的标签数量相关性很弱


<details>
  <summary>Details</summary>
Motivation: 验证共形预测方法是否能有效量化数据集中的偶然不确定性，特别是由类别重叠引起的固有歧义

Method: 使用三种共形预测方法为八个深度学习模型生成预测集，在四个包含多人标注的数据集上测量预测集大小与人类标注标签数量的相关性

Result: 绝大多数共形预测输出与人类标注呈现非常弱到弱的相关性，只有少数呈现中等相关性

Conclusion: 虽然共形预测可以提供更高的真实类别覆盖率，但其捕捉偶然不确定性的能力仍然有限，需要重新评估其生成的预测集

Abstract: Conformal prediction is a model-agnostic approach to generating prediction
sets that cover the true class with a high probability. Although its prediction
set size is expected to capture aleatoric uncertainty, there is a lack of
evidence regarding its effectiveness. The literature presents that prediction
set size can upper-bound aleatoric uncertainty or that prediction sets are
larger for difficult instances and smaller for easy ones, but a validation of
this attribute of conformal predictors is missing. This work investigates how
effectively conformal predictors quantify aleatoric uncertainty, specifically
the inherent ambiguity in datasets caused by overlapping classes. We perform
this by measuring the correlation between prediction set sizes and the number
of distinct labels assigned by human annotators per instance. We further assess
the similarity between prediction sets and human-provided annotations. We use
three conformal prediction approaches to generate prediction sets for eight
deep learning models trained on four datasets. The datasets contain annotations
from multiple human annotators (ranging from five to fifty participants) per
instance, enabling the identification of class overlap. We show that the vast
majority of the conformal prediction outputs show a very weak to weak
correlation with human annotations, with only a few showing moderate
correlation. These findings underscore the necessity of critically reassessing
the prediction sets generated using conformal predictors. While they can
provide a higher coverage of the true classes, their capability in capturing
aleatoric uncertainty remains limited.

</details>


### [57] [Finetuning LLMs for Human Behavior Prediction in Social Science Experiments](https://arxiv.org/abs/2509.05830)
*Akaash Kolluri,Shengguang Wu,Joon Sung Park,Michael S. Bernstein*

Main category: cs.LG

TL;DR: 通过在大规模社会科学实验数据集SocSci210上微调LLM，显著提高了对人类行为模拟的准确性，在未见过的研究中预测准确率提升26%，优于GPT-4o 13%。


<details>
  <summary>Details</summary>
Motivation: 利用LLM模拟社会科学实验结果，但现有模型在准确模拟人类行为方面存在局限，需要通过在真实实验数据上微调来提升性能。

Method: 构建包含290万条响应、40万参与者、210个实验的SocSci210数据集，使用Qwen2.5-14B作为基础模型进行微调，开发Socrates-Qwen-14B模型。

Result: 在完全未见过的研究中，预测准确率提升26%；在新条件下泛化能力提升71%；人口统计学偏差减少10.6%；整体性能优于GPT-4o 13%。

Conclusion: 通过在社会科学实验数据上微调LLM，可以显著提升对人类行为的模拟准确性，为实验假设筛选提供更可靠的模拟工具，并减少模型偏见。

Abstract: Large language models (LLMs) offer a powerful opportunity to simulate the
results of social science experiments. In this work, we demonstrate that
finetuning LLMs directly on individual-level responses from past experiments
meaningfully improves the accuracy of such simulations across diverse social
science domains. We construct SocSci210 via an automatic pipeline, a dataset
comprising 2.9 million responses from 400,491 participants in 210 open-source
social science experiments. Through finetuning, we achieve multiple levels of
generalization. In completely unseen studies, our strongest model,
Socrates-Qwen-14B, produces predictions that are 26% more aligned with
distributions of human responses to diverse outcome questions under varying
conditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by
13%. By finetuning on a subset of conditions in a study, generalization to new
unseen conditions is particularly robust, improving by 71%. Since SocSci210
contains rich demographic information, we reduce demographic parity, a measure
of bias, by 10.6% through finetuning. Because social sciences routinely
generate rich, topic-specific datasets, our findings indicate that finetuning
on such data could enable more accurate simulations for experimental hypothesis
screening. We release our data, models and finetuning code at
stanfordhci.github.io/socrates.

</details>


### [58] [Data-Driven Stochastic Modeling Using Autoregressive Sequence Models: Translating Event Tables to Queueing Dynamics](https://arxiv.org/abs/2509.05839)
*Daksh Mittal,Shunri Zheng,Jing Dong,Hongseok Namkoong*

Main category: cs.LG

TL;DR: 使用自回归序列模型从事件流数据自动学习排队网络模型，提高建模效率和可访问性


<details>
  <summary>Details</summary>
Motivation: 传统排队网络模型需要大量人工劳动和领域专业知识，为了使这种建模方法更具可扩展性和可访问性

Method: 提出一种基于自回归序列模型的数据驱动框架，通过Transformer类架构学习事件类型和事件时间的条件分布

Result: 在多种排队网络生成的事件表上验证了框架的有效性，展示了其在模拟、不确定性量化和反事实评估中的应用

Conclusion: 该框架利用AI进步和数据可用性，向更自动化的数据驱动建模流程进行了重要提升，支持排队网络模型在服务领域的更广泛采用

Abstract: While queueing network models are powerful tools for analyzing service
systems, they traditionally require substantial human effort and domain
expertise to construct. To make this modeling approach more scalable and
accessible, we propose a data-driven framework for queueing network modeling
and simulation based on autoregressive sequence models trained on event-stream
data. Instead of explicitly specifying arrival processes, service mechanisms,
or routing logic, our approach learns the conditional distributions of event
types and event times, recasting the modeling task as a problem of sequence
distribution learning. We show that Transformer-style architectures can
effectively parameterize these distributions, enabling automated construction
of high-fidelity simulators. As a proof of concept, we validate our framework
on event tables generated from diverse queueing networks, showcasing its
utility in simulation, uncertainty quantification, and counterfactual
evaluation. Leveraging advances in artificial intelligence and the growing
availability of data, our framework takes a step toward more automated,
data-driven modeling pipelines to support broader adoption of queueing network
models across service domains.

</details>


### [59] [The Measure of Deception: An Analysis of Data Forging in Machine Unlearning](https://arxiv.org/abs/2509.05865)
*Rishabh Dixit,Yuan Hui,Rayan Saab*

Main category: cs.LG

TL;DR: 该论文研究了机器学习中的遗忘验证问题，分析了对抗性伪造数据的可能性，证明了在大多数情况下随机采样伪造点的概率极小，为检测虚假遗忘声明提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 受隐私法规和消除有害数据影响的需求驱动，机器遗忘旨在修改训练好的模型以有效"忘记"指定数据。验证遗忘的关键挑战是对抗性伪造数据，即制造模仿目标点梯度的数据，造成已遗忘的假象而实际未移除信息。

Method: 提出了一个分析框架，考虑梯度近似目标梯度在容差ε内的数据点集合（称为ε-伪造集）。对线性回归和单层神经网络，分析了该集合的Lebesgue测度；在一般正则性假设下，证明了伪造集测度随ε^(d-r)/2衰减，其中d是数据维度，r是模型梯度变分矩阵的零度。

Result: 研究表明伪造集的测度很小，按ε的量级缩放，当ε足够小时为ε^d。扩展至批量SGD和几乎处处光滑损失函数得到相同的渐近缩放。建立了概率界限，显示在非退化数据分布下，随机采样伪造点的可能性极小。

Conclusion: 对抗性伪造在根本上受到限制，虚假的遗忘声明原则上可以被检测到，这为机器遗忘系统的可靠性提供了理论保证。

Abstract: Motivated by privacy regulations and the need to mitigate the effects of
harmful data, machine unlearning seeks to modify trained models so that they
effectively ``forget'' designated data. A key challenge in verifying unlearning
is forging -- adversarially crafting data that mimics the gradient of a target
point, thereby creating the appearance of unlearning without actually removing
information. To capture this phenomenon, we consider the collection of data
points whose gradients approximate a target gradient within tolerance
$\epsilon$ -- which we call an $\epsilon$-forging set -- and develop a
framework for its analysis. For linear regression and one-layer neural
networks, we show that the Lebesgue measure of this set is small. It scales on
the order of $\epsilon$, and when $\epsilon$ is small enough, $\epsilon^d$.
More generally, under mild regularity assumptions, we prove that the forging
set measure decays as $\epsilon^{(d-r)/2}$, where $d$ is the data dimension and
$r<d$ is the nullity of a variation matrix defined by the model gradients.
Extensions to batch SGD and almost-everywhere smooth loss functions yield the
same asymptotic scaling. In addition, we establish probability bounds showing
that, under non-degenerate data distributions, the likelihood of randomly
sampling a forging point is vanishingly small. These results provide evidence
that adversarial forging is fundamentally limited and that false unlearning
claims can, in principle, be detected.

</details>


### [60] [SPINN: An Optimal Self-Supervised Physics-Informed Neural Network Framework](https://arxiv.org/abs/2509.05886)
*Reza Pirayeshshirazinezhad*

Main category: cs.LG

TL;DR: 基于机器学习的代次模型预测液态钠在微型燥液器中的对流传热系数，包括自监睦物理信息神经网络和迁移学习方法，准确度高且计算效率优于传统CFD方法。


<details>
  <summary>Details</summary>
Motivation: 高保真度的流体力学模拟(CFD)方法在模拟液态金属的激流强式对流传热时需要大量计算资源和时间，因此需要开发更高效的代替模型方法。

Method: 首先使用核基机器学习技术和浅层神经网络处理87个孔瑞数数据，然后采用自监睦物理信息神经网络(通过额外层动态调整物理损失权重)和迁移学习(将基于水的模型调整用于钠)来提升预测性能。

Result: 自监睦物理信息神经网络成功预测钠的传热率，错误范围约为±8%；仅使用物理回归时错误保持在5%-10%之间；其他机器学习方法的预测错误主要在±8%以内。

Conclusion: 机器学习基于的代次模型为液态金属冷却微型燥液器的设计和优化提供了高效的替代工具，在保持高准确性的同时显著提高了计算效率。

Abstract: A surrogate model is developed to predict the convective heat transfer
coefficient of liquid sodium (Na) flow within rectangular miniature heat sinks.
Initially, kernel-based machine learning techniques and shallow neural network
are applied to a dataset with 87 Nusselt numbers for liquid sodium in
rectangular miniature heat sinks. Subsequently, a self-supervised
physics-informed neural network and transfer learning approach are used to
increase the estimation performance. In the self-supervised physics-informed
neural network, an additional layer determines the weight the of physics in the
loss function to balance data and physics based on their uncertainty for a
better estimation. For transfer learning, a shallow neural network trained on
water is adapted for use with Na. Validation results show that the
self-supervised physics-informed neural network successfully estimate the heat
transfer rates of Na with an error margin of approximately +8%. Using only
physics for regression, the error remains between 5% to 10%. Other machine
learning methods specify the prediction mostly within +8%. High-fidelity
modeling of turbulent forced convection of liquid metals using computational
fluid dynamics (CFD) is both time-consuming and computationally expensive.
Therefore, machine learning based models offer a powerful alternative tool for
the design and optimization of liquid-metal-cooled miniature heat sinks.

</details>


### [61] [Smoothed Online Optimization for Target Tracking: Robust and Learning-Augmented Algorithms](https://arxiv.org/abs/2509.05930)
*Ali Zeynali,Mahsa Sahebdel,Qingsong Liu,Mohammad Hajiesmaili,Ramesh K. Sitaraman*

Main category: cs.LG

TL;DR: 提出了SOOTT框架，整合目标追踪、对抗扰动和切换成本三个在线决策目标，并开发了BEST鲁棒算法和CoRT学习增强算法


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中需要同时处理动态目标追踪、不可预测扰动和决策平滑性的在线决策问题，如AI集群中的弹性工作负载调度

Method: 提出SOOTT问题框架，开发BEST算法提供理论竞争保证，并设计CoRT学习增强算法整合不可信的黑盒预测

Result: 理论分析显示CoRT在预测准确时优于BEST，同时在任意预测错误下保持鲁棒性。案例研究验证了算法在负载调度中的有效性

Conclusion: SOOTT框架成功整合了多个在线决策目标，BEST和CoRT算法提供了理论保证和实践性能的平衡，适用于需要同时处理追踪、扰动和平滑性的实际应用场景

Abstract: We introduce the Smoothed Online Optimization for Target Tracking (SOOTT)
problem, a new framework that integrates three key objectives in online
decision-making under uncertainty: (1) tracking cost for following a
dynamically moving target, (2) adversarial perturbation cost for withstanding
unpredictable disturbances, and (3) switching cost for penalizing abrupt
changes in decisions. This formulation captures real-world scenarios such as
elastic and inelastic workload scheduling in AI clusters, where operators must
balance long-term service-level agreements (e.g., LLM training) against sudden
demand spikes (e.g., real-time inference). We first present BEST, a robust
algorithm with provable competitive guarantees for SOOTT. To enhance practical
performance, we introduce CoRT, a learning-augmented variant that incorporates
untrusted black-box predictions (e.g., from ML models) into its decision
process. Our theoretical analysis shows that CoRT strictly improves over BEST
when predictions are accurate, while maintaining robustness under arbitrary
prediction errors. We validate our approach through a case study on workload
scheduling, demonstrating that both algorithms effectively balance trajectory
tracking, decision smoothness, and resilience to external disturbances.

</details>


### [62] [Unified Interaction Foundational Model (UIFM) for Predicting Complex User and System Behavior](https://arxiv.org/abs/2509.06025)
*Vignesh Ethiraj,Subhash Talluri*

Main category: cs.LG

TL;DR: 提出了统一交互基础模型(UIFM)，采用复合标记化方法将多属性事件作为语义连贯单元处理，解决了传统基础模型在序列化事件时丢失关键上下文的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于自然语言的基础模型无法理解电信、电商和金融等领域中结构化交互的整体性，将事件序列化为文本会导致语义碎片化和关键上下文丢失。

Method: 采用复合标记化原则，将每个多属性事件视为单个语义连贯单元，学习用户行为的底层"语法"，感知完整的交互而非离散的数据流。

Result: 该架构不仅更准确，而且代表了创建更适应性和智能预测系统的根本性进步。

Conclusion: UIFM模型是构建真正行为理解系统的重要步骤，能够更好地理解和预测复杂、演化的序列事件。

Abstract: A central goal of artificial intelligence is to build systems that can
understand and predict complex, evolving sequences of events. However, current
foundation models, designed for natural language, fail to grasp the holistic
nature of structured interactions found in domains like telecommunications,
e-commerce and finance. By serializing events into text, they disassemble them
into semantically fragmented parts, losing critical context. In this work, we
introduce the Unified Interaction Foundation Model (UIFM), a foundation model
engineered for genuine behavioral understanding. At its core is the principle
of composite tokenization, where each multi-attribute event is treated as a
single, semantically coherent unit. This allows UIFM to learn the underlying
"grammar" of user behavior, perceiving entire interactions rather than a
disconnected stream of data points. We demonstrate that this architecture is
not just more accurate, but represents a fundamental step towards creating more
adaptable and intelligent predictive systems.

</details>


### [63] [PolicyEvolve: Evolving Programmatic Policies by LLMs for multi-player games via Population-Based Training](https://arxiv.org/abs/2509.06053)
*Mingrui Lv,Hangzhi Liu,Zhi Luo,Hongjie Zhang,Jie Ou*

Main category: cs.LG

TL;DR: PolicyEvolve是一个基于LLM的多智能体强化学习框架，通过程序化策略生成减少对人工编码的依赖，提高策略可解释性和执行效率


<details>
  <summary>Details</summary>
Motivation: 传统MARL需要大量样本和计算资源，且策略缺乏可解释性，限制了实际部署。LLM在单智能体任务中成功生成可解释的程序化策略，启发将其扩展到多玩家游戏

Method: 包含四个模块：全局池（保存精英策略）、局部池（临时策略）、策略规划器（核心生成模块，采样精英策略并基于环境信息生成初始策略）、轨迹评判器（分析交互数据，识别漏洞并提供改进方向）。通过迭代优化过程生成高性能策略

Result: 显著减少对人工编写策略代码的依赖，以最少的环境交互实现高性能策略，提高策略的可解释性和执行效率

Conclusion: PolicyEvolve为多玩家游戏提供了一种高效的程序化策略生成框架，通过结合LLM能力和迭代优化过程，解决了传统MARL方法的计算资源需求和可解释性问题

Abstract: Multi-agent reinforcement learning (MARL) has achieved significant progress
in solving complex multi-player games through self-play. However, training
effective adversarial policies requires millions of experience samples and
substantial computational resources. Moreover, these policies lack
interpretability, hindering their practical deployment. Recently, researchers
have successfully leveraged Large Language Models (LLMs) to generate
programmatic policies for single-agent tasks, transforming neural network-based
policies into interpretable rule-based code with high execution efficiency.
Inspired by this, we propose PolicyEvolve, a general framework for generating
programmatic policies in multi-player games. PolicyEvolve significantly reduces
reliance on manually crafted policy code, achieving high-performance policies
with minimal environmental interactions. The framework comprises four modules:
Global Pool, Local Pool, Policy Planner, and Trajectory Critic. The Global Pool
preserves elite policies accumulated during iterative training. The Local Pool
stores temporary policies for the current iteration; only sufficiently
high-performing policies from this pool are promoted to the Global Pool. The
Policy Planner serves as the core policy generation module. It samples the top
three policies from the Global Pool, generates an initial policy for the
current iteration based on environmental information, and refines this policy
using feedback from the Trajectory Critic. Refined policies are then deposited
into the Local Pool. This iterative process continues until the policy achieves
a sufficiently high average win rate against the Global Pool, at which point it
is integrated into the Global Pool. The Trajectory Critic analyzes interaction
data from the current policy, identifies vulnerabilities, and proposes
directional improvements to guide the Policy Planner

</details>


### [64] [A novel biomass fluidized bed gasification model coupled with machine learning and CFD simulation](https://arxiv.org/abs/2509.06056)
*Chun Wang*

Main category: cs.LG

TL;DR: 提出基于机器学习和计算流体力学的生物质流化床气化耦合模型，提高复杂热化学反应过程的预测精度和计算效率


<details>
  <summary>Details</summary>
Motivation: 提高生物质流化床气化过程中复杂热化学反应过程的预测精度和计算效率

Method: 基于实验数据和高保真模拟结果构建高质量数据集，训练用于描述反应动力学特性的代理模型，并将其嵌入CFD框架实现反应速率和组分演化的实时更新

Result: 构建了耦合机器学习与CFD的预测模型

Conclusion: 该方法能够有效提高生物质流化床气化过程的模拟精度和计算效率

Abstract: A coupling model of biomass fluidized bed gasification based on machine
learning and computational fluid dynamics is proposed to improve the prediction
accuracy and computational efficiency of complex thermochemical reaction
process. By constructing a high-quality data set based on experimental data and
high fidelity simulation results, the agent model used to describe the
characteristics of reaction kinetics was trained and embedded into the
computational fluid dynamics (CFD) framework to realize the real-time update of
reaction rate and composition evolution.

</details>


### [65] [ARIES: Relation Assessment and Model Recommendation for Deep Time Series Forecasting](https://arxiv.org/abs/2509.06060)
*Fei Wang,Yujie Li,Zezhi Shao,Chengqing Yu,Yisong Fu,Zhulin An,Yongjun Xu,Xueqi Cheng*

Main category: cs.LG

TL;DR: ARIES框架通过构建包含多种时间序列模式的合成数据集，建立了时间序列属性与深度学习预测模型性能之间的关联，并开发了首个可解释的深度预测模型推荐系统。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测基准数据集缺乏多样化的时间模式，无法系统评估模型性能与数据属性之间的关系，且没有有效的模型推荐方法，导致测试不同架构时耗费大量时间和成本。

Method: 1) 构建具有多种明确时间模式的合成数据集；2) 设计全面的时间序列属性计算系统；3) 对50多个预测模型进行广泛基准测试；4) 建立时间序列属性与建模策略之间的关系

Result: 实验结果显示时间序列属性与建模策略之间存在明显相关性，基于这些发现开发了首个深度预测模型推荐器，能够为现实世界时间序列提供可解释的建议

Conclusion: ARIES是首个建立时间序列数据属性与建模策略之间关系的研究，同时实现了模型推荐系统，为时间序列预测领域提供了系统化的评估和推荐框架

Abstract: Recent advancements in deep learning models for time series forecasting have
been significant. These models often leverage fundamental time series
properties such as seasonality and non-stationarity, which may suggest an
intrinsic link between model performance and data properties. However, existing
benchmark datasets fail to offer diverse and well-defined temporal patterns,
restricting the systematic evaluation of such connections. Additionally, there
is no effective model recommendation approach, leading to high time and cost
expenditures when testing different architectures across different downstream
applications. For those reasons, we propose ARIES, a framework for assessing
relation between time series properties and modeling strategies, and for
recommending deep forcasting models for realistic time series. First, we
construct a synthetic dataset with multiple distinct patterns, and design a
comprehensive system to compute the properties of time series. Next, we conduct
an extensive benchmarking of over 50 forecasting models, and establish the
relationship between time series properties and modeling strategies. Our
experimental results reveal a clear correlation. Based on these findings, we
propose the first deep forecasting model recommender, capable of providing
interpretable suggestions for real-world time series. In summary, ARIES is the
first study to establish the relations between the properties of time series
data and modeling strategies, while also implementing a model recommendation
system. The code is available at: https://github.com/blisky-li/ARIES.

</details>


### [66] [A Surrogate model for High Temperature Superconducting Magnets to Predict Current Distribution with Neural Network](https://arxiv.org/abs/2509.06067)
*Mianjun Xiao,Peng Song,Yulong Liu,Cedric Korte,Ziyang Xu,Jiale Gao,Jiaqi Lu,Haoyang Nie,Qiantong Deng,Timing Qu*

Main category: cs.LG

TL;DR: 基于殊余神经网络的代模型提高高温超导磁铁计算效率，减少有限元法计算成本


<details>
  <summary>Details</summary>
Motivation: 传统有限元法在大规模REBCO超导磁铁计算中计算成本高、耗时长，限制了磁铁设计的速度

Method: 开发基于全连接殊余神经网络(FCRN)的代模型，预测REBCO螺线管中的空间-时间电流密度分布，使用不同圆盘数和圈数的FEM模拟数据进行训练

Result: 12个殊余块和每层256个神经元的网络配置表现最佳，能够外推50%的训练范围且最大误差低于10%，预测速度比FEM快几个数量级

Conclusion: 提出的FCRN代模型具有准确性和高效性，为大规模HTS磁铁的快速分析提供了有前景的工具

Abstract: Finite element method (FEM) is widely used in high-temperature
superconducting (HTS) magnets, but its computational cost increases with magnet
size and becomes time-consuming for meter-scale magnets, especially when
multi-physics couplings are considered, which limits the fast design of
large-scale REBCO magnet systems. In this work, a surrogate model based on a
fully connected residual neural network (FCRN) is developed to predict the
space-time current density distribution in REBCO solenoids. Training datasets
were generated from FEM simulations with varying numbers of turns and pancakes.
The results demonstrate that, for deeper networks, the FCRN architecture
achieves better convergence than conventional fully connected network (FCN),
with the configuration of 12 residual blocks and 256 neurons per layer
providing the most favorable balance between training accuracy and
generalization capability. Extrapolation studies show that the model can
reliably predict magnetization losses for up to 50% beyond the training range,
with maximum errors below 10%. The surrogate model achieves predictions several
orders of magnitude faster than FEM and still remains advantageous when
training costs are included. These results indicate that the proposed
FCRN-based surrogate model provides both accuracy and efficiency, offering a
promising tool for the rapid analysis of large-scale HTS magnets.

</details>


### [67] [Teaching Precommitted Agents: Model-Free Policy Evaluation and Control in Quasi-Hyperbolic Discounted MDPs](https://arxiv.org/abs/2509.06094)
*S. R. Eshwar*

Main category: cs.LG

TL;DR: 本文针对具有准双曲(QH)折扣偏好的预先承诺智能体，提出了理论分析和实用算法，证明了最优策略可简化为简单的一步非平稳形式，并设计了首个具有可证明收敛保证的模型无关算法。


<details>
  <summary>Details</summary>
Motivation: 准双曲折扣是人类和动物决策中时间不一致偏好的重要模型，但将其整合到强化学习框架中的研究有限，存在理论和算法上的空白需要填补。

Method: 通过理论分析形式化描述最优策略结构，证明其可简化为一步非平稳形式；设计模型无关的策略评估和Q学习算法，并提供收敛性证明。

Result: 首次证明了QH偏好下最优策略的简单结构特征，开发了首个实用的模型无关算法，为QH偏好融入RL提供了理论基础和算法工具。

Conclusion: 该研究为准双曲折扣偏好融入强化学习提供了重要的理论见解和实用算法基础，填补了该领域的关键空白。

Abstract: Time-inconsistent preferences, where agents favor smaller-sooner over
larger-later rewards, are a key feature of human and animal decision-making.
Quasi-Hyperbolic (QH) discounting provides a simple yet powerful model for this
behavior, but its integration into the reinforcement learning (RL) framework
has been limited. This paper addresses key theoretical and algorithmic gaps for
precommitted agents with QH preferences. We make two primary contributions: (i)
we formally characterize the structure of the optimal policy, proving for the
first time that it reduces to a simple one-step non-stationary form; and (ii)
we design the first practical, model-free algorithms for both policy evaluation
and Q-learning in this setting, both with provable convergence guarantees. Our
results provide foundational insights for incorporating QH preferences in RL.

</details>


### [68] [If generative AI is the answer, what is the question?](https://arxiv.org/abs/2509.06120)
*Ambuj Tewari*

Main category: cs.LG

TL;DR: 本文探讨生成式AI的理论基础，将生成视为与预测、压缩和决策相关的独立机器学习任务，系统综述了五大生成模型家族，并提出了概率和博弈论框架来理解生成过程。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI从文本图像扩展到音频、视频、代码和分子等领域，需要从理论层面深入理解生成作为机器学习任务的核心本质，而不仅仅是关注具体模型实现。

Method: 采用任务优先的框架，系统调查了自回归模型、变分自编码器、标准化流、生成对抗网络和扩散模型五大模型家族，提出了区分密度估计和生成的概率框架，以及基于双玩家对抗-学习者设置的博弈论框架。

Result: 建立了生成式AI的系统性理论框架，明确了生成任务的核心特征，并讨论了后训练修改和社会责任生成等重要议题。

Conclusion: 生成式AI需要从任务本质的角度进行理解，而不仅仅是技术实现，同时必须重视隐私保护、AI内容检测和版权等社会责任问题。

Abstract: Beginning with text and images, generative AI has expanded to audio, video,
computer code, and molecules. Yet, if generative AI is the answer, what is the
question? We explore the foundations of generation as a distinct machine
learning task with connections to prediction, compression, and decision-making.
We survey five major generative model families: autoregressive models,
variational autoencoders, normalizing flows, generative adversarial networks,
and diffusion models. We then introduce a probabilistic framework that
emphasizes the distinction between density estimation and generation. We review
a game-theoretic framework with a two-player adversary-learner setup to study
generation. We discuss post-training modifications that prepare generative
models for deployment. We end by highlighting some important topics in socially
responsible generation such as privacy, detection of AI-generated content, and
copyright and IP. We adopt a task-first framing of generation, focusing on what
generation is as a machine learning problem, rather than only on how models
implement it.

</details>


### [69] [Data-Efficient Time-Dependent PDE Surrogates: Graph Neural Simulators vs Neural Operators](https://arxiv.org/abs/2509.06154)
*Dibyajyoti Nayak,Somdatta Goswami*

Main category: cs.LG

TL;DR: 该论文提出了一种基于图神经网络模拟器(GNS)的框架，通过显式数值时间步进方案来学习PDE解，显著提高了数据效率并减少了误差积累。


<details>
  <summary>Details</summary>
Motivation: 神经算子需要大量数据集且在训练数据稀缺时表现不佳，许多神经算子公式没有显式编码物理演化的因果、局部时间结构。自回归模型虽然保持因果性但存在快速误差积累问题。

Method: 采用图神经网络模拟器(GNS)框架，结合显式数值时间步进方案，通过建模瞬时时间导数来学习PDE解。引入了PCA+KMeans轨迹选择策略来增强低数据性能。

Result: 在三个典型PDE系统上，GNS仅使用30个训练样本(占总数据3%)就实现了低于1%的相对L2误差。相比FNO AR和DON AR，GNS分别减少了82.48%和99.86%的自回归误差。

Conclusion: 将基于图的局部归纳偏置与传统时间积分器相结合，可以产生准确、物理一致且可扩展的时变PDE代理模型。

Abstract: Neural operators (NOs) approximate mappings between infinite-dimensional
function spaces but require large datasets and struggle with scarce training
data. Many NO formulations don't explicitly encode causal, local-in-time
structure of physical evolution. While autoregressive models preserve causality
by predicting next time-steps, they suffer from rapid error accumulation. We
employ Graph Neural Simulators (GNS) - a message-passing graph neural network
framework - with explicit numerical time-stepping schemes to construct accurate
forward models that learn PDE solutions by modeling instantaneous time
derivatives. We evaluate our framework on three canonical PDE systems: (1) 2D
Burgers' scalar equation, (2) 2D coupled Burgers' vector equation, and (3) 2D
Allen-Cahn equation. Rigorous evaluations demonstrate GNS significantly
improves data efficiency, achieving higher generalization accuracy with
substantially fewer training trajectories compared to neural operator baselines
like DeepONet and FNO. GNS consistently achieves under 1% relative L2 errors
with only 30 training samples out of 1000 (3% of available data) across all
three PDE systems. It substantially reduces error accumulation over extended
temporal horizons: averaged across all cases, GNS reduces autoregressive error
by 82.48% relative to FNO AR and 99.86% relative to DON AR. We introduce a
PCA+KMeans trajectory selection strategy enhancing low-data performance.
Results indicate combining graph-based local inductive biases with conventional
time integrators yields accurate, physically consistent, and scalable surrogate
models for time-dependent PDEs.

</details>


### [70] [Tracking daily paths in home contexts with RSSI fingerprinting based on UWB through deep learning models](https://arxiv.org/abs/2509.06161)
*Aurora Polo-Rodríguez,Juan Carlos Valera,Jesús Peral,David Gil,Javier Medina-Quero*

Main category: cs.LG

TL;DR: 本研究探讨了使用超宽带(UWB)技术和深度学习模型在家庭环境中追踪居住者路径的方法，通过RSSI指纹识别方法解决了墙壁和障碍物对定位精度的影响，混合CNN+LSTM模型取得了最佳效果，平均绝对误差接近50厘米。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备技术的发展，特别是个人设备的进步，人类活动识别领域得到了显著发展。UWB技术通过飞行时间和到达时间差方法估计用户位置，但在真实环境中受到墙壁和障碍物的显著影响，降低了定位精度。

Method: 提出基于指纹识别的方法，利用从两个公寓(60平方米和100平方米)中收集的RSSI数据，比较了CNN、LSTM和混合CNN+LSTM模型的性能，同时评估了蓝牙技术以及不同类型和时间长度的时序窗口(未来、过去或两者组合)的影响。

Result: 研究结果显示平均绝对误差接近50厘米，突出了混合模型在提供准确位置估计方面的优越性。

Conclusion: 混合CNN+LSTM模型在住宅环境中为日常人类活动识别应用提供了准确的位置估计能力，促进了该技术在智能家居环境中的实际应用。

Abstract: The field of human activity recognition has evolved significantly, driven
largely by advancements in Internet of Things (IoT) device technology,
particularly in personal devices. This study investigates the use of
ultra-wideband (UWB) technology for tracking inhabitant paths in home
environments using deep learning models. UWB technology estimates user
locations via time-of-flight and time-difference-of-arrival methods, which are
significantly affected by the presence of walls and obstacles in real
environments, reducing their precision. To address these challenges, we propose
a fingerprinting-based approach utilizing received signal strength indicator
(RSSI) data collected from inhabitants in two flats (60 m2 and 100 m2) while
performing daily activities. We compare the performance of convolutional neural
network (CNN), long short-term memory (LSTM), and hybrid CNN+LSTM models, as
well as the use of Bluetooth technology. Additionally, we evaluate the impact
of the type and duration of the temporal window (future, past, or a combination
of both). Our results demonstrate a mean absolute error close to 50 cm,
highlighting the superiority of the hybrid model in providing accurate location
estimates, thus facilitating its application in daily human activity
recognition in residential settings.

</details>


### [71] [An Improved Template for Approximate Computing](https://arxiv.org/abs/2509.06162)
*M. Rezaalipour,F. Costa,M. Biasion,R. Otoni,G. A. Constantinides,L. Pozzi*

Main category: cs.LG

TL;DR: 通过提出一种基于可参数化乘积共享模板的新方法，在保持精度的同时较现有技术更大幅度减小神经网络计算器件面积


<details>
  <summary>Details</summary>
Motivation: 在边缘设处上部署神经网络时，需要在能耗与准确性之间取得平衡，通过近似计算技术减少运算器的能耗

Method: 改进现有的XPAT布尔重写技术，提出一种基于可参数化乘积共享模板的新方法，将模板参数作为面积指标的代理

Result: 实验证明该方法能够更好地收敛到低面积解决方案，且较原始XPAT和其他两种现有技术能找到更好的近似解

Conclusion: 该研究提供了一种有效的方法来在神经网络计算中实现面积优化，为边缘设处的能耗优化提供了新的解决方案

Abstract: Deploying neural networks on edge devices entails a careful balance between
the energy required for inference and the accuracy of the resulting
classification. One technique for navigating this tradeoff is approximate
computing: the process of reducing energy consumption by slightly reducing the
accuracy of arithmetic operators. In this context, we propose a methodology to
reduce the area of the small arithmetic operators used in neural networks -
i.e., adders and multipliers - via a small loss in accuracy, and show that we
improve area savings for the same accuracy loss w.r.t. the state of the art. To
achieve our goal, we improve on a boolean rewriting technique recently
proposed, called XPAT, where the use of a parametrisable template to rewrite
circuits has proved to be highly beneficial. In particular, XPAT was able to
produce smaller circuits than comparable approaches while utilising a naive sum
of products template structure. In this work, we show that template parameters
can act as proxies for chosen metrics and we propose a novel template based on
parametrisable product sharing that acts as a close proxy to synthesised area.
We demonstrate experimentally that our methodology converges better to low-area
solutions and that it can find better approximations than both the original
XPAT and two other state-of-the-art approaches.

</details>


### [72] [Exploring Urban Factors with Autoencoders: Relationship Between Static and Dynamic Features](https://arxiv.org/abs/2509.06167)
*Ximena Pocco,Waqar Hassan,Karelia Salinas,Vladimir Molchanov,Luis G. Nonato*

Main category: cs.LG

TL;DR: 这篇论文研究了融合异质多模态城市数据的潜在表征在发现城市模式方面的效果，通过可视化框架评估融合表征与独立表征的优势


<details>
  <summary>Details</summary>
Motivation: 城市分析中的数据具有粒度细、异质性和多模态性特征，现有可视化工具少有探索融合数据是否能涉比独立数据源提供更深入的见解

Method: 开发了一个可视化辅助框架，用于分析融合潜在数据表征与独立表征在提取动态和静态城市数据模式方面的效果对比

Result: 分析显示结合的潜在表征能够产生更结构化的模式，而独立表征在特定情况下也有其用途

Conclusion: 融合异质多模态数据的潜在表征能够提供更好的城市模式发现能力，但不同的表征方式在不同场景下各有优势

Abstract: Urban analytics utilizes extensive datasets with diverse urban information to
simulate, predict trends, and uncover complex patterns within cities. While
these data enables advanced analysis, it also presents challenges due to its
granularity, heterogeneity, and multimodality. To address these challenges,
visual analytics tools have been developed to support the exploration of latent
representations of fused heterogeneous and multimodal data, discretized at a
street-level of detail. However, visualization-assisted tools seldom explore
the extent to which fused data can offer deeper insights than examining each
data source independently within an integrated visualization framework. In this
work, we developed a visualization-assisted framework to analyze whether fused
latent data representations are more effective than separate representations in
uncovering patterns from dynamic and static urban data. The analysis reveals
that combined latent representations produce more structured patterns, while
separate ones are useful in particular cases.

</details>


### [73] [Reasoning Language Model for Personalized Lung Cancer Screening](https://arxiv.org/abs/2509.06169)
*Chuang Niu,Ge Wang*

Main category: cs.LG

TL;DR: 提出基于推理语言模型(RLM)的个性化肺癌风险评估方法，整合影像学发现和纵向医疗记录，显著提升风险预测性能


<details>
  <summary>Details</summary>
Motivation: 现有Lung-RADS系统仅基于肺结节特征进行风险分层，缺乏对其他风险因素的整合，在敏感性和特异性之间存在权衡限制

Method: 构建推理语言模型(RLM)，通过数据集构建与蒸馏、监督微调、强化学习和综合评估的系统研究，将风险评估任务分解为子组件并整合多种风险因素

Result: 在国家肺癌筛查试验数据集上显著提升风险预测性能，模型能够分析不同风险因素的贡献并通过数据驱动系统方程计算最终风险评分

Conclusion: 该方法通过思维链推理过程提高了预测准确性和可监控性，有助于肺癌筛查的临床转化应用

Abstract: Accurate risk assessment in lung cancer screening is critical for enabling
early cancer detection and minimizing unnecessary invasive procedures. The Lung
CT Screening Reporting and Data System (Lung-RADS) has been widely used as the
standard framework for patient management and follow-up. Nevertheless,
Lung-RADS faces trade-offs between sensitivity and specificity, as it
stratifies risk solely based on lung nodule characteristics without
incorporating various risk factors. Here we propose a reasoning language model
(RLM) to integrate radiology findings with longitudinal medical records for
individualized lung cancer risk assessment. Through a systematic study
including dataset construction and distillation, supervised fine-tuning,
reinforcement learning, and comprehensive evaluation, our model makes
significant improvements in risk prediction performance on datasets in the
national lung screening trial. Notably, RLM can decompose the risk evaluation
task into sub-components, analyze the contributions of diverse risk factors,
and synthesize them into a final risk score computed using our data-driven
system equation. Our approach improves both predictive accuracy and
monitorability through the chain of thought reasoning process, thereby
facilitating clinical translation into lung cancer screening.

</details>


### [74] [Toward a Metrology for Artificial Intelligence: Hidden-Rule Environments and Reinforcement Learning](https://arxiv.org/abs/2509.06213)
*Christo Mathew,Wentian Wang,Lazaros Gallos,Paul Kantor,Vladimir Menkov,Hao Wang*

Main category: cs.LG

TL;DR: 研究在Game Of Hidden Rules环境中使用Transformer-based A2C算法，比较特征中心(FC)和对象中心(OC)两种状态表示策略，让智能体通过部分观察推断隐藏规则并学习最优策略


<details>
  <summary>Details</summary>
Motivation: 探索在复杂谜题环境中，智能体如何通过部分观察同时推断隐藏规则和学习最优策略，研究不同状态表示策略对学习效率的影响

Method: 使用Transformer-based Advantage Actor-Critic (A2C)算法，比较Feature-Centric和Object-Centric两种状态表示策略，在6×6棋盘环境中进行训练和评估

Result: 在多种基于规则和试验列表的实验设置中评估模型性能，分析迁移效果和表示策略对学习效率的影响

Conclusion: 不同状态表示策略在Game Of Hidden Rules环境中对智能体的规则推断和学习效率有重要影响，为复杂环境中的强化学习提供了有价值的见解

Abstract: We investigate reinforcement learning in the Game Of Hidden Rules (GOHR)
environment, a complex puzzle in which an agent must infer and execute hidden
rules to clear a 6$\times$6 board by placing game pieces into buckets. We
explore two state representation strategies, namely Feature-Centric (FC) and
Object-Centric (OC), and employ a Transformer-based Advantage Actor-Critic
(A2C) algorithm for training. The agent has access only to partial observations
and must simultaneously infer the governing rule and learn the optimal policy
through experience. We evaluate our models across multiple rule-based and
trial-list-based experimental setups, analyzing transfer effects and the impact
of representation on learning efficiency.

</details>


### [75] [Metric Embedding Initialization-Based Differentially Private and Explainable Graph Clustering](https://arxiv.org/abs/2509.06214)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: 基于度量嵌入初始化的差分隐私可解释图聚类方法，通过SDP优化和HST初始化提高效率和性能


<details>
  <summary>Details</summary>
Motivation: 当前差分隐私图聚类面临噪声高、效率低、可解释性差等挑战，严重制约了该领域发展

Method: 构建SDP优化模型，提取关键集，使用HST基础的初始化方法获得良好聚类配置，然后应用k-median聚类策略

Result: 在公开数据集上的实验表明，该框架在各种聚类指标上都超过现有方法，同时严格保证隐私性

Conclusion: 该方法有效解决了差分隐私图聚类中的噪声、效率和可解释性问题，为该领域提供了一种高效且可解释的解决方案

Abstract: Graph clustering under the framework of differential privacy, which aims to
process graph-structured data while protecting individual privacy, has been
receiving increasing attention. Despite significant achievements in current
research, challenges such as high noise, low efficiency and poor
interpretability continue to severely constrain the development of this field.
In this paper, we construct a differentially private and interpretable graph
clustering approach based on metric embedding initialization. Specifically, we
construct an SDP optimization, extract the key set and provide a
well-initialized clustering configuration using an HST-based initialization
method. Subsequently, we apply an established k-median clustering strategy to
derive the cluster results and offer comparative explanations for the query set
through differences from the cluster centers. Extensive experiments on public
datasets demonstrate that our proposed framework outperforms existing methods
in various clustering metrics while strictly ensuring privacy.

</details>


### [76] [UrbanMIMOMap: A Ray-Traced MIMO CSI Dataset with Precoding-Aware Maps and Benchmarks](https://arxiv.org/abs/2509.06270)
*Honggang Jia,Xiucheng Wang,Nan Cheng,Ruijin Sun,Changle Li*

Main category: cs.LG

TL;DR: 这篇论文提供了UrbanMIMOMap数据集，为6G系统的环境感知通信提供大规模、高精度的MIMO通道状态信息数据支持


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集主要集中在SISO系统和路径损耗信息，无法满足6G高级MIMO系统对详细通道状态信息(CSI)的需求

Method: 使用高精度光线追踪技术生成大规模城市MIMO CSI数据集，提供了在密集空间网格上的全面复数CSI矩阵

Result: 构建了UrbanMIMOMap数据集，超越传统路径损耗数据，为高保真度无线电地图构建和深度学习研究提供了基础资源

Conclusion: 该工作为高精度无线电地图生成、MIMO空间性能和6G环境感知的机器学习研究提供了关键数据集和参考标准

Abstract: Sixth generation (6G) systems require environment-aware communication, driven
by native artificial intelligence (AI) and integrated sensing and communication
(ISAC). Radio maps (RMs), providing spatially continuous channel information,
are key enablers. However, generating high-fidelity RM ground truth via
electromagnetic (EM) simulations is computationally intensive, motivating
machine learning (ML)-based RM construction. The effectiveness of these
data-driven methods depends on large-scale, high-quality training data. Current
public datasets often focus on single-input single-output (SISO) and limited
information, such as path loss, which is insufficient for advanced multi-input
multi-output (MIMO) systems requiring detailed channel state information (CSI).
To address this gap, this paper presents UrbanMIMOMap, a novel large-scale
urban MIMO CSI dataset generated using high-precision ray tracing. UrbanMIMOMap
offers comprehensive complex CSI matrices across a dense spatial grid, going
beyond traditional path loss data. This rich CSI is vital for constructing
high-fidelity RMs and serves as a fundamental resource for data-driven RM
generation, including deep learning. We demonstrate the dataset's utility
through baseline performance evaluations of representative ML methods for RM
construction. This work provides a crucial dataset and reference for research
in high-precision RM generation, MIMO spatial performance, and ML for 6G
environment awareness. The code and data for this work are available at:
https://github.com/UNIC-Lab/UrbanMIMOMap.

</details>


### [77] [IPR: Intelligent Prompt Routing with User-Controlled Quality-Cost Trade-offs](https://arxiv.org/abs/2509.06274)
*Aosong Feng,Zhichao Xu,Xian Wu,Kang Zhou,Sheng Guan,Yueyan Chen,Ninad Kulkarni,Yun Zhou,Balasubramaniam Srinivasan,Haibo Ding,Lin Lee Cheong*

Main category: cs.LG

TL;DR: IPR是一个智能提示路由框架，通过预测响应质量和用户容忍度动态选择最优LLM，在保持质量的同时显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 解决在大规模商业系统中路由查询到最具成本效益的LLM同时保持响应质量的基本挑战，优化性能-成本权衡。

Method: 采用模块化架构，包含轻量级质量估计器（基于150万标注提示训练）、用户控制的路由机制（容忍度参数τ）和可扩展设计（冻结编码器+模型特定适配器）。

Result: 在主要云平台部署中实现43.9%的成本降低，同时保持与Claude家族最强模型的质量相当，请求处理延迟低于150ms。

Conclusion: IPR框架有效解决了LLM路由的成本-质量权衡问题，提供了可扩展且高效的解决方案，显著降低了系统运营成本。

Abstract: Routing incoming queries to the most cost-effective LLM while maintaining
response quality poses a fundamental challenge in optimizing performance-cost
trade-offs for large-scale commercial systems. We present IPR\, a
quality-constrained Intelligent Prompt Routing framework that dynamically
selects optimal models based on predicted response quality and user-specified
tolerance levels. IPR introduces three key innovations: (1) a modular
architecture with lightweight quality estimators trained on 1.5M prompts
annotated with calibrated quality scores, enabling fine-grained quality
prediction across model families; (2) a user-controlled routing mechanism with
tolerance parameter $\tau \in [0,1]$ that provides explicit control over
quality-cost trade-offs; and (3) an extensible design using frozen encoders
with model-specific adapters, reducing new model integration from days to
hours. To rigorously train and evaluate IPR, we curate an industrial-level
dataset IPRBench\footnote{IPRBench will be released upon legal approval.}, a
comprehensive benchmark containing 1.5 million examples with response quality
annotations across 11 LLM candidates. Deployed on a major cloud platform, IPR
achieves 43.9\% cost reduction while maintaining quality parity with the
strongest model in the Claude family and processes requests with sub-150ms
latency.

</details>


### [78] [RecMind: LLM-Enhanced Graph Neural Networks for Personalized Consumer Recommendations](https://arxiv.org/abs/2509.06286)
*Chang Xue,Youwei Lu,Chen Yang,Jinming Xing*

Main category: cs.LG

TL;DR: RecMind是一种LLM增强的图推荐系统，将语言模型作为偏好先验而非单一排序器，通过对比学习对齐文本和协作视图，在冷启动和长尾场景中表现优异


<details>
  <summary>Details</summary>
Motivation: 个性化推荐面临稀疏交互、快速内容更新和异构文本信号的挑战，需要有效融合语言模型和图结构信息

Method: 使用冻结LLM加轻量适配器生成文本条件嵌入，LightGCN学习协作嵌入，通过对称对比目标对齐两个视图，采用层内门控融合

Result: 在Yelp和Amazon-Electronics数据集上所有8个指标均取得最佳结果，相对改进最高达+4.53%(Recall@40)和+4.01%(NDCG@40)

Conclusion: 跨视图对齐和门控融合策略对于推荐系统性能提升至关重要，LLM作为偏好先验比作为单一排序器更有效

Abstract: Personalization is a core capability across consumer technologies, streaming,
shopping, wearables, and voice, yet it remains challenged by sparse
interactions, fast content churn, and heterogeneous textual signals. We present
RecMind, an LLM-enhanced graph recommender that treats the language model as a
preference prior rather than a monolithic ranker. A frozen LLM equipped with
lightweight adapters produces text-conditioned user/item embeddings from
titles, attributes, and reviews; a LightGCN backbone learns collaborative
embeddings from the user-item graph. We align the two views with a symmetric
contrastive objective and fuse them via intra-layer gating, allowing language
to dominate in cold/long-tail regimes and graph structure to stabilize rankings
elsewhere. On Yelp and Amazon-Electronics, RecMind attains the best results on
all eight reported metrics, with relative improvements up to +4.53\%
(Recall@40) and +4.01\% (NDCG@40) over strong baselines. Ablations confirm both
the necessity of cross-view alignment and the advantage of gating over late
fusion and LLM-only variants.

</details>


### [79] [A Spatio-Temporal Graph Neural Networks Approach for Predicting Silent Data Corruption inducing Circuit-Level Faults](https://arxiv.org/abs/2509.06289)
*Shaoqi Wei,Senling Wang,Hiroshi Kai,Yoshinobu Higami,Ruijun Ma,Tianming Ni,Xiaoqing Wen,Hiroshi Takahashi*

Main category: cs.LG

TL;DR: 提出基于时空图卷积网络的快速故障影响概率预测方法，大幅减少仿真时间10倍以上，同时保持高精度，支持大规模时序电路的定量风险评估。


<details>
  <summary>Details</summary>
Motivation: 静默数据错误(SDEs)会损害安全关键系统，功能测试检测相关故障但仿真成本高昂，需要快速准确的长期故障影响预测方法。

Method: 将门级网表建模为时空图以捕捉拓扑和时序信息，使用专用的空间和时间编码器高效预测多周期故障影响概率。

Result: 在ISCAS-89基准测试中，方法减少仿真时间10倍以上，5周期预测的平均绝对误差为0.024，保持高精度。

Conclusion: 该方法可扩展至SoC级测试策略优化，适合下游电子设计自动化流程，通过预测FIPs选择观测点能更好检测长周期难测故障。

Abstract: Silent Data Errors (SDEs) from time-zero defects and aging degrade
safety-critical systems. Functional testing detects SDE-related faults but is
expensive to simulate. We present a unified spatio-temporal graph convolutional
network (ST-GCN) for fast, accurate prediction of long-cycle fault impact
probabilities (FIPs) in large sequential circuits, supporting quantitative risk
assessment. Gate-level netlists are modeled as spatio-temporal graphs to
capture topology and signal timing; dedicated spatial and temporal encoders
predict multi-cycle FIPs efficiently. On ISCAS-89 benchmarks, the method
reduces simulation time by more than 10x while maintaining high accuracy (mean
absolute error 0.024 for 5-cycle predictions). The framework accepts features
from testability metrics or fault simulation, allowing efficiency-accuracy
trade-offs. A test-point selection study shows that choosing observation points
by predicted FIPs improves detection of long-cycle, hard-to-detect faults. The
approach scales to SoC-level test strategy optimization and fits downstream
electronic design automation flows.

</details>


### [80] [LoaQ: Layer-wise Output Approximation Quantization](https://arxiv.org/abs/2509.06297)
*Li Lin,Xiaojun Wan*

Main category: cs.LG

TL;DR: LoaQ是一种面向输出一致性的层后训练量化方法，通过更好地近似模型输出而非局部权重激活，提升LLM量化效果


<details>
  <summary>Details</summary>
Motivation: 传统的层后训练量化方法只关注局部权重激活近似，无法实现完整的模型输出对齐，导致量化效果不理想

Method: 基于对主流LLM结构特征的深入理解，提出输出级一致性的层量化方法LoaQ，具有简单闭式解，可与现有技术正交集成

Result: 在LLaMA和Qwen模型系列上的实验表明，LoaQ在仅权重量化和权重激活联合量化中均表现有效

Conclusion: LoaQ通过无缝集成现有量化策略，进一步提升整体量化质量，具有推动后训练量化前沿的潜力

Abstract: A natural and intuitive idea in model quantization is to approximate each
component's quantized output to match its original. Layer-wise post-training
quantization (PTQ), though based on this idea, adopts a strictly local view and
can achieve, at best, only activation-aware approximations of weights. As a
result, it often leads to insufficient approximations and practical deviations
from this guiding intuition. Recent work has achieved a more accurate
approximation of linear-layer outputs within the framework of layer-wise PTQ,
but such refinements remain inadequate for achieving alignment with the full
model output. Based on a deeper understanding of the structural characteristics
of mainstream LLMs, we propose $LoaQ$, an output-approximation method for
layer-wise PTQ that explicitly targets output-level consistency. It better
aligns with this intuition and can feature a simple closed-form solution,
making it orthogonal to existing techniques and readily integrable into
existing quantization pipelines. Experiments on the LLaMA and Qwen model
families demonstrate that LoaQ performs effectively in both weight-only and
weight-activation joint quantization. By integrating seamlessly with existing
quantization strategies, it further enhances overall quantization quality and
shows strong potential to advance the frontier of post-training quantization.

</details>


### [81] [WindFM: An Open-Source Foundation Model for Zero-Shot Wind Power Forecasting](https://arxiv.org/abs/2509.06311)
*Hang Fan,Yu Shi,Zongliang Fu,Shuo Chen,Wei Wei,Wei Xu,Jian Li*

Main category: cs.LG

TL;DR: WindFM是一个轻量级的生成式基础模型，专门用于概率性风电功率预测，通过离散化-生成框架和Transformer架构，在零样本情况下实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法要么训练站点特定模型无法泛化到其他位置，要么依赖通用时间序列基础模型的微调，难以融入能源领域的特定领域数据。

Method: 采用离散化-生成框架：专用时间序列分词器将连续多变量观测转换为离散分层标记，解码器Transformer通过自回归预训练学习风电生成动态的通用表示。

Result: 8.1M参数的紧凑模型在确定性和概率性任务上实现零样本最先进性能，超越专用模型和更大基础模型，在不同大陆的分布外数据下表现出强适应性。

Conclusion: WindFM展示了学习表示的鲁棒性和可迁移性，为风电预测提供了有效的轻量级基础模型解决方案。

Abstract: High-quality wind power forecasting is crucial for the operation of modern
power grids. However, prevailing data-driven paradigms either train a
site-specific model which cannot generalize to other locations or rely on
fine-tuning of general-purpose time series foundation models which are
difficult to incorporate domain-specific data in the energy sector. This paper
introduces WindFM, a lightweight and generative Foundation Model designed
specifically for probabilistic wind power forecasting. WindFM employs a
discretize-and-generate framework. A specialized time-series tokenizer first
converts continuous multivariate observations into discrete, hierarchical
tokens. Subsequently, a decoder-only Transformer learns a universal
representation of wind generation dynamics by autoregressively pre-training on
these token sequences. Using the comprehensive WIND Toolkit dataset comprising
approximately 150 billion time steps from more than 126,000 sites, WindFM
develops a foundational understanding of the complex interplay between
atmospheric conditions and power output. Extensive experiments demonstrate that
our compact 8.1M parameter model achieves state-of-the-art zero-shot
performance on both deterministic and probabilistic tasks, outperforming
specialized models and larger foundation models without any fine-tuning. In
particular, WindFM exhibits strong adaptiveness under out-of-distribution data
from a different continent, demonstrating the robustness and transferability of
its learned representations. Our pre-trained model is publicly available at
https://github.com/shiyu-coder/WindFM.

</details>


### [82] [Evaluating the Efficiency of Latent Spaces via the Coupling-Matrix](https://arxiv.org/abs/2509.06314)
*Mehmet Can Yavuz,Berrin Yanikoglu*

Main category: cs.LG

TL;DR: 提出了一种新的冗余指数ρ(C)来直接量化潜在表示中的维度间依赖性，通过分析耦合矩阵并与正态分布比较来评估表示质量。


<details>
  <summary>Details</summary>
Motivation: 深度网络经常产生冗余的潜在空间，多个坐标编码重叠信息，降低有效容量并阻碍泛化。标准指标如准确率或重构损失只能提供间接证据，无法直接隔离冗余问题。

Method: 通过分析从潜在表示导出的耦合矩阵，使用能量距离比较其非对角线统计量与正态分布，构建紧凑、可解释且统计基础良好的表示质量度量。

Result: 在MNIST变体、Fashion-MNIST、CIFAR-10和CIFAR-100上验证，低ρ(C)可靠预测高分类准确率或低重构误差，而高冗余与性能崩溃相关。TPE优先探索低ρ区域。

Conclusion: ρ(C)通过暴露跨模型和任务的冗余瓶颈，为评估和改进学习表示的效率提供了理论视角和实用工具，可指导神经架构搜索和作为冗余感知正则化目标。

Abstract: A central challenge in representation learning is constructing latent
embeddings that are both expressive and efficient. In practice, deep networks
often produce redundant latent spaces where multiple coordinates encode
overlapping information, reducing effective capacity and hindering
generalization. Standard metrics such as accuracy or reconstruction loss
provide only indirect evidence of such redundancy and cannot isolate it as a
failure mode. We introduce a redundancy index, denoted rho(C), that directly
quantifies inter-dimensional dependencies by analyzing coupling matrices
derived from latent representations and comparing their off-diagonal statistics
against a normal distribution via energy distance. The result is a compact,
interpretable, and statistically grounded measure of representational quality.
We validate rho(C) across discriminative and generative settings on MNIST
variants, Fashion-MNIST, CIFAR-10, and CIFAR-100, spanning multiple
architectures and hyperparameter optimization strategies. Empirically, low
rho(C) reliably predicts high classification accuracy or low reconstruction
error, while elevated redundancy is associated with performance collapse.
Estimator reliability grows with latent dimension, yielding natural lower
bounds for reliable analysis. We further show that Tree-structured Parzen
Estimators (TPE) preferentially explore low-rho regions, suggesting that rho(C)
can guide neural architecture search and serve as a redundancy-aware
regularization target. By exposing redundancy as a universal bottleneck across
models and tasks, rho(C) offers both a theoretical lens and a practical tool
for evaluating and improving the efficiency of learned representations.

</details>


### [83] [Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics](https://arxiv.org/abs/2509.06322)
*Jiajun Bao,Nicolas Boullé,Toni J. B. Liu,Raphaël Sarfati,Christopher J. Earls*

Main category: cs.LG

TL;DR: 大语言模型能够通过上下文学习准确预测偏微分方程的时空动态，无需微调或自然语言提示，预测精度随上下文长度增加而提高，但随空间离散化变细而下降。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在零样本时间序列预测中的涌现能力，特别是对偏微分方程解的时空动态外推能力，以理解模型如何内部处理数值数据。

Method: 使用文本训练的基础模型处理离散化的偏微分方程解，分析多步滚动预测中的误差积累，并通过token级输出分布分析上下文学习过程。

Result: 模型预测精度随时间上下文增长而提高，但随空间离散化变细而下降；多步预测误差随时间范围代数增长；发现一致的ICL进展模式：从语法模式模仿到高熵探索，最终形成数值化预测。

Conclusion: 大语言模型展现出处理偏微分方程解的强大能力，其预测质量可通过上下文长度和输出长度的神经缩放定律来预测，为理解模型内部数值处理机制提供了新见解。

Abstract: Large language models (LLMs) have demonstrated emergent in-context learning
(ICL) capabilities across a range of tasks, including zero-shot time-series
forecasting. We show that text-trained foundation models can accurately
extrapolate spatiotemporal dynamics from discretized partial differential
equation (PDE) solutions without fine-tuning or natural language prompting.
Predictive accuracy improves with longer temporal contexts but degrades at
finer spatial discretizations. In multi-step rollouts, where the model
recursively predicts future spatial states over multiple time steps, errors
grow algebraically with the time horizon, reminiscent of global error
accumulation in classical finite-difference solvers. We interpret these trends
as in-context neural scaling laws, where prediction quality varies predictably
with both context length and output length. To better understand how LLMs are
able to internally process PDE solutions so as to accurately roll them out, we
analyze token-level output distributions and uncover a consistent ICL
progression: beginning with syntactic pattern imitation, transitioning through
an exploratory high-entropy phase, and culminating in confident, numerically
grounded predictions.

</details>


### [84] [Exploring approaches to computational representation and classification of user-generated meal logs](https://arxiv.org/abs/2509.06330)
*Guanlan Hu,Adit Anand,Pooja M. Desai,Iñigo Urteaga,Lena Mamykina*

Main category: cs.LG

TL;DR: 机器学习结合营养领域知识能够准确分类自由文本餐记录是否符合特定营养目标，效果超过自我评估


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用机器学习分析患者生成的健康数据（自由文本餐记录），实现对餐饮是否符合特定营养目标的自动分类

Method: 使用TFIDF和BERT文本嵌入，结合域特定信息丰富（本体论、成分解析器、宏量营养素内容），采用逻辑回归和多层感知机分类器，基于营养师专家评估进行评测

Result: 即使没有域信息丰富，机器学习也超过了自我评估。最佳组合（机器学习+丰富）达到更高准确度。解析成分、食物实体和宏量营养素信息的组合在多个营养目标上都表现良好

Conclusion: 机器学习能够利用非结构化自由文本餐记录实现可靠分类，特别是结合营养领域知识时效果更佳，为精准医疗中的患者个性化营养指导提供支持

Abstract: This study examined the use of machine learning and domain specific
enrichment on patient generated health data, in the form of free text meal
logs, to classify meals on alignment with different nutritional goals. We used
a dataset of over 3000 meal records collected by 114 individuals from a
diverse, low income community in a major US city using a mobile app. Registered
dietitians provided expert judgement for meal to goal alignment, used as gold
standard for evaluation. Using text embeddings, including TFIDF and BERT, and
domain specific enrichment information, including ontologies, ingredient
parsers, and macronutrient contents as inputs, we evaluated the performance of
logistic regression and multilayer perceptron classifiers using accuracy,
precision, recall, and F1 score against the gold standard and self assessment.
Even without enrichment, ML outperformed self assessments of individuals who
logged meals, and the best performing combination of ML classifier with
enrichment achieved even higher accuracies. In general, ML classifiers with
enrichment of Parsed Ingredients, Food Entities, and Macronutrients information
performed well across multiple nutritional goals, but there was variability in
the impact of enrichment and classification algorithm on accuracy of
classification for different nutritional goals. In conclusion, ML can utilize
unstructured free text meal logs and reliably classify whether meals align with
specific nutritional goals, exceeding self assessments, especially when
incorporating nutrition domain knowledge. Our findings highlight the potential
of ML analysis of patient generated health data to support patient centered
nutrition guidance in precision healthcare.

</details>


### [85] [A Fragile Number Sense: Probing the Elemental Limits of Numerical Reasoning in LLMs](https://arxiv.org/abs/2509.06332)
*Roussel Rahman,Aashwin Ananda Mishra*

Main category: cs.LG

TL;DR: LLM在数学计算能力测试中，前三类确定性算法执行表现良好，但在需要启发式搜索的组合数学谜题（Game of 24）上表现糟糕，表明其数值推理更像是模式匹配而非真正的分析思维


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型数值推理的鲁棒性，虽然LLM展现出卓越的涌现能力，但其数学计算能力的稳健性仍是一个开放性问题

Method: 测试多个最先进的LLM智能体在100个问题挑战上的表现，问题分为四类：基础算术、高级运算、素数检查、24点数字谜题

Result: 智能体在前三类需要确定性算法执行的类别上准确率很高，但在需要在大组合空间进行启发式搜索的数字谜题上持续失败

Conclusion: LLM的熟练度主要局限于回忆和执行已知算法，而非进行生成性解决问题，其表面上的数值推理更类似于复杂的模式匹配，限制了在需要新颖或创造性数值洞察任务中的潜力

Abstract: Large Language Models (LLMs) have demonstrated remarkable emergent
capabilities, yet the robustness of their numerical reasoning remains an open
question. While standard benchmarks evaluate LLM reasoning on complex problem
sets using aggregated metrics, they often obscure foundational weaknesses. In
this work, we probe LLM mathematical numeracy by evaluating performance on
problems of escalating complexity, from constituent operations to combinatorial
puzzles. We test several state-of-the-art LLM-based agents on a 100-problem
challenge comprising four categories: (1) basic arithmetic, (2) advanced
operations, (3) primality checking, and (4) the Game of 24 number puzzle. Our
results show that while the agents achieved high accuracy on the first three
categories, which require deterministic algorithmic execution, they
consistently failed at the number puzzle, underlining its demand for a
heuristic search over a large combinatorial space to be a significant
bottleneck. These findings reveal that the agents' proficiency is largely
confined to recalling and executing known algorithms, rather than performing
generative problem-solving. This suggests their apparent numerical reasoning is
more akin to sophisticated pattern-matching than flexible, analytical thought,
limiting their potential for tasks that require novel or creative numerical
insights.

</details>


### [86] [Ban&Pick: Achieving Free Performance Gains and Inference Speedup via Smarter Routing in MoE-LLMs](https://arxiv.org/abs/2509.06346)
*Yuanteng Chen,Peisong Wang,Yuantian Shao,Jian Cheng*

Main category: cs.LG

TL;DR: Ban&Pick是一种无需重新训练的后训练策略，通过强化关键专家和动态剪枝冗余专家，提升稀疏混合专家模型的性能和推理速度


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型的路由器在预训练中过早收敛并强制平衡使用，导致关键专家利用不足和冗余专家问题，限制了模型性能和效率

Method: 提出Ban&Pick后训练策略：Pick组件发现并强化对性能影响大的关键专家；Ban组件根据层和token敏感性动态剪枝冗余专家

Result: 在DeepSeek、Qwen3等MoE-LLM上，AIME2024准确率从80.67提升至84.66，GPQA-Diamond从65.66提升至68.18，推理速度提升1.25倍

Conclusion: Ban&Pick无需重新训练或架构改动，即可为细粒度MoE模型带来免费的性能提升和推理加速，证明了后训练优化MoE路由的有效性

Abstract: Sparse Mixture-of-Experts (MoE) has become a key architecture for scaling
large language models (LLMs) efficiently. Recent fine-grained MoE designs
introduce hundreds of experts per layer, with multiple experts activated per
token, enabling stronger specialization. However, during pre-training, routers
are optimized mainly for stability and robustness: they converge prematurely
and enforce balanced usage, limiting the full potential of model performance
and efficiency. In this work, we uncover two overlooked issues: (i) a few
highly influential experts are underutilized due to premature and balanced
routing decisions; and (ii) enforcing a fixed number of active experts per
token introduces substantial redundancy. Instead of retraining models or
redesigning MoE architectures, we introduce Ban&Pick, a post-training,
plug-and-play strategy for smarter MoE routing. Pick discovers and reinforces
key experts-a small group with outsized impact on performance-leading to
notable accuracy gains across domains. Ban complements this by dynamically
pruning redundant experts based on layer and token sensitivity, delivering
faster inference with minimal accuracy loss. Experiments on fine-grained
MoE-LLMs (DeepSeek, Qwen3) across math, code, and general reasoning benchmarks
demonstrate that Ban&Pick delivers free performance gains and inference
acceleration without retraining or architectural changes. For instance, on
Qwen3-30B-A3B, it improves accuracy from 80.67 to 84.66 on AIME2024 and from
65.66 to 68.18 on GPQA-Diamond, while accelerating inference by 1.25x under the
vLLM.

</details>


### [87] [Breaking SafetyCore: Exploring the Risks of On-Device AI Deployment](https://arxiv.org/abs/2509.06371)
*Victor Guyomard,Mathis Mauvisseau,Marie Paindavoine*

Main category: cs.LG

TL;DR: 本文通过SafetyCore案例研究揭示了设备端AI模型的安全风险，展示了模型可被提取和操纵以绕过敏感内容检测


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在设备端部署的增加，虽然提升了隐私保护和降低了延迟，但也带来了不同于传统软件的安全风险，需要深入研究这些新型威胁

Method: 通过Android系统服务SafetyCore的真实案例研究，分析设备端AI模型的安全漏洞，演示模型提取和操纵的技术方法

Result: 成功展示了如何提取和操纵设备端AI模型来绕过敏感图像内容检测，使保护机制失效

Conclusion: 设备端AI模型存在严重安全漏洞，攻击者可以实际利用这些漏洞，需要加强设备端AI系统的安全防护措施

Abstract: Due to hardware and software improvements, an increasing number of AI models
are deployed on-device. This shift enhances privacy and reduces latency, but
also introduces security risks distinct from traditional software. In this
article, we examine these risks through the real-world case study of
SafetyCore, an Android system service incorporating sensitive image content
detection. We demonstrate how the on-device AI model can be extracted and
manipulated to bypass detection, effectively rendering the protection
ineffective. Our analysis exposes vulnerabilities of on-device AI models and
provides a practical demonstration of how adversaries can exploit them.

</details>


### [88] [Variational Garrote for Statistical Physics-based Sparse and Robust Variable Selection](https://arxiv.org/abs/2509.06383)
*Hyungjoon Soh,Dongha Lee,Vipul Periwal,Junghyo Jo*

Main category: cs.LG

TL;DR: 本文重新审视并改进了基于统计物理的变分门控(VG)方法，用于高维数据的关键变量选择，通过自动微分技术实现可扩展优化，在高度稀疏场景下表现优于Ridge和LASSO回归。


<details>
  <summary>Details</summary>
Motivation: 在大数据时代，从高维数据中选择关键变量变得越来越重要。稀疏回归通过促进模型简洁性和可解释性为此提供了有力工具，但现有方法在高度稀疏场景下的表现有待改进。

Method: 重新利用基于统计物理的变分门控(VG)方法，引入显式特征选择自旋变量，利用变分推理推导可处理的损失函数，并整合现代自动微分技术实现可扩展的高效优化。

Result: VG在高度稀疏场景下表现优异，比Ridge和LASSO回归提供更一致和稳健的变量选择。发现了一个尖锐的过渡现象：当引入多余变量时，泛化性能会急剧下降，选择变量的不确定性增加。

Conclusion: VG在稀疏建模方面具有强大潜力，过渡点为估计相关变量数量提供了实用信号，可成功应用于识别现实数据中的关键预测因子，在压缩感知和机器学习模型剪枝等领域有广泛应用前景。

Abstract: Selecting key variables from high-dimensional data is increasingly important
in the era of big data. Sparse regression serves as a powerful tool for this
purpose by promoting model simplicity and explainability. In this work, we
revisit a valuable yet underutilized method, the statistical physics-based
Variational Garrote (VG), which introduces explicit feature selection spin
variables and leverages variational inference to derive a tractable loss
function. We enhance VG by incorporating modern automatic differentiation
techniques, enabling scalable and efficient optimization. We evaluate VG on
both fully controllable synthetic datasets and complex real-world datasets. Our
results demonstrate that VG performs especially well in highly sparse regimes,
offering more consistent and robust variable selection than Ridge and LASSO
regression across varying levels of sparsity. We also uncover a sharp
transition: as superfluous variables are admitted, generalization degrades
abruptly and the uncertainty of the selection variables increases. This
transition point provides a practical signal for estimating the correct number
of relevant variables, an insight we successfully apply to identify key
predictors in real-world data. We expect that VG offers strong potential for
sparse modeling across a wide range of applications, including compressed
sensing and model pruning in machine learning.

</details>


### [89] [Beyond the Pre-Service Horizon: Infusing In-Service Behavior for Improved Financial Risk Forecasting](https://arxiv.org/abs/2509.06385)
*Senhao Liu,Zhiyu Guo,Zhiyuan Ji,Yueguo Chen,Yateng Tang,Yunhai Wang,Xuehao Zheng,Xiang Ao*

Main category: cs.LG

TL;DR: 该文章提出了多粒度知识蓄纳(MGKD)框架，通过知识蓈纳把服务中用户行为数据转移到服务前风险预测中，提高风险评估性能。


<details>
  <summary>Details</summary>
Motivation: 传统金融风险管理中服务前风险评估和服务中违约检测通常分离建模，该研究尝试通过知识蓈纳把服务中数据优势转移到服务前模型中。

Method: 使用教师-学生模型框架，教师模型培养于历史服务中数据，通过软标签指导培养于服务前数据的学生模型。采用多粒度蓈纳策略（粗粒度、细粒度和自蓈纳）对齐表征和预测，并使用重新加权策略减少少数类偏差。

Result: 在腾讯移动支付的大规模实际数据集上进行实验，线上和线下场景都证明了该方法的有效性。

Conclusion: MGKD框架能够通过知识蓈纳把关键行为模式从服务中数据转移到服务前风险预测中，显著提高了服务前风险评估的性能。

Abstract: Typical financial risk management involves distinct phases for pre-service
risk assessment and in-service default detection, often modeled separately.
This paper proposes a novel framework, Multi-Granularity Knowledge Distillation
(abbreviated as MGKD), aimed at improving pre-service risk prediction through
the integration of in-service user behavior data. MGKD follows the idea of
knowledge distillation, where the teacher model, trained on historical
in-service data, guides the student model, which is trained on pre-service
data. By using soft labels derived from in-service data, the teacher model
helps the student model improve its risk prediction prior to service
activation. Meanwhile, a multi-granularity distillation strategy is introduced,
including coarse-grained, fine-grained, and self-distillation, to align the
representations and predictions of the teacher and student models. This
approach not only reinforces the representation of default cases but also
enables the transfer of key behavioral patterns associated with defaulters from
the teacher to the student model, thereby improving the overall performance of
pre-service risk assessment. Moreover, we adopt a re-weighting strategy to
mitigate the model's bias towards the minority class. Experimental results on
large-scale real-world datasets from Tencent Mobile Payment demonstrate the
effectiveness of our proposed approach in both offline and online scenarios.

</details>


### [90] [Graph Neural Networks for Resource Allocation in Interference-limited Multi-Channel Wireless Networks with QoS Constraints](https://arxiv.org/abs/2509.06395)
*Lili Chen,Changyang She,Jingge Zhu,Jamie Evans*

Main category: cs.LG

TL;DR: 提出了一种基于GNN和拉格朗日对偶优化的理论保证框架JCPGNN-M，用于无线通信系统中满足QoS约束的资源分配问题，在保持性能的同时显著提升计算效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法通过惩罚项处理QoS约束缺乏理论收敛保证，在实际场景中经常无法满足服务质量要求，需要一种理论上有保证的解决方案。

Method: 1. 扩展WMMSE算法到多信道QoS约束场景(eWMMSE)；2. 开发支持多信道分配的GNN算法JCPGNN-M；3. 提出GNN与拉格朗日原始-对偶优化结合的理论框架。

Result: JCPGNN-M在性能上匹配eWMMSE，同时在推理速度、大规模网络泛化能力和不完美信道信息下的鲁棒性方面有显著提升。

Conclusion: 该工作为未来无线网络中约束资源分配提供了一个可扩展且理论上有保证的解决方案，结合了传统优化方法和深度学习的优势。

Abstract: Meeting minimum data rate constraints is a significant challenge in wireless
communication systems, particularly as network complexity grows. Traditional
deep learning approaches often address these constraints by incorporating
penalty terms into the loss function and tuning hyperparameters empirically.
However, this heuristic treatment offers no theoretical convergence guarantees
and frequently fails to satisfy QoS requirements in practical scenarios.
Building upon the structure of the WMMSE algorithm, we first extend it to a
multi-channel setting with QoS constraints, resulting in the enhanced WMMSE
(eWMMSE) algorithm, which is provably convergent to a locally optimal solution
when the problem is feasible. To further reduce computational complexity and
improve scalability, we develop a GNN-based algorithm, JCPGNN-M, capable of
supporting simultaneous multi-channel allocation per user. To overcome the
limitations of traditional deep learning methods, we propose a principled
framework that integrates GNN with a Lagrangian-based primal-dual optimization
method. By training the GNN within the Lagrangian framework, we ensure
satisfaction of QoS constraints and convergence to a stationary point.
Extensive simulations demonstrate that JCPGNN-M matches the performance of
eWMMSE while offering significant gains in inference speed, generalization to
larger networks, and robustness under imperfect channel state information. This
work presents a scalable and theoretically grounded solution for constrained
resource allocation in future wireless networks.

</details>


### [91] [NeuroDeX: Unlocking Diverse Support in Decompiling Deep Neural Network Executables](https://arxiv.org/abs/2509.06402)
*Yilin Li,Guozhu Meng,Mingyang Sun,Yanzhong Wang,Kun Sun,Hailong Chang,Yuekang Li*

Main category: cs.LG

TL;DR: NeuroDeX是一个基于LLM语义理解和动态分析的DNN可执行文件反编译器，能够处理编译优化和量化模型，实现操作符识别、属性恢复和模型重建。


<details>
  <summary>Details</summary>
Motivation: 设备端深度学习模型需求广泛，但编译后的可执行文件面临逆向工程威胁。现有反编译方法难以处理编译优化和量化编译模型。

Method: 利用LLM的语义理解能力结合动态分析，准确高效地进行操作符类型识别、操作符属性恢复和模型重建。

Result: 在96个DNN可执行文件上测试，非量化可执行文件可反编译为近乎相同的高级模型，量化可执行文件可恢复功能相似模型，平均top-1准确率达72%。

Conclusion: NeuroDeX相比之前的DNN可执行文件反编译器提供了更全面有效的解决方案，支持编译优化、不同架构和量化编译模型。

Abstract: On-device deep learning models have extensive real world demands. Deep
learning compilers efficiently compile models into executables for deployment
on edge devices, but these executables may face the threat of reverse
engineering. Previous studies have attempted to decompile DNN executables, but
they face challenges in handling compilation optimizations and analyzing
quantized compiled models. In this paper, we present NeuroDeX to unlock diverse
support in decompiling DNN executables. NeuroDeX leverages the semantic
understanding capabilities of LLMs along with dynamic analysis to accurately
and efficiently perform operator type recognition, operator attribute recovery
and model reconstruction. NeuroDeX can recover DNN executables into high-level
models towards compilation optimizations, different architectures and quantized
compiled models. We conduct experiments on 96 DNN executables across 12 common
DNN models. Extensive experimental results demonstrate that NeuroDeX can
decompile non-quantized executables into nearly identical high-level models.
NeuroDeX can recover functionally similar high-level models for quantized
executables, achieving an average top-1 accuracy of 72%. NeuroDeX offers a more
comprehensive and effective solution compared to previous DNN executables
decompilers.

</details>


### [92] [CAPMix: Robust Time Series Anomaly Detection Based on Abnormal Assumptions with Dual-Space Mixup](https://arxiv.org/abs/2509.06419)
*Xudong Mou,Rui Wang,Tiejun Wang,Renyu Yang,Shiru Chen,Jie Sun,Tianyu Wo,Xudong Liu*

Main category: cs.LG

TL;DR: CAPMix是一个可控异常增强框架，通过CutAddPaste机制注入多样化异常，使用标签修正策略减少异常偏移，在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列异常检测中标注异常稀缺的问题，现有异常假设方法存在碎片化生成和异常偏移两个根本性限制。

Method: 设计CutAddPaste机制进行目标性异常注入，引入标签修正策略自适应优化异常标签，使用时序卷积网络中的双空间混合实现更平滑的决策边界。

Result: 在AIOps、UCR、SWaT、WADI和ESA五个基准数据集上的广泛实验显示，CAPMix相比最先进基线方法取得显著改进，对污染训练数据具有更强鲁棒性。

Conclusion: CAPMix有效解决了异常注入中的碎片化生成和异常偏移问题，为时间序列异常检测提供了更可靠的解决方案。

Abstract: Time series anomaly detection (TSAD) is a vital yet challenging task,
particularly in scenarios where labeled anomalies are scarce and temporal
dependencies are complex. Recent anomaly assumption (AA) approaches alleviate
the lack of anomalies by injecting synthetic samples and training
discriminative models. Despite promising results, these methods often suffer
from two fundamental limitations: patchy generation, where scattered anomaly
knowledge leads to overly simplistic or incoherent anomaly injection, and
Anomaly Shift, where synthetic anomalies either resemble normal data too
closely or diverge unrealistically from real anomalies, thereby distorting
classification boundaries. In this paper, we propose CAPMix, a controllable
anomaly augmentation framework that addresses both issues. First, we design a
CutAddPaste mechanism to inject diverse and complex anomalies in a targeted
manner, avoiding patchy generation. Second, we introduce a label revision
strategy to adaptively refine anomaly labels, reducing the risk of anomaly
shift. Finally, we employ dual-space mixup within a temporal convolutional
network to enforce smoother and more robust decision boundaries. Extensive
experiments on five benchmark datasets, including AIOps, UCR, SWaT, WADI, and
ESA, demonstrate that CAPMix achieves significant improvements over
state-of-the-art baselines, with enhanced robustness against contaminated
training data. The code is available at https://github.com/alsike22/CAPMix.

</details>


### [93] [CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction](https://arxiv.org/abs/2509.06465)
*Hongzong Li,Jiahao Ma,Zhanpeng Shi,Fanming Jin,Ye-Fan Hu,Jian-Dong Huang*

Main category: cs.LG

TL;DR: CAME-AB是一个基于多模态融合和混合专家机制的抗体结合位点预测框架，通过整合五种生物特征模态并采用自适应融合策略，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单视图特征，无法有效识别抗原上的抗体特异性结合位点，存在表示和预测的双重局限性。

Method: 整合五种生物特征模态（氨基酸编码、BLOSUM替换谱、预训练语言模型嵌入、结构感知特征、GCN精化生化图），采用自适应模态融合模块动态加权，结合Transformer编码器和MoE模块，引入监督对比学习和随机权重平均。

Result: 在基准抗体-抗原数据集上，CAME-AB在精确率、召回率、F1分数、AUC-ROC和MCC等多个指标上持续优于强基线方法。

Conclusion: 多模态特征整合和自适应融合策略有效提升了抗体结合位点预测性能，消融研究验证了各架构组件的有效性。

Abstract: Antibody binding site prediction plays a pivotal role in computational
immunology and therapeutic antibody design. Existing sequence or structure
methods rely on single-view features and fail to identify antibody-specific
binding sites on the antigens-a dual limitation in representation and
prediction. In this paper, we propose CAME-AB, a novel Cross-modality Attention
framework with a Mixture-of-Experts (MoE) backbone for robust antibody binding
site prediction. CAME-AB integrates five biologically grounded modalities,
including raw amino acid encodings, BLOSUM substitution profiles, pretrained
language model embeddings, structure-aware features, and GCN-refined
biochemical graphs-into a unified multimodal representation. To enhance
adaptive cross-modal reasoning, we propose an adaptive modality fusion module
that learns to dynamically weight each modality based on its global relevance
and input-specific contribution. A Transformer encoder combined with an MoE
module further promotes feature specialization and capacity expansion. We
additionally incorporate a supervised contrastive learning objective to
explicitly shape the latent space geometry, encouraging intra-class compactness
and inter-class separability. To improve optimization stability and
generalization, we apply stochastic weight averaging during training. Extensive
experiments on benchmark antibody-antigen datasets demonstrate that CAME-AB
consistently outperforms strong baselines on multiple metrics, including
Precision, Recall, F1-score, AUC-ROC, and MCC. Ablation studies further
validate the effectiveness of each architectural component and the benefit of
multimodal feature integration. The model implementation details and the codes
are available on https://anonymous.4open.science/r/CAME-AB-C525

</details>


### [94] [DyC-STG: Dynamic Causal Spatio-Temporal Graph Network for Real-time Data Credibility Analysis in IoT](https://arxiv.org/abs/2509.06483)
*Guanjie Cheng,Boyi Li,Peihan Wu,Feiyi Chen,Xinkui Zhao,Mengying Zhu,Shuiguang Deng*

Main category: cs.LG

TL;DR: DyC-STG是一个用于物联网数据可信度分析的动态因果时空图网络，通过事件驱动的动态图模块和因果推理模块解决传统STG模型在动态人本环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 物联网传感器产生大量时空数据流，但数据可信度是未解决的关键挑战。传统时空图模型在动态人本环境中存在两个根本限制：静态图拓扑无法捕捉物理动态，以及容易混淆虚假相关与真实因果关系。

Method: 提出DyC-STG框架，包含两个协同模块：事件驱动的动态图模块实时调整图拓扑以反映物理状态变化，因果推理模块通过严格强制执行时间优先来提取因果感知表示。

Result: 实验表明DyC-STG建立了新的最先进水平，比最强基线高出1.4个百分点，F1分数达到0.930。发布了两个新的真实世界数据集。

Conclusion: DyC-STG有效解决了传统时空图模型在动态人本环境中的局限性，为物联网数据可信度分析提供了创新解决方案，并在性能上实现了显著提升。

Abstract: The wide spreading of Internet of Things (IoT) sensors generates vast
spatio-temporal data streams, but ensuring data credibility is a critical yet
unsolved challenge for applications like smart homes. While spatio-temporal
graph (STG) models are a leading paradigm for such data, they often fall short
in dynamic, human-centric environments due to two fundamental limitations: (1)
their reliance on static graph topologies, which fail to capture physical,
event-driven dynamics, and (2) their tendency to confuse spurious correlations
with true causality, undermining robustness in human-centric environments. To
address these gaps, we propose the Dynamic Causal Spatio-Temporal Graph Network
(DyC-STG), a novel framework designed for real-time data credibility analysis
in IoT. Our framework features two synergistic contributions: an event-driven
dynamic graph module that adapts the graph topology in real-time to reflect
physical state changes, and a causal reasoning module to distill causally-aware
representations by strictly enforcing temporal precedence. To facilitate the
research in this domain we release two new real-world datasets. Comprehensive
experiments show that DyC-STG establishes a new state-of-the-art, outperforming
the strongest baselines by 1.4 percentage points and achieving an F1-Score of
up to 0.930.

</details>


### [95] [A machine-learned expression for the excess Gibbs energy](https://arxiv.org/abs/2509.06484)
*Marco Hoffmann,Thomas Specht,Quirin Göttl,Jakob Burger,Stephan Mandt,Hans Hasse,Fabian Jirasek*

Main category: cs.LG

TL;DR: HANNA是一个将物理定律作为硬约束集成到神经网络中的模型，用于从分子结构预测多组分混合物的过量吉布斯自由能，在准确性和范围上明显优于现有基准方法。


<details>
  <summary>Details</summary>
Motivation: 预测多组分混合物的过量吉布斯自由能仅从其组分的分子结构出发是一个长期存在的挑战，过量吉布斯自由能在化学工程和化学中起着核心作用，为液体混合物的热力学性质建模提供基础。

Method: 将物理定律作为硬约束集成到灵活的神经网络中，使用Dortmund数据银行中二元混合物的大量实验数据集进行端到端训练，开发了新颖的代理求解器以在训练过程中包含液-液平衡数据，并应用几何投影方法实现向多组分混合物的稳健外推。

Result: HANNA提供了出色的预测性能，在准确性和范围上明显优于最先进的基准方法。

Conclusion: 该工作成功解决了从分子结构预测多组分混合物过量吉布斯自由能的挑战，通过将物理约束与神经网络结合，实现了热力学一致且准确的预测，模型和代码已公开提供。

Abstract: The excess Gibbs energy plays a central role in chemical engineering and
chemistry, providing a basis for modeling the thermodynamic properties of
liquid mixtures. Predicting the excess Gibbs energy of multi-component mixtures
solely from the molecular structures of their components is a long-standing
challenge. In this work, we address this challenge by integrating physical laws
as hard constraints within a flexible neural network. The resulting model,
HANNA, was trained end-to-end on an extensive experimental dataset for binary
mixtures from the Dortmund Data Bank, guaranteeing thermodynamically consistent
predictions. A novel surrogate solver developed in this work enabled the
inclusion of liquid-liquid equilibrium data in the training process.
Furthermore, a geometric projection method was applied to enable robust
extrapolations to multi-component mixtures, without requiring additional
parameters. We demonstrate that HANNA delivers excellent predictions, clearly
outperforming state-of-the-art benchmark methods in accuracy and scope. The
trained model and corresponding code are openly available, and an interactive
interface is provided on our website, MLPROP.

</details>


### [96] [QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients](https://arxiv.org/abs/2509.06516)
*Zongheng Guo,Tao Chen,Manuela Ferrario*

Main category: cs.LG

TL;DR: QualityFM是一个用于PPG和ECG生理信号质量分析的多模态基础模型，通过大规模预训练和双轨架构实现信号质量评估，在三个临床任务中表现出色


<details>
  <summary>Details</summary>
Motivation: ICU和手术室中PPG和ECG信号质量差、不完整和不一致的问题导致误报警和诊断不准确，现有方法泛化性有限且需要大量标注数据

Method: 采用双轨架构处理不同质量的配对生理信号，使用自蒸馏策略，集成窗口稀疏注意力机制的Transformer模型，结合蒸馏损失和重构损失的复合损失函数

Result: 在超过2100万个30秒波形和17.9万小时数据上预训练，开发了三个参数规模不同的模型（9.6M到319M），在室速误报警、房颤识别和动脉血压估计三个临床任务中验证有效

Conclusion: QualityFM作为一个通用生理信号基础模型，能够有效处理信号质量问题，具有良好的跨任务迁移能力和临床应用价值

Abstract: Photoplethysmogram (PPG) and electrocardiogram (ECG) are commonly recorded in
intesive care unit (ICU) and operating room (OR). However, the high incidence
of poor, incomplete, and inconsistent signal quality, can lead to false alarms
or diagnostic inaccuracies. The methods explored so far suffer from limited
generalizability, reliance on extensive labeled data, and poor cross-task
transferability. To overcome these challenges, we introduce QualityFM, a novel
multimodal foundation model for these physiological signals, designed to
acquire a general-purpose understanding of signal quality. Our model is
pre-trained on an large-scale dataset comprising over 21 million 30-second
waveforms and 179,757 hours of data. Our approach involves a dual-track
architecture that processes paired physiological signals of differing quality,
leveraging a self-distillation strategy where an encoder for high-quality
signals is used to guide the training of an encoder for low-quality signals. To
efficiently handle long sequential signals and capture essential local
quasi-periodic patterns, we integrate a windowed sparse attention mechanism
within our Transformer-based model. Furthermore, a composite loss function,
which combines direct distillation loss on encoder outputs with indirect
reconstruction loss based on power and phase spectra, ensures the preservation
of frequency-domain characteristics of the signals. We pre-train three models
with varying parameter counts (9.6 M to 319 M) and demonstrate their efficacy
and practical value through transfer learning on three distinct clinical tasks:
false alarm of ventricular tachycardia detection, the identification of atrial
fibrillation and the estimation of arterial blood pressure (ABP) from PPG and
ECG signals.

</details>


### [97] [Lane Change Intention Prediction of two distinct Populations using a Transformer](https://arxiv.org/abs/2509.06529)
*Francesco De Cristofaro,Cornelia Lex,Jia Hu,Arno Eichberger*

Main category: cs.LG

TL;DR: 这篇论文研究了转换器模型在不同数据集上的车道更换意图预测性能，发现单一数据集训练的模型在异构数据集上性能严重下降，而多数据集训练可将准确率提升至86.71%。


<details>
  <summary>Details</summary>
Motivation: 当前车道更换意图预测研究存在严重偏差，大多数算法仅在单一数据集上训练和测试，未考虑模型在不同地区/人群数据上的演进性能。

Method: 使用转换器模型，在LevelX德国和香港收集的两个数据集上进行车道更换意图预测实验，测试单一数据集训练和多数据集同时训练的性能差异。

Result: 单一数据集训练的模型在异构数据集上准确率严重下降至39.43%，而同时在两个数据集上训练的模型能达到86.71%的高准确率。

Conclusion: 车道更换意图预测模型的演进性能很差，需要在多样化的数据集上进行训练才能获得良好的通用性能力。

Abstract: As a result of the growing importance of lane change intention prediction for
a safe and efficient driving experience in complex driving scenarios,
researchers have in recent years started to train novel machine learning
algorithms on available datasets with promising results. A shortcoming of this
recent research effort, though, is that the vast majority of the proposed
algorithms are trained on a single datasets. In doing so, researchers failed to
test if their algorithm would be as effective if tested on a different dataset
and, by extension, on a different population with respect to the one on which
they were trained. In this article we test a transformer designed for lane
change intention prediction on two datasets collected by LevelX in Germany and
Hong Kong. We found that the transformer's accuracy plummeted when tested on a
population different to the one it was trained on with accuracy values as low
as 39.43%, but that when trained on both populations simultaneously it could
achieve an accuracy as high as 86.71%. - This work has been submitted to the
IEEE for possible publication. Copyright may be transferred without notice,
after which this version may no longer be accessible.

</details>


### [98] [Learning Optimal Defender Strategies for CAGE-2 using a POMDP Model](https://arxiv.org/abs/2509.06539)
*Duc Huy Le,Rolf Stadler*

Main category: cs.LG

TL;DR: 本文为CAGE-2网络安全基准构建了POMDP形式化模型，提出了基于PPO和粒子滤波的BF-PPO方法，在防御策略和学习效率上超越了当前最优方法CARDIFF。


<details>
  <summary>Details</summary>
Motivation: CAGE-2是网络安全防御策略的重要基准，现有方法仍有改进空间，需要更高效的最优防御策略学习方法。

Method: 使用部分可观察马尔可夫决策过程(POMDP)对CAGE-2进行形式化建模，提出BF-PPO方法（基于PPO算法，结合粒子滤波处理大状态空间的计算复杂度）。

Result: 在CAGE-2 CybORG环境中评估，BF-PPO在防御策略性能和训练时间上都超越了当前排行榜最优方法CARDIFF。

Conclusion: 基于POMDP的形式化建模和BF-PPO方法能够有效学习CAGE-2环境中的最优防御策略，显著提升防御效果和学习效率。

Abstract: CAGE-2 is an accepted benchmark for learning and evaluating defender
strategies against cyberattacks. It reflects a scenario where a defender agent
protects an IT infrastructure against various attacks. Many defender methods
for CAGE-2 have been proposed in the literature. In this paper, we construct a
formal model for CAGE-2 using the framework of Partially Observable Markov
Decision Process (POMDP). Based on this model, we define an optimal defender
strategy for CAGE-2 and introduce a method to efficiently learn this strategy.
Our method, called BF-PPO, is based on PPO, and it uses particle filter to
mitigate the computational complexity due to the large state space of the
CAGE-2 model. We evaluate our method in the CAGE-2 CybORG environment and
compare its performance with that of CARDIFF, the highest ranked method on the
CAGE-2 leaderboard. We find that our method outperforms CARDIFF regarding the
learned defender strategy and the required training time.

</details>


### [99] [Predicting Fetal Outcomes from Cardiotocography Signals Using a Supervised Variational Autoencoder](https://arxiv.org/abs/2509.06540)
*John Tolladay,Beth Albert,Gabriel Davis Jones*

Main category: cs.LG

TL;DR: 这篇论文开发了一种可解释的监督变分自编码器模型，用于基于孕期结果的胎心电图信号分类，在保持竞争性预测性能的同时提供部分临床意义特征编码。


<details>
  <summary>Details</summary>
Motivation: 解决当前深度学习方法在胎心电图信号分析中的可解释性限制，开发能够同时进行信号重建和结果预测的可解释模型。

Method: 使用OxMat CTG数据集训练监督VAE模型，结5分钟胎心率段落，重点优化信号重建和结果预测，并通过Kullback-Leibler散度和总相关约束来结构潜在空间。

Result: 模型在段落层面获得0.752 AUROC，在CTG整体层面获得0.779 AUROC。潜在空间分析显示基线相关特征得到良好表征，但短期和长期变异性编码较弱。

Conclusion: 监督VAE能够在胎儿结果预测中达到竞争性能力，并部分编码具有临床意义的特征。虽未完全实现可解释性，但为未来可解释生成模型奠定了基础。

Abstract: Objective: To develop and interpret a supervised variational autoencoder
(VAE) model for classifying cardiotocography (CTG) signals based on pregnancy
outcomes, addressing interpretability limits of current deep learning
approaches. Methods: The OxMat CTG dataset was used to train a VAE on
five-minute fetal heart rate (FHR) segments, labeled with postnatal outcomes.
The model was optimised for signal reconstruction and outcome prediction,
incorporating Kullback-Leibler divergence and total correlation (TC)
constraints to structure the latent space. Performance was evaluated using area
under the receiver operating characteristic curve (AUROC) and mean squared
error (MSE). Interpretability was assessed using coefficient of determination,
latent traversals and unsupervised component analyses. Results: The model
achieved an AUROC of 0.752 at the segment level and 0.779 at the CTG level,
where predicted scores were aggregated. Relaxing TC constraints improved both
reconstruction and classification. Latent analysis showed that baseline-related
features (e.g., FHR baseline, baseline shift) were well represented and aligned
with model scores, while metrics like short- and long-term variability were
less strongly encoded. Traversals revealed clear signal changes for baseline
features, while other properties were entangled or subtle. Unsupervised
decompositions corroborated these patterns. Findings: This work demonstrates
that supervised VAEs can achieve competitive fetal outcome prediction while
partially encoding clinically meaningful CTG features. The irregular,
multi-timescale nature of FHR signals poses challenges for disentangling
physiological components, distinguishing CTG from more periodic signals such as
ECG. Although full interpretability was not achieved, the model supports
clinically useful outcome prediction and provides a basis for future
interpretable, generative models.

</details>


### [100] [Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs](https://arxiv.org/abs/2509.06550)
*Jack Wilkie,Hanan Hindy,Christos Tachtatzis,Robert Atkinson*

Main category: cs.LG

TL;DR: 提出CLAN方法，通过将增强样本作为负样本视图来改进网络入侵检测，在二元分类和多类分类任务中均优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有监督学习方法依赖大量标注数据，异常检测方法误报率高，自监督学习方法虽有所改进但仍需提升检测准确性和效率

Method: 提出CLAN（使用增强负样本对的对比学习）范式，将增强样本视为负视图（代表潜在恶意分布），其他良性样本作为正视图

Result: 在Lycos2017数据集上，二元分类任务超越现有自监督和异常检测技术；有限标注数据微调后，多类分类性能优于现有自监督模型

Conclusion: CLAN方法通过创新的负样本处理策略，显著提升了网络入侵检测的准确性和效率，为实际应用提供了更实用的解决方案

Abstract: Network intrusion detection remains a critical challenge in cybersecurity.
While supervised machine learning models achieve state-of-the-art performance,
their reliance on large labelled datasets makes them impractical for many
real-world applications. Anomaly detection methods, which train exclusively on
benign traffic to identify malicious activity, suffer from high false positive
rates, limiting their usability. Recently, self-supervised learning techniques
have demonstrated improved performance with lower false positive rates by
learning discriminative latent representations of benign traffic. In
particular, contrastive self-supervised models achieve this by minimizing the
distance between similar (positive) views of benign traffic while maximizing it
between dissimilar (negative) views. Existing approaches generate positive
views through data augmentation and treat other samples as negative. In
contrast, this work introduces Contrastive Learning using Augmented Negative
pairs (CLAN), a novel paradigm for network intrusion detection where augmented
samples are treated as negative views - representing potentially malicious
distributions - while other benign samples serve as positive views. This
approach enhances both classification accuracy and inference efficiency after
pretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset
demonstrates that the proposed method surpasses existing self-supervised and
anomaly detection techniques in a binary classification task. Furthermore, when
fine-tuned on a limited labelled dataset, the proposed approach achieves
superior multi-class classification performance compared to existing
self-supervised models.

</details>


### [101] [AI for Scientific Discovery is a Social Problem](https://arxiv.org/abs/2509.06580)
*Georgia Channing,Avijit Ghosh*

Main category: cs.LG

TL;DR: 论文认为AI科学发展的主要障碍是社会和制度性的，而非技术性的，需要将AI科学重新定义为集体社会项目，通过社区建设、跨学科教育和基础设施共享来解决。


<details>
  <summary>Details</summary>
Motivation: 人工智能有望加速科学发现，但其益处分布不均。虽然技术障碍如数据稀缺、标准分散和计算资源不平等是重要因素，但作者认为主要障碍是社会和制度性的。

Method: 通过分析社区功能障碍、研究优先级与上游需求不匹配、数据碎片化和基础设施不平等四个相互关联的挑战，指出其根源在于文化和组织实践。

Result: 识别出AI科学发展面临的核心社会制度障碍，提出了需要技术创新的同时，更需要有意识的社区建设、跨学科教育、共享基准和可访问基础设施的综合解决方案。

Conclusion: 需要将AI科学重新定义为集体社会项目，将可持续协作和公平参与视为技术进步的前提条件，通过社会和组织变革来真正实现AI对科学发现的加速作用。

Abstract: Artificial intelligence promises to accelerate scientific discovery, yet its
benefits remain unevenly distributed. While technical obstacles such as scarce
data, fragmented standards, and unequal access to computation are significant,
we argue that the primary barriers are social and institutional. Narratives
that defer progress to speculative "AI scientists," the undervaluing of data
and infrastructure contributions, misaligned incentives, and gaps between
domain experts and machine learning researchers all constrain impact. We
highlight four interconnected challenges: community dysfunction, research
priorities misaligned with upstream needs, data fragmentation, and
infrastructure inequities. We argue that their roots lie in cultural and
organizational practices. Addressing them requires not only technical
innovation but also intentional community-building, cross-disciplinary
education, shared benchmarks, and accessible infrastructure. We call for
reframing AI for science as a collective social project, where sustainable
collaboration and equitable participation are treated as prerequisites for
technical progress.

</details>


### [102] [Information-Theoretic Bounds and Task-Centric Learning Complexity for Real-World Dynamic Nonlinear Systems](https://arxiv.org/abs/2509.06599)
*Sri Satish Krishna Chaitanya Bulusu,Mikko Sillanpää*

Main category: cs.LG

TL;DR: 提出了一个理论框架来分析动态非线性系统中的静态和动态失真耦合问题，通过结构化分解、方差分析和任务感知复杂度边界来建立理论基础。


<details>
  <summary>Details</summary>
Motivation: 动态非线性系统中的静态和动态效应相互耦合，给数据驱动建模带来重大挑战，需要建立更基础的理论框架来理解这种纠缠关系。

Method: 采用结构化分解方法，扩展内积空间正交性到结构不对称设置，引入方向性下界、方差不等式、行为指标和记忆有限性指数，建立基于功率的条件来连接有限记忆和热力学第一定律。

Result: 提出了'行为不确定性原理'，证明静态和动态失真不能同时最小化；发现现实系统由于纠缠效应抵抗完全确定性分解；建立了函数方差与均方Lipschitz连续性和学习复杂度的通用定理。

Conclusion: 该框架提供了可扩展的理论基础方法，解释了结构化残差学习的经验优势（改进泛化、减少参数、降低训练成本），适用于复杂动态非线性系统建模。

Abstract: Dynamic nonlinear systems exhibit distortions arising from coupled static and
dynamic effects. Their intertwined nature poses major challenges for
data-driven modeling. This paper presents a theoretical framework grounded in
structured decomposition, variance analysis, and task-centric complexity
bounds.
  The framework employs a directional lower bound on interactions between
measurable system components, extending orthogonality in inner product spaces
to structurally asymmetric settings. This bound supports variance inequalities
for decomposed systems. Key behavioral indicators are introduced along with a
memory finiteness index. A rigorous power-based condition establishes a
measurable link between finite memory in realizable systems and the First Law
of Thermodynamics. This offers a more foundational perspective than classical
bounds based on the Second Law.
  Building on this foundation, we formulate a `Behavioral Uncertainty
Principle,' demonstrating that static and dynamic distortions cannot be
minimized simultaneously. We identify that real-world systems seem to resist
complete deterministic decomposition due to entangled static and dynamic
effects. We also present two general-purpose theorems linking function variance
to mean-squared Lipschitz continuity and learning complexity. This yields a
model-agnostic, task-aware complexity metric, showing that lower-variance
components are inherently easier to learn.
  These insights explain the empirical benefits of structured residual
learning, including improved generalization, reduced parameter count, and lower
training cost, as previously observed in power amplifier linearization
experiments. The framework is broadly applicable and offers a scalable,
theoretically grounded approach to modeling complex dynamic nonlinear systems.

</details>


### [103] [PAC-Bayesian Generalization Bounds for Graph Convolutional Networks on Inductive Node Classification](https://arxiv.org/abs/2509.06600)
*Huayi Tang,Yong Liu*

Main category: cs.LG

TL;DR: 本文提出了图卷积网络在动态图环境中的PAC-Bayesian理论分析框架，针对归纳式节点分类任务，推导了考虑数据依赖性和非平稳性的泛化边界。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图具有动态特性，新节点不断加入，现有连接随时间变化。现有的基于转导学习的理论研究无法充分建模这种时间演化和结构动态性。

Method: 采用PAC-Bayesian理论分析方法，将节点视为依赖且非同分布的数据点，推导单层和双层GCN的泛化边界，明确纳入数据依赖性和非平稳性的影响。

Result: 建立了单层GCN的泛化边界，确定了在节点数量增加时泛化间隙收敛到零的充分条件；发现双层GCN需要更强的图拓扑假设才能保证收敛。

Conclusion: 这项工作为理解和改进动态图环境中GNN的泛化能力奠定了理论基础，揭示了不同层数GCN在动态图环境中的理论特性差异。

Abstract: Graph neural networks (GNNs) have achieved remarkable success in processing
graph-structured data across various applications. A critical aspect of
real-world graphs is their dynamic nature, where new nodes are continually
added and existing connections may change over time. Previous theoretical
studies, largely based on the transductive learning framework, fail to
adequately model such temporal evolution and structural dynamics. In this
paper, we presents a PAC-Bayesian theoretical analysis of graph convolutional
networks (GCNs) for inductive node classification, treating nodes as dependent
and non-identically distributed data points. We derive novel generalization
bounds for one-layer GCNs that explicitly incorporate the effects of data
dependency and non-stationarity, and establish sufficient conditions under
which the generalization gap converges to zero as the number of nodes
increases. Furthermore, we extend our analysis to two-layer GCNs, and reveal
that it requires stronger assumptions on graph topology to guarantee
convergence. This work establishes a theoretical foundation for understanding
and improving GNN generalization in dynamic graph environments.

</details>


### [104] [Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in Molecular Tumor Boards](https://arxiv.org/abs/2509.06602)
*Noel Codella,Sam Preston,Hao Qiu,Leonardo Schettini,Wen-wai Yim,Mert Öz,Shrey Jain,Matthew P. Lungren,Thomas Osborne*

Main category: cs.LG

TL;DR: 使用LLM驱动的多代理系统HAO自动生成瞬症诊疗总结，通过TBFact框架评估摘要的完整性和简洁性，为分子诊疗讨论会提供可靠支持


<details>
  <summary>Details</summary>
Motivation: 传统手工编写瞬症诊疗总结工作量大、主观性强且容易遗漏关键信息，需要自动化解决方案提高效率和质量

Method: 开发HAO多代理系统协调生成患者摘要，设计TBFact"模型作为判官"框架评估摘要质量，包括完整性和简洁性指标

Result: 系统捕获了94%高重要性信息，在严格含义标准下达到0.84的TBFact回忆率，并支持无需共享敏感数据的本地化部署

Conclusion: HAO和TBFact为分子诊疗讨论会提供了可靠、可扩展的自动化支持平台，有效解决了临床摘要生成的效率和质量挑战

Abstract: Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology
specialists collaboratively assess complex patient cases to determine optimal
treatment strategies. A central element of this process is the patient summary,
typically compiled by a medical oncologist, radiation oncologist, or surgeon,
or their trained medical assistant, who distills heterogeneous medical records
into a concise narrative to facilitate discussion. This manual approach is
often labor-intensive, subjective, and prone to omissions of critical
information. To address these limitations, we introduce the Healthcare Agent
Orchestrator (HAO), a Large Language Model (LLM)-driven AI agent that
coordinates a multi-agent clinical workflow to generate accurate and
comprehensive patient summaries for MTBs. Evaluating predicted patient
summaries against ground truth presents additional challenges due to stylistic
variation, ordering, synonym usage, and phrasing differences, which complicate
the measurement of both succinctness and completeness. To overcome these
evaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework
designed to assess the comprehensiveness and succinctness of generated
summaries. Using a benchmark dataset derived from de-identified tumor board
discussions, we applied TBFact to evaluate our Patient History agent. Results
show that the agent captured 94% of high-importance information (including
partial entailments) and achieved a TBFact recall of 0.84 under strict
entailment criteria. We further demonstrate that TBFact enables a data-free
evaluation framework that institutions can deploy locally without sharing
sensitive clinical data. Together, HAO and TBFact establish a robust foundation
for delivering reliable and scalable support to MTBs.

</details>


### [105] [Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors](https://arxiv.org/abs/2509.06608)
*Viacheslav Sinii,Nikita Balagansky,Yaroslav Aksenov,Vadim Kurochkin,Daniil Laptev,Gleb Gerasimov,Alexey Gorbatovski,Boris Shaposhnikov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: 该研究通过轻量级导向向量分析推理训练如何重塑语言模型计算，发现不同层的导向向量通过不同机制提升推理性能：最后一层主要影响首个生成token的偏差，而倒数第二层通过MLP和未嵌入层增强过程词和结构符号。


<details>
  <summary>Details</summary>
Motivation: 理解推理训练如何改变语言模型的计算机制，目前这方面研究还不够深入。研究者希望开发一种既能保持模型可解释性又能达到全微调性能的方法。

Method: 使用强化学习目标训练轻量级导向向量插入基础模型的残差流中，结合logit-lens读取、路径修补和电路分析等技术，分析两个模型的内部机制。

Result: 发现最后一层导向向量表现为token替换偏差，集中在首个生成token上（如提升"To"和"Step"等token）；倒数第二层导向向量保持注意力模式基本不变，主要通过MLP和未嵌入层增强过程词和结构符号。

Conclusion: 该研究为解释推理训练引起的行为变化建立了一个原则性框架，展示了轻量级干预如何有效提升模型推理能力同时保持可解释性。

Abstract: The mechanisms by which reasoning training reshapes language-model
computations remain poorly understood. We study lightweight steering vectors
inserted into the base model's residual stream and trained with a
reinforcement-learning objective, which can match full fine-tuning performance
while retaining the interpretability of small, additive interventions. Using
logit-lens readouts, path patching, and circuit analyses, we analyze two models
and find: (i) the last-layer steering vector behaves like a token-substitution
bias concentrated on the first generated token, consistently boosting tokens
such as "To" and "Step"; and (ii) the penultimate-layer steering vector leaves
attention patterns largely unchanged and instead acts through the MLP and
unembedding, preferentially up-weighting process words and structure symbols.
These results establish a principled framework for interpreting the behavioral
changes induced by reasoning training.

</details>


### [106] [A Survey of Generalization of Graph Anomaly Detection: From Transfer Learning to Foundation Models](https://arxiv.org/abs/2509.06609)
*Junjun Pan,Yu Zheng,Yue Tan,Yixin Liu*

Main category: cs.LG

TL;DR: 本文对图异常检测（GAD）中的泛化问题进行了系统性综述，分析了现有方法的局限性并提出了分类框架。


<details>
  <summary>Details</summary>
Motivation: 现有GAD方法假设训练和测试分布相同且针对特定任务设计，在现实场景中面临数据分布偏移和训练样本稀缺的挑战，缺乏对泛化能力的系统理解。

Method: 通过追踪GAD泛化研究的发展历程，形式化问题设置，建立系统性分类框架，并对现有泛化GAD方法进行全面综述。

Result: 提出了细粒度的分类体系，对现有广义GAD方法进行了最新且全面的回顾，为理解GAD泛化问题提供了系统框架。

Conclusion: 识别了当前开放挑战并提出了未来研究方向，旨在激励这一新兴领域的未来研究，推动GAD模型在现实场景中的实际应用。

Abstract: Graph anomaly detection (GAD) has attracted increasing attention in recent
years for identifying malicious samples in a wide range of graph-based
applications, such as social media and e-commerce. However, most GAD methods
assume identical training and testing distributions and are tailored to
specific tasks, resulting in limited adaptability to real-world scenarios such
as shifting data distributions and scarce training samples in new applications.
To address the limitations, recent work has focused on improving the
generalization capability of GAD models through transfer learning that
leverages knowledge from related domains to enhance detection performance, or
developing "one-for-all" GAD foundation models that generalize across multiple
applications. Since a systematic understanding of generalization in GAD is
still lacking, in this paper, we provide a comprehensive review of
generalization in GAD. We first trace the evolution of generalization in GAD
and formalize the problem settings, which further leads to our systematic
taxonomy. Rooted in this fine-grained taxonomy, an up-to-date and comprehensive
review is conducted for the existing generalized GAD methods. Finally, we
identify current open challenges and suggest future directions to inspire
future research in this emerging field.

</details>


### [107] [BEAM: Brainwave Empathy Assessment Model for Early Childhood](https://arxiv.org/abs/2509.06620)
*Chen Xie,Gaofeng Wu,Kaidong Wang,Zihao Zhu,Xiaoshu Luo,Yan Liang,Feiyu Quan,Ruoxi Wu,Xianghui Huang,Han Zhang*

Main category: cs.LG

TL;DR: 基于多视角EEG信号的深度学习框架BEAM，通过空间-时间特征提取、特征融合和对比学习，较优地预测4-6岁儿童的兼思能力


<details>
  <summary>Details</summary>
Motivation: 传统的兼思能力评估方法存在偏差和主观性问题，EEG技术虽能提供客观测量，但现有方法主要关注静态模式，忽视了时间动态特征

Method: 提出BEAM框架，包含：1) LaBraM编码器进行空间-时间特征提取，2) 多视角特征融合模块，3) 对比学习模块提高类别分离度

Result: 在CBCP数据集上验证，BEAM在多个指标上超过现有最优方法，显示出对儿童兼思能力进行客观评估的潜力

Conclusion: BEAM框架为客观评估儿童兼思能力提供了有效方法，为早期干预措施提供了初步见解，有助于促进儿童社会情感发展

Abstract: Empathy in young children is crucial for their social and emotional
development, yet predicting it remains challenging. Traditional methods often
only rely on self-reports or observer-based labeling, which are susceptible to
bias and fail to objectively capture the process of empathy formation. EEG
offers an objective alternative; however, current approaches primarily extract
static patterns, neglecting temporal dynamics. To overcome these limitations,
we propose a novel deep learning framework, the Brainwave Empathy Assessment
Model (BEAM), to predict empathy levels in children aged 4-6 years. BEAM
leverages multi-view EEG signals to capture both cognitive and emotional
dimensions of empathy. The framework comprises three key components: 1) a
LaBraM-based encoder for effective spatio-temporal feature extraction, 2) a
feature fusion module to integrate complementary information from multi-view
signals, and 3) a contrastive learning module to enhance class separation.
Validated on the CBCP dataset, BEAM outperforms state-of-the-art methods across
multiple metrics, demonstrating its potential for objective empathy assessment
and providing a preliminary insight into early interventions in children's
prosocial development.

</details>


### [108] [Knowledge-Guided Machine Learning for Stabilizing Near-Shortest Path Routing](https://arxiv.org/abs/2509.06640)
*Yung-Fu Chen,Sen Lin,Anish Arora*

Main category: cs.LG

TL;DR: 通过深度神经网络学习本地路由策略，仅需单个图的少量数据样本即可学习到可在欧几里得空间几何随机图上沿用的路由策略，包括重现贪心转发策略和提出更优的GreedyTensile路由策略。


<details>
  <summary>Details</summary>
Motivation: 解决在欧几里得距离空间中的所有对近最短路径问题，通过深度学习方法让每个节点能够仅依靠本地和邻居节点状态高效地进行包转发，并保证学习结果的可推广性。

Method: 利用网络领域知识选择输入特征和设计策略函数，训练深度神经网络学习约最优路由策略。仅需从单个"种子图"采样数据即可实现可推广学习。输入特征包括到目的地距离和节点伸缩等。

Result: 训练的DNN中，仅使用到目的地距离特征的模型准确学习了贪心转发策略。新提出的GreedyTensile路由策略（结合距离和节点伸缩特征）几乎总是超过贪心转发。该策略可通过两个线性操作的低复杂度符号解释，具有超低延迟运行特性。

Conclusion: 该算法通过结合域知识和深度学习，能够从单个图的少量数据中学习到可推广的高效本地路由策略，为网络路由问题提供了一种新的深度学习解决方案。

Abstract: We propose a simple algorithm that needs only a few data samples from a
single graph for learning local routing policies that generalize across a rich
class of geometric random graphs in Euclidean metric spaces. We thus solve the
all-pairs near-shortest path problem by training deep neural networks (DNNs)
that let each graph node efficiently and scalably route (i.e., forward) packets
by considering only the node's state and the state of the neighboring nodes.
Our algorithm design exploits network domain knowledge in the selection of
input features and design of the policy function for learning an approximately
optimal policy. Domain knowledge also provides theoretical assurance that the
choice of a ``seed graph'' and its node data sampling suffices for
generalizable learning. Remarkably, one of these DNNs we train -- using
distance-to-destination as the only input feature -- learns a policy that
exactly matches the well-known Greedy Forwarding policy, which forwards packets
to the neighbor with the shortest distance to the destination. We also learn a
new policy, which we call GreedyTensile routing -- using both
distance-to-destination and node stretch as the input features -- that almost
always outperforms greedy forwarding. We demonstrate the explainability and
ultra-low latency run-time operation of Greedy Tensile routing by symbolically
interpreting its DNN in low-complexity terms of two linear actions.

</details>


### [109] [Group Effect Enhanced Generative Adversarial Imitation Learning for Individual Travel Behavior Modeling under Incentives](https://arxiv.org/abs/2509.06656)
*Yuanyuan Wu,Zhenlin Qin,Leizhen Wang,Xiaolei Ma,Zhenliang Ma*

Main category: cs.LG

TL;DR: 提出gcGAIL模型，通过利用乘客群体共享行为模式来提高个体出行行为建模效率，在准确性、泛化性和模式演示效率方面优于现有方法


<details>
  <summary>Details</summary>
Motivation: 个体出行行为响应建模对城市交通监管和政策评估至关重要，但传统MDP方法数据需求量大，面临数据量、时空覆盖和情境多样性等挑战

Method: 提出群体效应增强的生成对抗模仿学习(gcGAIL)模型，利用乘客群体间的共享行为模式来提升个体行为建模效率

Result: 在公共交通票价折扣案例研究中，gcGAIL在准确性、泛化性和模式演示效率方面优于AIRL、基线GAIL和条件GAIL等先进基准方法，对空间变化、数据稀疏性和行为多样性具有鲁棒性

Conclusion: gcGAIL模型能够预测任何时间的个体行为响应，为个性化激励提供基础，以引导可持续行为改变（更好的激励时机选择）

Abstract: Understanding and modeling individual travel behavior responses is crucial
for urban mobility regulation and policy evaluation. The Markov decision
process (MDP) provides a structured framework for dynamic travel behavior
modeling at the individual level. However, solving an MDP in this context is
highly data-intensive and faces challenges of data quantity, spatial-temporal
coverage, and situational diversity. To address these, we propose a
group-effect-enhanced generative adversarial imitation learning (gcGAIL) model
that improves the individual behavior modeling efficiency by leveraging shared
behavioral patterns among passenger groups. We validate the gcGAIL model using
a public transport fare-discount case study and compare against
state-of-the-art benchmarks, including adversarial inverse reinforcement
learning (AIRL), baseline GAIL, and conditional GAIL. Experimental results
demonstrate that gcGAIL outperforms these methods in learning individual travel
behavior responses to incentives over time in terms of accuracy,
generalization, and pattern demonstration efficiency. Notably, gcGAIL is robust
to spatial variation, data sparsity, and behavioral diversity, maintaining
strong performance even with partial expert demonstrations and underrepresented
passenger groups. The gcGAIL model predicts the individual behavior response at
any time, providing the basis for personalized incentives to induce sustainable
behavior changes (better timing of incentive injections).

</details>


### [110] [TrajAware: Graph Cross-Attention and Trajectory-Aware for Generalisable VANETs under Partial Observations](https://arxiv.org/abs/2509.06665)
*Xiaolu Fu,Ziyuan Bao,Eiman Kanjo*

Main category: cs.LG

TL;DR: TrajAware是一个基于强化学习的车载自组织网络路由框架，通过动作空间剪枝、图交叉注意力和轨迹感知预测来解决动态拓扑和部分观测问题，在边缘设备上实现高效路由。


<details>
  <summary>Details</summary>
Motivation: 车载自组织网络(VANETs)在智能交通系统中至关重要，但由于动态拓扑、不完全观测和边缘设备资源有限，路由仍然具有挑战性。现有的强化学习方法通常假设固定图结构，在网络条件变化时需要重新训练，不适合在受限硬件上部署。

Method: TrajAware框架包含三个组件：(1)动作空间剪枝：减少冗余邻居选项同时保持两跳可达性，缓解维度灾难；(2)图交叉注意力：将剪枝后的邻居映射到全局图上下文，生成适用于不同网络大小的特征；(3)轨迹感知预测：使用历史路由和路口信息估计部分观测下的实时位置。

Result: 在SUMO模拟器中使用真实城市地图进行评估，采用leave-one-city-out设置。结果显示TrajAware实现了接近最短路径和高交付率，同时在受限边缘设备上保持高效性能，在完全和部分观测场景下均优于最先进的基线方法。

Conclusion: TrajAware是一个适用于边缘AI部署的VANET路由框架，通过创新的动作剪枝、注意力机制和轨迹预测技术，有效解决了动态网络环境下的路由挑战，为智能交通系统提供了实用的解决方案。

Abstract: Vehicular ad hoc networks (VANETs) are a crucial component of intelligent
transportation systems; however, routing remains challenging due to dynamic
topologies, incomplete observations, and the limited resources of edge devices.
Existing reinforcement learning (RL) approaches often assume fixed graph
structures and require retraining when network conditions change, making them
unsuitable for deployment on constrained hardware. We present TrajAware, an
RL-based framework designed for edge AI deployment in VANETs. TrajAware
integrates three components: (i) action space pruning, which reduces redundant
neighbour options while preserving two-hop reachability, alleviating the curse
of dimensionality; (ii) graph cross-attention, which maps pruned neighbours to
the global graph context, producing features that generalise across diverse
network sizes; and (iii) trajectory-aware prediction, which uses historical
routes and junction information to estimate real-time positions under partial
observations. We evaluate TrajAware in the open-source SUMO simulator using
real-world city maps with a leave-one-city-out setup. Results show that
TrajAware achieves near-shortest paths and high delivery ratios while
maintaining efficiency suitable for constrained edge devices, outperforming
state-of-the-art baselines in both full and partial observation scenarios.

</details>


### [111] [Barycentric Neural Networks and Length-Weighted Persistent Entropy Loss: A Green Geometric and Topological Framework for Function Approximation](https://arxiv.org/abs/2509.06694)
*Victor Toscano-Duran,Rocio Gonzalez-Diaz,Miguel A. Gutiérrez-Naranjo*

Main category: cs.LG

TL;DR: 提出了一种新的小型浅层神经网络Barycentric Neural Network (BNN)，利用重心坐标和固定基点的概念来精确表示连续分段线性函数，并引入了长度加权持久熵(LWPE)作为新的拓扑特征，通过优化基点而非内部权重来实现快速高效的函数逼近。


<details>
  <summary>Details</summary>
Motivation: 传统深度或过参数化神经网络计算成本高，需要一种在资源受限环境下（有限基点和训练轮次）能够提供灵活且几何可解释的非线性连续函数逼近方法。

Method: 提出BNN网络结构，利用固定基点和重心坐标定义网络；引入长度加权持久熵(LWPE)作为损失函数；直接优化定义BNN的基点而非内部权重。

Result: 实验结果表明，该方法相比MSE、RMSE、MAE和log-cosh等经典损失函数，实现了更优越和更快速的逼近性能。

Conclusion: BNN结合LWPE损失函数的方法为资源受限环境下的函数逼近提供了一个灵活、可解释且高效的解决方案，特别是在基点和训练轮次有限的情况下表现出色。

Abstract: While it is well-established that artificial neural networks are
\emph{universal approximators} for continuous functions on compact domains,
many modern approaches rely on deep or overparameterized architectures that
incur high computational costs. In this paper, a new type of \emph{small
shallow} neural network, called the \emph{Barycentric Neural Network} ($\BNN$),
is proposed, which leverages a fixed set of \emph{base points} and their
\emph{barycentric coordinates} to define both its structure and its parameters.
We demonstrate that our $\BNN$ enables the exact representation of
\emph{continuous piecewise linear functions} ($\CPLF$s), ensuring strict
continuity across segments. Since any continuous function over a compact domain
can be approximated arbitrarily well by $\CPLF$s, the $\BNN$ naturally emerges
as a flexible and interpretable tool for \emph{function approximation}. Beyond
the use of this representation, the main contribution of the paper is the
introduction of a new variant of \emph{persistent entropy}, a topological
feature that is stable and scale invariant, called the \emph{length-weighted
persistent entropy} ($\LWPE$), which is weighted by the lifetime of topological
features. Our framework, which combines the $\BNN$ with a loss function based
on our $\LWPE$, aims to provide flexible and geometrically interpretable
approximations of nonlinear continuous functions in resource-constrained
settings, such as those with limited base points for $\BNN$ design and few
training epochs. Instead of optimizing internal weights, our approach directly
\emph{optimizes the base points that define the $\BNN$}. Experimental results
show that our approach achieves \emph{superior and faster approximation
performance} compared to classical loss functions such as MSE, RMSE, MAE, and
log-cosh.

</details>


### [112] [Probabilistic Modeling of Latent Agentic Substructures in Deep Neural Networks](https://arxiv.org/abs/2509.06701)
*Su Hyeong Lee,Risi Kondor,Richard Ngo*

Main category: cs.LG

TL;DR: 提出了基于概率建模的智能代理理论，通过加权对数池化实现代理组合，严格改善所有成员福利。证明了在线性池化或二元结果空间中不可能实现严格一致，但在三个或更多结果中可能。理论框架具有递归结构，并形式化了LLM中的代理对齐现象。


<details>
  <summary>Details</summary>
Motivation: 为神经模型开发一个基于概率建模的智能代理理论框架，以理解子代理如何聚合成连贯的高层实体，并为智能AI系统的对齐问题提供新的理论见解。

Method: 使用概率建模表示代理为结果分布，以对数分数作为认知效用，通过加权对数池化定义组合。采用克隆不变性、连续性和开放性构建递归结构，通过倾斜分析排除平凡重复。

Result: 证明了严格一致在线性池化或二元结果空间中不可能，但在三个或更多结果中可能。形式化了LLM中的Waluigi效应：引发仁慈角色会诱导对抗对应物，而先显现后抑制策略比单纯强化能更有效地减少一级错位。

Conclusion: 建立智能代理的数学理论框架为理解子代理聚合提供了原则性基础，对智能AI系统的对齐问题具有重要启示意义，展示了理论框架在实践对齐中的应用价值。

Abstract: We develop a theory of intelligent agency grounded in probabilistic modeling
for neural models. Agents are represented as outcome distributions with
epistemic utility given by log score, and compositions are defined through
weighted logarithmic pooling that strictly improves every member's welfare. We
prove that strict unanimity is impossible under linear pooling or in binary
outcome spaces, but possible with three or more outcomes. Our framework admits
recursive structure via cloning invariance, continuity, and openness, while
tilt-based analysis rules out trivial duplication. Finally, we formalize an
agentic alignment phenomenon in LLMs using our theory: eliciting a benevolent
persona ("Luigi'") induces an antagonistic counterpart ("Waluigi"), while a
manifest-then-suppress Waluigi strategy yields strictly larger first-order
misalignment reduction than pure Luigi reinforcement alone. These results
clarify how developing a principled mathematical framework for how subagents
can coalesce into coherent higher-level entities provides novel implications
for alignment in agentic AI systems.

</details>


### [113] [Nested Optimal Transport Distances](https://arxiv.org/abs/2509.06702)
*Ruben Bontorno,Songyan Hou*

Main category: cs.LG

TL;DR: 提出了基于嵌套最优传输距离的金融时间序列生成模型评估方法，并开发了高效并行计算算法


<details>
  <summary>Details</summary>
Motivation: 金融时间序列模拟对压力测试和决策制定至关重要，但缺乏统一的深度生成模型评估标准

Method: 使用时间因果的最优传输距离变体（嵌套最优传输距离），并提出统计一致且可并行化的计算算法

Result: 相比现有方法实现了显著的速度提升，对套期保值、最优停止和强化学习等任务具有鲁棒性

Conclusion: 为金融时间序列生成AI提供了可靠的评估指标和高效的计算方法，有助于提升决策制定质量

Abstract: Simulating realistic financial time series is essential for stress testing,
scenario generation, and decision-making under uncertainty. Despite advances in
deep generative models, there is no consensus metric for their evaluation. We
focus on generative AI for financial time series in decision-making
applications and employ the nested optimal transport distance, a time-causal
variant of optimal transport distance, which is robust to tasks such as
hedging, optimal stopping, and reinforcement learning. Moreover, we propose a
statistically consistent, naturally parallelizable algorithm for its
computation, achieving substantial speedups over existing approaches.

</details>


### [114] [RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn Directly on Robotic Platforms](https://arxiv.org/abs/2509.06714)
*Zakariae El Asri,Ibrahim Laiche,Clément Rambour,Olivier Sigaud,Nicolas Thome*

Main category: cs.LG

TL;DR: 提出了RT-HCP算法，解决机器人控制中样本效率低和推理时间长的双重挑战，通过在延迟推理框架下提供连续动作序列来满足高频控制需求。


<details>
  <summary>Details</summary>
Motivation: 机器人直接学习控制器需要极高的样本效率，而基于模型的强化学习方法虽然样本效率高，但推理时间过长，无法满足机器人控制频率要求。

Method: 定义了一个处理推理延迟的通用框架，让慢速推理的控制器提供连续动作序列来满足控制需求；提出了RT-HCP算法，在性能、样本效率和推理时间之间提供最佳权衡。

Result: 在简单但高频的FURUTA钟摆平台上验证了RT-HCP的优越性，实验结果表明该算法能够有效解决样本效率和推理时间的矛盾。

Conclusion: RT-HCP算法为机器人控制提供了一个优秀的解决方案，在保持高样本效率的同时满足了实时控制的时间要求。

Abstract: Learning a controller directly on the robot requires extreme sample
efficiency. Model-based reinforcement learning (RL) methods are the most sample
efficient, but they often suffer from a too long inference time to meet the
robot control frequency requirements. In this paper, we address the sample
efficiency and inference time challenges with two contributions. First, we
define a general framework to deal with inference delays where the slow
inference robot controller provides a sequence of actions to feed the
control-hungry robotic platform without execution gaps. Then, we compare
several RL algorithms in the light of this framework and propose RT-HCP, an
algorithm that offers an excellent trade-off between performance, sample
efficiency and inference time. We validate the superiority of RT-HCP with
experiments where we learn a controller directly on a simple but high frequency
FURUTA pendulum platform. Code: github.com/elasriz/RTHCP

</details>


### [115] [Long-Range Graph Wavelet Networks](https://arxiv.org/abs/2509.06743)
*Filippo Guerranti,Fabrizio Forte,Simon Geisler,Stephan Günnemann*

Main category: cs.LG

TL;DR: LR-GWN通过将小波滤波器分解为局部和全局组件，统一处理短距离和长距离信息流，在长距离图基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 解决图机器学习中长距离交互建模的挑战，现有基于小波的图神经网络受限于有限阶多项式近似，限制了感受野并阻碍长距离传播

Method: 将小波滤波器分解为互补的局部和全局组件：局部聚合使用高效低阶多项式处理，长距离交互通过灵活的谱域参数化捕获

Result: 在长距离基准测试中达到基于小波方法的最先进性能，同时在短距离数据集上保持竞争力

Conclusion: LR-GWN的混合设计在原则性小波框架内统一了短距离和长距离信息流，有效解决了图长距离交互建模问题

Abstract: Modeling long-range interactions, the propagation of information across
distant parts of a graph, is a central challenge in graph machine learning.
Graph wavelets, inspired by multi-resolution signal processing, provide a
principled way to capture both local and global structures. However, existing
wavelet-based graph neural networks rely on finite-order polynomial
approximations, which limit their receptive fields and hinder long-range
propagation. We propose Long-Range Graph Wavelet Networks (LR-GWN), which
decompose wavelet filters into complementary local and global components. Local
aggregation is handled with efficient low-order polynomials, while long-range
interactions are captured through a flexible spectral domain parameterization.
This hybrid design unifies short- and long-distance information flow within a
principled wavelet framework. Experiments show that LR-GWN achieves
state-of-the-art performance among wavelet-based methods on long-range
benchmarks, while remaining competitive on short-range datasets.

</details>


### [116] [Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization](https://arxiv.org/abs/2509.06759)
*Thanh Thi Nguyen,Campbell Wilson,Janis Dalins*

Main category: cs.LG

TL;DR: 该论文综述了使用深度强化学习(DRL)和直接偏好优化(DPO)技术来微调大型视觉语言模型(LVLMs)，使其与人类价值观对齐并提升任务性能的方法。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态理解方面取得显著进展，但如何通过微调使其与人类价值观对齐、适应特定任务仍是一个关键挑战。DRL和DPO提供了有前景的对齐框架。

Method: 探索DRL和DPO两种范式：DRL通过奖励信号优化模型行为，DPO直接根据偏好数据对齐策略而无需显式奖励模型。分析偏好数据来源、奖励信号以及不同方法分类。

Result: 论文系统梳理了LVLMs微调的对齐方法，提供了技术框架理解，但未报告具体的实验结果数据。

Conclusion: DRL和DPO技术有助于开发更鲁棒且与人类对齐的大型视觉语言模型，但仍面临可扩展性、样本效率、持续学习、泛化能力和安全性等开放挑战。

Abstract: Large Vision-Language Models (LVLMs) or multimodal large language models
represent a significant advancement in artificial intelligence, enabling
systems to understand and generate content across both visual and textual
modalities. While large-scale pretraining has driven substantial progress,
fine-tuning these models for aligning with human values or engaging in specific
tasks or behaviors remains a critical challenge. Deep Reinforcement Learning
(DRL) and Direct Preference Optimization (DPO) offer promising frameworks for
this aligning process. While DRL enables models to optimize actions using
reward signals instead of relying solely on supervised preference data, DPO
directly aligns the policy with preferences, eliminating the need for an
explicit reward model. This overview explores paradigms for fine-tuning LVLMs,
highlighting how DRL and DPO techniques can be used to align models with human
preferences and values, improve task performance, and enable adaptive
multimodal interaction. We categorize key approaches, examine sources of
preference data, reward signals, and discuss open challenges such as
scalability, sample efficiency, continual learning, generalization, and safety.
The goal is to provide a clear understanding of how DRL and DPO contribute to
the evolution of robust and human-aligned LVLMs.

</details>


### [117] [Asynchronous Message Passing for Addressing Oversquashing in Graph Neural Networks](https://arxiv.org/abs/2509.06777)
*Kushal Bose,Swagatam Das*

Main category: cs.LG

TL;DR: 提出异步消息传递框架解决GNN中的过度挤压问题，通过基于节点中心性的批次更新来提升长距离交互性能


<details>
  <summary>Details</summary>
Motivation: GNN在需要长距离交互的任务中存在过度挤压问题，现有图重连方法会破坏归纳偏置并导致信息损失，而增加通道容量又会增加模型复杂度

Method: 提出模型无关的异步更新框架，每层基于节点中心性创建批次，仅更新批次内节点特征，避免同步消息传递中的信息压缩

Result: 在6个标准图数据集和2个长距离数据集上测试，在REDDIT-BINARY和Peptides-struct上分别获得5%和4%的性能提升

Conclusion: 异步消息传递框架能有效缓解过度挤压问题，保持更高的特征敏感度边界，在长距离任务中表现优异

Abstract: Graph Neural Networks (GNNs) suffer from Oversquashing, which occurs when
tasks require long-range interactions. The problem arises from the presence of
bottlenecks that limit the propagation of messages among distant nodes.
Recently, graph rewiring methods modify edge connectivity and are expected to
perform well on long-range tasks. Yet, graph rewiring compromises the inductive
bias, incurring significant information loss in solving the downstream task.
Furthermore, increasing channel capacity may overcome information bottlenecks
but enhance the parameter complexity of the model. To alleviate these
shortcomings, we propose an efficient model-agnostic framework that
asynchronously updates node features, unlike traditional synchronous message
passing GNNs. Our framework creates node batches in every layer based on the
node centrality values. The features of the nodes belonging to these batches
will only get updated. Asynchronous message updates process information
sequentially across layers, avoiding simultaneous compression into
fixed-capacity channels. We also theoretically establish that our proposed
framework maintains higher feature sensitivity bounds compared to standard
synchronous approaches. Our framework is applied to six standard graph datasets
and two long-range datasets to perform graph classification and achieves
impressive performances with a $5\%$ and $4\%$ improvements on REDDIT-BINARY
and Peptides-struct, respectively.

</details>


### [118] [Physics-informed Value Learner for Offline Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2509.06782)
*Vittorio Giammarino,Ruiqi Ni,Ahmed H. Qureshi*

Main category: cs.LG

TL;DR: 提出基于物理信息的正则化损失函数，通过Eikonal偏微分方程在离线目标条件强化学习中引入几何归纳偏置，提升值函数学习效果和泛化能力


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习在自主导航和运动控制等领域有重要应用，但面临状态-动作空间覆盖有限和长时程任务泛化的挑战

Method: 基于Eikonal偏微分方程推导物理信息正则化损失，结合分层隐式Q学习算法，形成Pi-HIQL方法

Result: 在性能表现和泛化能力方面取得显著提升，特别是在stitching机制和大规模导航任务中效果明显

Conclusion: 物理信息正则化方法为离线GCRL提供了有效的几何归纳偏置，能够改善值函数学习并提升算法性能

Abstract: Offline Goal-Conditioned Reinforcement Learning (GCRL) holds great promise
for domains such as autonomous navigation and locomotion, where collecting
interactive data is costly and unsafe. However, it remains challenging in
practice due to the need to learn from datasets with limited coverage of the
state-action space and to generalize across long-horizon tasks. To improve on
these challenges, we propose a Physics-informed (Pi) regularized loss for value
learning, derived from the Eikonal Partial Differential Equation (PDE) and
which induces a geometric inductive bias in the learned value function. Unlike
generic gradient penalties that are primarily used to stabilize training, our
formulation is grounded in continuous-time optimal control and encourages value
functions to align with cost-to-go structures. The proposed regularizer is
broadly compatible with temporal-difference-based value learning and can be
integrated into existing Offline GCRL algorithms. When combined with
Hierarchical Implicit Q-Learning (HIQL), the resulting method, Physics-informed
HIQL (Pi-HIQL), yields significant improvements in both performance and
generalization, with pronounced gains in stitching regimes and large-scale
navigation tasks.

</details>


### [119] [\texttt{R$^\textbf{2}$AI}: Towards Resistant and Resilient AI in an Evolving World](https://arxiv.org/abs/2509.06786)
*Youbang Sun,Xiang Wang,Jie Fu,Chaochao Lu,Bowen Zhou*

Main category: cs.LG

TL;DR: 该论文提出了一种新的AI安全框架R²AI，通过对抗性和迅速恢复能力的协同进化，解决当前AI能力增长与安全进步之间的差距问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全方法分为两种范式："让AI变安全"（后置对齐和护栏）存在脆弱性，"制造安全AI"（本质安全）难以应对未预见风险。需要一种新的方法来应对开放式环境中的动态风险。

Method: 受生物免疫机制启发，提出安全协同进化概念，并提出R²AI框架，包括：1）快速和慢速安全模型；2）通过安全风测进行对抗性模拟和验证；3）持续反馈循环，促使安全性和能力协同进化。

Result: R²AI框架能够统一对抗已知威胁和应对未知风险的能力，提供了一种可扩展的主动安全方案。

Conclusion: 该框架为在动态环境中维持持续安全提供了一条可行的路径，能够同时应对近期脆弱性和长期存在风险，适用于AI向通用人工智能（AGI）和超级智能（ASI）的发展过程。

Abstract: In this position paper, we address the persistent gap between rapidly growing
AI capabilities and lagging safety progress. Existing paradigms divide into
``Make AI Safe'', which applies post-hoc alignment and guardrails but remains
brittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety
but struggles to address unforeseen risks in open-ended environments. We
therefore propose \textit{safe-by-coevolution} as a new formulation of the
``Make Safe AI'' paradigm, inspired by biological immunity, in which safety
becomes a dynamic, adversarial, and ongoing learning process. To operationalize
this vision, we introduce \texttt{R$^2$AI} -- \textit{Resistant and Resilient
AI} -- as a practical framework that unites resistance against known threats
with resilience to unforeseen risks. \texttt{R$^2$AI} integrates \textit{fast
and slow safe models}, adversarial simulation and verification through a
\textit{safety wind tunnel}, and continual feedback loops that guide safety and
capability to coevolve. We argue that this framework offers a scalable and
proactive path to maintain continual safety in dynamic environments, addressing
both near-term vulnerabilities and long-term existential risks as AI advances
toward AGI and ASI.

</details>


### [120] [floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL](https://arxiv.org/abs/2509.06863)
*Bhavya Agrawalla,Michal Nauman,Khush Agarwal,Aviral Kumar*

Main category: cs.LG

TL;DR: FLOQ方法将强化学习中的Q函数参数化为速度场，使用流匹配技术进行训练，通过数值积分步骤控制模型容量，在离线RL基准测试中性能提升1.8倍


<details>
  <summary>Details</summary>
Motivation: 受现代大规模机器学习技术使用密集监督训练中间计算的启发，研究迭代计算在时序差分方法中的潜在优势

Method: 使用流匹配技术参数化Q函数为速度场，通过时序差分目标训练速度场，利用目标速度场的多步数值积分进行自举

Result: 在具有挑战性的离线RL基准测试和在线微调任务中，性能提升近1.8倍，容量扩展能力显著优于标准TD学习架构

Conclusion: 迭代计算在价值学习方面具有巨大潜力，FLOQ方法通过控制积分步骤数实现了比传统架构更精细的容量控制和扩展

Abstract: A hallmark of modern large-scale machine learning techniques is the use of
training objectives that provide dense supervision to intermediate
computations, such as teacher forcing the next token in language models or
denoising step-by-step in diffusion models. This enables models to learn
complex functions in a generalizable manner. Motivated by this observation, we
investigate the benefits of iterative computation for temporal difference (TD)
methods in reinforcement learning (RL). Typically they represent value
functions in a monolithic fashion, without iterative compute. We introduce floq
(flow-matching Q-functions), an approach that parameterizes the Q-function
using a velocity field and trains it using techniques from flow-matching,
typically used in generative modeling. This velocity field underneath the flow
is trained using a TD-learning objective, which bootstraps from values produced
by a target velocity field, computed by running multiple steps of numerical
integration. Crucially, floq allows for more fine-grained control and scaling
of the Q-function capacity than monolithic architectures, by appropriately
setting the number of integration steps. Across a suite of challenging offline
RL benchmarks and online fine-tuning tasks, floq improves performance by nearly
1.8x. floq scales capacity far better than standard TD-learning architectures,
highlighting the potential of iterative computation for value learning.

</details>


### [121] [Concolic Testing on Individual Fairness of Neural Network Models](https://arxiv.org/abs/2509.06864)
*Ming-I Huang,Chih-Duo Hong,Fang Yu*

Main category: cs.LG

TL;DR: PyFair是一个用于评估和验证深度神经网络个体公平性的形式化框架，通过改进PyCT工具生成公平性路径约束，采用双网络架构提供完整性保证，在25个基准模型上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决深度神经网络中存在的歧视性问题，需要一种系统性的方法来评估和验证模型的个体公平性，特别是在关键领域中确保算法的公平性。

Method: 通过改进concolic测试工具PyCT，生成公平性特定的路径约束来系统探索DNN行为，采用双网络架构进行全面的公平性评估，并为特定网络类型提供完整性保证。

Result: 在25个基准模型（包括经过现有偏见缓解技术增强的模型）上评估显示，PyFair能有效检测歧视性实例并验证公平性，但也揭示了复杂模型的可扩展性挑战。

Conclusion: PyFair为预训练DNN的公平性测试和验证提供了一种严格、系统的方法，推动了关键领域中算法公平性的发展。

Abstract: This paper introduces PyFair, a formal framework for evaluating and verifying
individual fairness of Deep Neural Networks (DNNs). By adapting the concolic
testing tool PyCT, we generate fairness-specific path constraints to
systematically explore DNN behaviors. Our key innovation is a dual network
architecture that enables comprehensive fairness assessments and provides
completeness guarantees for certain network types. We evaluate PyFair on 25
benchmark models, including those enhanced by existing bias mitigation
techniques. Results demonstrate PyFair's efficacy in detecting discriminatory
instances and verifying fairness, while also revealing scalability challenges
for complex models. This work advances algorithmic fairness in critical domains
by offering a rigorous, systematic method for fairness testing and verification
of pre-trained DNNs.

</details>


### [122] [AxelSMOTE: An Agent-Based Oversampling Algorithm for Imbalanced Classification](https://arxiv.org/abs/2509.06875)
*Sukumar Kishanthan,Asela Hevapathige*

Main category: cs.LG

TL;DR: AxelSMOTE是一种基于智能体交互的新型过采样方法，通过文化传播模型解决传统过采样技术的局限性，在8个不平衡数据集上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统过采样技术存在特征独立处理、缺乏相似性控制、样本多样性有限和合成多样性管理不足等问题，需要更有效的方法来解决类别不平衡问题

Method: 基于Axelrod文化传播模型，采用四个关键创新：(1)基于特征的分组保留相关性；(2)基于相似性的概率交换机制；(3)Beta分布混合实现真实插值；(4)受控多样性注入避免过拟合

Result: 在8个不平衡数据集上的实验表明，AxelSMOTE在性能上优于最先进的采样方法，同时保持了计算效率

Conclusion: AxelSMOTE通过智能体交互框架有效解决了传统过采样方法的局限性，为类别不平衡问题提供了更优的解决方案

Abstract: Class imbalance in machine learning poses a significant challenge, as skewed
datasets often hinder performance on minority classes. Traditional oversampling
techniques, which are commonly used to alleviate class imbalance, have several
drawbacks: they treat features independently, lack similarity-based controls,
limit sample diversity, and fail to manage synthetic variety effectively. To
overcome these issues, we introduce AxelSMOTE, an innovative agent-based
approach that views data instances as autonomous agents engaging in complex
interactions. Based on Axelrod's cultural dissemination model, AxelSMOTE
implements four key innovations: (1) trait-based feature grouping to preserve
correlations; (2) a similarity-based probabilistic exchange mechanism for
meaningful interactions; (3) Beta distribution blending for realistic
interpolation; and (4) controlled diversity injection to avoid overfitting.
Experiments on eight imbalanced datasets demonstrate that AxelSMOTE outperforms
state-of-the-art sampling methods while maintaining computational efficiency.

</details>


### [123] [Not All Samples Are Equal: Quantifying Instance-level Difficulty in Targeted Data Poisoning](https://arxiv.org/abs/2509.06896)
*William Xu,Yiwei Lu,Yihan Wang,Matthew Y. R. Yang,Zuoqiu Liu,Gautam Kamath,Yaoliang Yu*

Main category: cs.LG

TL;DR: 本文研究了目标数据投毒攻击中不同测试样本的易受攻击性差异，提出了三个预测标准（遍历预测准确率、毒物距离、毒物预算）来评估攻击难度，并通过实验验证了这些指标的有效性。


<details>
  <summary>Details</summary>
Motivation: 目标数据投毒攻击因其易于部署和高成功率而构成严重威胁，但现有研究对攻击难度在不同测试样本间的差异性缺乏理解，需要识别影响漏洞的关键特征。

Method: 提出了三个预测标准：1）通过干净训练动态分析的遍历预测准确率；2）毒物距离；3）毒物预算。通过实验验证这些指标在不同场景下的有效性。

Result: 实验结果表明，这三个指标能够有效预测现实世界中目标投毒攻击的难度变化，为漏洞评估提供了实用工具。

Conclusion: 研究揭示了目标数据投毒攻击的难度差异性，提出的预测标准为从业者提供了评估漏洞和理解数据投毒攻击的重要见解。

Abstract: Targeted data poisoning attacks pose an increasingly serious threat due to
their ease of deployment and high success rates. These attacks aim to
manipulate the prediction for a single test sample in classification models.
Unlike indiscriminate attacks that aim to decrease overall test performance,
targeted attacks present a unique threat to individual test instances. This
threat model raises a fundamental question: what factors make certain test
samples more susceptible to successful poisoning than others? We investigate
how attack difficulty varies across different test instances and identify key
characteristics that influence vulnerability. This paper introduces three
predictive criteria for targeted data poisoning difficulty: ergodic prediction
accuracy (analyzed through clean training dynamics), poison distance, and
poison budget. Our experimental results demonstrate that these metrics
effectively predict the varying difficulty of real-world targeted poisoning
attacks across diverse scenarios, offering practitioners valuable insights for
vulnerability assessment and understanding data poisoning attacks.

</details>


### [124] [Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition](https://arxiv.org/abs/2509.06918)
*Tarhib Al Azad,Shahana Ibrahim*

Main category: cs.LG

TL;DR: 提出了一种针对训练标签噪声的鲁棒OOD检测框架，结合损失校正和低秩稀疏分解方法，在噪声标签环境下显著优于现有技术


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法在训练标签噪声环境下性能显著下降，但缺乏针对这一问题的系统解决方案，直接将噪声标签鲁棒方法与OOD检测策略结合效果不佳

Method: 整合噪声标签学习中的损失校正技术和信号处理中的低秩稀疏分解方法，构建鲁棒的OOD检测框架

Result: 在合成和真实数据集上的大量实验表明，该方法在严重噪声标签设置下显著优于最先进的OOD检测技术

Conclusion: 该框架有效解决了噪声标签环境下的OOD检测问题，为安全关键AI系统提供了更可靠的异常检测能力

Abstract: Robust out-of-distribution (OOD) detection is an indispensable component of
modern artificial intelligence (AI) systems, especially in safety-critical
applications where models must identify inputs from unfamiliar classes not seen
during training. While OOD detection has been extensively studied in the
machine learning literature--with both post hoc and training-based
approaches--its effectiveness under noisy training labels remains
underexplored. Recent studies suggest that label noise can significantly
degrade OOD performance, yet principled solutions to this issue are lacking. In
this work, we demonstrate that directly combining existing label noise-robust
methods with OOD detection strategies is insufficient to address this critical
challenge. To overcome this, we propose a robust OOD detection framework that
integrates loss correction techniques from the noisy label learning literature
with low-rank and sparse decomposition methods from signal processing.
Extensive experiments on both synthetic and real-world datasets demonstrate
that our method significantly outperforms the state-of-the-art OOD detection
techniques, particularly under severe noisy label settings.

</details>


### [125] [Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding](https://arxiv.org/abs/2509.06923)
*Ziheng Li,Zexu Sun,Jinman Zhao,Erxue Min,Yongcheng Zeng,Hui Wu,Hengyi Cai,Shuaiqiang Wang,Dawei Yin,Xu Chen,Zhi-Hong Deng*

Main category: cs.LG

TL;DR: SEELE是一个监督辅助的强化学习框架，通过动态调整问题难度（自适应提示长度）来提高语言模型推理能力，在数学推理基准上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在探索效率低的问题，当问题难度与模型能力不匹配时：问题太难模型找不到可行推理路径，问题太简单模型学习不到新能力

Method: 提出SEELE框架，为每个训练样本附加自适应长度的提示（部分解决方案），通过多轮采样和项目反应理论模型动态调整提示长度，使问题难度保持在高效学习区间

Result: 在六个数学推理基准上平均比GRPO和SFT分别高出11.8和10.5个百分点，比之前最好的监督辅助方法高出3.6个百分点

Conclusion: 通过实例级实时难度调整，SEELE有效解决了RLVR中的探索效率问题，显著提升了语言模型的推理性能

Abstract: Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable
success in enhancing the reasoning capabilities of large language models
(LLMs). However, existing RLVR methods often suffer from exploration
inefficiency due to mismatches between the training data's difficulty and the
model's capability. LLMs fail to discover viable reasoning paths when problems
are overly difficult, while learning little new capability when problems are
too simple. In this work, we formalize the impact of problem difficulty by
quantifying the relationship between loss descent speed and rollout accuracy.
Building on this analysis, we propose SEELE, a novel supervision-aided RLVR
framework that dynamically adjusts problem difficulty to stay within the
high-efficiency region. SEELE augments each training sample by appending a hint
(part of a full solution) after the original problem. Unlike previous
hint-based approaches, SEELE deliberately and adaptively adjusts the hint
length for each problem to achieve an optimal difficulty. To determine the
optimal hint length, SEELE employs a multi-round rollout sampling strategy. In
each round, it fits an item response theory model to the accuracy-hint pairs
collected in preceding rounds to predict the required hint length for the next
round. This instance-level, real-time difficulty adjustment aligns problem
difficulty with the evolving model capability, thereby improving exploration
efficiency. Experimental results show that SEELE outperforms Group Relative
Policy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5
points, respectively, and surpasses the best previous supervision-aided
approach by +3.6 points on average across six math reasoning benchmarks.

</details>


### [126] [Neutron Reflectometry by Gradient Descent](https://arxiv.org/abs/2509.06924)
*Max D. ~Champneys,Andrew J. ~Parnell,Philipp Gutfreund,Maximilian W. A. Skoda,. Patrick A. Fairclough,Timothy J. ~Rogers,Stephanie L. ~Burg*

Main category: cs.LG

TL;DR: 使用自动微分技术直接在中子反射模型上进行梯度下降优化，提高反射数据分析效率保持物理直觉


<details>
  <summary>Details</summary>
Motivation: 中子反射法作为间接测量技术，需要解决复杂的逆向模型问题，现有方法在大量数据或复杂多层结构下效率低下，而机器学习代理模型又失去了物理直觉

Method: 采用自动微分技术计算错误函数对关键参数的准确梯度，直接在前向反射模型上进行梯度下降优化

Result: 在厚氧化石英橡胶甲基框架和有机LED多层设备两个案例中展现出领先的性能，并开源了可微反射内核Python库

Conclusion: 该方法为中子反射数据分析提供了高效保持物理直觉的优化方案，开启了现代优化技术在该领域的应用

Abstract: Neutron reflectometry (NR) is a powerful technique to probe surfaces and
interfaces. NR is inherently an indirect measurement technique, access to the
physical quantities of interest (layer thickness, scattering length density,
roughness), necessitate the solution of an inverse modelling problem, that is
inefficient for large amounts of data or complex multiplayer structures (e.g.
lithium batteries / electrodes). Recently, surrogate machine learning models
have been proposed as an alternative to existing optimisation routines.
Although such approaches have been successful, physical intuition is lost when
replacing governing equations with fast neural networks. Instead, we propose a
novel and efficient approach; to optimise reflectivity data analysis by
performing gradient descent on the forward reflection model itself. Herein,
automatic differentiation techniques are used to evaluate exact gradients of
the error function with respect to the parameters of interest. Access to these
quantities enables users of neutron reflectometry to harness a host of powerful
modern optimisation and inference techniques that remain thus far unexploited
in the context of neutron reflectometry. This paper presents two benchmark case
studies; demonstrating state-of-the-art performance on a thick oxide quartz
film, and robust co-fitting performance in the high complexity regime of
organic LED multilayer devices. Additionally, we provide an open-source library
of differentiable reflectometry kernels in the python programming language so
that gradient based approaches can readily be applied to other NR datasets.

</details>


### [127] [Learning words in groups: fusion algebras, tensor ranks and grokking](https://arxiv.org/abs/2509.06931)
*Maor Shutman,Oren Louidor,Ran Tessler*

Main category: cs.LG

TL;DR: 两层神经网络可以学习有限群中的任意词操作，并在学习过程中表现出grokking现象。通过将问题重构为学习低秩3-张量，网络能够找到低秩实现方式，使用有限宽度来近似词张量。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络如何学习群论中的词操作，特别是探索grokking现象背后的机制，以及网络如何通过低秩分解来实现高效计算。

Method: 将词操作学习问题重构为3-张量学习问题，利用群的自共轭表示和融合结构进行低秩分解。通过替代模型分析网络如何找到低秩实现方式。

Result: 证明标准激活函数的两层神经网络能够学习任意有限群中的词操作，网络能够发现低秩实现方式，在乘法词操作中实现了类似Strassen矩阵乘法的高效计算。

Conclusion: 神经网络通过低秩分解机制学习群论操作，这种机制解释了grokking现象，并为理解梯度下降如何找到此类解提供了新的见解。

Abstract: In this work, we demonstrate that a simple two-layer neural network with
standard activation functions can learn an arbitrary word operation in any
finite group, provided sufficient width is available and exhibits grokking
while doing so. To explain the mechanism by which this is achieved, we reframe
the problem as that of learning a particular $3$-tensor, which we show is
typically of low rank. A key insight is that low-rank implementations of this
tensor can be obtained by decomposing it along triplets of basic self-conjugate
representations of the group and leveraging the fusion structure to rule out
many components. Focusing on a phenomenologically similar but more tractable
surrogate model, we show that the network is able to find such low-rank
implementations (or approximations thereof), thereby using limited width to
approximate the word-tensor in a generalizable way. In the case of the simple
multiplication word, we further elucidate the form of these low-rank
implementations, showing that the network effectively implements efficient
matrix multiplication in the sense of Strassen. Our work also sheds light on
the mechanism by which a network reaches such a solution under gradient
descent.

</details>


### [128] [From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers](https://arxiv.org/abs/2509.06938)
*Praneet Suresh,Jack Stanley,Sonia Joseph,Luca Scimeca,Danilo Bzdok*

Main category: cs.LG

TL;DR: 本文通过稀疏自编码器分析transformer模型中的幻觉现象，发现在输入不确定性增加时，模型会激活与输入无关的语义特征导致幻觉输出，甚至纯噪声输入也能触发有意义的内部概念。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI系统在科学、商业和政府领域的普及，理解其失败模式（如幻觉现象）对于高风险应用中的信任和采用至关重要。

Method: 使用稀疏自编码器捕获预训练transformer模型的概念表示，在实验控制输入空间不确定性的场景下系统分析幻觉产生机制。

Result: 研究发现随着输入信息变得非结构化，transformer模型使用的语义概念数量增加；在输入不确定性增大时，模型倾向于激活连贯但输入不敏感的语义特征；纯噪声输入也能在模型中间激活中触发有意义的概念。

Conclusion: 该研究为AI模型与人类价值观对齐、AI安全、对抗攻击防御以及自动量化模型幻觉风险提供了理论基础和实用方法。

Abstract: As generative AI systems become competent and democratized in science,
business, and government, deeper insight into their failure modes now poses an
acute need. The occasional volatility in their behavior, such as the propensity
of transformer models to hallucinate, impedes trust and adoption of emerging AI
solutions in high-stakes areas. In the present work, we establish how and when
hallucinations arise in pre-trained transformer models through concept
representations captured by sparse autoencoders, under scenarios with
experimentally controlled uncertainty in the input space. Our systematic
experiments reveal that the number of semantic concepts used by the transformer
model grows as the input information becomes increasingly unstructured. In the
face of growing uncertainty in the input space, the transformer model becomes
prone to activate coherent yet input-insensitive semantic features, leading to
hallucinated output. At its extreme, for pure-noise inputs, we identify a wide
variety of robustly triggered and meaningful concepts in the intermediate
activations of pre-trained transformer models, whose functional integrity we
confirm through targeted steering. We also show that hallucinations in the
output of a transformer model can be reliably predicted from the concept
patterns embedded in transformer layer activations. This collection of insights
on transformer internal processing mechanics has immediate consequences for
aligning AI models with human values, AI safety, opening the attack surface for
potential adversarial attacks, and providing a basis for automatic
quantification of a model's hallucination risk.

</details>


### [129] [Outcome-based Exploration for LLM Reasoning](https://arxiv.org/abs/2509.06941)
*Yuda Song,Julia Kempe,Remi Munos*

Main category: cs.LG

TL;DR: 强化学习后训练会降低语言模型生成多样性，提出基于结果的探索方法（历史探索和批量探索）来同时提升准确性和多样性


<details>
  <summary>Details</summary>
Motivation: 基于结果的强化学习虽然能提高LLM推理准确性，但会导致生成多样性系统性下降，这在实际部署中影响性能扩展

Method: 提出基于结果的探索方法：1）历史探索-使用UCB式奖励鼓励罕见答案；2）批量探索-惩罚批次内重复以促进测试时多样性

Result: 在数学竞赛数据集上的实验表明，两种方法都能在提高准确性的同时缓解多样性崩溃问题

Conclusion: 基于结果的探索为RL方法提供了一条实用路径，既能增强推理能力又不牺牲部署所需的多样性

Abstract: Reinforcement learning (RL) has emerged as a powerful method for improving
the reasoning abilities of large language models (LLMs). Outcome-based RL,
which rewards policies solely for the correctness of the final answer, yields
substantial accuracy gains but also induces a systematic loss in generation
diversity. This collapse undermines real-world performance, where diversity is
critical for test-time scaling. We analyze this phenomenon by viewing RL
post-training as a sampling process and show that, strikingly, RL can reduce
effective diversity even on the training set relative to the base model. Our
study highlights two central findings: (i) a transfer of diversity degradation,
where reduced diversity on solved problems propagates to unsolved ones, and
(ii) the tractability of the outcome space, since reasoning tasks admit only a
limited set of distinct answers. Motivated by these insights, we propose
outcome-based exploration, which assigns exploration bonuses according to final
outcomes. We introduce two complementary algorithms: historical exploration,
which encourages rarely observed answers via UCB-style bonuses, and batch
exploration, which penalizes within-batch repetition to promote test-time
diversity. Experiments on standard competition math with Llama and Qwen models
demonstrate that both methods improve accuracy while mitigating diversity
collapse. On the theoretical side, we formalize the benefit of outcome-based
exploration through a new model of outcome-based bandits. Together, these
contributions chart a practical path toward RL methods that enhance reasoning
without sacrificing the diversity essential for scalable deployment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [130] [Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts](https://arxiv.org/abs/2509.05323)
*Adam Cole,Mick Grierson*

Main category: cs.AI

TL;DR: 通过提取和可视化视频渣果模型中的交叉注意力地图，探索注意力机制在文本生成视频中的时空行为，并将其作为创意媒介进行艺术创作。


<details>
  <summary>Details</summary>
Motivation: 受早期视频艺术家操控模拟视频信号的启发，希望通过解释性AI技术探索视频渣果模型的内部机制，让艺术家能够将AI的内部工作过程重新当作创意媒介。

Method: 基于开源Wan模型构建工具，提取和可视化生成式视频模型中的交叉注意力地图，通过探索性探针和艺术案例研究进行分析。

Result: 开发了能够提供解释性视窗的工具，揭示了文本生成视频过程中注意力的时空行为特征，并证明注意力地图既可作为分析工具也可作为原始艺术材料。

Conclusion: 这项工作为艺术解释性AI（XAIxArts）领域做出贡献，鼓励艺术家将AI的内部工作机制重新当作创意媒介进行创作，促进人工智能与艺术创作的融合。

Abstract: This paper presents an artistic and technical investigation into the
attention mechanisms of video diffusion transformers. Inspired by early video
artists who manipulated analog video signals to create new visual aesthetics,
this study proposes a method for extracting and visualizing cross-attention
maps in generative video models. Built on the open-source Wan model, our tool
provides an interpretable window into the temporal and spatial behavior of
attention in text-to-video generation. Through exploratory probes and an
artistic case study, we examine the potential of attention maps as both
analytical tools and raw artistic material. This work contributes to the
growing field of Explainable AI for the Arts (XAIxArts), inviting artists to
reclaim the inner workings of AI as a creative medium.

</details>


### [131] [Perception Graph for Cognitive Attack Reasoning in Augmented Reality](https://arxiv.org/abs/2509.05324)
*Rongqian Chen,Shu Hong,Rifatul Islam,Mahdi Imani,G. Gary Tan,Tian Lan*

Main category: cs.AI

TL;DR: 提出Perception Graph模型来检测和分析AR系统中的认知攻击，通过量化感知扭曲程度来保护用户决策能力


<details>
  <summary>Details</summary>
Motivation: AR系统在战术环境中部署增加，但其依赖无缝人机交互的特性使其容易受到认知攻击，这些攻击会操纵用户感知并严重损害用户决策能力

Method: 引入Perception Graph模型，首先模拟人类从MR环境中解释关键信息的过程，然后用语义有意义的结构表示结果，计算量化分数反映感知扭曲程度

Result: 模型能够计算反映感知扭曲水平的量化分数，为检测和分析认知攻击效果提供了稳健可测量的方法

Conclusion: Perception Graph模型为AR系统提供了一种有效的认知攻击检测和分析工具，有助于保护用户免受感知操纵的影响

Abstract: Augmented reality (AR) systems are increasingly deployed in tactical
environments, but their reliance on seamless human-computer interaction makes
them vulnerable to cognitive attacks that manipulate a user's perception and
severely compromise user decision-making. To address this challenge, we
introduce the Perception Graph, a novel model designed to reason about human
perception within these systems. Our model operates by first mimicking the
human process of interpreting key information from an MR environment and then
representing the outcomes using a semantically meaningful structure. We
demonstrate how the model can compute a quantitative score that reflects the
level of perception distortion, providing a robust and measurable method for
detecting and analyzing the effects of such cognitive attacks.

</details>


### [132] [SynDelay: A Synthetic Dataset for Delivery Delay Prediction](https://arxiv.org/abs/2509.05325)
*Liming Xu,Yunbo Long,Alexandra Brintrup*

Main category: cs.AI

TL;DR: SynDelay是一个用于配送延迟预测的合成数据集，通过先进生成模型基于真实数据创建，既保持真实配送模式又确保隐私，为预测建模提供实用测试平台


<details>
  <summary>Details</summary>
Motivation: 现有配送延迟预测数据集多为专有、小型或不一致维护，阻碍了研究的可复现性和基准测试，需要高质量开放数据集来推动AI在供应链管理中的发展

Method: 使用基于真实数据的先进生成模型创建合成数据集，保留真实配送模式同时确保隐私保护，并提供基线结果和评估指标作为基准参考

Result: 开发了SynDelay合成数据集，虽然不完全无噪声或不一致，但为配送延迟预测提供了具有挑战性和实用性的测试环境，并通过Supply Chain Data Hub公开提供

Conclusion: SynDataset为供应链AI研究提供了有价值的开放数据集资源，鼓励社区贡献数据集、模型和评估实践，共同推动该领域研究发展

Abstract: Artificial intelligence (AI) is transforming supply chain management, yet
progress in predictive tasks -- such as delivery delay prediction -- remains
constrained by the scarcity of high-quality, openly available datasets.
Existing datasets are often proprietary, small, or inconsistently maintained,
hindering reproducibility and benchmarking. We present SynDelay, a synthetic
dataset designed for delivery delay prediction. Generated using an advanced
generative model trained on real-world data, SynDelay preserves realistic
delivery patterns while ensuring privacy. Although not entirely free of noise
or inconsistencies, it provides a challenging and practical testbed for
advancing predictive modelling. To support adoption, we provide baseline
results and evaluation metrics as initial benchmarks, serving as reference
points rather than state-of-the-art claims. SynDelay is publicly available
through the Supply Chain Data Hub, an open initiative promoting dataset sharing
and benchmarking in supply chain AI. We encourage the community to contribute
datasets, models, and evaluation practices to advance research in this area.
All code is openly accessible at https://supplychaindatahub.org.

</details>


### [133] [MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset](https://arxiv.org/abs/2509.05330)
*Seyed Muhammad Hossein Mousavi,Atiye Ilanloo*

Main category: cs.AI

TL;DR: MVRS数据集是一个多模态情感识别数据集，包含VR情绪刺激下的眼动、身体运动、EMG和GSR信号，通过特征提取和融合技术验证了数据集质量和情感可分性


<details>
  <summary>Details</summary>
Motivation: 当前缺乏包含身体运动和生理信号的多模态数据集，限制了情感识别领域的发展

Method: 收集13名参与者在VR情绪刺激下的同步多模态数据（眼动、身体运动、EMG、GSR），进行特征提取和早期/晚期融合，使用分类器评估

Result: 数据集质量良好，情感类别具有可分性，验证了多模态方法的有效性

Conclusion: MVRS数据集为多模态情感计算领域提供了有价值的贡献，有助于推动相关研究发展

Abstract: Automatic emotion recognition has become increasingly important with the rise
of AI, especially in fields like healthcare, education, and automotive systems.
However, there is a lack of multimodal datasets, particularly involving body
motion and physiological signals, which limits progress in the field. To
address this, the MVRS dataset is introduced, featuring synchronized recordings
from 13 participants aged 12 to 60 exposed to VR based emotional stimuli
(relaxation, fear, stress, sadness, joy). Data were collected using eye
tracking (via webcam in a VR headset), body motion (Kinect v2), and EMG and GSR
signals (Arduino UNO), all timestamp aligned. Participants followed a unified
protocol with consent and questionnaires. Features from each modality were
extracted, fused using early and late fusion techniques, and evaluated with
classifiers to confirm the datasets quality and emotion separability, making
MVRS a valuable contribution to multimodal affective computing.

</details>


### [134] [Benchmarking Large Language Models for Personalized Guidance in AI-Enhanced Learning](https://arxiv.org/abs/2509.05346)
*Bo Yuan,Jiazi Hu*

Main category: cs.AI

TL;DR: 这篇论文通过实验比较GPT-4o、DeepSeek-V3和GLM-4.5在个性化教学辅助任务中的表现，发现GPT-4o在准确性、清晰度和可操作性方面更优，为LLM在教育领域的应甠提供了实证基础。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型被趋势性地视为个性化学习的智能助手，但在真实学习场景中的系统性对比评估仍有限。本研究旨在填补这一空白，通过实验比较不同LLM在教学辅助任务中的表现。

Method: 使用包含学生答案和正确标签的数据集，要求三个先进LLM进行：(1)分析测验识别知识组成部分，(2)推断学生掌握情况，(3)生成有针对性的改进建议。采用Gemini作为虚拟判官进行双向比较，评估准确性、清晰度、可操作性和适宜性。

Result: 通过Bradley-Terry模型分析显示，GPT-4o通常被更偏好，能够产生更信息丰富、结构更好的反馈。DeepSeek-V3和GLM-4.5显示出间息性强项但一致性较低。

Conclusion: 研究结果证明了LLM作为高级教学助手提供个性化支持的可行性，为未来LLM驱动的个性化学习实证研究提供了方法论指导。

Abstract: While Large Language Models (LLMs) are increasingly envisioned as intelligent
assistants for personalized learning, systematic head-to-head evaluations
within authentic learning scenarios remain limited. This study conducts an
empirical comparison of three state-of-the-art LLMs on a tutoring task that
simulates a realistic learning setting. Using a dataset comprising a student's
answers to ten questions of mixed formats with correctness labels, each LLM is
required to (i) analyze the quiz to identify underlying knowledge components,
(ii) infer the student's mastery profile, and (iii) generate targeted guidance
for improvement. To mitigate subjectivity and evaluator bias, we employ Gemini
as a virtual judge to perform pairwise comparisons along various dimensions:
accuracy, clarity, actionability, and appropriateness. Results analyzed via the
Bradley-Terry model indicate that GPT-4o is generally preferred, producing
feedback that is more informative and better structured than its counterparts,
while DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower
consistency. These findings highlight the feasibility of deploying LLMs as
advanced teaching assistants for individualized support and provide
methodological guidance for future empirical research on LLM-driven
personalized learning.

</details>


### [135] [SasAgent: Multi-Agent AI System for Small-Angle Scattering Data Analysis](https://arxiv.org/abs/2509.05363)
*Lijie Ding,Changwoo Do*

Main category: cs.AI

TL;DR: SasAgent是一个基于大语言模型的多智能体系统，用于自动化小角散射数据分析，通过SasView工具和文本交互实现高效科学工作流。


<details>
  <summary>Details</summary>
Motivation: 为了解决小角散射数据分析的复杂性，提高自动化水平，利用LLM技术简化科学工作流程。

Method: 采用多智能体架构，包括协调器和三个专门代理（SLD计算、合成数据生成、实验数据拟合），结合SasView工具和RAG技术。

Result: 系统能够准确解释复杂提示、计算散射长度密度、生成精确散射数据，并以高精度拟合实验数据集。

Conclusion: 展示了LLM驱动的AI系统在简化科学工作流程和增强SAS研究自动化方面的巨大潜力。

Abstract: We introduce SasAgent, a multi-agent AI system powered by large language
models (LLMs) that automates small-angle scattering (SAS) data analysis by
leveraging tools from the SasView software and enables user interaction via
text input. SasAgent features a coordinator agent that interprets user prompts
and delegates tasks to three specialized agents for scattering length density
(SLD) calculation, synthetic data generation, and experimental data fitting.
These agents utilize LLM-friendly tools to execute tasks efficiently. These
tools, including the model data tool, Retrieval-Augmented Generation (RAG)
documentation tool, bump fitting tool, and SLD calculator tool, are derived
from the SasView Python library. A user-friendly Gradio-based interface
enhances user accessibility. Through diverse examples, we demonstrate
SasAgent's ability to interpret complex prompts, calculate SLDs, generate
accurate scattering data, and fit experimental datasets with high precision.
This work showcases the potential of LLM-driven AI systems to streamline
scientific workflows and enhance automation in SAS research.

</details>


### [136] [Characterizing Fitness Landscape Structures in Prompt Engineering](https://arxiv.org/abs/2509.05375)
*Arend Hintze*

Main category: cs.AI

TL;DR: 本文通过自相关分析揭示了提示工程优化景观的拓扑结构，发现系统化提示生成产生平滑衰减的自相关，而多样化生成则呈现非单调模式，表明存在崎岖、分层结构的景观。


<details>
  <summary>Details</summary>
Motivation: 当前提示工程方法将优化视为黑盒问题，缺乏对优化景观拓扑结构的理解。本文旨在系统分析提示工程中的适应度景观结构。

Method: 使用语义嵌入空间的自相关分析，在错误检测任务上进行实验，采用两种提示生成策略：系统枚举（1,024个提示）和新颖性驱动的多样化（1,000个提示）。

Result: 发现两种策略产生根本不同的景观拓扑：系统化生成呈现平滑衰减的自相关，多样化生成显示非单调模式，在中间语义距离处出现峰值相关性。不同错误类型显示出不同程度的崎岖性。

Conclusion: 研究结果为理解提示工程景观中优化的复杂性提供了实证基础，揭示了不同提示生成策略产生的不同景观拓扑特征。

Abstract: While prompt engineering has emerged as a crucial technique for optimizing
large language model performance, the underlying optimization landscape remains
poorly understood. Current approaches treat prompt optimization as a black-box
problem, applying sophisticated search algorithms without characterizing the
landscape topology they navigate. We present a systematic analysis of fitness
landscape structures in prompt engineering using autocorrelation analysis
across semantic embedding spaces. Through experiments on error detection tasks
with two distinct prompt generation strategies -- systematic enumeration (1,024
prompts) and novelty-driven diversification (1,000 prompts) -- we reveal
fundamentally different landscape topologies. Systematic prompt generation
yields smoothly decaying autocorrelation, while diversified generation exhibits
non-monotonic patterns with peak correlation at intermediate semantic
distances, indicating rugged, hierarchically structured landscapes.
Task-specific analysis across 10 error detection categories reveals varying
degrees of ruggedness across different error types. Our findings provide an
empirical foundation for understanding the complexity of optimization in prompt
engineering landscapes.

</details>


### [137] [Code Like Humans: A Multi-Agent Solution for Medical Coding](https://arxiv.org/abs/2509.05378)
*Andreas Motzfeldt,Joakim Edin,Casper L. Christensen,Christian Hardmeier,Lars Maaløe,Anna Rogers*

Main category: cs.AI

TL;DR: 提出了Code Like Humans框架，这是首个支持完整ICD-10编码系统（7万+标签）的医疗编码解决方案，在罕见诊断代码上达到最佳性能


<details>
  <summary>Details</summary>
Motivation: 医疗编码需要将非结构化临床记录映射到诊断和程序的字母数字代码，现有方法在处理完整ICD-10系统和罕见代码方面存在局限

Method: 基于大语言模型的代理框架，实现了官方的人类专家编码指南，支持完整的ICD-10编码系统

Result: 在罕见诊断代码上达到最佳性能，但微调判别分类器在高频代码上仍保持优势

Conclusion: 贡献了系统性能分析并识别了系统的'盲点'（系统性地编码不足的代码），为未来工作指明方向

Abstract: In medical coding, experts map unstructured clinical notes to alphanumeric
codes for diagnoses and procedures. We introduce Code Like Humans: a new
agentic framework for medical coding with large language models. It implements
official coding guidelines for human experts, and it is the first solution that
can support the full ICD-10 coding system (+70K labels). It achieves the best
performance to date on rare diagnosis codes (fine-tuned discriminative
classifiers retain an advantage for high-frequency codes, to which they are
limited). Towards future work, we also contribute an analysis of system
performance and identify its `blind spots' (codes that are systematically
undercoded).

</details>


### [138] [Murphys Laws of AI Alignment: Why the Gap Always Wins](https://arxiv.org/abs/2509.05381)
*Madhava Gaikwad*

Main category: cs.AI

TL;DR: 本文提出了Alignment Gap概念，统一解释RLHF等对齐方法的失败模式，通过KL-tilting形式化说明优化压力如何放大代理奖励与人类意图的差异，并提出了Alignment Trilemma和MAPS框架来指导未来设计。


<details>
  <summary>Details</summary>
Motivation: 现有的人类偏好对齐方法（如RLHF、DPO等）虽然有效，但存在奖励攻击、奉承、标注者漂移和错误泛化等重复性失败模式，需要统一的理论框架来理解这些结构性限制。

Method: 使用KL-tilting形式化方法分析优化压力对代理奖励与真实人类意图差异的放大效应，通过小规模实证研究进行验证，并提出Alignment Trilemma和MAPS框架。

Result: 建立了Alignment Gap概念和Murphys Laws of AI Alignment目录，揭示了优化强度、价值捕获和泛化之间的三重困境，为对齐方法的设计提供了结构性指导。

Conclusion: 本文不是提出不可能定理，而是提供了一个重新构建对齐辩论的视角，围绕结构性限制和权衡，为未来设计提供更清晰的指导。

Abstract: Large language models are increasingly aligned to human preferences through
reinforcement learning from human feedback (RLHF) and related methods such as
Direct Preference Optimization (DPO), Constitutional AI, and RLAIF. While
effective, these methods exhibit recurring failure patterns i.e., reward
hacking, sycophancy, annotator drift, and misgeneralization. We introduce the
concept of the Alignment Gap, a unifying lens for understanding recurring
failures in feedback-based alignment. Using a KL-tilting formalism, we
illustrate why optimization pressure tends to amplify divergence between proxy
rewards and true human intent. We organize these failures into a catalogue of
Murphys Laws of AI Alignment, and propose the Alignment Trilemma as a way to
frame trade-offs among optimization strength, value capture, and
generalization. Small-scale empirical studies serve as illustrative support.
Finally, we propose the MAPS framework (Misspecification, Annotation, Pressure,
Shift) as practical design levers. Our contribution is not a definitive
impossibility theorem but a perspective that reframes alignment debates around
structural limits and trade-offs, offering clearer guidance for future design.

</details>


### [139] [From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation](https://arxiv.org/abs/2509.05469)
*Chenguang Wang,Xiang Yan,Yilong Dai,Ziyi Wang,Susu Xu*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Realistic visual renderings of street-design scenarios are essential for
public engagement in active transportation planning. Traditional approaches are
labor-intensive, hindering collective deliberation and collaborative
decision-making. While AI-assisted generative design shows transformative
potential by enabling rapid creation of design scenarios, existing generative
approaches typically require large amounts of domain-specific training data and
struggle to enable precise spatial variations of design/configuration in
complex street-view scenes. We introduce a multi-agent system that edits and
redesigns bicycle facilities directly on real-world street-view imagery. The
framework integrates lane localization, prompt optimization, design generation,
and automated evaluation to synthesize realistic, contextually appropriate
designs. Experiments across diverse urban scenarios demonstrate that the system
can adapt to varying road geometries and environmental conditions, consistently
yielding visually coherent and instruction-compliant results. This work
establishes a foundation for applying multi-agent pipelines to transportation
infrastructure planning and facility design.

</details>


### [140] [TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation](https://arxiv.org/abs/2509.05550)
*Zixi Li*

Main category: cs.AI

TL;DR: TreeGPT是一种新颖的神经架构，结合了transformer注意力机制和全局父子聚合，用于处理抽象语法树，在神经程序合成任务中表现出色，在ARC Prize 2025数据集上达到96%准确率。


<details>
  <summary>Details</summary>
Motivation: 传统方法要么依赖序列处理，要么使用图神经网络，无法有效捕捉抽象语法树的层次结构和全局依赖关系，需要一种能够同时处理局部依赖和层次结构的混合设计。

Method: 采用混合设计：使用self-attention捕捉局部依赖，专门的Tree Feed-Forward Network通过迭代消息传递建模层次树结构。核心创新是全局父子聚合机制，使每个节点能够逐步聚合整个树结构的信息。

Result: 在ARC Prize 2025数据集上达到96%准确率，显著优于transformer基线(1.3%)、Grok-4(15.9%)和SOAR(52%)等模型，仅使用150万参数。消融研究表明边缘投影是最关键组件。

Conclusion: TreeGPT通过结合注意力机制和树结构聚合，在程序合成任务中取得了最先进的性能，证明了混合架构在处理层次化树结构数据方面的有效性。

Abstract: We introduce TreeGPT, a novel neural architecture that combines
transformer-based attention mechanisms with global parent-child aggregation for
processing Abstract Syntax Trees (ASTs) in neural program synthesis tasks.
Unlike traditional approaches that rely solely on sequential processing or
graph neural networks, TreeGPT employs a hybrid design that leverages both
self-attention for capturing local dependencies and a specialized Tree
Feed-Forward Network (TreeFFN) for modeling hierarchical tree structures
through iterative message passing.
  The core innovation lies in our Global Parent-Child Aggregation mechanism,
formalized as: $$h_i^{(t+1)} = \sigma \Big( h_i^{(0)} + W_{pc} \sum_{(p,c) \in
E_i} f(h_p^{(t)}, h_c^{(t)}) + b \Big)$$ where $h_i^{(t)}$ represents the
hidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edges
involving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. This
formulation enables each node to progressively aggregate information from the
entire tree structure through $T$ iterations.
  Our architecture integrates optional enhancements including gated aggregation
with learnable edge weights, residual connections for gradient stability, and
bidirectional propagation for capturing both bottom-up and top-down
dependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challenging
visual reasoning benchmark requiring abstract pattern recognition and rule
inference. Experimental results demonstrate that TreeGPT achieves 96\%
accuracy, significantly outperforming transformer baselines (1.3\%),
large-scale models like Grok-4 (15.9\%), and specialized program synthesis
methods like SOAR (52\%) while using only 1.5M parameters. Our comprehensive
ablation study reveals that edge projection is the most critical component,
with the combination of edge projection and gating achieving optimal
performance.

</details>


### [141] [OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision](https://arxiv.org/abs/2509.05578)
*Ruixun Liu,Lingyu Kong,Derun Li,Hang Zhao*

Main category: cs.AI

TL;DR: OccVLA是一个创新的多模态框架，通过将3D占据表示集成到统一推理过程中，解决了MLLMs在3D空间理解方面的局限性，无需显式3D输入即可实现自动驾驶的先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉-语言推理方面表现出色，但缺乏强大的3D空间理解能力，这对于自动驾驶至关重要。主要挑战包括：构建有效3D表示的难度和大规模3D视觉语言预训练的缺失。

Method: 提出OccVLA框架，将密集3D占据既作为预测输出又作为监督信号，使模型能够直接从2D视觉输入学习细粒度空间结构。占据预测被视为隐式推理过程，在推理时可跳过而不影响性能。

Result: 在nuScenes基准测试中实现了轨迹规划的最先进结果，在3D视觉问答任务上表现出优越性能，为自动驾驶提供了可扩展、可解释且完全基于视觉的解决方案。

Conclusion: OccVLA通过创新的3D占据表示集成方法，成功解决了MLLMs在3D空间理解方面的核心挑战，为自动驾驶领域提供了高效且无需额外计算开销的解决方案。

Abstract: Multimodal large language models (MLLMs) have shown strong vision-language
reasoning abilities but still lack robust 3D spatial understanding, which is
critical for autonomous driving. This limitation stems from two key challenges:
(1) the difficulty of constructing accessible yet effective 3D representations
without expensive manual annotations, and (2) the loss of fine-grained spatial
details in VLMs due to the absence of large-scale 3D vision-language
pretraining. To address these challenges, we propose OccVLA, a novel framework
that integrates 3D occupancy representations into a unified multimodal
reasoning process. Unlike prior approaches that rely on explicit 3D inputs,
OccVLA treats dense 3D occupancy as both a predictive output and a supervisory
signal, enabling the model to learn fine-grained spatial structures directly
from 2D visual inputs. The occupancy predictions are regarded as implicit
reasoning processes and can be skipped during inference without performance
degradation, thereby adding no extra computational overhead. OccVLA achieves
state-of-the-art results on the nuScenes benchmark for trajectory planning and
demonstrates superior performance on 3D visual question-answering tasks,
offering a scalable, interpretable, and fully vision-based solution for
autonomous driving.

</details>


### [142] [MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions](https://arxiv.org/abs/2509.05685)
*Jian Yang,Jiahui Wu,Li Fang,Hongchao Fan,Bianying Zhang,Huijie Zhao,Guangyi Yang,Rui Xin,Xiong You*

Main category: cs.AI

TL;DR: MSRFormer是一个新颖的道路网络表示学习框架，通过多尺度空间交互和轨迹数据整合，解决了道路网络的异质性和层次性挑战，在道路网络分析任务中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 城市道路网络具有异质性和层次性特点，传统图神经网络因同质性假设和单一结构尺度限制而难以准确学习表示，需要新的方法来处理空间交互的流异质性和长距离依赖关系。

Method: 提出MSRFormer框架：使用空间流卷积从轨迹数据提取小尺度特征，识别尺度依赖的空间交互区域，利用图变换器捕获多尺度复杂空间依赖，通过残差连接融合特征，最后采用对比学习算法生成最终表示。

Result: 在两个真实数据集上的验证显示，MSRFormer在道路网络分析任务中优于基线方法，性能提升达16%，特别是在交通相关任务和复杂道路网络结构中表现更佳。

Conclusion: 该研究为开发任务无关的道路网络表示模型提供了实用框架，揭示了尺度效应与空间交互流异质性之间的关联模式，证明了整合轨迹数据对交通相关任务的显著益处。

Abstract: Transforming road network data into vector representations using deep
learning has proven effective for road network analysis. However, urban road
networks' heterogeneous and hierarchical nature poses challenges for accurate
representation learning. Graph neural networks, which aggregate features from
neighboring nodes, often struggle due to their homogeneity assumption and focus
on a single structural scale. To address these issues, this paper presents
MSRFormer, a novel road network representation learning framework that
integrates multi-scale spatial interactions by addressing their flow
heterogeneity and long-distance dependencies. It uses spatial flow convolution
to extract small-scale features from large trajectory datasets, and identifies
scale-dependent spatial interaction regions to capture the spatial structure of
road networks and flow heterogeneity. By employing a graph transformer,
MSRFormer effectively captures complex spatial dependencies across multiple
scales. The spatial interaction features are fused using residual connections,
which are fed to a contrastive learning algorithm to derive the final road
network representation. Validation on two real-world datasets demonstrates that
MSRFormer outperforms baseline methods in two road network analysis tasks. The
performance gains of MSRFormer suggest the traffic-related task benefits more
from incorporating trajectory data, also resulting in greater improvements in
complex road network structures with up to 16% improvements compared to the
most competitive baseline method. This research provides a practical framework
for developing task-agnostic road network representation models and highlights
distinct association patterns of the interplay between scale effects and flow
heterogeneity of spatial interactions.

</details>


### [143] [Towards Meta-Cognitive Knowledge Editing for Multimodal LLMs](https://arxiv.org/abs/2509.05714)
*Zhaoyu Fan,Kaihang Pan,Mingze Zhou,Bosheng Qin,Juncheng Li,Shengyu Zhang,Wenqiao Zhang,Siliang Tang,Fei Wu,Yueting Zhuang*

Main category: cs.AI

TL;DR: CogEdit是一个评估多模态大语言模型元认知知识编辑能力的新基准，包含三个层次：反事实驱动编辑、边界约束编辑和噪声鲁棒编辑。作者提出了MIND框架来提升元认知编辑能力，实验证明其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑基准主要关注认知层面的修改，缺乏对更深层元认知过程的关注。需要填补这一空白，评估MLLMs在元认知层面的知识编辑能力。

Method: 提出了MIND框架：构建元知识记忆库用于自我意识，采用博弈论交互监控知识激活，结合标签精炼实现噪声鲁棒更新。

Result: 大量实验表明，MIND框架在传统和元认知知识编辑基准上都显著优于现有的认知编辑方法，取得了强劲的性能表现。

Conclusion: CogEdit基准和MIND框架有效提升了多模态大语言模型的元认知知识编辑能力，为更深层次的知识更新和评估提供了新的解决方案。

Abstract: Knowledge editing enables multimodal large language models (MLLMs) to
efficiently update outdated or incorrect information. However, existing
benchmarks primarily emphasize cognitive-level modifications while lacking a
focus on deeper meta-cognitive processes. To bridge this gap, we introduce
CogEdit, a novel benchmark designed to evaluate MLLMs' meta-cognitive knowledge
editing abilities across three levels: (1) Counterfactual-Driven Editing,
assessing self-awareness of knowledge correctness changes; (2) Boundary
Constraint Editing, ensuring appropriate generalization without unintended
interference; and (3) Noise-Robust Editing, promoting reflective evaluation of
uncertain information. To advance meta-cognitive editing, we propose MIND
(Meta-cognitive INtegrated Dynamic Knowledge Editing), a framework that
constructs a meta-knowledge memory for self-awareness, employs game-theoretic
interactions to monitor knowledge activation, and incorporates label refinement
for noise-robust updates. Extensive experiments show that MIND significantly
outperforms existing cognitive editing approaches, achieving strong performance
on both traditional and meta-cognitive knowledge editing benchmarks.

</details>


### [144] [Hyperbolic Large Language Models](https://arxiv.org/abs/2509.05757)
*Sarang Patil,Zeyong Zhang,Yiran Huang,Tengfei Ma,Mengjia Xu*

Main category: cs.AI

TL;DR: 该论文综述了双曲几何在大型语言模型中的应用，通过四种主要技术类别（指数/对数映射、双曲微调模型、全双曲LLM和双曲状态空间模型）来增强语义表示学习和多尺度推理能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据往往具有高度非欧几里得的层次结构（如蛋白质网络、交通网络、语言结构等），而传统LLM在处理这类数据的语义蕴含和层次关系方面存在局限，双曲几何因其对树状层次结构的有效建模能力而成为有前景的解决方案。

Method: 提出了双曲LLM的分类法，包括四种主要技术：通过指数/对数映射的双曲LLM、双曲微调模型、全双曲LLM以及双曲状态空间模型。

Result: 建立了双曲几何在LLM中应用的系统分类框架，为复杂数据的语义表示学习提供了新的研究方向和技术路径。

Conclusion: 双曲几何为LLM处理层次化非欧几里得数据提供了有前景的表示空间，未来需要在应用开发和理论研究方面进一步探索。

Abstract: Large language models (LLMs) have achieved remarkable success and
demonstrated superior performance across various tasks, including natural
language processing (NLP), weather forecasting, biological protein folding,
text generation, and solving mathematical problems. However, many real-world
data exhibit highly non-Euclidean latent hierarchical anatomy, such as protein
networks, transportation networks, financial networks, brain networks, and
linguistic structures or syntactic trees in natural languages. Effectively
learning intrinsic semantic entailment and hierarchical relationships from
these raw, unstructured input data using LLMs remains an underexplored area.
Due to its effectiveness in modeling tree-like hierarchical structures,
hyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity
as an expressive latent representation space for complex data modeling across
domains such as graphs, images, languages, and multi-modal data. Here, we
provide a comprehensive and contextual exposition of recent advancements in
LLMs that leverage hyperbolic geometry as a representation space to enhance
semantic representation learning and multi-scale reasoning. Specifically, the
paper presents a taxonomy of the principal techniques of Hyperbolic LLMs
(HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log
maps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4)
hyperbolic state-space models. We also explore crucial potential applications
and outline future research directions. A repository of key papers, models,
datasets, and code implementations is available at
https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main.

</details>


### [145] [DRF: LLM-AGENT Dynamic Reputation Filtering Framework](https://arxiv.org/abs/2509.05764)
*Yuwei Lou,Hao Hu,Shaocong Ma,Zongfei Zhang,Liang Wang,Jidong Ge,Xianping Tao*

Main category: cs.AI

TL;DR: DRF框架通过动态信誉评分和过滤机制，解决了多智能体系统中智能体性能量化和信誉评估的问题，显著提升了任务完成质量和协作效率。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和多智能体系统的发展，现有系统缺乏量化智能体性能和评估智能体可信度的机制，这限制了系统处理复杂任务的能力。

Method: 提出DRF动态信誉过滤框架：构建交互评分网络量化智能体性能，设计信誉评分机制衡量智能体诚实度和能力，集成基于上置信界策略提升智能体选择效率。

Result: 实验表明，DRF在逻辑推理和代码生成任务中显著提高了任务完成质量和协作效率。

Conclusion: DRF为多智能体系统处理大规模任务提供了新的解决方案，通过信誉机制有效提升了系统性能。

Abstract: With the evolution of generative AI, multi - agent systems leveraging large -
language models(LLMs) have emerged as a powerful tool for complex tasks.
However, these systems face challenges in quantifying agent performance and
lack mechanisms to assess agent credibility. To address these issues, we
introduce DRF, a dynamic reputation filtering framework. DRF constructs an
interactive rating network to quantify agent performance, designs a reputation
scoring mechanism to measure agent honesty and capability, and integrates an
Upper Confidence Bound - based strategy to enhance agent selection efficiency.
Experiments show that DRF significantly improves task completion quality and
collaboration efficiency in logical reasoning and code - generation tasks,
offering a new approach for multi - agent systems to handle large - scale
tasks.

</details>


### [146] [Decision-Focused Learning Enhanced by Automated Feature Engineering for Energy Storage Optimisation](https://arxiv.org/abs/2509.05772)
*Nasser Alkhulaifi,Ismail Gokay Dogan,Timothy R. Cargan,Alexander L. Bowler,Direnc Pekaslan,Nicholas J. Watson,Isaac Triguero*

Main category: cs.AI

TL;DR: 这篇论文提出了一种结合自动特征工程(AFE)和决策聚焦学习(DFL)的桌面，用于解决能源管理中的不确定性问题，特别是电池能量存储系统(BESS)的优化运营。该方法在实际数据集上验证，对比传统预测后优化方法，显著降低了运营成本。


<details>
  <summary>Details</summary>
Motivation: 能源管理中的不确定性参数影响优化策略，特别是BESS运营。传统预测后优化方法存在预测错误导致次优决策的问题，而决策聚焦学习方法虽然充分集成预测和优化，但在实际应用中面临数据稀缺和变异性挑战。

Method: 提出AFE-DFL框架，结合自动特征工程和决策聚焦学习方法。AFE用于提取更丰富的表征表示，而DFL集成电价和需求预测与BESS运营优化，以最小化成本。在英国实际房屋数据集上进行验证。

Result: 结果显示DFL方法比传统PTO方法平均优化成本更低。添加AFE后，DFL方法的性能美比未使用AFE的模型提高22.9-56.5%。

Conclusion: 这些发现为DFL在实际应用中的可行性提供了实证支撑，表明领域特定的AFE能够增强DFL的性能，减少对领域专业知识的依赖，为面临类似挑战的能源管理系统带来经济效益。

Abstract: Decision-making under uncertainty in energy management is complicated by
unknown parameters hindering optimal strategies, particularly in Battery Energy
Storage System (BESS) operations. Predict-Then-Optimise (PTO) approaches treat
forecasting and optimisation as separate processes, allowing prediction errors
to cascade into suboptimal decisions as models minimise forecasting errors
rather than optimising downstream tasks. The emerging Decision-Focused Learning
(DFL) methods overcome this limitation by integrating prediction and
optimisation; however, they are relatively new and have been tested primarily
on synthetic datasets or small-scale problems, with limited evidence of their
practical viability. Real-world BESS applications present additional
challenges, including greater variability and data scarcity due to collection
constraints and operational limitations. Because of these challenges, this work
leverages Automated Feature Engineering (AFE) to extract richer representations
and improve the nascent approach of DFL. We propose an AFE-DFL framework
suitable for small datasets that forecasts electricity prices and demand while
optimising BESS operations to minimise costs. We validate its effectiveness on
a novel real-world UK property dataset. The evaluation compares DFL methods
against PTO, with and without AFE. The results show that, on average, DFL
yields lower operating costs than PTO and adding AFE further improves the
performance of DFL methods by 22.9-56.5% compared to the same models without
AFE. These findings provide empirical evidence for DFL's practical viability in
real-world settings, indicating that domain-specific AFE enhances DFL and
reduces reliance on domain expertise for BESS optimisation, yielding economic
benefits with broader implications for energy management systems facing similar
challenges.

</details>


### [147] [Chatbot To Help Patients Understand Their Health](https://arxiv.org/abs/2509.05818)
*Won Seok Jang,Hieu Tran,Manav Mistry,SaiKiran Gandluri,Yifan Zhang,Sharmin Sultana,Sunjae Kown,Yuan Zhang,Zonghai Yao,Hong Yu*

Main category: cs.AI

TL;DR: NoteAid-Chatbot是一个基于多代理LLM和强化学习的对话AI系统，通过"学习即对话"框架促进患者理解医疗信息，无需人工标注数据，在轻量级LLaMA模型上训练，在医疗对话中展现出关键的新兴行为。


<details>
  <summary>Details</summary>
Motivation: 患者需要具备必要的知识来积极参与自己的医疗护理，因此需要开发能够促进患者理解的对话AI系统。

Method: 基于轻量级LLaMA 3.2 3B模型的两阶段训练：先在合成医疗对话数据上进行监督微调，然后通过强化学习（PPO）在模拟医院出院场景中进行奖励优化，奖励基于患者理解评估。

Result: NoteAid-Chatbot展现出清晰性、相关性和结构化对话等关键新兴行为，在Turing测试中超越了非专家人类表现，能够处理多轮交互并整合多样化的教育策略。

Conclusion: 该框架证明了低成本PPO强化学习在现实开放对话领域的可行性，扩展了基于RL的对齐方法的适用性，虽然当前聚焦医疗领域但具有广泛的应用前景。

Abstract: Patients must possess the knowledge necessary to actively participate in
their care. We present NoteAid-Chatbot, a conversational AI that promotes
patient understanding via a novel 'learning as conversation' framework, built
on a multi-agent large language model (LLM) and reinforcement learning (RL)
setup without human-labeled data. NoteAid-Chatbot was built on a lightweight
LLaMA 3.2 3B model trained in two stages: initial supervised fine-tuning on
conversational data synthetically generated using medical conversation
strategies, followed by RL with rewards derived from patient understanding
assessments in simulated hospital discharge scenarios. Our evaluation, which
includes comprehensive human-aligned assessments and case studies, demonstrates
that NoteAid-Chatbot exhibits key emergent behaviors critical for patient
education, such as clarity, relevance, and structured dialogue, even though it
received no explicit supervision for these attributes. Our results show that
even simple Proximal Policy Optimization (PPO)-based reward modeling can
successfully train lightweight, domain-specific chatbots to handle multi-turn
interactions, incorporate diverse educational strategies, and meet nuanced
communication objectives. Our Turing test demonstrates that NoteAid-Chatbot
surpasses non-expert human. Although our current focus is on healthcare, the
framework we present illustrates the feasibility and promise of applying
low-cost, PPO-based RL to realistic, open-ended conversational domains,
broadening the applicability of RL-based alignment methods.

</details>


### [148] [MapAgent: A Hierarchical Agent for Geospatial Reasoning with Dynamic Map Tool Integration](https://arxiv.org/abs/2509.05933)
*Md Hasebul Hasan,Mahir Labib Dihan,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: MapAgent是一个分层多智能体框架，专门用于地理空间推理任务，通过解耦规划与执行来提高工具选择准确性和API协调能力


<details>
  <summary>Details</summary>
Motivation: 现有AI智能体框架主要针对数学、编程等领域，在地理空间任务中表现不足，需要空间推理、多跳规划和实时地图交互能力

Method: 采用分层多智能体架构：高层规划器分解复杂查询为子目标，专用地图工具代理并行协调相关API获取地理空间数据，简单模块无需额外代理开销

Result: 在四个地理空间基准测试(MapEval-Textual、MapEval-API、MapEval-Visual、MapQA)上显著优于现有工具增强和智能体基线方法

Conclusion: MapAgent通过分层设计有效解决了地理空间推理中的认知负载和API协调问题，为地图集成的地理空间推理提供了高效框架

Abstract: Agentic AI has significantly extended the capabilities of large language
models (LLMs) by enabling complex reasoning and tool use. However, most
existing frameworks are tailored to domains such as mathematics, coding, or web
automation, and fall short on geospatial tasks that require spatial reasoning,
multi-hop planning, and real-time map interaction. To address these challenges,
we introduce MapAgent, a hierarchical multi-agent plug-and-play framework with
customized toolsets and agentic scaffolds for map-integrated geospatial
reasoning. Unlike existing flat agent-based approaches that treat tools
uniformly-often overwhelming the LLM when handling similar but subtly different
geospatial APIs-MapAgent decouples planning from execution. A high-level
planner decomposes complex queries into subgoals, which are routed to
specialized modules. For tool-heavy modules-such as map-based services-we then
design a dedicated map-tool agent that efficiently orchestrates related APIs
adaptively in parallel to effectively fetch geospatial data relevant for the
query, while simpler modules (e.g., solution generation or answer extraction)
operate without additional agent overhead. This hierarchical design reduces
cognitive load, improves tool selection accuracy, and enables precise
coordination across similar APIs. We evaluate MapAgent on four diverse
geospatial benchmarks-MapEval-Textual, MapEval-API, MapEval-Visual, and
MapQA-and demonstrate substantial gains over state-of-the-art tool-augmented
and agentic baselines. We open-source our framwork at
https://github.com/Hasebul/MapAgent.

</details>


### [149] [Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL](https://arxiv.org/abs/2509.06024)
*Haoyang He,Zihua Rong,Kun Ji,Chenyang Li,Qing Huang,Chong Xia,Lan Yang,Honggang Zhang*

Main category: cs.AI

TL;DR: 提出了动态推理效率奖励（DRER）框架，通过质量奖励和动态长度优势来优化强化学习中的推理链训练，在Logictree数据集上7B模型达到GPT-o3-mini水平


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的奖励函数只评估答案格式和正确性，无法判断推理链是否真正提升答案质量，且任务特定训练对逻辑深度控制有限

Method: DRER框架包含：（1）推理质量奖励-对提升正确答案概率的推理链给予细粒度奖励；（2）动态长度优势-对偏离验证阈值的响应长度进行优势衰减

Result: 7B模型在400训练步数下达到GPT-o3-mini水平，CoT增强答案的平均置信度提升30%，在多种逻辑推理数据集和AIME24数学基准上表现出良好泛化能力

Conclusion: DRER有效塑造了CoT行为，为增强大语言模型的形式推理能力提供了实用路径

Abstract: Reinforcement learning (RL) has recently become the dominant paradigm for
strengthening the reasoning abilities of large language models (LLMs). Yet the
rule-based reward functions commonly used on mathematical or programming
benchmarks assess only answer format and correctness, providing no signal as to
whether the induced Chain-of-Thought (CoT) actually improves the answer.
Furthermore, such task-specific training offers limited control over logical
depth and therefore may fail to reveal a model's genuine reasoning capacity. We
propose Dynamic Reasoning Efficiency Reward (DRER) -- a plug-and-play RL reward
framework that reshapes both reward and advantage signals. (i) A Reasoning
Quality Reward assigns fine-grained credit to those reasoning chains that
demonstrably raise the likelihood of the correct answer, directly incentivising
the trajectories with beneficial CoT tokens. (ii) A Dynamic Length Advantage
decays the advantage of responses whose length deviates from a
validation-derived threshold, stabilising training. To facilitate rigorous
assessment, we also release Logictree, a dynamically constructed deductive
reasoning dataset that functions both as RL training data and as a
comprehensive benchmark. Experiments confirm the effectiveness of DRER: our 7B
model attains GPT-o3-mini level performance on Logictree with 400 trianing
steps, while the average confidence of CoT-augmented answers rises by 30%. The
model further exhibits generalisation across diverse logical-reasoning
datasets, and the mathematical benchmark AIME24. These results illuminate how
RL shapes CoT behaviour and chart a practical path toward enhancing
formal-reasoning skills in large language models. All code and data are
available in repository https://github.com/Henryhe09/DRER.

</details>


### [150] [Reverse-Engineered Reasoning for Open-Ended Generation](https://arxiv.org/abs/2509.06160)
*Haozhe Wang,Haoran Que,Qixin Xu,Minghao Liu,Wangchunshu Zhou,Jiazhan Feng,Wanjun Zhong,Wei Ye,Tong Yang,Wenhao Huang,Ge Zhang,Fangzhen Lin*

Main category: cs.AI

TL;DR: REER方法通过从已知优质解决方案反向推导推理过程，解决了开放式创作任务中深度推理的难题，超越了传统RL和蒸馏方法的局限


<details>
  <summary>Details</summary>
Motivation: 传统深度推理方法在开放式创作任务中存在局限：强化学习缺乏清晰奖励信号，指令蒸馏成本高昂且受限于教师模型能力

Method: 提出REER（反向工程推理）范式，从已知优质解决方案反向计算推导出潜在的逐步深度推理过程，采用可扩展的无梯度方法

Result: 构建了DeepWriting-20K大规模数据集（2万条深度推理轨迹），训练的DeepWriter-8B模型在性能上超越开源基线，与GPT-4o和Claude 3.5等专有模型竞争甚至更优

Conclusion: REER为开放式创作任务的深度推理提供了新的有效范式，通过反向工程方法成功解决了传统方法的局限性

Abstract: While the ``deep reasoning'' paradigm has spurred significant advances in
verifiable domains like mathematics, its application to open-ended, creative
generation remains a critical challenge. The two dominant methods for
instilling reasoning -- reinforcement learning (RL) and instruction
distillation -- falter in this area; RL struggles with the absence of clear
reward signals and high-quality reward models, while distillation is
prohibitively expensive and capped by the teacher model's capabilities. To
overcome these limitations, we introduce REverse-Engineered Reasoning (REER), a
new paradigm that fundamentally shifts the approach. Instead of building a
reasoning process ``forwards'' through trial-and-error or imitation, REER works
``backwards'' from known-good solutions to computationally discover the latent,
step-by-step deep reasoning process that could have produced them. Using this
scalable, gradient-free approach, we curate and open-source DeepWriting-20K, a
large-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks.
Our model, DeepWriter-8B, trained on this data, not only surpasses strong
open-source baselines but also achieves performance competitive with, and at
times superior to, leading proprietary models like GPT-4o and Claude 3.5.

</details>


### [151] [From Long to Short: LLMs Excel at Trimming Own Reasoning Chains](https://arxiv.org/abs/2509.06174)
*Wei Han,Geng Zhan,Sicheng Yu,Chenyu Wang,Bryan Hooi*

Main category: cs.AI

TL;DR: EDIT是一种测试时缩放方法，通过约束引导生成和联合跟踪长度与答案分布，帮助大型推理模型找到最短的正确推理路径，平衡简洁性和正确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRMs)在复杂推理任务中表现出色，但容易过度思考，产生冗长复杂的推理轨迹，影响可解释性。需要解决多生成目标(正确性和简洁性)平衡的难题。

Method: 提出EDIT(Efficient Dynamic Inference Trimming)方法，采用约束引导生成，同时在不同约束下联合跟踪长度和答案分布，选择在简洁性和正确性之间达到最佳平衡的响应。

Result: 在多样化模型和数据集上的广泛实验表明，EDIT显著提高了推理效率，产生紧凑且信息丰富的输出，改善了可读性和用户体验。

Conclusion: EDIT方法有效缓解了大型推理模型的过度思考问题，通过动态推理修剪实现了推理效率的显著提升，为LRMs的实际应用提供了重要改进。

Abstract: O1/R1 style large reasoning models (LRMs) signal a substantial leap forward
over conventional instruction-following LLMs. By applying test-time scaling to
generate extended reasoning paths, they establish many SOTAs across a wide
range of complex reasoning tasks. However, recent studies show that LRMs are
prone to suffer from overthinking -- the tendency to overcomplicate simple
problems, leading to excessive strategy switching and long, convoluted
reasoning traces that hinder their interpretability. To mitigate this issue, we
conduct a systematic investigation into the reasoning efficiency of a broad set
of LRMs and uncover a common dilemma: the difficulty in balancing multiple
generation objectives such as correctness and brevity. Based on this discovery,
we propose a test-time scaling method, EDIT (Efficient Dynamic Inference
Trimming), which efficiently guides LRMs to identify the shortest correct
reasoning paths at test time. EDIT employs constraint-guided generation while
jointly tracking length and answer distributions under varying constraints,
allowing it to select responses that strike an optimal balance between
conciseness and correctness. Extensive experiments across diverse models and
datasets show that EDIT substantially enhance the reasoning efficiency,
producing compact yet informative outputs that improve readability and user
experience.

</details>


### [152] [PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft Team Environments](https://arxiv.org/abs/2509.06235)
*Olivier Schipper,Yudi Zhang,Yali Du,Mykola Pechenizkiy,Meng Fang*

Main category: cs.AI

TL;DR: 提出了PillagerBench框架用于评估Minecraft中的实时竞争性多智能体系统，并开发了TactiCrafter智能体系统，该系统通过人类可读战术促进团队合作，在竞争中表现出色并展示了自适应学习能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在合作和策略推理任务中表现良好，但在竞争性多智能体环境中的有效性尚未充分探索，需要专门的评估框架。

Method: 开发了PillagerBench评估框架，提供可扩展API、多轮测试和基于规则的对手；提出了TactiCrafter多智能体系统，使用人类可读战术、学习因果依赖并适应对手策略。

Result: TactiCrafter在性能上优于基线方法，通过自我对弈展示了自适应学习能力，并在多轮游戏中分析了其学习过程和策略演化。

Conclusion: PillagerBench为竞争性环境中的多智能体AI研究提供了可复现的比较基准，TactiCrafter展示了LLM智能体在竞争性团队场景中的潜力，框架已开源以促进进一步研究。

Abstract: LLM-based agents have shown promise in various cooperative and strategic
reasoning tasks, but their effectiveness in competitive multi-agent
environments remains underexplored. To address this gap, we introduce
PillagerBench, a novel framework for evaluating multi-agent systems in
real-time competitive team-vs-team scenarios in Minecraft. It provides an
extensible API, multi-round testing, and rule-based built-in opponents for
fair, reproducible comparisons. We also propose TactiCrafter, an LLM-based
multi-agent system that facilitates teamwork through human-readable tactics,
learns causal dependencies, and adapts to opponent strategies. Our evaluation
demonstrates that TactiCrafter outperforms baseline approaches and showcases
adaptive learning through self-play. Additionally, we analyze its learning
process and strategic evolution over multiple game episodes. To encourage
further research, we have open-sourced PillagerBench, fostering advancements in
multi-agent AI for competitive environments.

</details>


### [153] [Proof2Silicon: Prompt Repair for Verified Code and Hardware Generation via Reinforcement Learning](https://arxiv.org/abs/2509.06239)
*Manvi Jha,Jiaxin Wan,Deming Chen*

Main category: cs.AI

TL;DR: Proof2Silicon是一个端到端的硬件合成框架，通过PREFACE的强化学习提示优化确保Dafny代码正确性，自动转换为C代码，最终生成RTL实现，在100任务基准测试中达到72%的端到端硬件合成成功率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成的代码经常无法通过形式验证的问题，特别是在硬件和安全关键领域需要正确性保证的场景。

Method: 1) 使用PREFACE的验证器驱动RL代理迭代优化提示生成，确保Dafny代码正确性；2) 将验证后的Dafny程序自动转换为可合成的高级C代码；3) 使用Vivado HLS生成RTL实现。

Result: PREFACE的RL引导提示优化将Dafny验证成功率提高了21%，Proof2Silicon实现了高达72%的端到端硬件合成成功率。

Conclusion: 该工作展示了一个强大、可扩展且自动化的流水线，能够从自然语言规范生成经过形式验证的硬件，实现了从规范到硅实现的桥梁。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
automated code generation but frequently produce code that fails formal
verification, an essential requirement for hardware and safety-critical
domains. To overcome this fundamental limitation, we previously proposed
PREFACE, a model-agnostic framework based on reinforcement learning (RL) that
iteratively repairs the prompts provided to frozen LLMs, systematically
steering them toward generating formally verifiable Dafny code without costly
fine-tuning. This work presents Proof2Silicon, a novel end-to-end synthesis
framework that embeds the previously proposed PREFACE flow to enable the
generation of correctness-by-construction hardware directly from natural
language specifications. Proof2Silicon operates by: (1) leveraging PREFACE's
verifier-driven RL agent to optimize prompt generation iteratively, ensuring
Dafny code correctness; (2) automatically translating verified Dafny programs
into synthesizable high-level C using Dafny's Python backend and PyLog; and (3)
employing Vivado HLS to produce RTL implementations. Evaluated rigorously on a
challenging 100-task benchmark, PREFACE's RL-guided prompt optimization
consistently improved Dafny verification success rates across diverse LLMs by
up to 21%. Crucially, Proof2Silicon achieved an end-to-end hardware synthesis
success rate of up to 72%, generating RTL designs through Vivado HLS synthesis
flows. These results demonstrate a robust, scalable, and automated pipeline for
LLM-driven, formally verified hardware synthesis, bridging natural-language
specification and silicon realization.

</details>


### [154] [REMI: A Novel Causal Schema Memory Architecture for Personalized Lifestyle Recommendation Agents](https://arxiv.org/abs/2509.06269)
*Vishal Raman,Vijai Aravindh R,Abhijith Ragav*

Main category: cs.AI

TL;DR: REMI是一个基于因果模式记忆的多模态生活方式助手，通过整合个人因果知识图谱、因果推理引擎和模式规划模块，提供可解释的个性化推荐


<details>
  <summary>Details</summary>
Motivation: 现有AI助手难以整合复杂的个人数据和因果知识，导致建议过于通用且缺乏解释性

Method: 使用个人因果图谱记录用户生活事件和习惯，通过目标导向的因果遍历、外部知识增强和假设推理，结合可适应计划模式生成定制行动方案，由大语言模型协调各组件

Result: 基于CSM的代理比基准LLM代理能提供更上下文感知、用户对齐的推荐

Conclusion: 这项工作展示了在个性化代理中进行记忆增强因果推理的新方法，推动了透明可信AI生活方式助手的发展

Abstract: Personalized AI assistants often struggle to incorporate complex personal
data and causal knowledge, leading to generic advice that lacks explanatory
power. We propose REMI, a Causal Schema Memory architecture for a multimodal
lifestyle agent that integrates a personal causal knowledge graph, a causal
reasoning engine, and a schema based planning module. The idea is to deliver
explainable, personalized recommendations in domains like fashion, personal
wellness, and lifestyle planning. Our architecture uses a personal causal graph
of the user's life events and habits, performs goal directed causal traversals
enriched with external knowledge and hypothetical reasoning, and retrieves
adaptable plan schemas to generate tailored action plans. A Large Language
Model orchestrates these components, producing answers with transparent causal
explanations. We outline the CSM system design and introduce new evaluation
metrics for personalization and explainability, including Personalization
Salience Score and Causal Reasoning Accuracy, to rigorously assess its
performance. Results indicate that CSM based agents can provide more context
aware, user aligned recommendations compared to baseline LLM agents. This work
demonstrates a novel approach to memory augmented, causal reasoning in
personalized agents, advancing the development of transparent and trustworthy
AI lifestyle assistants.

</details>


### [155] [TableMind: An Autonomous Programmatic Agent for Tool-Augmented Table Reasoning](https://arxiv.org/abs/2509.06278)
*Chuang Jiang,Mingyue Cheng,Xiaoyu Tao,Qingyang Mao,Jie Ouyang,Qi Liu*

Main category: cs.AI

TL;DR: TableMind是一个基于大语言模型的表格推理代理，通过自主工具调用、代码执行和自适应策略，显著提升表格数据推理的准确性和计算精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于纯文本的方法在复杂数值计算和细粒度操作方面存在不足，而工具集成推理方法又缺乏真正的自主适应性，需要更灵活的表格推理解决方案。

Method: 采用两阶段微调范式：首先进行监督微调建立有效的工具使用模式，然后通过强化学习微调优化多目标策略，特别是提出了Rank-Aware Policy Optimization (RAPO)方法。

Result: 在多个主流基准测试中，TableMind相比竞争基线实现了卓越性能，在推理准确性和计算精度方面都取得了显著提升。

Conclusion: TableMind通过自主工具调用、安全代码执行和自适应策略，为表格推理任务提供了一个高效且准确的解决方案，证明了其在复杂数值计算场景中的优势。

Abstract: Table reasoning is crucial for leveraging structured data in domains such as
finance, healthcare, and scientific research. While large language models
(LLMs) show promise in multi-step reasoning, purely text-based methods often
struggle with the complex numerical computations and fine-grained operations
inherently required in this task. Tool-integrated reasoning improves
computational accuracy via explicit code execution, yet existing systems
frequently rely on rigid patterns, supervised imitation, and lack true
autonomous adaptability. In this paper, we present TableMind, an LLM-driven
table reasoning agent that (i) autonomously performs multi-turn tool
invocation, (ii) writes and executes data-analyzing code in a secure sandbox
environment for data analysis and precise numerical reasoning, and (iii)
exhibits high-level capabilities such as planning and self-reflection to adapt
strategies. To realize these capabilities, we adopt a two-stage fine-tuning
paradigm built on top of a powerful pre-trained language model: supervised
fine-tuning on high-quality reasoning trajectories to establish effective tool
usage patterns, followed by reinforcement fine-tuning to optimize
multi-objective strategies. In particular, we propose Rank-Aware Policy
Optimization (RAPO), which increases the update weight of high-quality
trajectories when their output probabilities are lower than those of
low-quality ones, thereby guiding the model more consistently toward better and
more accurate answers. Extensive experiments on several mainstream benchmarks
demonstrate that TableMind achieves superior performance compared to
competitive baselines, yielding substantial gains in both reasoning accuracy
and computational precision.

</details>


### [156] [SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents](https://arxiv.org/abs/2509.06283)
*Xuan-Phi Nguyen,Shrey Pandit,Revanth Gangi Reddy,Austin Xu,Silvio Savarese,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 本文提出一种使用合成数据的简单强化学习方法，通过持续强化学习优化理性模型，开发了自主单代理深度研究模型，在Humanity's Last Exam测试中达到28.7%的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解锁深度研究等重要应用，需要为大语言模型配备复杂的交错理性和工具使用能力，特别是发展自主单代理模型来动态决策下一步动作。

Method: 采用持续强化学习方法，使用全部合成数据对理性优化模型进行训练，集成最小化网络爬取和Python工具。

Result: 最佳模型SFR-DR-20B在Humanity's Last Exam测试中达到28.7%的性能水平，显著提升了自主代理的深度研究能力。

Conclusion: 通过持续强化学习理性优化模型，可以有效提升自主单代理的深度研究能力，为代理智能研究提供了新的方向。

Abstract: Equipping large language models (LLMs) with complex, interleaved reasoning
and tool-use capabilities has become a key focus in agentic AI research,
especially with recent advances in reasoning-oriented (``thinking'') models.
Such capabilities are key to unlocking a number of important applications. One
such application is Deep Research (DR), which requires extensive search and
reasoning over many sources. Our work in this paper focuses on the development
of native Autonomous Single-Agent models for DR featuring minimal web crawling
and Python tool integration. Unlike multi-agent systems, where agents take up
pre-defined roles and are told what to do at each step in a static workflow, an
autonomous single-agent determines its next action dynamically based on
context, without manual directive. While prior work has proposed training
recipes for base or instruction-tuned LLMs, we focus on continual reinforcement
learning (RL) of reasoning-optimized models to further enhance agentic skills
while preserving reasoning ability. Towards this end, we propose a simple RL
recipe with entirely synthetic data, which we apply to various open-source
LLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam
benchmark. In addition, we conduct key analysis experiments to provide more
insights into our methodologies.

</details>


### [157] [From Implicit Exploration to Structured Reasoning: Leveraging Guideline and Refinement for LLMs](https://arxiv.org/abs/2509.06284)
*Jiaxiang Chen,Zhuo Wang,Mingxi Zou,Zhucong Li,Zhijian Zhou,Song Wang,Zenglin Xu*

Main category: cs.AI

TL;DR: 提出了一个从隐式探索转向结构化推理的框架，通过指导原则和精炼机制来提升大语言模型的推理稳定性、错误纠正能力和经验学习


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖隐式探索，导致推理路径不稳定、缺乏错误纠正机制、无法从过往经验中学习

Method: 从成功轨迹中提取结构化推理模式，从失败中获取反思信号；推理时逐步遵循指导原则，每一步后进行精炼以纠正错误和稳定推理过程

Result: 在BBH、GSM8K、MATH-500、MBPP、HumanEval等多个基准测试中 consistently 超越强基线，提升了推理稳定性和泛化能力

Conclusion: 结构化推理配合逐步执行和精炼机制有效提升了推理质量，指导原则具有良好的跨域迁移能力，支持跨模型协作，在效果和可扩展性上达到或超过监督微调

Abstract: Large language models (LLMs) have advanced general-purpose reasoning, showing
strong performance across diverse tasks. However, existing methods often rely
on implicit exploration, where the model follows stochastic and unguided
reasoning paths-like walking without a map. This leads to unstable reasoning
paths, lack of error correction, and limited learning from past experience. To
address these issues, we propose a framework that shifts from implicit
exploration to structured reasoning through guideline and refinement. First, we
extract structured reasoning patterns from successful trajectories and
reflective signals from failures. During inference, the model follows these
guidelines step-by-step, with refinement applied after each step to correct
errors and stabilize the reasoning process. Experiments on BBH and four
additional benchmarks (GSM8K, MATH-500, MBPP, HumanEval) show that our method
consistently outperforms strong baselines across diverse reasoning tasks.
Structured reasoning with stepwise execution and refinement improves stability
and generalization, while guidelines transfer well across domains and flexibly
support cross-model collaboration, matching or surpassing supervised
fine-tuning in effectiveness and scalability.

</details>


### [158] [Can AI Make Energy Retrofit Decisions? An Evaluation of Large Language Models](https://arxiv.org/abs/2509.06307)
*Lei Shu,Dong Zhao*

Main category: cs.AI

TL;DR: 评估7种大型语言模型在住宅能源改造决策中的表现，发现LLMs在技术目标上表现更好，但在社会技术决策中受限于经济权衡和本地背景，需要提高准确性、一致性和上下文处理能力。


<details>
  <summary>Details</summary>
Motivation: 传统建筑能源改造决策方法存在通用性有限和可解释性低的问题，阻碍了在多样化住宅环境中的采用。随着智能连接社区的发展，生成式AI特别是大型语言模型可能通过处理上下文信息来提供可读建议。

Method: 评估7种LLMs（ChatGPT、DeepSeek、Gemini、Grok、Llama和Claude）在住宅改造决策中的表现，使用包含49个美国州400个住宅的数据集，从准确性、一致性、敏感性和推理四个维度进行评估。

Result: LLMs在许多情况下能生成有效建议，无需微调即可达到54.5%的top 1匹配率和92.8%的top 5匹配率。技术目标表现更强，模型间一致性低，高表现模型往往与其他模型不同。对位置和建筑几何形状敏感，但对技术和居住者行为不太敏感。

Conclusion: LLMs在能源改造决策中是很有前景的助手，但需要在准确性、一致性和上下文处理方面进行改进才能实现可靠实践。

Abstract: Conventional approaches to building energy retrofit decision making suffer
from limited generalizability and low interpretability, hindering adoption in
diverse residential contexts. With the growth of Smart and Connected
Communities, generative AI, especially large language models (LLMs), may help
by processing contextual information and producing practitioner readable
recommendations. We evaluate seven LLMs (ChatGPT, DeepSeek, Gemini, Grok,
Llama, and Claude) on residential retrofit decisions under two objectives:
maximizing CO2 reduction (technical) and minimizing payback period
(sociotechnical). Performance is assessed on four dimensions: accuracy,
consistency, sensitivity, and reasoning, using a dataset of 400 homes across 49
US states. LLMs generate effective recommendations in many cases, reaching up
to 54.5 percent top 1 match and 92.8 percent within top 5 without fine tuning.
Performance is stronger for the technical objective, while sociotechnical
decisions are limited by economic trade offs and local context. Agreement
across models is low, and higher performing models tend to diverge from others.
LLMs are sensitive to location and building geometry but less sensitive to
technology and occupant behavior. Most models show step by step, engineering
style reasoning, but it is often simplified and lacks deeper contextual
awareness. Overall, LLMs are promising assistants for energy retrofit decision
making, but improvements in accuracy, consistency, and context handling are
needed for reliable practice.

</details>


### [159] [Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation](https://arxiv.org/abs/2509.06337)
*Jianpeng Zhao,Chenyu Yuan,Weiming Luo,Haoling Xie,Guangwei Zhang,Steven Jige Quan,Zixuan Yuan,Pengyang Wang,Denghui Zhang*

Main category: cs.AI

TL;DR: 使用大型语言模型模拟虚拟调查受访者，提出了PAS和FAS两种模拟设置，构建了LLM-S^3基准测试套件，评估了多个主流LLM在社会科学调查模拟中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统问卷调查方法成本高、耗时长、规模有限，需要探索更高效、可扩展的替代方案来支持社会科学研究和政策制定。

Method: 提出两种模拟设置：PAS（基于部分属性预测缺失属性）和FAS（生成完整合成数据集），使用11个真实数据集构建LLM-S^3基准，评估GPT-3.5/4 Turbo、LLaMA 3.0/3.1-8B等模型。

Result: 发现LLM在预测性能上表现一致趋势，识别了失败模式，证明上下文和提示设计对模拟保真度有重要影响。

Conclusion: 为LLM驱动的调查模拟建立了严谨基础，为社会学研究和政策评估提供了可扩展、成本效益高的工具。

Abstract: Questionnaire-based surveys are foundational to social science research and
public policymaking, yet traditional survey methods remain costly,
time-consuming, and often limited in scale. This paper explores a new paradigm:
simulating virtual survey respondents using Large Language Models (LLMs). We
introduce two novel simulation settings, namely Partial Attribute Simulation
(PAS) and Full Attribute Simulation (FAS), to systematically evaluate the
ability of LLMs to generate accurate and demographically coherent responses. In
PAS, the model predicts missing attributes based on partial respondent
profiles, whereas FAS involves generating complete synthetic datasets under
both zero-context and context-enhanced conditions. We curate a comprehensive
benchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey
Simulation), that spans 11 real-world public datasets across four sociological
domains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA
3.0/3.1-8B) reveals consistent trends in prediction performance, highlights
failure modes, and demonstrates how context and prompt design impact simulation
fidelity. This work establishes a rigorous foundation for LLM-driven survey
simulations, offering scalable and cost-effective tools for sociological
research and policy evaluation. Our code and dataset are available at:
https://github.com/dart-lab-research/LLM-S-Cube-Benchmark

</details>


### [160] [Evaluating Multi-Turn Bargain Skills in LLM-Based Seller Agent](https://arxiv.org/abs/2509.06341)
*Issue Yishu Wang,Kakam Chong,Xiaofeng Wang,Xu Yan,DeXin Kong,Chen Ju,Ming Chen,Shuai Xiao,Shuguang Han,jufeng chen*

Main category: cs.AI

TL;DR: 提出了一个基于心理理论的多轮电子商务议价评估框架，用于测试卖家代理在长期谈判中提取和跟踪买家意图的能力。


<details>
  <summary>Details</summary>
Motivation: 在线二手市场中，多轮议价是买卖双方互动的关键环节。LLM可以作为卖家代理在给定业务约束下与买家谈判，但需要准确跟踪和解释长期谈判中的累积买家意图。

Method: 开发了一个多轮评估框架，包括：(1)大规模电子商务议价基准数据集；(2)基于心理理论的回合级评估框架；(3)从海量对话数据中提取可靠意图的自动化流程。

Result: 构建了覆盖622个类别、9,892种产品和3,014个任务的大规模基准，提供了超越结果指标的意图标注评估方法。

Conclusion: 该框架为评估卖家代理的议价能力提供了有效的工具，特别关注意图跟踪和理解能力，对电子商务对话系统的发展具有重要意义。

Abstract: In online second-hand marketplaces, multi-turn bargaining is a crucial part
of seller-buyer interactions. Large Language Models (LLMs) can act as seller
agents, negotiating with buyers on behalf of sellers under given business
constraints. A critical ability for such agents is to track and accurately
interpret cumulative buyer intents across long negotiations, which directly
impacts bargaining effectiveness. We introduce a multi-turn evaluation
framework for measuring the bargaining ability of seller agents in e-commerce
dialogues. The framework tests whether an agent can extract and track buyer
intents. Our contributions are: (1) a large-scale e-commerce bargaining
benchmark spanning 622 categories, 9,892 products, and 3,014 tasks; (2) a
turn-level evaluation framework grounded in Theory of Mind (ToM) with annotated
buyer intents, moving beyond outcome-only metrics; and (3) an automated
pipeline that extracts reliable intent from massive dialogue data.

</details>


### [161] [A data-driven discretized CS:GO simulation environment to facilitate strategic multi-agent planning research](https://arxiv.org/abs/2509.06355)
*Yunzhe Wang,Volkan Ustun,Chris McGroarty*

Main category: cs.AI

TL;DR: DECOY是一个新颖的多智能体模拟器，将3D地形中的战略长期规划抽象为高级离散化模拟，同时保持低层次环境保真度。使用CS:GO作为测试平台，仅通过移动决策来准确模拟游戏玩法。


<details>
  <summary>Details</summary>
Motivation: 现代复杂多智能体交互的模拟环境需要在高度保真细节和计算效率之间取得平衡。需要一种能够抽象战略规划同时保持环境真实性的模拟框架。

Method: 采用路径点系统简化和离散化连续状态和动作，配合基于真实CS:GO比赛数据训练的神经预测和生成模型来重建事件结果。不显式建模瞄准和射击等低层次机制。

Result: 广泛评估表明，从人类数据生成的DECOY回放与原始游戏中观察到的回放高度匹配。模拟环境能够准确重现游戏玩法。

Conclusion: DECOY提供了一个有价值的工具，可用于推进战略多智能体规划和行为生成的研究，为复杂交互模拟提供了高效且保真的解决方案。

Abstract: Modern simulation environments for complex multi-agent interactions must
balance high-fidelity detail with computational efficiency. We present DECOY, a
novel multi-agent simulator that abstracts strategic, long-horizon planning in
3D terrains into high-level discretized simulation while preserving low-level
environmental fidelity. Using Counter-Strike: Global Offensive (CS:GO) as a
testbed, our framework accurately simulates gameplay using only movement
decisions as tactical positioning -- without explicitly modeling low-level
mechanics such as aiming and shooting. Central to our approach is a waypoint
system that simplifies and discretizes continuous states and actions, paired
with neural predictive and generative models trained on real CS:GO tournament
data to reconstruct event outcomes. Extensive evaluations show that replays
generated from human data in DECOY closely match those observed in the original
game. Our publicly available simulation environment provides a valuable tool
for advancing research in strategic multi-agent planning and behavior
generation.

</details>


### [162] [Teaching AI Stepwise Diagnostic Reasoning with Report-Guided Chain-of-Thought Learning](https://arxiv.org/abs/2509.06409)
*Yihong Luo,Wenwu He,Zhuo-Xu Cui,Dong Liang*

Main category: cs.AI

TL;DR: DiagCoT是一个多阶段框架，通过监督微调通用视觉语言模型，仅使用自由文本报告模拟放射科医生的逐步诊断推理，显著提升了疾病分类、病理定位和报告生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在医疗诊断中缺乏放射科医生的逐步推理能力，且难以从非结构化的临床报告中学习诊断逻辑。DiagCoT旨在通过模拟医生的诊断思维过程，开发可解释且诊断能力强的AI系统。

Method: 采用三阶段方法：1）对比图像-报告调优实现领域对齐；2）思维链监督捕获推理逻辑；3）强化学习调优结合临床奖励信号提升事实准确性和流畅性。仅使用自由文本报告作为监督信号。

Result: 在MIMIC-CXR基准测试中：疾病分类AUC从0.52提升至0.76（+0.24），病理定位mIoU从0.08提升至0.31（+0.23），报告生成BLEU从0.11提升至0.33（+0.22）。在长尾疾病和外部数据集上优于LLaVA-Med和CXR-LLAVA等先进模型。

Conclusion: DiagCoT通过将非结构化临床叙述转化为结构化监督，为开发可解释且诊断能力强的放射学AI系统提供了一种可扩展的方法，显著提升了模型的诊断性能和推理能力。

Abstract: This study presents DiagCoT, a multi-stage framework that applies supervised
fine-tuning to general-purpose vision-language models (VLMs) to emulate
radiologists' stepwise diagnostic reasoning using only free-text reports.
DiagCoT combines contrastive image-report tuning for domain alignment,
chain-of-thought supervision to capture inferential logic, and reinforcement
tuning with clinical reward signals to enhance factual accuracy and fluency. On
the MIMIC-CXR benchmark, DiagCoT improved zero-shot disease classification AUC
from 0.52 to 0.76 (absolute gain of 0.24), pathology grounding mIoU from 0.08
to 0.31 (absolute gain of 0.23), and report generation BLEU from 0.11 to 0.33
(absolute gain of 0.22). It outperformed state-of-the-art models including
LLaVA-Med and CXR-LLAVA on long-tailed diseases and external datasets. By
converting unstructured clinical narratives into structured supervision,
DiagCoT offers a scalable approach for developing interpretable and
diagnostically competent AI systems for radiology.

</details>


### [163] [Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning](https://arxiv.org/abs/2509.06436)
*Song Yu,Xiaofei Xu,Ke Deng,Li Li,Lin Tian*

Main category: cs.AI

TL;DR: 提出了Tree of Agents (TOA)多智能体推理框架，通过将长输入分段处理并动态交换信息，有效解决LLM长上下文处理中的"中间信息丢失"问题，在保持API开销相当的情况下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在处理长上下文任务时面临的"中间信息丢失"问题，现有方法要么可能丢弃关键信息，要么导致注意力分散。

Method: 采用多智能体推理框架，将输入分段由独立智能体处理，生成局部认知后通过树状结构路径动态交换信息进行协作推理，结合前缀哈希缓存和自适应剪枝策略提高效率。

Result: 实验表明，基于紧凑的LLaMA3.1-8B模型的TOA显著优于多个基线模型，并在各种长上下文任务上达到与最新大型商业模型（如Gemini1.5-pro）相当的性能。

Conclusion: TOA框架通过多智能体协作推理有效缓解位置偏见和减少幻觉，为长上下文处理提供了高效解决方案，代码已开源。

Abstract: Large language models (LLMs) face persistent challenges when handling
long-context tasks, most notably the lost in the middle issue, where
information located in the middle of a long input tends to be underutilized.
Some existing methods that reduce input have the risk of discarding key
information, while others that extend context windows often lead to attention
dispersion. To address these limitations, we propose Tree of Agents (TOA), a
multi-agent reasoning framework that segments the input into chunks processed
by independent agents. Each agent generates its local cognition, then agents
dynamically exchange information for collaborative reasoning along
tree-structured paths. TOA enables agents to probe different reasoning orders
for multi-perspective understanding, effectively mitigating position bias and
reducing hallucinations. To improve processing efficiency, we incorporate
prefix-hash caching and adaptive pruning strategies, achieving significant
performance improvements with comparable API overhead. Experiments show that
TOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple
baselines and demonstrates comparable performance to the latest and much larger
commercial models, such as Gemini1.5-pro, on various long-context tasks. Code
is available at https://github.com/Aireduce952/Tree-of-Agents.

</details>


### [164] [HyFedRAG: A Federated Retrieval-Augmented Generation Framework for Heterogeneous and Privacy-Sensitive Data](https://arxiv.org/abs/2509.06444)
*Cheng Qian,Hainan Zhang,Yongxin Tong,Hong-Wei Zheng,Zhiming Zheng*

Main category: cs.AI

TL;DR: HyFedRAG是一个面向混合数据模态的联邦RAG框架，通过边缘-云协作机制解决医疗领域异构隐私数据的检索问题，在保持数据隐私的同时支持SQL、知识图谱和文档等多种数据格式。


<details>
  <summary>Details</summary>
Motivation: 集中式RAG系统在处理异构和隐私敏感数据时存在困难，特别是在分布式医疗环境中，患者数据分散在SQL、知识图谱和临床笔记等多种格式中，且传统云基RAG系统无法有效处理边缘设备和隐私约束。

Method: 1) 基于Flower设计边缘-云协作RAG框架，边缘侧LLM将异构数据转换为标准化隐私保护表示，服务器侧LLM进行全局推理和生成；2) 集成轻量级本地检索器和隐私感知LLM，提供三种匿名化工具；3) 设计三级缓存策略优化响应延迟。

Result: 在PMC-Patients数据集上的实验表明，HyFedRAG在检索质量、生成一致性和系统效率方面优于现有基线方法。

Conclusion: 该框架为结构化异构数据提供了可扩展且符合隐私要求的RAG解决方案，在敏感和多样化数据环境中释放了LLMs的潜力。

Abstract: Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive
data, especially in distributed healthcare settings where patient data spans
SQL, knowledge graphs, and clinical notes. Clinicians face difficulties
retrieving rare disease cases due to privacy constraints and the limitations of
traditional cloud-based RAG systems in handling diverse formats and edge
devices. To address this, we introduce HyFedRAG, a unified and efficient
Federated RAG framework tailored for Hybrid data modalities. By leveraging an
edge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across
diverse data sources while preserving data privacy. Our key contributions are:
(1) We design an edge-cloud collaborative RAG framework built on Flower, which
supports querying structured SQL data, semi-structured knowledge graphs, and
unstructured documents. The edge-side LLMs convert diverse data into
standardized privacy-preserving representations, and the server-side LLMs
integrates them for global reasoning and generation. (2) We integrate
lightweight local retrievers with privacy-aware LLMs and provide three
anonymization tools that enable each client to produce semantically rich,
de-identified summaries for global inference across devices. (3) To optimize
response latency and reduce redundant computation, we design a three-tier
caching strategy consisting of local cache, intermediate representation cache,
and cloud inference cache. Experimental results on PMC-Patients demonstrate
that HyFedRAG outperforms existing baselines in terms of retrieval quality,
generation consistency, and system efficiency. Our framework offers a scalable
and privacy-compliant solution for RAG over structural-heterogeneous data,
unlocking the potential of LLMs in sensitive and diverse data environments.

</details>


### [165] [Accelerate Scaling of LLM Alignment via Quantifying the Coverage and Depth of Instruction Set](https://arxiv.org/abs/2509.06463)
*Chengwei Wu,Li Du,Hanyu Zhao,Yiming Ju,Jiapu Wang,Tengfei Pan*

Main category: cs.AI

TL;DR: 本文提出了一种新的指令数据选择方法，通过最大化指令深度和语义覆盖率来持续提升大语言模型在下游任务中的对齐性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在下游任务中的应用需求增长，提升模型对齐性能和效率变得至关重要。当前指令集优化方法在指令池不断扩展时无法持续提升性能，需要找到影响对齐模型性能的关键因素。

Method: 首先研究指令数据集分布与对齐模型性能关系的关键因素，发现指令深度和语义空间覆盖率是关键因素。然后设计指令选择算法，同时最大化所选指令的深度和语义覆盖率。

Result: 实验结果表明，相比最先进的基线方法，该方法能够以更快的速度持续提升模型性能，实现"加速扩展"。指令深度和语义覆盖率可以解释开发集上70%以上的模型损失。

Conclusion: 指令深度和语义空间覆盖率是决定下游性能的关键因素，提出的指令选择方法能够有效提升大语言模型的对齐性能，实现可持续的性能改进。

Abstract: With the growing demand for applying large language models to downstream
tasks, improving model alignment performance and efficiency has become crucial.
Such a process involves selecting informative instructions from a candidate
pool. However, due to the complexity of instruction set distributions, the key
factors driving the performance of aligned models remain unclear. As a result,
current instruction set refinement methods fail to improve performance as the
instruction pool expands continuously. To address this issue, we first
investigate the key factors that influence the relationship between instruction
dataset distribution and aligned model performance. Based on these insights, we
propose a novel instruction data selection method. We identify that the depth
of instructions and the coverage of the semantic space are the crucial factors
determining downstream performance, which could explain over 70\% of the model
loss on the development set. We then design an instruction selection algorithm
to simultaneously maximize the depth and semantic coverage of the selected
instructions. Experimental results demonstrate that, compared to
state-of-the-art baseline methods, it can sustainably improve model performance
at a faster pace and thus achieve \emph{``Accelerated Scaling''}.

</details>


### [166] [MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents](https://arxiv.org/abs/2509.06477)
*Pengxiang Zhao,Guangyi Liu,Yaozhen Liang,Weiqing He,Zhengxi Lu,Yuehao Huang,Yaxuan Guo,Kexin Zhang,Hao Wang,Liang Liu,Yong Liu*

Main category: cs.AI

TL;DR: MAS-Bench是一个用于评估GUI-快捷方式混合智能体在移动领域性能的基准测试，包含139个复杂任务和88个预定义快捷方式，填补了该领域的评估空白。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统评估GUI操作与快捷方式（API、深度链接等）混合智能体的框架，需要建立基准来推动更高效智能代理的发展。

Method: 构建包含139个跨11个真实应用复杂任务的基准测试，提供88个预定义快捷方式知识库，设计7个评估指标来测试智能体自主生成快捷方式和工作流的能力。

Result: 实验显示混合智能体相比纯GUI操作的智能体在成功率和效率方面都有显著提升，证明了评估方法的有效性。

Conclusion: MAS-Bench填补了关键评估空白，为创建更高效、鲁棒的智能代理提供了基础平台，推动了GUI-快捷方式混合范式的发展。

Abstract: To enhance the efficiency of GUI agents on various platforms like smartphones
and computers, a hybrid paradigm that combines flexible GUI operations with
efficient shortcuts (e.g., API, deep links) is emerging as a promising
direction. However, a framework for systematically benchmarking these hybrid
agents is still underexplored. To take the first step in bridging this gap, we
introduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut
hybrid agents with a specific focus on the mobile domain. Beyond merely using
predefined shortcuts, MAS-Bench assesses an agent's capability to autonomously
generate shortcuts by discovering and creating reusable, low-cost workflows. It
features 139 complex tasks across 11 real-world applications, a knowledge base
of 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation
metrics. The tasks are designed to be solvable via GUI-only operations, but can
be significantly accelerated by intelligently embedding shortcuts. Experiments
show that hybrid agents achieve significantly higher success rates and
efficiency than their GUI-only counterparts. This result also demonstrates the
effectiveness of our method for evaluating an agent's shortcut generation
capabilities. MAS-Bench fills a critical evaluation gap, providing a
foundational platform for future advancements in creating more efficient and
robust intelligent agents.

</details>


### [167] [MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply Chain Optimization](https://arxiv.org/abs/2509.06490)
*Niki Kotecha,Ehecatl Antonio del Rio Chanona*

Main category: cs.AI

TL;DR: 提出结合强化学习和多目标进化算法的方法，用于供应链动态多目标优化，通过CVaR引入风险敏感决策，在库存管理案例中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统多目标优化方法难以适应供应链的动态性和不确定性，需要实时灵活的决策方案来平衡成本、服务水平和环境可持续性等冲突目标

Method: 使用多目标进化算法搜索策略神经网络的参数空间，生成帕累托前沿策略集，结合条件风险价值(CVaR)进行风险敏感决策，支持动态策略切换

Result: 在案例研究中展示了应对供应链动态变化的能力，在库存管理案例中优于最先进方法，提高了决策效率和系统韧性

Conclusion: 该方法为供应链不确定性管理和性能优化提供了更鲁棒的框架，实现了实时灵活的多目标决策

Abstract: In supply chain management, decision-making often involves balancing multiple
conflicting objectives, such as cost reduction, service level improvement, and
environmental sustainability. Traditional multi-objective optimization methods,
such as linear programming and evolutionary algorithms, struggle to adapt in
real-time to the dynamic nature of supply chains. In this paper, we propose an
approach that combines Reinforcement Learning (RL) and Multi-Objective
Evolutionary Algorithms (MOEAs) to address these challenges for dynamic
multi-objective optimization under uncertainty. Our method leverages MOEAs to
search the parameter space of policy neural networks, generating a Pareto front
of policies. This provides decision-makers with a diverse population of
policies that can be dynamically switched based on the current system
objectives, ensuring flexibility and adaptability in real-time decision-making.
We also introduce Conditional Value-at-Risk (CVaR) to incorporate
risk-sensitive decision-making, enhancing resilience in uncertain environments.
We demonstrate the effectiveness of our approach through case studies,
showcasing its ability to respond to supply chain dynamics and outperforming
state-of-the-art methods in an inventory management case study. The proposed
strategy not only improves decision-making efficiency but also offers a more
robust framework for managing uncertainty and optimizing performance in supply
chains.

</details>


### [168] [Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers](https://arxiv.org/abs/2509.06493)
*Ran Xin,Zeyu Zheng,Yanchen Nie,Kun Yuan,Xia Xiao*

Main category: cs.AI

TL;DR: BFS-Prover-V2是一个解决LLM在自动定理证明中训练和推理双重扩展问题的系统，通过多轮离线RL框架和规划增强的多智能体搜索架构，在形式数学基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动定理证明中面临训练时强化学习和推理时计算资源扩展的双重挑战，需要克服性能平台期和搜索空间过大的问题

Method: 1) 多轮离线RL框架，采用AlphaZero启发的多阶段专家迭代流程，包含自适应策略级数据过滤和周期性重训练；2) 规划增强的多智能体搜索架构，使用通用推理模型作为高层规划器分解复杂定理，通过共享证明缓存实现并行证明器协作

Result: 在MiniF2F和ProofNet测试集上分别达到95.08%和41.4%的准确率，在形式数学基准测试中取得最先进结果

Conclusion: 该系统成功解决了LLM在定理证明中的双重扩展问题，提出的RL和推理技术具有广泛适用性，可应用于其他需要长视野多轮推理和复杂搜索的领域

Abstract: The integration of Large Language Models (LLMs) into automated theorem
proving has shown immense promise, yet is fundamentally constrained by
challenges in scaling up both training-time reinforcement learning (RL) and
inference-time compute. This paper introduces \texttt{BFS-Prover-V2}, a system
designed to address this dual scaling problem. We present two primary
innovations. The first is a novel multi-turn off-policy RL framework for
continually improving the performance of LLM step-prover at training time. This
framework, inspired by the principles of AlphaZero, utilizes a multi-stage
expert iteration pipeline featuring adaptive tactic-level data filtering and
periodic retraining to surmount the performance plateaus that typically curtail
long-term RL in LLM-based agents. The second innovation is a planner-enhanced
multi-agent search architecture that scales reasoning capabilities at inference
time. This architecture employs a general reasoning model as a high-level
planner to iteratively decompose complex theorems into a sequence of simpler
subgoals. This hierarchical approach substantially reduces the search space,
enabling a team of parallel prover agents to collaborate efficiently by
leveraging a shared proof cache. We demonstrate that this dual approach to
scaling yields state-of-the-art results on established formal mathematics
benchmarks. \texttt{BFS-Prover-V2} achieves 95.08\% and 41.4\% on the MiniF2F
and ProofNet test sets respectively. While demonstrated in the domain of formal
mathematics, the RL and inference techniques presented in this work are of
broader interest and may be applied to other domains requiring long-horizon
multi-turn reasoning and complex search.

</details>


### [169] [An AI system to help scientists write expert-level empirical software](https://arxiv.org/abs/2509.06503)
*Eser Aygün,Anastasiya Belyaeva,Gheorghe Comanici,Marc Coram,Hao Cui,Jake Garrison,Renee Johnston Anton Kast,Cory Y. McLean,Peter Norgaard,Zahra Shamsi,David Smalling,James Thompson,Subhashini Venugopalan,Brian P. Williams,Chujun He,Sarah Martinson,Martyna Plomecka,Lai Wei,Yuchen Zhou,Qian-Ze Zhu,Matthew Abraham,Erica Brand,Anna Bulanova,Jeffrey A. Cardille,Chris Co,Scott Ellsworth,Grace Joseph,Malcolm Kane,Ryan Krueger,Johan Kartiwa,Dan Liebling,Jan-Matthis Lueckmann,Paul Raccuglia,Xuefei,Wang,Katherine Chou,James Manyika,Yossi Matias,John C. Platt,Lizzie Dorfman,Shibl Mourad,Michael P. Brenner*

Main category: cs.AI

TL;DR: AI系统使用大语言模型和树搜索自动生成专家级科学软件，在多个领域超越人类专家方法


<details>
  <summary>Details</summary>
Motivation: 科学发现过程中手动创建计算实验软件速度慢，成为瓶颈，需要自动化工具加速科学进步

Method: 结合大语言模型(LLM)和树搜索(TS)方法，系统性地改进质量指标并智能探索解决方案空间

Result: 在生物信息学中发现40种优于人类方法的新单细胞分析方法；在流行病学中生成14个超越CDC集合模型的COVID-19住院预测模型；在多个领域产生最先进软件

Conclusion: 该系统通过为多样化任务设计新颖解决方案，代表了加速科学进步的重要一步

Abstract: The cycle of scientific discovery is frequently bottlenecked by the slow,
manual creation of software to support computational experiments. To address
this, we present an AI system that creates expert-level scientific software
whose goal is to maximize a quality metric. The system uses a Large Language
Model (LLM) and Tree Search (TS) to systematically improve the quality metric
and intelligently navigate the large space of possible solutions. The system
achieves expert-level results when it explores and integrates complex research
ideas from external sources. The effectiveness of tree search is demonstrated
across a wide range of benchmarks. In bioinformatics, it discovered 40 novel
methods for single-cell data analysis that outperformed the top human-developed
methods on a public leaderboard. In epidemiology, it generated 14 models that
outperformed the CDC ensemble and all other individual models for forecasting
COVID-19 hospitalizations. Our method also produced state-of-the-art software
for geospatial analysis, neural activity prediction in zebrafish, time series
forecasting and numerical solution of integrals. By devising and implementing
novel solutions to diverse tasks, the system represents a significant step
towards accelerating scientific progress.

</details>


### [170] [CogGuide: Human-Like Guidance for Zero-Shot Omni-Modal Reasoning](https://arxiv.org/abs/2509.06641)
*Zhou-Peng Shou,Zhi-Qiang You,Fang Wang,Hai-Bo Liu*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于"意图草笼"的零检验多模态推理组件，通过模仿人类认知策略来解决多模态大模型中的"短接"问题和上下文理解不足问题。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大模型在复杂跨模态推理中存在的"短接"问题和上下文理解不足的挑战，提高模型的深度理解能力。

Method: 设计了一个插拔式三模块流水线：意图感知器、策略生成器和策略选择器，显式构建"理解-规划-选择"的认知过程，通过生成和筛选"意图草笼"策略来指导最终推理。

Result: 在IntentBench、WorldSense和Daily-Omni数据集上验证了方法的普适性和稳健收益，相比各自基线，完整的"三模块"方案在不同推理引擎和流水线组合中均实现了一致收益，最高提升约9.51个百分点。

Conclusion: 该方法无需参数微调，仅通过上下文工程实现跨模型转移，信息论分析显示其能够降低条件熵并提高信息利用效率，压制意外的短接推理，在零检验场景中具有实际价值和可移植性。

Abstract: Targeting the issues of "shortcuts" and insufficient contextual understanding
in complex cross-modal reasoning of multimodal large models, this paper
proposes a zero-shot multimodal reasoning component guided by human-like
cognitive strategies centered on an "intent sketch". The component comprises a
plug-and-play three-module pipeline-Intent Perceiver, Strategy Generator, and
Strategy Selector-that explicitly constructs a "understand-plan-select"
cognitive process. By generating and filtering "intent sketch" strategies to
guide the final reasoning, it requires no parameter fine-tuning and achieves
cross-model transfer solely through in-context engineering.
Information-theoretic analysis shows that this process can reduce conditional
entropy and improve information utilization efficiency, thereby suppressing
unintended shortcut reasoning. Experiments on IntentBench, WorldSense, and
Daily-Omni validate the method's generality and robust gains; compared with
their respective baselines, the complete "three-module" scheme yields
consistent improvements across different reasoning engines and pipeline
combinations, with gains up to approximately 9.51 percentage points,
demonstrating the practical value and portability of the "intent sketch"
reasoning component in zero-shot scenarios.

</details>


### [171] [Reinforcement Learning Foundations for Deep Research Systems: A Survey](https://arxiv.org/abs/2509.06733)
*Wenjun Li,Zhi Chen,Jingru Lin,Hannan Cao,Wei Han,Sheng Liang,Zhi Zhang,Kuicai Dong,Dexun Li,Chen Zhang,Yong Liu*

Main category: cs.AI

TL;DR: 这是首个专注于深度研究系统强化学习基础的综述，系统化了以DeepSeek-R1为基础的研究进展，包括数据合成、RL方法、训练系统等多个方面，为基于RL的深度研究机器人训练提供实践指南。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究系统主要依赖SFT和DPO方法，但这些方法存在模仿偏差、暴露偏差、代理依赖性、偏好对齐问题等限制，而强化学习能够通过优化轨迹级攻略、支持探索和恢复行为、减少对人类先验知识的依赖来充分利用环境反馈。

Method: 本综述沿三个轴心系统化相关工作：(i)数据合成与管理；(ii)研究机器人RL方法，包括稳定性、样本效率、长上下文处理、奖励与信贷设计、多目标优化、多模态集成等；(iii)机器人RL训练系统与框架。同时涵盖代理7结构、协调、评估标准等内容。

Result: 本综述归纳了重复模式、持续的基础设施瓶颈，并为使用RL训练健壮、透明的深度研究机器人提供了实践指南。

Conclusion: 强化学习在深度研究系统中具有重要价值，能够充分利用环境反馈、支持探索和恢复行为、减少对人类先验知识的依赖。本综述为这一领域的研究提供了系统化的知识基础和实践指引。

Abstract: Deep research systems, agentic AI that solve complex, multi-step tasks by
coordinating reasoning, search across the open web and user files, and tool
use, are moving toward hierarchical deployments with a Planner, Coordinator,
and Executors. In practice, training entire stacks end-to-end remains
impractical, so most work trains a single planner connected to core tools such
as search, browsing, and code. While SFT imparts protocol fidelity, it suffers
from imitation and exposure biases and underuses environment feedback.
Preference alignment methods such as DPO are schema and proxy-dependent,
off-policy, and weak for long-horizon credit assignment and multi-objective
trade-offs. A further limitation of SFT and DPO is their reliance on human
defined decision points and subskills through schema design and labeled
comparisons. Reinforcement learning aligns with closed-loop, tool-interaction
research by optimizing trajectory-level policies, enabling exploration,
recovery behaviors, and principled credit assignment, and it reduces dependence
on such human priors and rater biases.
  This survey is, to our knowledge, the first dedicated to the RL foundations
of deep research systems. It systematizes work after DeepSeek-R1 along three
axes: (i) data synthesis and curation; (ii) RL methods for agentic research
covering stability, sample efficiency, long context handling, reward and credit
design, multi-objective optimization, and multimodal integration; and (iii)
agentic RL training systems and frameworks. We also cover agent architecture
and coordination, as well as evaluation and benchmarks, including recent QA,
VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We
distill recurring patterns, surface infrastructure bottlenecks, and offer
practical guidance for training robust, transparent deep research agents with
RL.

</details>


### [172] [VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction](https://arxiv.org/abs/2509.06736)
*Jie Yang,Jiajun Chen,Zhangyue Yin,Shuo Chen,Yuxin Wang,Yiran Guo,Yuan Li,Yining Zheng,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的State-based Function Call (SFC)方法，通过维护显式系统状态感知和直接状态迁移，在车辆室内环境控制任务中显著超越传统函数调用方法。


<details>
  <summary>Details</summary>
Motivation: 智能车辆室对API代理构成特殊挑战，需要协调紧密耦合的子系统。传统函数调用方法存在状态不明、需多次探索调用的问题，导致效率低下和错误恢复能力有限。

Method: 构建了VehicleWorld环境，包含30个模块、250个API和680个属性，提供实时状态信息。提出State-based Function Call (SFC)方法，通过维护显式系统状态感知和实现直接状态迁移来达到目标条件。

Result: 实验结果显示，SFC方法显著超越传统FC方法，实现了更高的执行准确性和更低的延迟。直接状态预测在环境控制任务中表现更优。

Conclusion: SFC方法通过维护显式状态感知和直接状态迁移，有效解决了车辆室环境中API代理的效率和准确性问题，为汽车领域的代理系统设计提供了新的解决方案。

Abstract: Intelligent vehicle cockpits present unique challenges for API Agents,
requiring coordination across tightly-coupled subsystems that exceed typical
task environments' complexity. Traditional Function Calling (FC) approaches
operate statelessly, requiring multiple exploratory calls to build
environmental awareness before execution, leading to inefficiency and limited
error recovery. We introduce VehicleWorld, the first comprehensive environment
for the automotive domain, featuring 30 modules, 250 APIs, and 680 properties
with fully executable implementations that provide real-time state information
during agent execution. This environment enables precise evaluation of vehicle
agent behaviors across diverse, challenging scenarios. Through systematic
analysis, we discovered that direct state prediction outperforms function
calling for environmental control. Building on this insight, we propose
State-based Function Call (SFC), a novel approach that maintains explicit
system state awareness and implements direct state transitions to achieve
target conditions. Experimental results demonstrate that SFC significantly
outperforms traditional FC approaches, achieving superior execution accuracy
and reduced latency. We have made all implementation code publicly available on
Github https://github.com/OpenMOSS/VehicleWorld.

</details>


### [173] [Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting](https://arxiv.org/abs/2509.06770)
*Shashidhar Reddy Javaji,Bhavul Gauri,Zining Zhu*

Main category: cs.AI

TL;DR: 这篇论文提出了一个评估框架，用于量化分析大语言模型在多轮迭代精细化任务中的表现，包括思想创造、代码生成和数学计算领域，通过控制对话流程和多种提示策略来评估迭代的效果和模式。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型已经被广泛用于多轮工作流程，但目前仍缺乏明确的方法来衡量迭代在什么情况下能够提升性能，什么情况下可能造成负面影响。需要一个统一的评估框架来对比不同模型和任务中的迭代效果。

Method: 设计了一个控制对话协议，每个任务进行12轮对话，使用从模糊"改进它"反馈到有目标导向的多种提示策略。记录每轮输出，使用领域适当的检查方法进行评分（代码用单元测试，数学用答案等效性加推理合理性，思想创造用原创性和可行性），并使用三组指标跟踪每轮行为：语义变化、轮间变化和输出大小增长。

Result: 在不同模型和任务中，进步效果存在领域差异：思想创造和代码在早期轮次就有改善，而数学在后期轮次通过详细提示才能获得显著改善。模糊反馈在前几轮后常平台或造成正确性倒退，而有目标的提示能够可靠地提升指定质量指标。同时观察到一致的领域模式：思想创造在语义上变化更大，代码输出大小增长但语义变化小，数学初始固定但通过后期详细迭代可以突破。

Conclusion: 该框架和指标体系使得迭代过程可以被量化和在不同模型间进行比较，并为决策者提供了什么时候应该进行导向、停止或更换策略的信号。这为优化多轮工作流程中的迭代策略提供了重要的实践指南。

Abstract: Large language models (LLMs) are now used in multi-turn workflows, but we
still lack a clear way to measure when iteration helps and when it hurts. We
present an evaluation framework for iterative refinement that spans ideation,
code, and math. Our protocol runs controlled 12-turn conversations per task,
utilizing a variety of prompts ranging from vague ``improve it'' feedback to
targeted steering, and logs per-turn outputs. We score outcomes with
domain-appropriate checks (unit tests for code; answer-equivalence plus
reasoning-soundness for math; originality and feasibility for ideation) and
track turn-level behavior with three families of metrics: semantic movement
across turns, turn-to-turn change, and output size growth. Across models and
tasks, gains are domain-dependent: they arrive early in ideas and code, but in
math late turns matter when guided by elaboration. After the first few turns,
vague feedback often plateaus or reverses correctness, while targeted prompts
reliably shift the intended quality axis (novelty vs. feasibility in ideation;
speed vs. readability in code; in math, elaboration outperforms exploration and
drives late-turn gains). We also observe consistent domain patterns: ideation
moves more in meaning across turns, code tends to grow in size with little
semantic change, and math starts fixed but can break that path with late,
elaborative iteration.Together, the framework and metrics make iteration
measurable and comparable across models, and signal when to steer, stop, or
switch strategies.

</details>


### [174] [RAFFLES: Reasoning-based Attribution of Faults for LLM Systems](https://arxiv.org/abs/2509.06822)
*Chenyang Zhu,Spencer Hong,Jingyu Wu,Kushal Chawla,Charlotte Tang,Youbing Yin,Nathan Wolfe,Erin Babinsky,Daben Liu*

Main category: cs.AI

TL;DR: RAFFLES是一个用于评估长时程多组件LLM代理系统的迭代式评估架构，通过推理和迭代精化来识别系统故障点和原因，在故障检测准确率上显著超越现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理系统的评估方法存在局限，主要关注单一指标或端到端结果，难以识别复杂长时程系统中故障的具体位置和原因。需要能够推理、探测、迭代和理解系统复杂逻辑的评估框架。

Method: RAFFLES采用迭代式多组件管道架构，使用中央Judge系统调查故障，并通过专门的Evaluators评估系统组件和Judge本身的推理质量，建立假设历史记录。

Result: 在Who&When数据集上，RAFFLES在算法生成数据集上达到43%以上的代理-步骤故障对准确率（相比之前最佳16.6%大幅提升），在手工制作数据集上达到20%以上准确率（超越之前最佳8.8%）。

Conclusion: RAFFLES代表了向自动化故障检测迈出的关键一步，能够替代劳动密集型的人工审查，为自主系统的故障诊断提供了有效解决方案。

Abstract: We have reached a critical roadblock in the development and enhancement of
long-horizon, multi-component LLM agentic systems: it is incredibly tricky to
identify where these systems break down and why. Evaluation capabilities that
currently exist today (e.g., single pass LLM-as-a-judge) are limited in that
they often focus on individual metrics or capabilities, end-to-end outcomes,
and are narrowly grounded on the preferences of humans. We argue that to match
the agentic capabilities, evaluation frameworks must also be able to reason,
probe, iterate, and understand the complex logic passing through these systems
over long horizons. In this paper, we present RAFFLES - an evaluation
architecture that incorporates reasoning and iterative refinement.
Specifically, RAFFLES operates as an iterative, multi-component pipeline, using
a central Judge to systematically investigate faults and a set of specialized
Evaluators to assess not only the system's components but also the quality of
the reasoning by the Judge itself, thereby building a history of hypotheses. We
tested RAFFLES against several baselines on the Who&When dataset, a benchmark
designed to diagnose the "who" (agent) and "when" (step) of a system's failure.
RAFFLES outperforms these baselines, achieving an agent-step fault pair
accuracy of over 43% on the Algorithmically-Generated dataset (a substantial
increase from the previously published best of 16.6%) and over 20% on the
Hand-Crafted dataset (surpassing the previously published best of 8.8%). These
results demonstrate a key step towards introducing automated fault detection
for autonomous systems over labor-intensive manual human review.

</details>


### [175] [Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet](https://arxiv.org/abs/2509.06861)
*James Xu Zhao,Bryan Hooi,See-Kiong Ng*

Main category: cs.AI

TL;DR: 这篇论文研究了测试时扩展（test-time scaling）在知识密集任务中的效果，发现增加推理链长度并不能持续提高准确性，反而可能导更多幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 测试时扩展技术在多个领域表现优异，但在需要高事实准确性和低幻觉率的知识密集任务中的效果尚不明确。

Method: 使用12个推理模型在2个知识密集测试集上进行综合评估，分析扩展推理对准确性和幻觉行为的影响。

Result: 增加测试时计算并不能持续提高准确性，常导致更多幻觉；减少的幻觉多来自模型选择避免回答，而非收回记忆改善；过长推理可能导致确认偏见和过份自信的幻觉。

Conclusion: 虽然存在限制，但与不进行推理相比，启用推理仍然有益。

Abstract: Test-time scaling increases inference-time computation by allowing models to
generate long reasoning chains, and has shown strong performance across many
domains. However, in this work, we show that this approach is not yet effective
for knowledge-intensive tasks, where high factual accuracy and low
hallucination rates are essential. We conduct a comprehensive evaluation of
test-time scaling using 12 reasoning models on two knowledge-intensive
benchmarks. Our results reveal that increasing test-time computation does not
consistently improve accuracy and, in many cases, it even leads to more
hallucinations. We then analyze how extended reasoning affects hallucination
behavior. We find that reduced hallucinations often result from the model
choosing to abstain after thinking more, rather than from improved factual
recall. Conversely, for some models, longer reasoning encourages attempts on
previously unanswered questions, many of which result in hallucinations. Case
studies show that extended reasoning can induce confirmation bias, leading to
overconfident hallucinations. Despite these limitations, we observe that
compared to non-thinking, enabling thinking remains beneficial. Code and data
are available at https://github.com/XuZhao0/tts-knowledge

</details>


### [176] [Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents](https://arxiv.org/abs/2509.06917)
*Jiacheng Miao,Joe R. Davis,Jonathan K. Pritchard,James Zou*

Main category: cs.AI

TL;DR: Paper2Agent是一个自动化框架，可将研究论文转换为AI代理，使静态论文变成动态交互式研究助手，通过自然语言执行复杂科学查询。


<details>
  <summary>Details</summary>
Motivation: 传统研究论文需要读者投入大量精力理解代码、数据和方法，阻碍了知识的传播和重用。Paper2Agent旨在将研究成果从被动产物转变为主动系统，加速下游使用和发现。

Method: 使用多代理系统分析论文和相关代码库，构建模型上下文协议(MCP)服务器，通过迭代生成和运行测试来优化MCP。然后将论文MCP与聊天代理连接，通过自然语言执行查询并调用原始论文的工具和工作流。

Result: 通过深入案例研究验证有效性，成功创建了基于AlphaGenome、ScanPy和TISSUE的论文代理，能够重现原始论文结果并正确执行新用户查询。

Conclusion: Paper2Agent通过将静态论文转变为动态交互式AI代理，为知识传播引入了新范式，为AI协作科学生态系统奠定了基础。

Abstract: We introduce Paper2Agent, an automated framework that converts research
papers into AI agents. Paper2Agent transforms research output from passive
artifacts into active systems that can accelerate downstream use, adoption, and
discovery. Conventional research papers require readers to invest substantial
effort to understand and adapt a paper's code, data, and methods to their own
work, creating barriers to dissemination and reuse. Paper2Agent addresses this
challenge by automatically converting a paper into an AI agent that acts as a
knowledgeable research assistant. It systematically analyzes the paper and the
associated codebase using multiple agents to construct a Model Context Protocol
(MCP) server, then iteratively generates and runs tests to refine and robustify
the resulting MCP. These paper MCPs can then be flexibly connected to a chat
agent (e.g. Claude Code) to carry out complex scientific queries through
natural language while invoking tools and workflows from the original paper. We
demonstrate Paper2Agent's effectiveness in creating reliable and capable paper
agents through in-depth case studies. Paper2Agent created an agent that
leverages AlphaGenome to interpret genomic variants and agents based on ScanPy
and TISSUE to carry out single-cell and spatial transcriptomics analyses. We
validate that these paper agents can reproduce the original paper's results and
can correctly carry out novel user queries. By turning static papers into
dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for
knowledge dissemination and a foundation for the collaborative ecosystem of AI
co-scientists.

</details>


### [177] [Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference](https://arxiv.org/abs/2509.06942)
*Xiangwei Shen,Zhimin Li,Zhantao Yang,Shiyi Zhang,Yingfang Zhang,Donghao Li,Chunyu Wang,Qinglin Lu,Yansong Tang*

Main category: cs.AI

TL;DR: 通过提前定义噪声先验和语义相对偏好优化方法，解决了人类偏好对齐中多步去噪计算成本高和需要离线奖励模型调整的问题，显著提升了图像真实感和美学质量


<details>
  <summary>Details</summary>
Motivation: 解决现有模型在人类偏好对齐中遇到的两大挑战：(1)多步去噪计算成本高，只能在少数步骤进行优化；(2)需要离线奖励模型调整来达到目标美学质量

Method: 提出Direct-Align方法，通过预定义噪声先验来恢复原始图像，避免后期时间步过度优化；提出语义相对偏好优化(SRPO)，将奖励形式化为文本条件信号，支持在线奖励调整

Result: 对FLUX.1.dev模型进行精调后，人类评估的真实感和美学质量提升了3倍以上

Conclusion: 该方法通过提前定义噪声先验和在线奖励调整机制，有效解决了人类偏好对齐中的计算效率和灵活性问题，显著提升了模型的生成质量

Abstract: Recent studies have demonstrated the effectiveness of directly aligning
diffusion models with human preferences using differentiable reward. However,
they exhibit two primary challenges: (1) they rely on multistep denoising with
gradient computation for reward scoring, which is computationally expensive,
thus restricting optimization to only a few diffusion steps; (2) they often
need continuous offline adaptation of reward models in order to achieve desired
aesthetic quality, such as photorealism or precise lighting effects. To address
the limitation of multistep denoising, we propose Direct-Align, a method that
predefines a noise prior to effectively recover original images from any time
steps via interpolation, leveraging the equation that diffusion states are
interpolations between noise and target images, which effectively avoids
over-optimization in late timesteps. Furthermore, we introduce Semantic
Relative Preference Optimization (SRPO), in which rewards are formulated as
text-conditioned signals. This approach enables online adjustment of rewards in
response to positive and negative prompt augmentation, thereby reducing the
reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model
with optimized denoising and online reward adjustment, we improve its
human-evaluated realism and aesthetic quality by over 3x.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [178] [Effectively obtaining acoustic, visual and textual data from videos](https://arxiv.org/abs/2509.05786)
*Jorge E. León,Miguel Carrasco*

Main category: cs.MM

TL;DR: 通过视频提取音频-图像-文本多模态数据的方法，构建大规模高质量多模态数据集


<details>
  <summary>Details</summary>
Motivation: 机器学习模型需要大规模高质量多模态数据集，特别是音频、视觉和文本组合的数据，但目前这类数据集有限

Method: 从视频中提取相关音频-图像-文本观测数据，包括选择合适视频、提取数据对、使用图像-文本模型生成描述性文本

Result: 构建了公开可用的多模态数据集，确保了模态间的稳健语义联系，提高了数据集的实用性

Conclusion: 该方法有助于解决多模态数据短缺问题，支持多模态数据分析和机器学习研究的发展

Abstract: The increasing use of machine learning models has amplified the demand for
high-quality, large-scale multimodal datasets. However, the availability of
such datasets, especially those combining acoustic, visual and textual data,
remains limited. This paper addresses this gap by proposing a method to extract
related audio-image-text observations from videos. We detail the process of
selecting suitable videos, extracting relevant data pairs, and generating
descriptive texts using image-to-text models. Our approach ensures a robust
semantic connection between modalities, enhancing the utility of the created
datasets for various applications. We also discuss the challenges encountered
and propose solutions to improve data quality. The resulting datasets, publicly
available, aim to support and advance research in multimodal data analysis and
machine learning.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [179] [Knowledge-Augmented Relation Learning for Complementary Recommendation with Large Language Models](https://arxiv.org/abs/2509.05564)
*Chihiro Yamasaki,Kai Sugahara,Kazushi Okamoto*

Main category: cs.IR

TL;DR: KARL框架通过结合主动学习和LLM，低成本扩展高质量功能标签数据集，在OOD场景下显著提升互补推荐准确性


<details>
  <summary>Details</summary>
Motivation: 解决互补推荐中行为标签噪声大与功能标签标注成本高的两难问题，需要一种能高效生成高质量标签的方法

Method: 提出KARL框架，使用主动学习选择分类器最难判别的样本，利用LLM进行标签扩展，低成本扩充高质量FBL数据集

Result: 在OOD设置下准确率提升高达37%，但在ID设置下提升不足0.5%，长时间学习甚至可能降低准确率

Conclusion: KARL通过知识扩展增加数据多样性，建议需要根据预测上下文（ID或OOD）动态调整采样策略的多样性

Abstract: Complementary recommendations play a crucial role in e-commerce by enhancing
user experience through suggestions of compatible items. Accurate
classification of complementary item relationships requires reliable labels,
but their creation presents a dilemma. Behavior-based labels are widely used
because they can be easily generated from interaction logs; however, they often
contain significant noise and lack reliability. While function-based labels
(FBLs) provide high-quality definitions of complementary relationships by
carefully articulating them based on item functions, their reliance on costly
manual annotation severely limits a model's ability to generalize to diverse
items. To resolve this trade-off, we propose Knowledge-Augmented Relation
Learning (KARL), a framework that strategically fuses active learning with
large language models (LLMs). KARL efficiently expands a high-quality FBL
dataset at a low cost by selectively sampling data points that the classifier
finds the most difficult and uses the label extension of the LLM. Our
experiments showed that in out-of-distribution (OOD) settings, an unexplored
item feature space, KARL improved the baseline accuracy by up to 37%. In
contrast, in in-distribution (ID) settings, the learned item feature space, the
improvement was less than 0.5%, with prolonged learning could degrade accuracy.
These contrasting results are due to the data diversity driven by KARL's
knowledge expansion, suggesting the need for a dynamic sampling strategy that
adjusts diversity based on the prediction context (ID or OOD).

</details>


### [180] [LESER: Learning to Expand via Search Engine-feedback Reinforcement in e-Commerce](https://arxiv.org/abs/2509.05570)
*Yipeng Zhang,Bowen Liu,Xiaoshuang Zhang,Aritra Mandal,Zhe Wu,Canran Xu*

Main category: cs.IR

TL;DR: LESER是一个基于搜索引擎反馈强化学习的查询扩展框架，通过实时搜索反馈优化LLM，显著提升电商搜索的语义覆盖和检索相关性


<details>
  <summary>Details</summary>
Motivation: 电商搜索查询通常模糊简短，现有方法难以准确捕捉用户意图且难以规模化生产部署，需要更有效的查询扩展解决方案

Method: 使用搜索引擎实时反馈作为监督信号，通过Group Relative Policy Optimization进行强化学习，训练上下文感知LLM进行查询扩展

Result: 在大规模真实电商数据集上，LESER在离线和在线设置中都显著提升了语义覆盖、检索相关性和用户参与度

Conclusion: LESER为现代搜索系统提供了一个实用且可扩展的解决方案，能够有效处理模糊查询并提升搜索质量

Abstract: User queries in e-commerce search are often vague, short, and underspecified,
making it difficult for retrieval systems to match them accurately against
structured product catalogs. This challenge is amplified by the one-to-many
nature of user intent, where a single query can imply diverse and competing
needs. Existing methods, including neural query expansion and prompting-based
LLM approaches, fall short in real-world settings: they struggle to capture
nuanced user intent, often generate outputs that violate platform constraints,
and rely on workflows that are difficult to scale in production. We propose
Learning to Expand via Search Engine-feedback Reinforcement (LESER), a novel
framework that fine-tunes a context-aware LLM using real-time search engine
feedback as supervision. LESER formulates query expansion as a retrieval
optimization task and leverages Group Relative Policy Optimization to learn
directly from relevance and coverage metrics. LESER is trained to reason over
search results and produce high quality query expansions that align with
platform rules and retrieval objectives. We evaluate LESER on large-scale,
real-world e-commerce datasets, demonstrating substantial improvements in both
offline and online settings. Our results show that LESER not only enhances
semantic coverage and retrieval relevance but also delivers measurable gains in
user engagement, making it a practical and scalable solution for modern search
systems.

</details>


### [181] [Toward Efficient and Scalable Design of In-Memory Graph-Based Vector Search](https://arxiv.org/abs/2509.05750)
*Ilias Azizi,Karima Echihab,Themis Palpanas,Vassilis Christophides*

Main category: cs.IR

TL;DR: 这篇论文对12种最先进的图基向量搜索算法进行了系统性实验评估，在7个达到10亿向量的实际数据集上测试，发现增量插入咉邻居多样化的方法表现最佳


<details>
  <summary>Details</summary>
Motivation: 向量数据在商业和科学应用中趋于普遍，集合规模达到十亿级别，向量搜索成为关键分析任务的核心，但缺乏对各种图基算法的系统性比较

Method: 在7个实际数据集上对12种独立的图基向量搜索算法进行完整的实验评估，数据集规模达到10亿向量

Result: 最佳方法通常基于增量插入和邻居多样化，基图的选择会影响算法的扩展性，需要更加复杂的数据适应性种子选择和多样化策略

Conclusion: 该研究为图基向量搜索领域提供了重要的见解，指明了当前方法的优势和局限性，并提出了未来研究方向

Abstract: Vector data is prevalent across business and scientific applications, and its
popularity is growing with the proliferation of learned embeddings. Vector data
collections often reach billions of vectors with thousands of dimensions, thus,
increasing the complexity of their analysis. Vector search is the backbone of
many critical analytical tasks, and graph-based methods have become the best
choice for analytical tasks that do not require guarantees on the quality of
the answers. Although several paradigms (seed selection, incremental insertion,
neighborhood propagation, neighborhood diversification, and divide-and-conquer)
have been employed to design in-memory graph-based vector search algorithms, a
systematic comparison of the key algorithmic advances is still missing. We
conduct an exhaustive experimental evaluation of twelve state-of-the-art
methods on seven real data collections, with sizes up to 1 billion vectors. We
share key insights about the strengths and limitations of these methods; e.g.,
the best approaches are typically based on incremental insertion and
neighborhood diversification, and the choice of the base graph can hurt
scalability. Finally, we discuss open research directions, such as the
importance of devising more sophisticated data adaptive seed selection and
diversification strategies.

</details>


### [182] [A Survey of Real-World Recommender Systems: Challenges, Constraints, and Industrial Perspectives](https://arxiv.org/abs/2509.06002)
*Kuan Zou,Aixin Sun*

Main category: cs.IR

TL;DR: 这篇论文系统性评估了产业界推荐系统与学术界的差异，提出了交易导向和内容导向的新分类方法，并指出了推进学术研究的具体建议。


<details>
  <summary>Details</summary>
Motivation: 学术研究因为无法获取真实用户数据和大规模推荐平台，导致实践相关性不足、技术进步缓慢和对推荐系统核心挑战的理解不深入。

Method: 通过系统性评估产业界推荐系统，对比学术界推荐系统在数据规模、实时要求和评估方法上的差异，总结主要实际推荐场景及挑战，并以物品特征和推荐目标为基础提出新的分类方法。

Result: 识别了产业界实践者在交易导向和内容导向推荐系统中解决挑战的方法，并提出了有前景的研究方向。

Conclusion: 论文的目标是提升学术界对实践推荐系统的理解，缩小产学界发展差距，促进产学界更深入合作。

Abstract: Recommender systems have generated tremendous value for both users and
businesses, drawing significant attention from academia and industry alike.
However, due to practical constraints, academic research remains largely
confined to offline dataset optimizations, lacking access to real user data and
large-scale recommendation platforms. This limitation reduces practical
relevance, slows technological progress, and hampers a full understanding of
the key challenges in recommender systems. In this survey, we provide a
systematic review of industrial recommender systems and contrast them with
their academic counterparts. We highlight key differences in data scale,
real-time requirements, and evaluation methodologies, and we summarize major
real-world recommendation scenarios along with their associated challenges. We
then examine how industry practitioners address these challenges in
Transaction-Oriented Recommender Systems and Content-Oriented Recommender
Systems, a new classification grounded in item characteristics and
recommendation objectives. Finally, we outline promising research directions,
including the often-overlooked role of user decision-making, the integration of
economic and psychological theories, and concrete suggestions for advancing
academic research. Our goal is to enhance academia's understanding of practical
recommender systems, bridge the growing development gap, and foster stronger
collaboration between industry and academia.

</details>


### [183] [Modeling shopper interest broadness with entropy-driven dialogue policy in the context of arbitrarily large product catalogs](https://arxiv.org/abs/2509.06185)
*Firas Jarboui,Issa Memari*

Main category: cs.IR

TL;DR: 通过检索分数分布的熵来模型用户兴趣广度，动态路由对话策略：低熵查询直接推荐，高熵查询进行探索性询问


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型驱动的对话推荐系统中探索（清晰用户需求）和利用（推荐商品）的平衡问题，特别是在面对巨大商品目录时

Method: 使用神经网络检索器获取相关商品，计算重排分数分布的熵来以动态路由对话策略

Result: 该方法使大语言模型能够在不扩大上下文窗口的情况下，实时处理任意大小的商品目录

Conclusion: 通过检索分数分布熵来模型用户兴趣广度的简单策略，有效地解决了对话推荐系统中探索与利用的平衡挑战

Abstract: Conversational recommender systems promise rich interactions for e-commerce,
but balancing exploration (clarifying user needs) and exploitation (making
recommendations) remains challenging, especially when deploying large language
models (LLMs) with vast product catalogs. We address this challenge by modeling
the breadth of user interest via the entropy of retrieval score distributions.
Our method uses a neural retriever to fetch relevant items for a user query and
computes the entropy of the re-ranked scores to dynamically route the dialogue
policy: low-entropy (specific) queries trigger direct recommendations, whereas
high-entropy (ambiguous) queries prompt exploratory questions. This simple yet
effective strategy allows an LLM-driven agent to remain aware of an arbitrarily
large catalog in real-time without bloating its context window.

</details>


### [184] [Language Bias in Information Retrieval: The Nature of the Beast and Mitigation Methods](https://arxiv.org/abs/2509.06195)
*Jinrui Yang,Fan Jiang,Timothy Baldwin*

Main category: cs.IR

TL;DR: 本文提出语言公平性在跨语言信息检索中的重要性，基于相同语义查询应获得相似排序结果的假设，评估现有方法的语言偏见，并提出LaKDA损失函数来缓解神经检索模型中的语言偏差。


<details>
  <summary>Details</summary>
Motivation: 确保多语言信息检索系统的语言公平性，使不同语言但语义相同的查询在检索相同多语言文档时能获得等效的排序结果，解决当前技术中存在的内在语言偏见问题。

Method: 使用传统检索方法和基于mBERT和XLM-R的DPR神经排序器评估语言公平性程度，并设计新颖的LaKDA损失函数来减轻神经MLIR方法中的语言偏见。

Result: 分析揭示了当前MLIR技术中存在内在语言偏见，不同检索方法间存在显著差异，同时证明了LaKDA在提升语言公平性方面的有效性。

Conclusion: LaKDA损失函数能有效缓解多语言信息检索系统中的语言偏见，为提高跨语言信息检索的公平性提供了有效解决方案。

Abstract: Language fairness in multilingual information retrieval (MLIR) systems is
crucial for ensuring equitable access to information across diverse languages.
This paper sheds light on the issue, based on the assumption that queries in
different languages, but with identical semantics, should yield equivalent
ranking lists when retrieving on the same multilingual documents. We evaluate
the degree of fairness using both traditional retrieval methods, and a DPR
neural ranker based on mBERT and XLM-R. Additionally, we introduce `LaKDA', a
novel loss designed to mitigate language biases in neural MLIR approaches. Our
analysis exposes intrinsic language biases in current MLIR technologies, with
notable disparities across the retrieval methods, and the effectiveness of
LaKDA in enhancing language fairness.

</details>


### [185] [AudioBoost: Increasing Audiobook Retrievability in Spotify Search with Synthetic Query Generation](https://arxiv.org/abs/2509.06452)
*Enrico Palumbo,Gustavo Penha,Alva Liu,Marcus Eltscheminov,Jefferson Carvalho dos Santos,Alice Wang,Hugues Bouchard,Humberto Jesús Corona Pampin,Michelle Tran Luu*

Main category: cs.IR

TL;DR: AudioBoost系统使用LLM生成合成查询来提升Spotify中audiobook的可检索性，解决了冷启动问题，通过离线和在线测试验证了效果


<details>
  <summary>Details</summary>
Motivation: Spotify新增有声书内容，但面临冷启动问题，用户交互数据少导致检索偏差，需要支持用户通过探索性查询发现有声书

Method: 利用大型语言模型基于有声书元数据生成合成查询，同时在查询自动补全和搜索检索引擎中索引这些查询

Result: 离线评估显示合成查询提高了可检索性和质量；在线A/B测试显示audiobook展示量+0.7%，点击量+1.22%，探索性查询补全+1.82%

Conclusion: AudioBoost系统有效解决了有声书冷启动检索问题，通过合成查询生成显著提升了用户发现和访问有声书内容的能力

Abstract: Spotify has recently introduced audiobooks as part of its catalog,
complementing its music and podcast offering. Search is often the first entry
point for users to access new items, and an important goal for Spotify is to
support users in the exploration of the audiobook catalog. More specifically,
we would like to enable users without a specific item in mind to broadly search
by topic, genre, story tropes, decade, and discover audiobooks, authors and
publishers they may like. To do this, we need to 1) inspire users to type more
exploratory queries for audiobooks and 2) augment our retrieval systems to
better deal with exploratory audiobook queries. This is challenging in a
cold-start scenario, where we have a retrievabiliy bias due to the little
amount of user interactions with audiobooks compared to previously available
items such as music and podcast content. To address this, we propose
AudioBoost, a system to boost audiobook retrievability in Spotify's Search via
synthetic query generation. AudioBoost leverages Large Language Models (LLMs)
to generate synthetic queries conditioned on audiobook metadata. The synthetic
queries are indexed both in the Query AutoComplete (QAC) and in the Search
Retrieval engine to improve query formulation and retrieval at the same time.
We show through offline evaluation that synthetic queries increase
retrievability and are of high quality. Moreover, results from an online A/B
test show that AudioBoost leads to a +0.7% in audiobook impressions, +1.22% in
audiobook clicks, and +1.82% in audiobook exploratory query completions.

</details>


### [186] [Rethinking LLM Parametric Knowledge as Post-retrieval Confidence for Dynamic Retrieval and Reranking](https://arxiv.org/abs/2509.06472)
*Haoxiang Jin,Ronghan Li,Qiguang Miao,Zixiang Lu*

Main category: cs.IR

TL;DR: 这篇论文提出了一种基于大语言模型内部隐藏状态的知识过滤方法，通过信心检测模型和动态检索机制来提升RAG系统的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在超出知识范围时产生幻觉的问题，并充分利用检索到的上下文信息，克服当前方法在知识边界识别上的不足。

Method: 构建基于LLM内部隐藏状态的信心检测模型，创建偏好数据集微调重排器，并提出基于信心的动态检索机制(CBDR)。

Result: 实验结果显示在上下文筛选和终端到终端RAG性能上都取得了显著改善，同时大幅减少了检索成本保持竞争力准确性。

Conclusion: 该方法通过利用LLM内部连续状态信息，有效提升了RAG系统的知识边界识别能力和整体性能。

Abstract: Large Language Models (LLMs) often generate inaccurate responses
(hallucinations) when faced with questions beyond their knowledge scope.
Retrieval-Augmented Generation (RAG) addresses this by leveraging external
knowledge, but a critical challenge remains: determining whether retrieved
contexts effectively enhance the model`s ability to answer specific queries.
This challenge underscores the importance of knowledge boundary awareness,
which current methods-relying on discrete labels or limited signals-fail to
address adequately, as they overlook the rich information in LLMs` continuous
internal hidden states. To tackle this, we propose a novel post-retrieval
knowledge filtering approach. First, we construct a confidence detection model
based on LLMs` internal hidden states to quantify how retrieved contexts
enhance the model`s confidence. Using this model, we build a preference dataset
(NQ_Rerank) to fine-tune a reranker, enabling it to prioritize contexts
preferred by the downstream LLM during reranking. Additionally, we introduce
Confidence-Based Dynamic Retrieval (CBDR), which adaptively triggers retrieval
based on the LLM`s initial confidence in the original question, reducing
knowledge conflicts and improving efficiency. Experimental results demonstrate
significant improvements in accuracy for context screening and end-to-end RAG
performance, along with a notable reduction in retrieval costs while
maintaining competitive accuracy.

</details>


### [187] [Reasoning-enhanced Query Understanding through Decomposition and Interpretation](https://arxiv.org/abs/2509.06544)
*Yunfei Zhong,Jun Yang,Yixing Fan,Jiafeng Guo,Lixin Su,Maarten de Rijke,Ruqing Zhang,Dawei Yin,Xueqi Cheng*

Main category: cs.IR

TL;DR: ReDI是一个基于LLM的三阶段推理增强查询理解方法，通过分解复杂查询、语义解释增强和结果融合，显著提升了长查询的文档检索效果


<details>
  <summary>Details</summary>
Motivation: 现有LLM在查询理解方面主要针对短关键词查询进行评估，但随着AI搜索发展，复杂长查询日益普遍且缺乏充分研究

Method: 三阶段管道：1)分解复杂查询为子查询；2)为每个子查询添加详细语义解释；3)独立检索并融合结果

Result: 在BRIGHT和BEIR数据集上，ReDI在稀疏和稠密检索范式中均持续超越强基线方法

Conclusion: ReDI方法有效解决了复杂长查询的理解问题，证明了推理增强方法在查询理解任务中的有效性

Abstract: Accurate inference of user intent is crucial for enhancing document retrieval
in modern search engines. While large language models (LLMs) have made
significant strides in this area, their effectiveness has predominantly been
assessed with short, keyword-based queries. As AI-driven search evolves,
long-form queries with intricate intents are becoming more prevalent, yet they
remain underexplored in the context of LLM-based query understanding (QU). To
bridge this gap, we introduce ReDI: a Reasoning-enhanced approach for query
understanding through Decomposition and Interpretation. ReDI leverages the
reasoning and comprehension capabilities of LLMs in a three-stage pipeline: (i)
it breaks down complex queries into targeted sub-queries to accurately capture
user intent; (ii) it enriches each sub-query with detailed semantic
interpretations to improve the query-document matching; and (iii) it
independently retrieves documents for each sub-query and employs a fusion
strategy to aggregate the results for the final ranking. We compiled a
large-scale dataset of real-world complex queries from a major search engine
and distilled the query understanding capabilities of teacher models into
smaller models for practical application. Experiments on BRIGHT and BEIR
demonstrate that ReDI consistently surpasses strong baselines in both sparse
and dense retrieval paradigms, affirming its effectiveness.

</details>


### [188] [UniSearch: Rethinking Search System with a Unified Generative Architecture](https://arxiv.org/abs/2509.06887)
*Jiahui Chen,Xiaoze Jiang,Zhibo Wang,Quanzhi Zhu,Junyao Zhao,Feng Hu,Kang Pan,Ao Xie,Maohua Pei,Zhiheng Qin,Hongjing Zhang,Zhixin Zhai,Xiaobo Guo,Runbin Zhou,Kefeng Wang,Mingyang Geng,Cheng Chen,Jingshan Lv,Yupeng Huang,Xiao Liang,Han Li*

Main category: cs.IR

TL;DR: UniSearch是一个统一的生成式搜索框架，通过端到端架构替代传统的级联搜索系统，结合搜索生成器和视频编码器，并引入搜索偏好优化来提升搜索效果。


<details>
  <summary>Details</summary>
Motivation: 传统搜索引擎采用级联架构，多个模块的设计和维护复杂，难以实现整体性能提升。现有生成式搜索方法不是真正的端到端，存在目标不一致和泛化能力有限的问题。

Method: 提出UniSearch框架，包含搜索生成器（生成相关项目的语义标识符）和视频编码器（学习潜在项目嵌入并提供标记化表示），采用统一训练框架联合优化两个组件，并引入搜索偏好优化（SPO）利用奖励模型和真实用户反馈来对齐生成结果与用户偏好。

Result: 在工业规模数据集上的广泛实验以及在短视频和直播搜索场景中的在线A/B测试表明，UniSearch具有强大的有效性和部署潜力。在直播搜索中的部署实现了产品历史上近年来最大的单次实验改进。

Conclusion: UniSearch框架通过端到端的统一生成式搜索架构，有效解决了传统级联搜索系统的问题，在实际应用中展现出显著的性能提升和实用价值。

Abstract: Modern search systems play a crucial role in facilitating information
acquisition. Traditional search engines typically rely on a cascaded
architecture, where results are retrieved through recall, pre-ranking, and
ranking stages. The complexity of designing and maintaining multiple modules
makes it difficult to achieve holistic performance gains. Recent advances in
generative recommendation have motivated the exploration of unified generative
search as an alternative. However, existing approaches are not genuinely
end-to-end: they typically train an item encoder to tokenize candidates first
and then optimize a generator separately, leading to objective inconsistency
and limited generalization. To address these limitations, we propose UniSearch,
a unified generative search framework for Kuaishou Search. UniSearch replaces
the cascaded pipeline with an end-to-end architecture that integrates a Search
Generator and a Video Encoder. The Generator produces semantic identifiers of
relevant items given a user query, while the Video Encoder learns latent item
embeddings and provides their tokenized representations. A unified training
framework jointly optimizes both components, enabling mutual enhancement and
improving representation quality and generation accuracy. Furthermore, we
introduce Search Preference Optimization (SPO), which leverages a reward model
and real user feedback to better align generation with user preferences.
Extensive experiments on industrial-scale datasets, together with online A/B
testing in both short-video and live search scenarios, demonstrate the strong
effectiveness and deployment potential of UniSearch. Notably, its deployment in
live search yields the largest single-experiment improvement in recent years of
our product's history, highlighting its practical value for real-world
applications.

</details>
