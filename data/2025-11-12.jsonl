{"id": "2511.07731", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.07731", "abs": "https://arxiv.org/abs/2511.07731", "authors": ["Lukas Rapp", "Muriel Médard", "Ken R. Duffy"], "title": "Group Probability Decoding of Turbo Product Codes over Higher-Order Fields", "comment": null, "summary": "Binary turbo product codes (TPCs) are powerful error-correcting codes constructed from short component codes. Traditionally, turbo product decoding passes log likelihood ratios (LLRs) between the component decoders, inherently losing information when bit correlation exists. Such correlation can arise exogenously from sources like intersymbol interference and endogenously during component code decoding. To preserve these correlations and improve performance, we propose turbo product decoding based on group probabilities. We theoretically predict mutual information and signal-to-noise ratio (SNR) gains of group over bit-probability decoding. To translate these theoretical insights to practice, we revisit non-binary TPCs that naturally support group-probability decoding. We show that any component list decoder that takes group probabilities as input and outputs block-wise soft-output can partially preserve bit correlation, which we demonstrate with symbol-level ORBGRAND combined with soft-output GRAND (SOGRAND). Our results demonstrate that group-probability-based turbo product decoding achieves SNR gains of up to 0.3 dB for endogenous correlation and 0.7 dB for exogenous correlation, compared to bit-probability decoding."}
{"id": "2511.07789", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.07789", "abs": "https://arxiv.org/abs/2511.07789", "authors": ["Mingjie Zhu", "Yejian Lyu", "Chong Han"], "title": "Digital Twin Empowered In-Vehicular Channel Modeling and Wireless Planning in the Terahertz Band", "comment": null, "summary": "Vehicle-to-everything (V2X) technology has emerged as a key enabler of intelligent transportation systems, while the Terahertz (THz) band offers abundant spectrum resources to support ultra-high-speed and low-latency V2X communications. This paper investigates the in-vehicle wireless channel in the 300~GHz band. First, channel measurement based on vector-network-analyzer (VNA) is conducted under typical V2X scenarios, including with/without human, and window-on/off cases. Then, a digital twin (DT) of the vehicle is constructed from high-resolution point cloud data and a measurement-based material property database. The DT is integrated into an open-source ray-tracing (RT) simulator, Sionna, to model multipath propagation. The DT-empowered simulation results are analyzed and validated with the measurement data, showing strong agreement and validating the feasibility. Finally, a hybrid ray-tracing-statistic channel model is established, combining the RT results and measurement data. Leveraging the validated model, further wireless planning is carried out, including signal-to-interference-plus-noise ratio (SINR) analysis, coverage probability evaluation, and optimal transmitter (Tx) placement. These findings provide valuable insights for the design and deployment of future THz in-vehicle communication systems."}
{"id": "2511.07826", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.07826", "abs": "https://arxiv.org/abs/2511.07826", "authors": ["Yujie Zhou", "Rulong Wang", "Yong Xiao", "Yingyu Li", "Guangming Shi"], "title": "Variable-Length Joint Source-Channel Coding for Semantic Communication", "comment": null, "summary": "This paper investigates a key challenge faced by joint source-channel coding (JSCC) in digital semantic communication (SemCom): the incompatibility between existing JSCC schemes that yield continuous encoded representations and digital systems that employ discrete variable-length codewords. It further results in feasibility issues in achieving physical bit-level rate control via such JSCC approaches for efficient semantic transmission. In this paper, we propose a novel end-to-end coding (E2EC) framework to tackle it. The semantic coding problem is formed by extending the information bottleneck (IB) theory over noisy channels, which is a tradeoff between bit-level communication rate and semantic distortion. With a structural decomposition of encoding to handle code length and content respectively, we can construct an end-to-end trainable encoder that supports the direct compression of a data source into a finite codebook. To optimize our E2EC across non-differentiable operations, e.g., sampling, we use the powerful policy gradient to support gradient-based updates. Experimental results illustrate that E2EC achieves high inference quality with low bit rates, outperforming representative baselines compatible with digital SemCom systems."}
{"id": "2511.08182", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.08182", "abs": "https://arxiv.org/abs/2511.08182", "authors": ["Yuxin Li", "Guangyue Lu", "Yinghui Ye", "Liqin Shi", "Daniel Benevides da Costa"], "title": "Symbol Detection in Multi-channel Multi-tag Ambient Backscatter Communication Under IQ Imbalance", "comment": null, "summary": "Ambient backscatter communication (AmBC) offers low-cost and low-power connectivity for Internet of Things (IoT), where a backscatter tag (BT) modulates incident signals transmitted by an ambient radio frequency (RF) source and reflects them to its associated AmBC receiver. In multi-channel multi-tag AmBC, one of major challenges from the aspect of symbol detection is the image channel crosstalk, which is induced by the inevitable in-phase/quadrature (IQ) imbalance. To address this issue, in this paper, we study symbol detection in multi-channel multi-tag AmBC under IQ imbalance. Considering the differential encoding scheme at the BTs, we propose a novel symbol detection model that incorporates IQ imbalance parameters, the presence or absence of both the incident signal and the backscattered signal of the image channel. On this basis, considering an energy difference detector at the AmBC receiver, we derive the closed-form expressions for the bit error rate (BER) as well as the near-optimal detection threshold to minimize BER. However, calculating the near-optimal detection threshold requires prior information, such as the IQ imbalance parameters, the presence probability of the incident signal of the image channel and the backscattered signal of the image channel, the signal power of the ambient RF source, and the noise power, which are typically unknown to the AmBC receiver in practice. To eliminate the need for the prior information, we propose a threshold estimation method using the received samples. Numerical results indicate that under IQ imbalance, directly using the existing method leads to a significant degradation in BER performance. However, this degradation can be effectively mitigated by our derived detection threshold."}
{"id": "2511.07975", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07975", "abs": "https://arxiv.org/abs/2511.07975", "authors": ["Li Peng", "Jiayao Zhang", "Yihang Wu", "Weiran Liu", "Jinfei Liu", "Zheng Yan", "Kui Ren", "Lei Zhang", "Lin Qu"], "title": "Reliable and Private Utility Signaling for Data Markets", "comment": null, "summary": "The explosive growth of data has highlighted its critical role in driving economic growth through data marketplaces, which enable extensive data sharing and access to high-quality datasets. To support effective trading, signaling mechanisms provide participants with information about data products before transactions, enabling informed decisions and facilitating trading. However, due to the inherent free-duplication nature of data, commonly practiced signaling methods face a dilemma between privacy and reliability, undermining the effectiveness of signals in guiding decision-making.\n  To address this, this paper explores the benefits and develops a non-TCP-based construction for a desirable signaling mechanism that simultaneously ensures privacy and reliability. We begin by formally defining the desirable utility signaling mechanism and proving its ability to prevent suboptimal decisions for both participants and facilitate informed data trading. To design a protocol to realize its functionality, we propose leveraging maliciously secure multi-party computation (MPC) to ensure the privacy and robustness of signal computation and introduce an MPC-based hash verification scheme to ensure input reliability. In multi-seller scenarios requiring fair data valuation, we further explore the design and optimization of the MPC-based KNN-Shapley method with improved efficiency. Rigorous experiments demonstrate the efficiency and practicality of our approach."}
{"id": "2511.07663", "categories": ["cs.DB", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07663", "abs": "https://arxiv.org/abs/2511.07663", "authors": ["Paritosh Aggarwal", "Bowei Chen", "Anupam Datta", "Benjamin Han", "Boxin Jiang", "Nitish Jindal", "Zihan Li", "Aaron Lin", "Pawel Liskowski", "Jay Tayade", "Dimitris Tsirogiannis", "Nathan Wiegand", "Weicheng Zhao"], "title": "Cortex AISQL: A Production SQL Engine for Unstructured Data", "comment": null, "summary": "Snowflake's Cortex AISQL is a production SQL engine that integrates native semantic operations directly into SQL. This integration allows users to write declarative queries that combine relational operations with semantic reasoning, enabling them to query both structured and unstructured data effortlessly. However, making semantic operations efficient at production scale poses fundamental challenges. Semantic operations are more expensive than traditional SQL operations, possess distinct latency and throughput characteristics, and their cost and selectivity are unknown during query compilation. Furthermore, existing query engines are not designed to optimize semantic operations. The AISQL query execution engine addresses these challenges through three novel techniques informed by production deployment data from Snowflake customers. First, AI-aware query optimization treats AI inference cost as a first-class optimization objective, reasoning about large language model (LLM) cost directly during query planning to achieve 2-8$\\times$ speedups. Second, adaptive model cascades reduce inference costs by routing most rows through a fast proxy model while escalating uncertain cases to a powerful oracle model, achieving 2-6$\\times$ speedups while maintaining 90-95% of oracle model quality. Third, semantic join query rewriting lowers the quadratic time complexity of join operations to linear through reformulation as multi-label classification tasks, achieving 15-70$\\times$ speedups with often improved prediction quality. AISQL is deployed in production at Snowflake, where it powers diverse customer workloads across analytics, search, and content understanding."}
{"id": "2511.07573", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.07573", "abs": "https://arxiv.org/abs/2511.07573", "authors": ["Kamand Kalashi", "Babak Teimourpour"], "title": "A Hybrid Multimodal Deep Learning Framework for Intelligent Fashion Recommendation", "comment": "8 pages, 1 figure", "summary": "The rapid expansion of online fashion platforms has created an increasing demand for intelligent recommender systems capable of understanding both visual and textual cues. This paper proposes a hybrid multimodal deep learning framework for fashion recommendation that jointly addresses two key tasks: outfit compatibility prediction and complementary item retrieval. The model leverages the visual and textual encoders of the CLIP architecture to obtain joint latent representations of fashion items, which are then integrated into a unified feature vector and processed by a transformer encoder. For compatibility prediction, an \"outfit token\" is introduced to model the holistic relationships among items, achieving an AUC of 0.95 on the Polyvore dataset. For complementary item retrieval, a \"target item token\" representing the desired item description is used to retrieve compatible items, reaching an accuracy of 69.24% under the Fill-in-the-Blank (FITB) metric. The proposed approach demonstrates strong performance across both tasks, highlighting the effectiveness of multimodal learning for fashion recommendation."}
{"id": "2511.07835", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2511.07835", "abs": "https://arxiv.org/abs/2511.07835", "authors": ["Yiqiao Bao", "Anindya De", "Shivam Nadimpalli", "Rocco A. Servedio", "Nathan White"], "title": "Testing noisy low-degree polynomials for sparsity", "comment": "63 pages", "summary": "We consider the problem of testing whether an unknown low-degree polynomial $p$ over $\\mathbb{R}^n$ is sparse versus far from sparse, given access to noisy evaluations of the polynomial $p$ at \\emph{randomly chosen points}. This is a property-testing analogue of classical problems on learning sparse low-degree polynomials with noise, extending the work of Chen, De, and Servedio (2020) from noisy \\emph{linear} functions to general low-degree polynomials.\n  Our main result gives a \\emph{precise characterization} of when sparsity testing for low-degree polynomials admits constant sample complexity independent of dimension, together with a matching constant-sample algorithm in that regime. For any mean-zero, variance-one finitely supported distribution $\\boldsymbol{X}$ over the reals, degree $d$, and any sparsity parameters $s \\leq T$, we define a computable function $\\mathrm{MSG}_{\\boldsymbol{X},d}(\\cdot)$, and:\n  - For $T \\ge \\mathrm{MSG}_{\\boldsymbol{X},d}(s)$, we give an $O_{s,\\boldsymbol{X},d}(1)$-sample algorithm that distinguishes whether a multilinear degree-$d$ polynomial over $\\mathbb{R}^n$ is $s$-sparse versus $\\varepsilon$-far from $T$-sparse, given examples $(\\boldsymbol{x},\\, p(\\boldsymbol{x}) + \\mathrm{noise})_{\\boldsymbol{x} \\sim \\boldsymbol{X}^{\\otimes n}}$. Crucially, the sample complexity is \\emph{completely independent} of the ambient dimension $n$.\n  - For $T \\leq \\mathrm{MSG}_{\\boldsymbol{X},d}(s) - 1$, we show that even without noise, any algorithm given samples $(\\boldsymbol{x},p(\\boldsymbol{x}))_{\\boldsymbol{x} \\sim \\boldsymbol{X}^{\\otimes n}}$ must use $Ω_{\\boldsymbol{X},d,s}(\\log n)$ examples.\n  Our techniques employ a generalization of the results of Dinur et al. (2007) on the Fourier tails of bounded functions over $\\{0,1\\}^n$ to a broad range of finitely supported distributions, which may be of independent interest."}
{"id": "2511.08188", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.08188", "abs": "https://arxiv.org/abs/2511.08188", "authors": ["Sourav Mukherjee", "Bho Matthiesen", "Armin Dekorsy", "Petar Popovski"], "title": "Dynamic Downlink-Uplink Spectrum Sharing between Terrestrial and Non-Terrestrial Networks", "comment": "Submitted for peer review", "summary": "6G networks are expected to integrate low Earth orbit satellites to ensure global connectivity by extending coverage to underserved and remote regions. However, the deployment of dense mega-constellations introduces severe interference among satellites operating over shared frequency bands. This is, in part, due to the limited flexibility of conventional frequency division duplex (FDD) systems, where fixed bands for downlink (DL) and uplink (UL) transmissions are employed. In this work, we propose dynamic re-assignment of FDD bands for improved interference management in dense deployments and evaluate the performance gain of this approach. To this end, we formulate a joint optimization problem that incorporates dynamic band assignment, user scheduling, and power allocation in both directions. This non-convex mixed integer problem is solved using a combination of equivalence transforms, alternating optimization, and state-of-the-art industrial-grade mixed integer solvers. Numerical results demonstrate that the proposed approach of dynamic FDD band assignment significantly enhances system performance over conventional FDD, achieving up to 94\\% improvement in throughput in dense deployments."}
{"id": "2511.07984", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.07984", "abs": "https://arxiv.org/abs/2511.07984", "authors": ["Ying Wang", "Jiaqian Li", "Tianze Wei", "Hau Chan", "Minming Li"], "title": "Centralized Group Equitability and Individual Envy-Freeness in the Allocation of Indivisible Items", "comment": "Appears in the 40th AAAI Conference on Artificial Intelligence (AAAI), 2026", "summary": "We study the fair allocation of indivisible items for groups of agents from the perspectives of the agents and a centralized allocator. In our setting, the centralized allocator is interested in ensuring the allocation is fair among the groups and between agents. This setting applies to many real-world scenarios, including when a school administrator wants to allocate resources (e.g., office spaces and supplies) to staff members in departments and when a city council allocates limited housing units to various families in need across different communities. To ensure fair allocation between agents, we consider the classical envy-freeness (EF) notion. To ensure fairness among the groups, we define the notion of centralized group equitability (CGEQ) to capture the fairness for the groups from the allocator's perspective. Because an EF or CGEQ allocation does not always exist in general, we consider their corresponding natural relaxations of envy-freeness to one item (EF1) and centralized group equitability up to one item (CGEQ1). For different classes of valuation functions of the agents and the centralized allocator, we show that allocations satisfying both EF1 and CGEQ1 always exist and design efficient algorithms to compute these allocations. We also consider the centralized group maximin share (CGMMS) from the centralized allocator's perspective as a group-level fairness objective with EF1 for agents and present several results."}
{"id": "2511.07886", "categories": ["cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.07886", "abs": "https://arxiv.org/abs/2511.07886", "authors": ["Dechuang Chen", "Sibo Wang", "Qintian Guo"], "title": "ACGraph: An Efficient Asynchronous Out-of-Core Graph Processing Framework", "comment": "Accepted by SIGMOD'26", "summary": "Graphs are a ubiquitous data structure in diverse domains such as machine learning, social networks, and data mining. As real-world graphs continue to grow beyond the memory capacity of single machines, out-of-core graph processing systems have emerged as a viable solution. Yet, existing systems that rely on strictly synchronous, iteration-by-iteration execution incur significant overheads. In particular, their scheduling mechanisms lead to I/O inefficiencies, stemming from read and work amplification, and induce costly synchronization stalls hindering sustained disk utilization. To overcome these limitations, we present {\\em ACGraph}, a novel asynchronous graph processing system optimized for SSD-based environments with constrained memory resources. ACGraph employs a dynamic, block-centric priority scheduler that adjusts in real time based on workload, along with an online asynchronous worklist that minimizes redundant disk accesses by efficiently reusing active blocks in memory. Moreover, ACGraph unifies asynchronous I/O with computation in a pipelined execution model that maintains sustained I/O activation, and leverages a highly optimized hybrid storage format to expedite access to low-degree vertices. We implement popular graph algorithms, such as Breadth-First Search (BFS), Weakly Connected Components (WCC), personalized PageRank (PPR), PageRank (PR), and $k$-core on ACGraph and demonstrate that ACGraph substantially outperforms state-of-the-art out-of-core graph processing systems in both runtime and I/O efficiency."}
{"id": "2511.07595", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.07595", "abs": "https://arxiv.org/abs/2511.07595", "authors": ["Özay Ezerceli", "Gizem Gümüşçekiçci", "Tuğba Erkoç", "Berke Özenç"], "title": "TurkEmbed4Retrieval: Turkish Embedding Model for Retrieval Task", "comment": "4 pages, in Turkish language, 1 figure, conference", "summary": "In this work, we introduce TurkEmbed4Retrieval, a retrieval specialized variant of the TurkEmbed model originally designed for Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. By fine-tuning the base model on the MS MARCO TR dataset using advanced training techniques, including Matryoshka representation learning and a tailored multiple negatives ranking loss, we achieve SOTA performance for Turkish retrieval tasks. Extensive experiments demonstrate that our model outperforms Turkish colBERT by 19,26% on key retrieval metrics for the Scifact TR dataset, thereby establishing a new benchmark for Turkish information retrieval."}
{"id": "2511.07846", "categories": ["cs.DS", "cs.CC", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.07846", "abs": "https://arxiv.org/abs/2511.07846", "authors": ["Xi Chen", "Anindya De", "Yizhi Huang", "Shivam Nadimpalli", "Rocco A. Servedio", "Tianqi Yang"], "title": "Model-agnostic super-resolution in high dimensions", "comment": "41 pages", "summary": "The problem of \\emph{super-resolution}, roughly speaking, is to reconstruct an unknown signal to high accuracy, given (potentially noisy) information about its low-degree Fourier coefficients. Prior results on super-resolution have imposed strong modeling assumptions on the signal, typically requiring that it is a linear combination of spatially separated point sources.\n  In this work we analyze a very general version of the super-resolution problem, by considering completely general signals over the $d$-dimensional torus $[0,1)^d$; we do not assume any spatial separation between point sources, or even that the signal is a finite linear combination of point sources. We obtain two sets of results, corresponding to two natural notions of reconstruction.\n  - {\\bf Reconstruction in Wasserstein distance:} We give essentially matching upper and lower bounds on the cutoff frequency $T$ and the magnitude $κ$ of the noise for which accurate reconstruction in Wasserstein distance is possible. Roughly speaking, our results here show that for $d$-dimensional signals, estimates of $\\approx \\exp(d)$ many Fourier coefficients are necessary and sufficient for accurate reconstruction under the Wasserstein distance.\n  - {\\bf \"Heavy hitter\" reconstruction:} For nonnegative signals (equivalently, probability distributions), we introduce a new notion of \"heavy hitter\" reconstruction that essentially amounts to achieving high-accuracy reconstruction of all \"sufficiently dense\" regions of the distribution. Here too we give essentially matching upper and lower bounds on the cutoff frequency $T$ and the magnitude $κ$ of the noise for which accurate reconstruction is possible. Our results show that -- in sharp contrast with Wasserstein reconstruction -- accurate estimates of only $\\approx \\exp(\\sqrt{d})$ many Fourier coefficients are necessary and sufficient for heavy hitter reconstruction."}
{"id": "2511.08255", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.08255", "abs": "https://arxiv.org/abs/2511.08255", "authors": ["Yuting Tang", "Yufan He", "Yi Zhong", "Xijun Wang", "Tony Q. S. Quek", "Howard H. Yang"], "title": "Analysis of SINR Coverage in LEO Satellite Networks through Spatial Network Calculus", "comment": null, "summary": "We introduce a new analytical framework, developed based on the spatial network calculus, for performance assessment of Low Earth Orbit (LEO) satellite networks. Specifically, we model the satellites' spatial positions as a strong ball-regulated point process on the sphere. Under this model, proximal points in space exhibit a locally repulsive property, reflecting the fact that intersatellite links are protected by a safety distance and would not be arbitrarily close. Subsequently, we derive analytical lower bounds on the conditional coverage probabilities under Nakagami-$m$ and Rayleigh fading, respectively. These expressions have a low computational complexity, enabling efficient numerical evaluations. We validate the effectiveness of our theoretical model by contrasting the coverage probability obtained from our analysis with that estimated from a Starlink constellation. The results show that our analysis provides a tight lower bound on the actual value and, surprisingly, matches the empirical simulations almost perfectly with a 1 dB shift. This demonstrates our framework as an appropriate theoretical model for LEO satellite networks."}
{"id": "2511.08033", "categories": ["cs.GT", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.08033", "abs": "https://arxiv.org/abs/2511.08033", "authors": ["Chuanzhe Zhang", "Yuke Li", "Wenjun Mei"], "title": "Nash-equilibrium Seeking Algorithm for Power-Allocation Games on Networks of International Relations", "comment": null, "summary": "In the field of international security, understanding the strategic interactions between countries within a networked context is crucial. Our previous research has introduced a ``games-on-signed graphs'' framework~\\cite{LiMorse2022} to analyze these interactions. While the framework is intended to be basic and general, there is much left to be explored, particularly in capturing the complexity of strategic scenarios in international relations. Our paper aims to fill this gap in two key ways. First, we modify the existing preference axioms to allow for a more nuanced understanding of how countries pursue self-survival, defense of allies, and offense toward adversaries. Second, we introduce a novel algorithm that proves the existence of a pure-strategy Nash equilibrium for these revised games. To validate our model, we employ historical data from the year 1940 as the game input and predict countries' survivability. Our contributions thus extend the real-world applicability of the original framework, offering a more comprehensive view of strategic interactions in a networked security environment."}
{"id": "2511.08006", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.08006", "abs": "https://arxiv.org/abs/2511.08006", "authors": ["Peiyu Hu", "Wayne Lu", "Jia Wang"], "title": "From IDs to Semantics: A Generative Framework for Cross-Domain Recommendation with Adaptive Semantic Tokenization", "comment": null, "summary": "Cross-domain recommendation (CDR) is crucial for improving recommendation accuracy and generalization, yet traditional methods are often hindered by the reliance on shared user/item IDs, which are unavailable in most real-world scenarios. Consequently, many efforts have focused on learning disentangled representations through multi-domain joint training to bridge the domain gaps. Recent Large Language Model (LLM)-based approaches show promise, they still face critical challenges, including: (1) the \\textbf{item ID tokenization dilemma}, which leads to vocabulary explosion and fails to capture high-order collaborative knowledge; and (2) \\textbf{insufficient domain-specific modeling} for the complex evolution of user interests and item semantics. To address these limitations, we propose \\textbf{GenCDR}, a novel \\textbf{Gen}erative \\textbf{C}ross-\\textbf{D}omain \\textbf{R}ecommendation framework. GenCDR first employs a \\textbf{Domain-adaptive Tokenization} module, which generates disentangled semantic IDs for items by dynamically routing between a universal encoder and domain-specific adapters. Symmetrically, a \\textbf{Cross-domain Autoregressive Recommendation} module models user preferences by fusing universal and domain-specific interests. Finally, a \\textbf{Domain-aware Prefix-tree} enables efficient and accurate generation. Extensive experiments on multiple real-world datasets demonstrate that GenCDR significantly outperforms state-of-the-art baselines. Our code is available in the supplementary materials."}
{"id": "2511.07859", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.07859", "abs": "https://arxiv.org/abs/2511.07859", "authors": ["Jason Li"], "title": "Deterministic Padded Decompositions and Negative-Weight Shortest Paths", "comment": "12 pages", "summary": "We obtain the first near-linear time deterministic algorithm for negative-weight single-source shortest paths on integer-weighted graphs. Our main ingredient is a deterministic construction of a padded decomposition on directed graphs, which may be of independent interest."}
{"id": "2511.08278", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.08278", "abs": "https://arxiv.org/abs/2511.08278", "authors": ["Chen Zhao", "Haobo Jia", "Zhuqing Jia"], "title": "Robust Dynamic Coded Distributed Storage with Partially Storage Constrained Servers", "comment": null, "summary": "We consider the problem of Robust Dynamic Coded Distributed Storage (RDCDS) with partially storage constrained servers where the goal is to enable robust (resilient to server dropouts) and efficient (as measured by the communication costs) read and update operations, subject to the constraint that the storage at $S$ out of $N$ servers is limited by $1/K_c$ the size of the message. Building upon previously established converse arguments and achievability schemes by Jia et al., in this work we develop a set of new converse arguments and coding designs that enable us to completely characterize the fundamental limits of RDCDS with partially storage constrained servers, i.e., the minimum number of available servers for feasible update operation and the minimum communication costs for read and update operations across various server dropout scenarios."}
{"id": "2511.08160", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2511.08160", "abs": "https://arxiv.org/abs/2511.08160", "authors": ["Argyris Deligkas", "Eduard Eiben", "Tiger-Lily Goldsmith", "Dušan Knop", "Šimon Schierreich"], "title": "Dividing Indivisible Items for the Benefit of All: It is Hard to Be Fair Without Social Awareness", "comment": "A preliminary version appeared in AAAI '26", "summary": "In standard fair division models, we assume that all agents are selfish. However, in many scenarios, division of resources has a direct impact on the whole group or even society. Therefore, we study fair allocations of indivisible items that, at the same time, maximize social impact. In this model, each agent is associated with two additive functions that define their value and social impact for each item. The goal is to allocate items so that the social impact is maximized while maintaining some fairness criterion. We reveal that the complexity of the problem heavily depends on whether the agents are socially aware, i.e., they take into consideration the social impact functions. For socially unaware agents, we prove that the problem is NP-hard for a variety of fairness notions, and that it is tractable only for very restricted cases, e.g., if, for every agent, the valuation equals social impact and it is binary. On the other hand, social awareness allows for fair allocations that maximize social impact, and such allocations can be computed in polynomial time. Interestingly, the problem becomes again intractable as soon as the definition of social awareness is relaxed."}
{"id": "2511.08029", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08029", "abs": "https://arxiv.org/abs/2511.08029", "authors": ["Aarush Sinha", "Pavan Kumar S", "Roshan Balaji", "Nirav Pravinbhai Bhatt"], "title": "BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives", "comment": "Accepted for oral presentation at AAAI 2026", "summary": "Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation."}
{"id": "2511.07869", "categories": ["cs.DS", "cs.DC", "cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.07869", "abs": "https://arxiv.org/abs/2511.07869", "authors": ["Nima Anari", "Carlo Baronio", "CJ Chen", "Alireza Haqi", "Frederic Koehler", "Anqi Li", "Thuy-Duong Vuong"], "title": "Parallel Sampling via Autospeculation", "comment": null, "summary": "We present parallel algorithms to accelerate sampling via counting in two settings: any-order autoregressive models and denoising diffusion models. An any-order autoregressive model accesses a target distribution $μ$ on $[q]^n$ through an oracle that provides conditional marginals, while a denoising diffusion model accesses a target distribution $μ$ on $\\mathbb{R}^n$ through an oracle that provides conditional means under Gaussian noise. Standard sequential sampling algorithms require $\\widetilde{O}(n)$ time to produce a sample from $μ$ in either setting. We show that, by issuing oracle calls in parallel, the expected sampling time can be reduced to $\\widetilde{O}(n^{1/2})$. This improves the previous $\\widetilde{O}(n^{2/3})$ bound for any-order autoregressive models and yields the first parallel speedup for diffusion models in the high-accuracy regime, under the relatively mild assumption that the support of $μ$ is bounded.\n  We introduce a novel technique to obtain our results: speculative rejection sampling. This technique leverages an auxiliary ``speculative'' distribution~$ν$ that approximates~$μ$ to accelerate sampling. Our technique is inspired by the well-studied ``speculative decoding'' techniques popular in large language models, but differs in key ways. Firstly, we use ``autospeculation,'' namely we build the speculation $ν$ out of the same oracle that defines~$μ$. In contrast, speculative decoding typically requires a separate, faster, but potentially less accurate ``draft'' model $ν$. Secondly, the key differentiating factor in our technique is that we make and accept speculations at a ``sequence'' level rather than at the level of single (or a few) steps. This last fact is key to unlocking our parallel runtime of $\\widetilde{O}(n^{1/2})$."}
{"id": "2511.08304", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.08304", "abs": "https://arxiv.org/abs/2511.08304", "authors": ["Cícero Carvalho", "Hiram H. López", "Rodrigo San-José"], "title": "Cartesian square-free codes", "comment": null, "summary": "The generalized Hamming weights (GHWs) of a linear code C extend the concept of minimum distance, which is the minimum cardinality of the support of all one-dimensional subspaces of C, to the minimum cardinality of the support of all r-dimensional subspaces of the code. In this work, we introduce Cartesian square-free codes, which are linear codes generated by evaluating square-free monomials over a Cartesian set. We use commutative algebraic tools, specifically the footprint bound, to provide explicit formulas for some of the GHWs of this family of codes, and we show how we can translate these results to evaluation codes over the projective space."}
{"id": "2511.08347", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2511.08347", "abs": "https://arxiv.org/abs/2511.08347", "authors": ["Elizabeth Maggie Penn", "John W. Patty"], "title": "Classification in Equilibrium: Structure of Optimal Decision Rules", "comment": null, "summary": "This paper characterizes optimal classification when individuals adjust their behavior in response to the classification rule. We model the interaction between a designer and a population as a Stackelberg game: the designer selects a classification rule anticipating how individuals will comply, cheat, or abstain in order to obtain a favorable classification. Under standard monotone likelihood ratio assumptions, optimal rules belong to a small and interpretable family (single-threshold and two-cut rules) that encompass both conventional and counterintuitive designs. Our results depart sharply from prior findings that optimal classifiers reward higher signals: in equilibrium, the designer may deliberately reward those with lower likelihood ratios or concentrate rewards/penalties in a middle band to improve informational quality."}
{"id": "2511.08150", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.08150", "abs": "https://arxiv.org/abs/2511.08150", "authors": ["Xinpeng Zhao", "Yukun Zhao", "Zhenyang Li", "Mengqi Zhang", "Jun Feng", "Ran Chen", "Ying Zhou", "Zhumin Chen", "Shuaiqiang Wang", "Zhaochun Ren", "Dawei Yin", "Xin Xin"], "title": "DiffuGR: Generative Document Retrieval with Diffusion Language Models", "comment": "This paper is under review", "summary": "Generative retrieval (GR) re-frames document retrieval as a sequence-based document identifier (DocID) generation task, memorizing documents with model parameters and enabling end-to-end retrieval without explicit indexing. Existing GR methods are based on auto-regressive generative models, i.e., the token generation is performed from left to right. However, such auto-regressive methods suffer from: (1) mismatch between DocID generation and natural language generation, e.g., an incorrect DocID token generated in early left steps would lead to totally erroneous retrieval; and (2) failure to balance the trade-off between retrieval efficiency and accuracy dynamically, which is crucial for practical applications. To address these limitations, we propose generative document retrieval with diffusion language models, dubbed DiffuGR. It models DocID generation as a discrete diffusion process: during training, DocIDs are corrupted through a stochastic masking process, and a diffusion language model is learned to recover them under a retrieval-aware objective. For inference, DiffuGR attempts to generate DocID tokens in parallel and refines them through a controllable number of denoising steps. In contrast to conventional left-to-right auto-regressive decoding, DiffuGR provides a novel mechanism to first generate more confident DocID tokens and refine the generation through diffusion-based denoising. Moreover, DiffuGR also offers explicit runtime control over the qualitylatency tradeoff. Extensive experiments on benchmark retrieval datasets show that DiffuGR is competitive with strong auto-regressive generative retrievers, while offering flexible speed and accuracy tradeoffs through variable denoising budgets. Overall, our results indicate that non-autoregressive diffusion models are a practical and effective alternative for generative document retrieval."}
{"id": "2511.08210", "categories": ["cs.DS", "cs.DC", "math.CO"], "pdf": "https://arxiv.org/pdf/2511.08210", "abs": "https://arxiv.org/abs/2511.08210", "authors": ["Taisuke Izumi", "Naoki Kitamura", "Yutaro Yamaguchi"], "title": "Forgetting Alternation and Blossoms: A New Framework for Fast Matching Augmentation and Its Applications to Sequential/Distributed/Streaming Computation", "comment": "39 pages, 6 figures", "summary": "Finding a maximum cardinality matching in a graph is one of the most fundamental problems. An algorithm proposed by Micali and Vazirani (1980) is well-known to solve the problem in $O(m\\sqrt{n})$ time, which is still one of the fastest algorithms in general. While the MV algorithm itself is not so complicated and is indeed convincing, its correctness proof is extremely challenging, which can be seen from the history: after the first algorithm paper had appeared in 1980, Vazirani has made several attempts to give a complete proof for more than 40 years. It seems, roughly speaking, caused by the nice but highly complex structure of the shortest alternating paths in general graphs that are deeply intertwined with the so-called (nested) blossoms.\n  In this paper, we propose a new structure theorem on the shortest alternating paths in general graphs without taking into the details of blossoms. The high-level idea is to forget the alternation (of matching and non-matching edges) as early as possible. A key ingredient is a notion of alternating base trees (ABTs) introduced by Izumi, Kitamura, and Yamaguchi (2024) to develop a nearly linear-time distributed algorithm. Our structure theorem refines the properties of ABTs exploited in their algorithm, and we also give simpler alternative proofs for them. Based on our structure theorem, we propose a new algorithm, which is slightly slower but more implementable and much easier to confirm its correctness than the MV algorithm.\n  As applications of our framework, we also present new $(1 - ε)$-approximation algorithms in the distributed and semi-streaming settings. Both algorithms are deterministic, and substantially improve the best known upper bounds on the running time. The algorithms are built on the top of a novel framework of amplifying approximation factors of given matchings, which is of independent interest."}
{"id": "2511.08326", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2511.08326", "abs": "https://arxiv.org/abs/2511.08326", "authors": ["Mohammadreza Bakhshizadeh Mohajer", "Daniela Tuninetti", "Luca Barletta"], "title": "A General Ziv-Zakai Bound for DoA Estimation in MIMO Radar Systems", "comment": "6 pages, 3 figures, submitted to IEEE ICC 2026", "summary": "This paper derives a Ziv-Zakai Bound (ZZB) on the Mean Squared Error (MSE) for Direction-of-Arrival (DoA) estimation in co-located Multiple-Input Multiple-Output (MIMO) radar systems and provides closed-form expressions that hold for multi-target scenarios. Unlike classical results that address single-input multiple-output systems with complex Gaussian input signals, the developed ZZB in this paper explicitly accounts for a general input covariance matrix, target radar cross-section statistics and multiple snapshot effects, and admits a compact expression that reveals the dependence of the MSE on the number of transmit antennas, number of targets, Signal-to-Noise Ratio (SNR) and the transmit covariance matrix. Numerical simulations validate the tightness of the ZZB in the a priori dominated region and show how the increase of the number of transmit antennas compresses the threshold SNR for the transition to the Cramer-Rao bound (CRB) while the variation of the number of targets shifts the bound's behavior across SNR regimes. The analytical results and numerical simulations demonstrate that the ZZB is tighter than the CRB, particularly in the low SNR regime."}
{"id": "2511.08538", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.08538", "abs": "https://arxiv.org/abs/2511.08538", "authors": ["Yannan Bai", "Kamesh Munagala", "Yiheng Shen", "Davidson Zhu"], "title": "Fair Multi-agent Persuasion with Submodular Constraints", "comment": null, "summary": "We study the problem of selection in the context of Bayesian persuasion. We are given multiple agents with hidden values (or quality scores), to whom resources must be allocated by a welfare-maximizing decision-maker. An intermediary with knowledge of the agents' values seeks to influence the outcome of the selection by designing informative signals and providing tie-breaking policies, so that when the receiver maximizes welfare over the resulting posteriors, the expected utilities of the agents (where utility is defined as allocation times value) achieve certain fairness properties. The fairness measure we will use is majorization, which simultaneously approximately maximizes all symmetric, monotone, concave functions of the utilities. We consider the general setting where the allocation to the agents needs to respect arbitrary submodular constraints, as given by the corresponding polymatroid.\n  We present a signaling policy that, under a mild bounded rationality assumption on the receiver, achieves a logarithmically approximate majorized policy in this setting. The approximation ratio is almost best possible, and that significantly outperforms generic results that only yield linear approximations. A key component of our result is a structural characterization showing that the vector of agent utilities for a given signaling policy defines the base polytope of a different polymatroid, a result that may be of independent interest. In addition, we show that an arbitrarily good additive approximation to this vector can be produced in (weakly) polynomial time via the multiplicative weights update method."}
{"id": "2511.08181", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08181", "abs": "https://arxiv.org/abs/2511.08181", "authors": ["Seung Hwan Cho", "Yujin Yang", "Danik Baeck", "Minjoo Kim", "Young-Min Kim", "Heejung Lee", "Sangjin Park"], "title": "MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System", "comment": "13 pages, 2 figures, Accepted at RDGENAI at CIKM 2025 workshop", "summary": "Recommender systems (RS) are currently being studied to mitigate limitations during cold-start conditions by leveraging modality information or introducing Agent concepts based on the exceptional reasoning capabilities of Large Language Models (LLMs). Meanwhile, food and beverage recommender systems have traditionally used knowledge graph and ontology concepts due to the domain's unique data attributes and relationship characteristics. On this background, we propose MARC, a multimodal and multi-task cocktail recommender system based on Agentic Retrieval-Augmented Generation (RAG) utilizing graph database under cold-start conditions. The proposed system generates high-quality, contextually appropriate answers through two core processes: a task recognition router and a reflection process. The graph database was constructed by processing cocktail data from Kaggle, and its effectiveness was evaluated using 200 manually crafted questions. The evaluation used both LLM-as-a-judge and human evaluation to demonstrate that answers generated via the graph database outperformed those from a simple vector database in terms of quality. The code is available at https://github.com/diddbwls/cocktail_rec_agentrag"}
{"id": "2511.08485", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.08485", "abs": "https://arxiv.org/abs/2511.08485", "authors": ["Sayan Bhattacharya", "Ruoxu Cen", "Debmalya Panigrahi"], "title": "Fully Dynamic Set Cover: Worst-Case Recourse and Update Time", "comment": null, "summary": "In (fully) dynamic set cover, the goal is to maintain an approximately optimal solution to a dynamically evolving instance of set cover, where in each step either an element is added to or removed from the instance. The two main desiderata of a dynamic set cover algorithm are to minimize at each time-step, the recourse, which is the number of sets removed from or added to the solution, and the update time to compute the updated solution. This problem has been extensively studied over the last decade leading to many results that achieve ever-improving bounds on the recourse and update time, while maintaining a solution whose cost is comparable to that of offline approximation algorithms.\n  In this paper, we give the first algorithms to simultaneously achieve non-trivial worst-case bounds for recourse and update time. Specifically, we give fully-dynamic set cover algorithms that simultaneously achieve $O(\\log n)$ recourse and $f\\cdot \\textrm{poly}\\log(n)$ update time in the worst-case, for both approximation regimes: $O(\\log n)$ and $O(f)$ approximation. (Here, $n, f$ respectively denote the maximum number of elements and maximum frequency of an element across all instances.) Prior to our work, all results for this problem either settled for amortized bounds on recourse and update time, or obtained $f\\cdot \\textrm{poly}\\log(n)$ update time in the worst-case but at the cost of $Ω(m)$ worst-case recourse. (Here, $m$ denotes the number of sets. Note that any algorithm has recourse at most $m$.)"}
{"id": "2511.08378", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08378", "abs": "https://arxiv.org/abs/2511.08378", "authors": ["Xiao Wang", "Ke Qin", "Dongyang Zhang", "Xiurui Xie", "Shuang Liang"], "title": "Bid Farewell to Seesaw: Towards Accurate Long-tail Session-based Recommendation via Dual Constraints of Hybrid Intents", "comment": null, "summary": "Session-based recommendation (SBR) aims to predict anonymous users' next interaction based on their interaction sessions. In the practical recommendation scenario, low-exposure items constitute the majority of interactions, creating a long-tail distribution that severely compromises recommendation diversity. Existing approaches attempt to address this issue by promoting tail items but incur accuracy degradation, exhibiting a \"see-saw\" effect between long-tail and accuracy performance. We attribute such conflict to session-irrelevant noise within the tail items, which existing long-tail approaches fail to identify and constrain effectively. To resolve this fundamental conflict, we propose \\textbf{HID} (\\textbf{H}ybrid \\textbf{I}ntent-based \\textbf{D}ual Constraint Framework), a plug-and-play framework that transforms the conventional \"see-saw\" into \"win-win\" through introducing the hybrid intent-based dual constraints for both long-tail and accuracy. Two key innovations are incorporated in this framework: (i) \\textit{Hybrid Intent Learning}, where we reformulate the intent extraction strategies by employing attribute-aware spectral clustering to reconstruct the item-to-intent mapping. Furthermore, discrimination of session-irrelevant noise is achieved through the assignment of the target and noise intents to each session. (ii) \\textit{Intent Constraint Loss}, which incorporates two novel constraint paradigms regarding the \\textit{diversity} and \\textit{accuracy} to regulate the representation learning process of both items and sessions. These two objectives are unified into a single training loss through rigorous theoretical derivation. Extensive experiments across multiple SBR models and datasets demonstrate that HID can enhance both long-tail performance and recommendation accuracy, establishing new state-of-the-art performance in long-tail recommender systems."}
{"id": "2511.08551", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2511.08551", "abs": "https://arxiv.org/abs/2511.08551", "authors": ["Bernhard Haeupler", "Yonggang Jiang", "Thatchaphol Saranurak"], "title": "Deterministic Negative-Weight Shortest Paths in Nearly Linear Time via Path Covers", "comment": null, "summary": "We present the first deterministic nearly-linear time algorithm for single-source shortest paths with negative edge weights on directed graphs: given a directed graph $G$ with $n$ vertices, $m$ edges whose weights are integer in $\\{-W,\\dots,W\\}$, our algorithm either computes all distances from a source $s$ or reports a negative cycle in time $\\tilde{O}(m)\\cdot \\log(nW)$ time.\n  All known near-linear time algorithms for this problem have been inherently randomized, as they crucially rely on low-diameter decompositions.\n  To overcome this barrier, we introduce a new structural primitive for directed graphs called the path cover. This plays a role analogous to neighborhood covers in undirected graphs, which have long been central to derandomizing algorithms that use low-diameter decomposition in the undirected setting. We believe that path covers will serve as a fundamental tool for the design of future deterministic algorithms on directed graphs."}
{"id": "2511.08476", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.08476", "abs": "https://arxiv.org/abs/2511.08476", "authors": ["Hadi Ghaemi", "Lauren Snyder", "Markus Stocker"], "title": "Advancing Scientific Knowledge Retrieval and Reuse with a Novel Digital Library for Machine-Readable Knowledge", "comment": null, "summary": "Digital libraries for research, such as the ACM Digital Library or Semantic Scholar, do not enable the machine-supported, efficient reuse of scientific knowledge (e.g., in synthesis research). This is because these libraries are based on document-centric models with narrative text knowledge expressions that require manual or semi-automated knowledge extraction, structuring, and organization. We present ORKG reborn, an emerging digital library that supports finding, accessing, and reusing accurate, fine-grained, and reproducible machine-readable expressions of scientific knowledge that relate scientific statements and their supporting evidence in terms of data and code. The rich expressions of scientific knowledge are published as reborn (born-reusable) articles and provide novel possibilities for scientific knowledge retrieval, for instance by statistical methods, software packages, variables, or data matching specific constraints. We describe the proposed system and demonstrate its practical viability and potential for information retrieval in contrast to state-of-the-art digital libraries and document-centric scholarly communication using several published articles in research fields ranging from computer science to soil science. Our work underscores the enormous potential of scientific knowledge databases and a viable approach to their construction."}
{"id": "2511.08556", "categories": ["cs.DS", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.08556", "abs": "https://arxiv.org/abs/2511.08556", "authors": ["Shaleen Baral", "Robert Kleinberg", "Sylvan Martin", "Henry Rogers", "Tegan Wilson", "Ruogu Zhang"], "title": "Universal Connection Schedules for Reconfigurable Networking", "comment": "32 pages, 1 figure. This paper was accepted to the ACM-SIAM Symposium on Discrete Algorithms (SODA) 2026. This is a pre-print version", "summary": "Reconfigurable networks are a novel communication paradigm in which the pattern of connectivity between hosts varies rapidly over time. Prior theoretical work explored the inherent tradeoffs between throughput (or, hop-count) and latency, and showed the existence of infinitely many Pareto-optimal designs as the network size tends to infinity. Existing Pareto-optimal designs use a connection schedule which is fine-tuned to the desired hop-count $h$, permitting lower latency as $h$ increases. However, in reality datacenter workloads contain a mix of low-latency and high-latency requests. Using a connection schedule fine-tuned for one request type leads to inefficiencies when serving other types.\n  A more flexible and efficient alternative is a {\\em universal schedule}, a single connection schedule capable of attaining many Pareto-optimal tradeoff points simultaneously, merely by varying the choice of routing paths. In this work we present the first universal schedules for oblivious routing. Our constructions yield universal schedules which are near-optimal for all possible hop-counts $h$. The key technical idea is to specialize to a type of connection schedule based on cyclic permutations and to develop a novel Fourier-analytic method for analyzing randomized routing on these connection schedules. We first show that a uniformly random connection schedule suffices with multiplicative error in throughput, and latency optimal up to a $\\log N$ factor. We then show that a more carefully designed random connection schedule suffices with additive error in throughput, but improved latency optimal up to only constant factors. Finally, we show that our first randomized construction can be made deterministic using a derandomized version of the Lovett-Meka discrepancy minimization algorithm to obtain the same result."}
{"id": "2511.08538", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.08538", "abs": "https://arxiv.org/abs/2511.08538", "authors": ["Yannan Bai", "Kamesh Munagala", "Yiheng Shen", "Davidson Zhu"], "title": "Fair Multi-agent Persuasion with Submodular Constraints", "comment": null, "summary": "We study the problem of selection in the context of Bayesian persuasion. We are given multiple agents with hidden values (or quality scores), to whom resources must be allocated by a welfare-maximizing decision-maker. An intermediary with knowledge of the agents' values seeks to influence the outcome of the selection by designing informative signals and providing tie-breaking policies, so that when the receiver maximizes welfare over the resulting posteriors, the expected utilities of the agents (where utility is defined as allocation times value) achieve certain fairness properties. The fairness measure we will use is majorization, which simultaneously approximately maximizes all symmetric, monotone, concave functions of the utilities. We consider the general setting where the allocation to the agents needs to respect arbitrary submodular constraints, as given by the corresponding polymatroid.\n  We present a signaling policy that, under a mild bounded rationality assumption on the receiver, achieves a logarithmically approximate majorized policy in this setting. The approximation ratio is almost best possible, and that significantly outperforms generic results that only yield linear approximations. A key component of our result is a structural characterization showing that the vector of agent utilities for a given signaling policy defines the base polytope of a different polymatroid, a result that may be of independent interest. In addition, we show that an arbitrarily good additive approximation to this vector can be produced in (weakly) polynomial time via the multiplicative weights update method."}
